<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10087" opendate="2015-3-25 00:00:00" fixdate="2015-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline&amp;#39;s --silent option should suppress query from being echoed when running with -f option</summary>
      <description>The -e and the -f options behave differently. beeline -u jdbc:hive2://localhost:10000/default --showHeader=false --silent=true -f select.sql0: jdbc:hive2://localhost:10000/default&gt; select * from sample_07 limit 5;--------------------------------------------------------------------------------------00-0000 All Occupations 134354250 4069011-0000 Management occupations 6003930 9615011-1011 Chief executives 299160 15137011-1021 General and operations managers 1655410 10378011-1031 Legislators 61110 33880--------------------------------------------------------------------------------------beeline -u jdbc:hive2://localhost:10000/default --showHeader=false --silent=true -e "select * from sample_07 limit 5;"--------------------------------------------------------------------------------------00-0000 All Occupations 134354250 4069011-0000 Management occupations 6003930 9615011-1011 Chief executives 299160 15137011-1021 General and operations managers 1655410 10378011-1031 Legislators 61110 33880--------------------------------------------------------------------------------------</description>
      <version>0.13.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="10140" opendate="2015-3-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  <bug id="10284" opendate="2015-4-9 00:00:00" fixdate="2015-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable container reuse for grace hash join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridhashjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.context.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1038" opendate="2010-1-8 00:00:00" fixdate="2010-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin dies if the join prunes all the columns</summary>
      <description>The query:select /*+ mapjoin(a) */ count(1) from src a join src b on a.key = b.keydies.It is a blocker for 0.5</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1042" opendate="2010-1-11 00:00:00" fixdate="2010-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>function in a transform with more than 1 argument fails</summary>
      <description>select transform(substr(key, 1, 3)) USING '/bin/cat' FROM srcthrows an error:FAILED: Error in semantic analysis: AS clause has an invalid number of aliases</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10678" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update sql standard authorization configuration whitelist - more optimization flags</summary>
      <description>hive.exec.parallel and hive.groupby.orderby.position.alias are optimization config parameters that should be settable when sql standard authorization is enabled.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10679" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonSerde ignores varchar and char size limit specified during table creation</summary>
      <description>JsonSerde ignores varchar and char size limit specified during table creation and always creates varchar or char column with max length.steps to reproduce the issue:create table jsonserde_1 (v varchar(50), c char(50)) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';desc jsonserde_1;OKv varchar(65535) from deserializer c char(255) from deserializer Time taken: 0.468 seconds, Fetched: 2 row(s)</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10682" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="10683" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a mechanism for daemons to inform the AM about killed tasks</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="10835" opendate="2015-5-27 00:00:00" fixdate="2015-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="10841" opendate="2015-5-27 00:00:00" fixdate="2015-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WHERE col is not null] does not work sometimes for queries with many JOIN statements</summary>
      <description>The result from the following SELECT query is 3 rows but it should be 1 row.I checked it in MySQL - it returned 1 row.To reproduce the issue in Hive1. prepare tablesdrop table if exists L;drop table if exists LA;drop table if exists FR;drop table if exists A;drop table if exists PI;drop table if exists acct;create table L as select 4436 id;create table LA as select 4436 loan_id, 4748 aid, 4415 pi_id;create table FR as select 4436 loan_id;create table A as select 4748 id;create table PI as select 4415 id;create table acct as select 4748 aid, 10 acc_n, 122 brn;insert into table acct values(4748, null, null);insert into table acct values(4748, null, null);2. run SELECT queryselect acct.ACC_N, acct.brnFROM LJOIN LA ON L.id = LA.loan_idJOIN FR ON L.id = FR.loan_idJOIN A ON LA.aid = A.idJOIN PI ON PI.id = LA.pi_idJOIN acct ON A.id = acct.aidWHERE L.id = 4436 and acct.brn is not null;the result is 3 rows10 122NULL NULLNULL NULLbut it should be 1 row10 1222.1 "explain select ..." output for hive-1.3.0 MRSTAGE DEPENDENCIES: Stage-12 is a root stage Stage-9 depends on stages: Stage-12 Stage-0 depends on stages: Stage-9STAGE PLANS: Stage: Stage-12 Map Reduce Local Work Alias -&gt; Map Local Tables: a Fetch Operator limit: -1 acct Fetch Operator limit: -1 fr Fetch Operator limit: -1 l Fetch Operator limit: -1 pi Fetch Operator limit: -1 Alias -&gt; Map Local Operator Tree: a TableScan alias: a Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: id is not null (type: boolean) Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE HashTable Sink Operator keys: 0 _col5 (type: int) 1 id (type: int) 2 aid (type: int) acct TableScan alias: acct Statistics: Num rows: 3 Data size: 31 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: aid is not null (type: boolean) Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE HashTable Sink Operator keys: 0 _col5 (type: int) 1 id (type: int) 2 aid (type: int) fr TableScan alias: fr Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (loan_id = 4436) (type: boolean) Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE HashTable Sink Operator keys: 0 4436 (type: int) 1 4436 (type: int) 2 4436 (type: int) l TableScan alias: l Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (id = 4436) (type: boolean) Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE HashTable Sink Operator keys: 0 4436 (type: int) 1 4436 (type: int) 2 4436 (type: int) pi TableScan alias: pi Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: id is not null (type: boolean) Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE HashTable Sink Operator keys: 0 _col6 (type: int) 1 id (type: int) Stage: Stage-9 Map Reduce Map Operator Tree: TableScan alias: la Statistics: Num rows: 1 Data size: 14 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (((loan_id is not null and aid is not null) and pi_id is not null) and (loan_id = 4436)) (type: boolean) Statistics: Num rows: 1 Data size: 14 Basic stats: COMPLETE Column stats: NONE Map Join Operator condition map: Inner Join 0 to 1 Inner Join 0 to 2 keys: 0 4436 (type: int) 1 4436 (type: int) 2 4436 (type: int) outputColumnNames: _col5, _col6 Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: NONE Map Join Operator condition map: Inner Join 0 to 1 Inner Join 1 to 2 keys: 0 _col5 (type: int) 1 id (type: int) 2 aid (type: int) outputColumnNames: _col6, _col19, _col20 Statistics: Num rows: 4 Data size: 17 Basic stats: COMPLETE Column stats: NONE Map Join Operator condition map: Inner Join 0 to 1 keys: 0 _col6 (type: int) 1 id (type: int) outputColumnNames: _col19, _col20 Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: _col19 (type: int), _col20 (type: int) outputColumnNames: _col0, _col1 Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE File Output Operator compressed: false Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Local Work: Map Reduce Local Work Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: ListSinkTime taken: 0.57 seconds, Fetched: 142 row(s)2.2. "explain select..." output for hive-0.13.1 TezSTAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Tez Edges: Reducer 2 &lt;- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE) Reducer 3 &lt;- Reducer 2 (SIMPLE_EDGE), Map 9 (SIMPLE_EDGE) Reducer 6 &lt;- Map 5 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (SIMPLE_EDGE) DagName: lcapp_20150528111717_06c57a5b-8dc6-4ce9-bce7-b9e0a7818fe4:1 Vertices: Map 1 Map Operator Tree: TableScan alias: acct Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: aid (type: int) sort order: + Map-reduce partition columns: aid (type: int) Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE value expressions: acc_n (type: int), brn (type: int) Map 4 Map Operator Tree: TableScan alias: a Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: id (type: int) sort order: + Map-reduce partition columns: id (type: int) Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Map 5 Map Operator Tree: TableScan alias: la Statistics: Num rows: 28 Data size: 347 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (loan_id = 4436) (type: boolean) Statistics: Num rows: 14 Data size: 173 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: loan_id (type: int) sort order: + Map-reduce partition columns: loan_id (type: int) Statistics: Num rows: 14 Data size: 173 Basic stats: COMPLETE Column stats: NONE value expressions: aid (type: int), pi_id (type: int) Map 7 Map Operator Tree: TableScan alias: fr Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (loan_id = 4436) (type: boolean) Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: loan_id (type: int) sort order: + Map-reduce partition columns: loan_id (type: int) Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE Map 8 Map Operator Tree: TableScan alias: l Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (id = 4436) (type: boolean) Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: id (type: int) sort order: + Map-reduce partition columns: id (type: int) Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE Map 9 Map Operator Tree: TableScan alias: pi Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: id (type: int) sort order: + Map-reduce partition columns: id (type: int) Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE Reducer 2 Reduce Operator Tree: Join Operator condition map: Inner Join 0 to 1 Inner Join 1 to 2 condition expressions: 0 {VALUE._col2} 1 2 {VALUE._col1} {VALUE._col2} outputColumnNames: _col2, _col15, _col16 Statistics: Num rows: 110 Data size: 448 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: _col2 (type: int) sort order: + Map-reduce partition columns: _col2 (type: int) Statistics: Num rows: 110 Data size: 448 Basic stats: COMPLETE Column stats: NONE value expressions: _col15 (type: int), _col16 (type: int) Reducer 3 Reduce Operator Tree: Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {VALUE._col1} {VALUE._col2} 1 outputColumnNames: _col1, _col2 Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: _col1 (type: int), _col2 (type: int) outputColumnNames: _col0, _col1 Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE File Output Operator compressed: false Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Reducer 6 Reduce Operator Tree: Join Operator condition map: Inner Join 0 to 1 Inner Join 0 to 2 condition expressions: 0 1 {VALUE._col1} {VALUE._col2} 2 outputColumnNames: _col4, _col5 Statistics: Num rows: 50 Data size: 204 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: _col4 (type: int) sort order: + Map-reduce partition columns: _col4 (type: int) Statistics: Num rows: 50 Data size: 204 Basic stats: COMPLETE Column stats: NONE value expressions: _col5 (type: int) Stage: Stage-0 Fetch Operator limit: -1Time taken: 1.377 seconds, Fetched: 146 row(s)3. The workaround is to put "acct.brn is not null" to join conditionselect acct.ACC_N, acct.brnFROM LJOIN LA ON L.id = LA.loan_idJOIN FR ON L.id = FR.loan_idJOIN A ON LA.aid = A.idJOIN PI ON PI.id = LA.pi_idJOIN acct ON A.id = acct.aid and acct.brn is not nullWHERE L.id = 4436;OK10 122Time taken: 23.479 seconds, Fetched: 1 row(s)I tried it on hive-1.3.0 (MR) and hive-0.13.1 (MR and Tez) - all combinations have the issue</description>
      <version>0.13.0,0.13.1,0.14.0,1.2.0,1.3.0</version>
      <fixedVersion>1.0.2,1.2.1,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10842" opendate="2015-5-28 00:00:00" fixdate="2015-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: DAGs get stuck in yet another way</summary>
      <description>Looks exactly like HIVE-10744. Last comment there has internal app IDs. Logs upon request.6 (number of slots) tasks from a machine are stuck.jstack for target daemon sayeth: 7 Found one Java-level deadlock: 8 ============================= 9 10 "IPC Server handler 4 on 15001": 11 waiting to lock Monitor@0x00007f3cb0005cb8 (Object@0x000000008cc3ce98, a java/lang/Object), 12 which is held by "Wait-Queue-Scheduler-0" 13 "Wait-Queue-Scheduler-0": 14 waiting to lock Monitor@0x00007f3cb0004d98 (Object@0x000000009234cf58, a org/apache/hadoop/hive/llap/daemon/impl/Q ueryInfo$FinishableStateTracker), 15 which is held by "IPC Server handler 4 on 15001"Oh, this time it is not q1; I was running bunch of TPCDS queries in sequence for some cache test. No parallel queries. There may have been task failures before.The query that got stuck had lots and lots of reducersMap 1: 1/1 Map 10: 1/1 Map 11: 85/85 Map 13: 1/1 Map 14: 1/1 Map 15: 1/1 Map 16: 1/1 Map 17: 94/94 Map 19: 1/1 Map 2: 1/1 Map 20: 1/1 Map 3: 91/91 Map 7: 1/1 Map 8: 1/1 Map 9: 1/1 Reducer 12: 391/391 Reducer 18: 197/197 Reducer 4: 1009/1009 Reducer 5: 1003(+6)/1009 Reducer 6: 0(+1)/1I think it's query 58</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="10843" opendate="2015-5-28 00:00:00" fixdate="2015-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>desc database and show tables commands don&amp;#39;t pass db to HiveAuthorizer check</summary>
      <description>'show tables' and 'describe database' command should pass the database information for the command to HiveAuthorizer . This is needed for any auditing the hive authorizer might implement, or any authorization check it might decide to do based on the given database name.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.names.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.drop.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.database.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.db.owner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.change.db.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.add.part.exist.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.ctas2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.hooks.TestHs2Hooks.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.java</file>
      <file type="M">contrib.src.test.results.clientpositive.url.hook.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="10846" opendate="2015-5-28 00:00:00" fixdate="2015-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: preemption in AM due to failures / out of order scheduling</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.Converters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="10896" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10925" opendate="2015-6-4 00:00:00" fixdate="2015-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-static threadlocals in metastore code can potentially cause memory leak</summary>
      <description>There are many places where non-static threadlocals are used. I can't seem to find a good logic for using them. However, they can potentially result in leaking objects if for example they are created in a long running thread every time the thread handles a new session.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="10927" opendate="2015-6-4 00:00:00" fixdate="2015-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add number of HMS/HS2 connection metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmPauseMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="11027" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: Bucket map joins fail when hashcode goes negative</summary>
      <description>Seeing an issue when dynamic sort optimization is enabled while doing an insert into bucketed table. We seem to be flipping the negative sign on the hashcode instead of taking the complement of it for routing the data correctly. This results in correctness issues in bucket map joins in hive on tez when the hash code goes negative.</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11029" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop.proxyuser.mapr.groups does not work to restrict the groups that can be impersonated</summary>
      <description>In the core-site.xml, the hadoop.proxyuser.&lt;user&gt;.groups specifies the user groups which can be impersonated by the HS2 &lt;user&gt;. However, this does not work properly in Hive. In my core-site.xml, I have the following configs:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.groups&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;I would expect with this configuration that 'mapr' can impersonate only members of the Unix group 'root'. However if I submit a query as user 'jon' the query is running as user 'jon' even though 'mapr' should not be able to impersonate this user.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
    </fixedFiles>
  </bug>
  <bug id="1103" opendate="2010-1-26 00:00:00" fixdate="2010-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add .gitignore file</summary>
      <description>Add a .gitignore file (equivalent to svn:ignore) for those using git-svn.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11031" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC concatenation of old files can fail while merging column statistics</summary>
      <description>Column statistics in ORC are optional protobuf fields. Old ORC files might not have statistics for newly added types like decimal, date, timestamp etc. But column statistics merging assumes column statistics exists for these types and invokes merge. For example, merging of TimestampColumnStatistics directly casts the received ColumnStatistics object without doing instanceof check. If the ORC file contains time stamp column statistics then this will work else it will throw ClassCastException.Also, the file merge operator swallows the exception.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11099" opendate="2015-6-24 00:00:00" fixdate="2015-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for running negative q-tests [Spark Branch]</summary>
      <description>Add support for TestSparkNegativeCliDriver TestMiniSparkOnYarnNegativeCliDriver to negative q-tests</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="111" opendate="2008-12-3 00:00:00" fixdate="2008-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>join without a ON clause dies</summary>
      <description>join without a ON clause diesFor eg: the following query:select x.* from x JOIN yresults in a null pointer exception.It should be treated as a cartesian product</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1110" opendate="2010-1-27 00:00:00" fixdate="2010-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add counters to show that skew join triggered</summary>
      <description>It would be very useful to debug, and quickly find out if the skew join was triggered.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11150" opendate="2015-6-30 00:00:00" fixdate="2015-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove wrong warning message related to chgrp</summary>
      <description>When using other file system other than hdfs, users see warning message regarding hdfs chgrp. The warning is very annoying and confusing. We'd better remove it. The warning example:hive&gt; insert overwrite table s3_test select total_emp, salary, description from sample_07 limit 5;-chgrp: '' does not match expected pattern for groupUsage: hadoop fs [generic options] -chgrp [-R] GROUP PATH...Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="11409" opendate="2015-7-30 00:00:00" fixdate="2015-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): add SEL before UNION</summary>
      <description>Two purpose: (1) to ensure that the data type of non-primary branch (the 1st branch is the primary branch) of union can be casted to that of the primary branch; (2) to make UnionProcessor optimizer work; (3) if the SEL is redundant, it will be removed by IdentidyProjectRemover optimizer.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11525" opendate="2015-8-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning</summary>
      <description>Logically and functionally bucketing and partitioning are quite similar - both provide mechanism to segregate and separate the table's data based on its content. Thanks to that significant further optimisations like &amp;#91;partition&amp;#93; PRUNING or &amp;#91;bucket&amp;#93; MAP JOIN are possible.The difference seems to be imposed by design where the PARTITIONing is open/explicit while BUCKETing is discrete/implicit.Partitioning seems to be very common if not a standard feature in all current RDBMS while BUCKETING seems to be HIVE specific only.In a way BUCKETING could be also called by "hashing" or simply "IMPLICIT PARTITIONING".Regardless of the fact that these two are recognised as two separate features available in Hive there should be nothing to prevent leveraging same existing query/join optimisations across the two.BUCKET pruningEnable partition PRUNING equivalent optimisation for queries on BUCKETED tablesSimplest example is for queries like:"SELECT … FROM x WHERE colA=123123"to read only the relevant bucket file rather than all file-buckets that belong to a table.</description>
      <version>0.13.0,0.13.1,0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11526" opendate="2015-8-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  <bug id="11533" opendate="2015-8-11 00:00:00" fixdate="2015-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loop optimization for SIMD in integer comparisons</summary>
      <description>Long*CompareLong* classes can be optimized with subtraction and bitwise operators for better SIMD optimization.for(int i = 0; i != n; i++) { outputVector[i] = vector1[0] &gt; vector2[i] ? 1 : 0;}This issue will cover following classes; LongColEqualLongColumn LongColNotEqualLongColumn LongColGreaterLongColumn LongColGreaterEqualLongColumn LongColLessLongColumn LongColLessEqualLongColumn LongScalarEqualLongColumn LongScalarNotEqualLongColumn LongScalarGreaterLongColumn LongScalarGreaterEqualLongColumn LongScalarLessLongColumn LongScalarLessEqualLongColumn LongColEqualLongScalar LongColNotEqualLongScalar LongColGreaterLongScalar LongColGreaterEqualLongScalar LongColLessLongScalar LongColLessEqualLongScalar</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizationBench.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="11538" opendate="2015-8-12 00:00:00" fixdate="2015-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to skip init script while running tests</summary>
      <description>q_test_init.sql has grown over time. Now, it takes substantial amount of time. When debugging a particular query which doesn't need such initialization, this delay is annoyance.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11695" opendate="2015-8-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If user have no permission to create LOCAL DIRECTORY ，the Hql does not throw any exception and fail silently.</summary>
      <description>If user have no permission to create LOCAL DIRECTORY such as "/data/wangmeng/hiveserver2" ,the query does not throw any exception and fail silently.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11729" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update to use Tez 0.8</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12244" opendate="2015-10-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring code for avoiding of comparison of Strings and do comparison on Path</summary>
      <description>In Hive often String is used for representation path and it causes new issues.We need to compare it with equals() but comparing Strings often is not right in terms comparing paths .I think if we use Path from org.apache.hadoop.fs we will avoid new problems in future.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestConditionalResolverCommonJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOPrepareCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestCombineHiveInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12245" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support column comments for an HBase backed table</summary>
      <description>Currently the column comments of an HBase backed table are always returned as "from deserializer". For example,CREATE TABLE hbasetbl (key string comment 'It is key', state string comment 'It is state', country string comment 'It is country', country_id int comment 'It is country_id')STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = "info:state,info:country,info:country_id");hive&gt; describe hbasetbl;key string from deserializer state string from deserializer country string from deserializer country_id int from deserializer</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.format.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12566" opendate="2015-12-2 00:00:00" fixdate="2015-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result returns when using COALESCE in WHERE condition with LEFT JOIN</summary>
      <description>The left join query with on/where clause returns incorrect result (more rows are returned). See the reproducible sample below.Left table with data:CREATE TABLE ltable (i int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---1,\N,CD5415192314304,000712,\N,CD5415192225530,00071Right table with data:CREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---1,CD5415192314304,0007145,CD5415192314304,00072Query:SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');Result returns:1 NULL CD5415192314304 00071 NULL NULL NULL2 NULL CD5415192225530 00071 NULL NULL NULLThe correct result should be2 NULL CD5415192225530 00071 NULL NULL NULL</description>
      <version>0.13.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading</summary>
      <description>There are many redundant calls to metastore which is not needed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15551" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>memory leak in directsql for mysql+bonecp specific initialization</summary>
      <description>We observed HMS memory leak when directsql is enabled for MySQL metastore DB. The affected code is in the method MetaStoreDirecdtSql.executeNoResult():((Connection)jdoConn.getNativeConnection()).createStatement().execute(queryText);The statement object (from createStatement()) is unfortunately referenced in the Connection object. Although close() is called on the Connection object in finally block, the BoneCP just moves it to a freeConnection list. Hence, statement object never get chances to be closed.The leaked statement object is not huge (~1KB as observed in memory analyzer). However long running Hive Metastore Server is very likely ended up with bad performance doing frequent garbage collection.</description>
      <version>0.13.0</version>
      <fixedVersion>1.3.0,2.0.2,2.1.2,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="15848" opendate="2017-2-8 00:00:00" fixdate="2017-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>count or sum distinct incorrect when hive.optimize.reducededuplication set to true</summary>
      <description>Test Table:create table test(id int,key int,name int);Data：idkeyname1 1 21 2 31 3 21 4 21 5 3Test SQL1:select id,count(Distinct key),count(Distinct name)from (select id,key,name from count_distinct_test group by id,key,name)mgroup by id;result：154expect:152Test SQL2:select id,count(Distinct name),count(Distinct key)from (select id,key,name from count_distinct_test group by id,name,key)mgroup by id;result:125</description>
      <version>0.13.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1851" opendate="2010-12-14 00:00:00" fixdate="2010-6-14 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>wrong number of rows inserted reported by Hive</summary>
      <description>The counters that hive uses to report the number of rows inserted are not very reliable.Unless they become correct, it is a good idea to disable these reports.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="18510" opendate="2018-1-22 00:00:00" fixdate="2018-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable running checkstyle on test sources as well</summary>
      <description>Currently only source files are in the scope of checkstyle testing. We should expand the scope to include our testing code as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18511" opendate="2018-1-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix generated checkstyle errors</summary>
      <description>HIVE-18510 identified, that checkstyle was not running for test sources.After running checkstyle several errors are identified</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.RemoteMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.MiniHMS.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.EmbeddedMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.MetaStoreFactoryForTests.java</file>
    </fixedFiles>
  </bug>
  <bug id="22055" opendate="2019-7-26 00:00:00" fixdate="2019-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select count gives incorrect result after loading data from text file</summary>
      <description>Add one more load to mm_loaddata.q:Load data 3 times (both kv1.txt and kv2.txt contains 500 records)create table load0_mm (key string, value string) stored as textfile tblproperties("transactional"="true", "transactional_properties"="insert_only");load data local inpath '../../data/files/kv1.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;Expected outputPREHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPREHOOK: type: LOAD#### A masked pattern was here ####PREHOOK: Output: default@load0_mmPOSTHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPOSTHOOK: type: LOAD#### A masked pattern was here ####POSTHOOK: Output: default@load0_mmPREHOOK: query: select count(1) from load0_mmPREHOOK: type: QUERYPREHOOK: Input: default@load0_mm#### A masked pattern was here ####POSTHOOK: query: select count(1) from load0_mmPOSTHOOK: type: QUERYPOSTHOOK: Input: default@load0_mm#### A masked pattern was here ####1500Got:&amp;#91;ERROR&amp;#93;   TestMiniLlapLocalCliDriver.testCliDriver:59 Client Execution succeeded but contained differences (error code = 1) after executing mm_loaddata.q63c63&lt; 1480—&gt; 1500 </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3405" opendate="2012-8-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4756" opendate="2013-6-19 00:00:00" fixdate="2013-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hadoop 0.23 profile to 2.0.5-alpha</summary>
      <description>The minimr tests fail at present with the 0.23 profile. In my tests upgrading to 2.0.5-alpha fixes this.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4764" opendate="2013-6-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Kerberos HTTP authentication for HiveServer2 running in http mode</summary>
      <description>Support Kerberos authentication for HiveServer2 running in http mode.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpBasicAuthInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="4810" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor exec package</summary>
      <description>The exec package contains both operators and classes used to execute the job. Moving the latter into a sub package makes the package slightly more manageable and will make it easier to provide a tez-based implementation.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart.err.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.reflect.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udfnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.aggregator.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.serde.regex2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.minimr.broken.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.local.mapred.error.cache.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fatal.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part.max.per.node.q.out</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Throttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.java</file>
      <file type="M">ql.src.test.results.clientnegative.autolocal1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cachingprintstream.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cluster.tasklog.retrieval.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4812" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logical explain plan</summary>
      <description>In various situations it would have been useful to me to glance at the operator plan before we break it into tasks and apply join, total order sort, etc optimizations.I've added this as an options to explain. "Explain logical &lt;QUERY&gt;" will output the full operator tree (not the stage plans, tasks, AST etc).Again, I don't think this has to even be documented for users, but might be useful to developers.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="5217" opendate="2013-9-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add long polling to asynchronous execution in HiveServer2</summary>
      <description>HIVE-4617 provides support for async execution in HS2. The client gets an operation handle which it can poll to check on the operation status. However, the polling frequency is entirely left to the client which can be resource inefficient. Long polling will solve this, by blocking the client request to check the operation status for a configurable amount of time (a new HS2 config) if the data is not available, but responding immediately if the data is available.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5229" opendate="2013-9-5 00:00:00" fixdate="2013-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better thread management for HiveServer2 async threads</summary>
      <description>HIVE-4617 provides support for async execution in HS2. The async (background) thread pool currently creates N threads (server config), which are alive all the time. If all the threads in the pool are busy, a new request is added to a blocking queue. However, we can improve the strategy by not having all the async (background) threads alive when there are no corresponding requests. The async threads should die after a certain timeout if there are no new requests to handle.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5232" opendate="2013-9-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make JDBC use the new HiveServer2 async execution API by default</summary>
      <description>HIVE-4617 provides support for async execution in HS2. There are some proposed improvements in followup JIRAs:HIVE-5217HIVE-5229HIVE-5230HIVE-5441There is also HIVE-5060 which assumes that execute to be asynchronous by default.Once they are in, we can think of using the async API as the default for JDBC. This can enable the server to report back error sooner to the client. It can also be useful in cases where a statement.cancel is done in a different thread - the original thread will now be able to detect the cancel, as opposed to the use of the blocking execute calls, in which statement.cancel will be a no-op.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="5277" opendate="2013-9-12 00:00:00" fixdate="2013-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase handler skips rows with null valued first cells when only row key is selected</summary>
      <description>HBaseStorageHandler skips rows with null valued first cells when only row key is selected.SELECT key, col1, col2 FROM hbase_table;key1 cell1 cell2 key2 NULL cell3SELECT COUNT(key) FROM hbase_table;1HiveHBaseTableInputFormat.getRecordReader makes first cell selected to avoid skipping rows. But when the first cell is null, HBase skips that row.http://hbase.apache.org/book/perf.reading.html 12.9.6. Optimal Loading of Row Keys describes how to deal with this problem.I tried to find an existing issue, but I couldn't. If you find a same issue, please make this issue duplicated.</description>
      <version>0.11.0,0.12.0,0.11.1,0.13.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5278" opendate="2013-9-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move some string UDFs to GenericUDFs, for better varchar support</summary>
      <description>To better support varchar/char types in string UDFs, select UDFs should be converted to GenericUDFs. This allows the UDF to return the resulting char/varchar length in the type metadata.This work is being split off as a separate task from HIVE-4844. The initial UDFs as part of this work are concat/lower/upper.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5290" opendate="2013-9-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some HCatalog tests have been behaving flaky</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatContext.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.common.HCatContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5296" opendate="2013-9-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak: OOM Error after multiple open/closed JDBC connections.</summary>
      <description>Multiple connections to Hiveserver2, all of which are closed and disposed of properly show the Java heap size to grow extremely quickly. This issue can be recreated using the following codeimport java.sql.DriverManager;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;import org.apache.hive.service.cli.HiveSQLException;import org.apache.log4j.Logger;/* * Class which encapsulates the lifecycle of a query or statement. * Provides functionality which allows you to create a connection */public class HiveClient { Connection con; Logger logger; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; private String db; public HiveClient(String db) { logger = Logger.getLogger(HiveClient.class); this.db=db; try{ Class.forName(driverName); }catch(ClassNotFoundException e){ logger.info("Can't find Hive driver"); } String hiveHost = GlimmerServer.config.getString("hive/host"); String hivePort = GlimmerServer.config.getString("hive/port"); String connectionString = "jdbc:hive2://"+hiveHost+":"+hivePort +"/default"; logger.info(String.format("Attempting to connect to %s",connectionString)); try{ con = DriverManager.getConnection(connectionString,"",""); }catch(Exception e){ logger.error("Problem instantiating the connection"+e.getMessage()); } } public int update(String query) { Integer res = 0; Statement stmt = null; try{ stmt = con.createStatement(); String switchdb = "USE "+db; logger.info(switchdb); stmt.executeUpdate(switchdb); logger.info(query); res = stmt.executeUpdate(query); logger.info("Query passed to server"); stmt.close(); }catch(HiveSQLException e){ logger.info(String.format("HiveSQLException thrown, this can be valid, " + "but check the error: %s from the query %s",query,e.toString())); }catch(SQLException e){ logger.error(String.format("Unable to execute query SQLException %s. Error: %s",query,e)); }catch(Exception e){ logger.error(String.format("Unable to execute query %s. Error: %s",query,e)); } if(stmt!=null) try{ stmt.close(); }catch(SQLException e){ logger.error("Cannot close the statment, potentially memory leak "+e); } return res; } public void close() { if(con!=null){ try { con.close(); } catch (SQLException e) { logger.info("Problem closing connection "+e); } } } }And by creating and closing many HiveClient objects. The heap space used by the hiveserver2 runjar process is seen to increase extremely quickly, without such space being released.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="5322" opendate="2013-9-19 00:00:00" fixdate="2013-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FsPermission is initialized incorrectly in HIVE 5513</summary>
      <description>The change in HIVE-5313 converts the octal string into short using Short.parseShort(scratchDirPermission) but Short.parseShort function expects decimal. So "700" gets converted to 700 instead of 448.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="5325" opendate="2013-9-20 00:00:00" fixdate="2013-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement statistics providing ORC writer and reader interfaces</summary>
      <description>HIVE-5324 adds new interfaces that can be implemented by ORC reader/writer to provide statistics. Writer provided statistics is used to update table/partition level statistics in metastore. Reader provided statistics can be used for reducer estimation, CBO etc. in the absence of metastore statistics.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StringColumnStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="5327" opendate="2013-9-20 00:00:00" fixdate="2013-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential leak and cleanup in utilities.java</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="5349" opendate="2013-9-24 00:00:00" fixdate="2013-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QTestutil does not properly set UTF-8</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="538" opendate="2009-6-3 00:00:00" fixdate="2009-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make hive_jdbc.jar self-containing</summary>
      <description>Currently, most jars in hive/build/dist/lib and the hadoop-*-core.jar are required in the classpath to run jdbc applications on hive. We need to do atleast the following to get rid of most unnecessary dependencies:1. get rid of dynamic serde and use a standard serialization format, maybe tab separated, json or avro2. dont use hadoop configuration parameters3. repackage thrift and fb303 classes into hive_jdbc.jar</description>
      <version>0.3.0,0.4.0,0.6.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5380" opendate="2013-9-27 00:00:00" fixdate="2013-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-default OI constructors should be supported for backwards compatibility</summary>
      <description>In HIVE-5263 we started serializing OI's when cloning the plan. This was a great boost in speed for many queries. In the future we'd like to stop copying the OI's, perhaps in HIVE-4396.Until then Custom Serdes will not work on trunk. This is a fix to allow custom serdes such as the Hive JSon Serde work until we address the fact we don't want to have to copy the OI's. Since this is modifying the byte code, we should recommend that the no-arg constructor be added.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="5382" opendate="2013-9-27 00:00:00" fixdate="2013-8-27 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Allow strings represented as exponential notation to be typecasted to int/smallint/bigint/tinyint</summary>
      <description>Follow up jira for HIVE-5352</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.java</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="5403" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move loading of filesystem, ugi, metastore client to hive session</summary>
      <description>As part of HIVE-5184, the metastore connection, loading filesystem were done as part of the tez session so as to speed up query times while paying a cost at startup. We can do this more generally in hive to apply to both the mapreduce and tez side of things.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
    </fixedFiles>
  </bug>
  <bug id="5411" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate expression serialization to Kryo</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexSearchCondition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgument.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5425" opendate="2013-10-2 00:00:00" fixdate="2013-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a configuration option to control the default stripe size for ORC</summary>
      <description>We should provide a configuration option to control the default stripe size.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5446" opendate="2013-10-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive can CREATE an external table but not SELECT from it when file path have spaces</summary>
      <description>Create external table table1 (age int, gender string, totBil float, dirBill float, alkphos int,sgpt int, sgot int, totProt float, aLB float, aG float, sel int) ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILELOCATION 'hdfs://namenodehost:9000/hive newtable';select * from table1;return nothing even there is file in the target folder</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5449" opendate="2013-10-4 00:00:00" fixdate="2013-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive schematool info option incorrectly reports error for Postgres metastore</summary>
      <description>The schema tool has an option to verify the schema version stored in the metastore. This is implemented as a simple select query executed via JDBC. The problem is that Postgres requires object names to be quoted due to the way tables are created. It's a similar issues hit by metastore direct SQL (HIVE-5264, HIVE-5265 etc).</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="5490" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR(col, 1, 0) returns wrong result in vectorized mode</summary>
      <description>The query select substr(cstring1, 1, 0) from alltypesorc;returns all empty strings when set hive.vectorized.execution.enabled = false;which is the correct result, and returns non-empty strings whenset hive.vectorized.execution.enabled = true;which is not correct.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
    </fixedFiles>
  </bug>
  <bug id="5522" opendate="2013-10-11 00:00:00" fixdate="2013-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move split generation into the AM</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="554" opendate="2009-6-10 00:00:00" fixdate="2009-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GenericUDF to create arrays, maps</summary>
      <description>Here is an example:SELECT array(1,2,3)[3], map("a":1,"b":2,"c":3)["a"], struct(user_id:3, revenue: sum(rev))FROM tableGROUP BY user_id;This is relatively easy to do with the GenericUDF framework, and will greatly increase the flexibility of the language.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5540" opendate="2013-10-15 00:00:00" fixdate="2013-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat e2e test failures: "Expect 1 jobs in logs, but get 1"</summary>
      <description>With current state of trunk repo (see below) WebHCat e2e tests have 4 errors all of the same type."Expect 1 jobs in logs, but get 1"Test run log file attachedcommit b612a91f7f09f45474f593f99039ec78d2c03b68Author: Edward Capriolo &lt;ecapriolo@apache.org&gt;Date: Mon Oct 14 21:40:44 2013 +0000 An explode function that includes the item's position in the array (Niko Stahl via egc) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532108 13f79535-47bb-0310-9956-ffa450edef68commit ad18f0747a3448fc8cda2197df6223e8abc93dc6Author: Brock Noland &lt;brock@apache.org&gt;Date: Mon Oct 14 21:22:12 2013 +0000 HIVE-5423 - Speed up testing of scalar UDFS (Edward Capriolo via Brock Noland) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532103 13f79535-47bb-0310-9956-ffa450edef68commit 4a2152b0f7df5a3f42f8577a27c2a9269697def1Author: Thejas Madhavan Nair &lt;thejas@apache.org&gt;Date: Mon Oct 14 20:31:22 2013 +0000 HIVE-5508 : &amp;#91;WebHCat&amp;#93; ignore log collector e2e tests for Hadoop 2 (Daniel Dai via Thejas Nair) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532077 13f79535-47bb-0310-9956-ffa450edef68commit 83865be207bc044c506eb957c01e8fcbf551b7d1Author: Thejas Madhavan Nair &lt;thejas@apache.org&gt;Date: Mon Oct 14 20:14:34 2013 +0000 HIVE-5535 : &amp;#91;WebHCat&amp;#93; Webhcat e2e test JOBS_2 fail due to permission when hdfs umask setting is 022 (Daniel Dai via Thejas Nair) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532054 13f79535-47bb-0310-9956-ffa450edef68commit 78da38b50d2264ebdcca0651f7c6f3750eaf1221Author: Brock Noland &lt;brock@apache.org&gt;Date: Mon Oct 14 19:50:55 2013 +0000 HIVE-5526 - NPE in ConstantVectorExpression.evaluate(vrg) (Remus Rusanu via Brock Noland) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532044 13f79535-47bb-0310-9956-ffa450edef68commit 58a3275477fc2c4f85dfb0a729150732a8230579Author: Thejas Madhavan Nair &lt;thejas@apache.org&gt;Date: Mon Oct 14 19:02:22 2013 +0000 HIVE-5509 : &amp;#91;WebHCat&amp;#93; TestDriverCurl to use string comparison for jobid (Daniel Dai via Thejas Nair) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532026 13f79535-47bb-0310-9956-ffa450edef68commit 95e45ede68be95603c6f43e06d9e68b20218b54fAuthor: Thejas Madhavan Nair &lt;thejas@apache.org&gt;Date: Mon Oct 14 19:00:44 2013 +0000 HIVE-5507: &amp;#91;WebHCat&amp;#93; test.other.user.name parameter is missing from build.xml in e2e harness (Daniel Dai via Thejas Nair) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532025 13f79535-47bb-0310-9956-ffa450edef68commit 976ece58f134e02384e4f54474c1749b85c03934Author: Jianyong Dai &lt;daijy@apache.org&gt;Date: Mon Oct 14 18:38:29 2013 +0000 HIVE-5448: webhcat duplicate test TestMapReduce_2 should be removed (Thejas M Nair via Daniel Dai) git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532018 13f79535-47bb-0310-9956-ffa450edef68</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="5562" opendate="2013-10-16 00:00:00" fixdate="2013-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide stripe level column statistics in ORC</summary>
      <description>ORC maintains two levels of column statistics. Index statistics (for every rowgroup) and file level column statistics for the entire file. It is useful to have stripe level column statistics which will be intermediate to index and file statistics. The reason to maintain stripe level statistics is that, the current input split computation logic is based on stripe boundaries. So if stripe level statistics are available and if a stripe doesn't satisfy a predicate condition then that entire stripe (also split) can be eliminated from split computation.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="5576" opendate="2013-10-17 00:00:00" fixdate="2013-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blank lines missing from .q.out files created on Windows for testcase=TestCliDriver</summary>
      <description>If you create a .q.out file on Windows using a command like this:ant test "-Dhadoop.security.version=1.1.0-SNAPSHOT" "-Dhadoop.root=c:\hw\project\hadoop-monarch" "-Dresolvers=internal" "-Dhadoop-0.20S.version=1.1.0-SNAPSHOT" "-Dhadoop.mr.rev=20S" "-Dhive.support.concurrency=false" "-Dshims.include=0.20S" "-Dtest.continue.on.failure=true" "-Dtest.halt.on.failure=no" "-Dtest.print.classpath=true" "-Dtestcase=TestCliDriver" "-Dqfile=vectorized_math_funcs.q,vectorized_string_funcs.q,vectorized_casts.q" "-Doverwrite=true" "-Dtest.silent=false"Then the .q.out files generated in the hive directory underql\src\test\results\clientpositivehaving missing blank lines.So, the .q tests will pass on your Windows machine. But when you upload them in a patch, they fail on the automated build server. See HIVE-5517 for an example. HIVE-5517.3.patch has .q.out files with missing blank lines. Hive-5517.4.patch has .q.out files created on a Linux or Mac system. Those have blank lines.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="558" opendate="2009-6-12 00:00:00" fixdate="2009-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>describe extended table/partition output is cryptic</summary>
      <description>describe extended table prints out the Thrift metadata object directly. The information from it is not easy to read or parse. Output should be easily read and can be simple parsed to get table location etc by programs.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out.0.17</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.queries.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5580" opendate="2013-10-17 00:00:00" fixdate="2013-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>push down predicates with an and-operator between non-SARGable predicates will get NPE</summary>
      <description>When all of the predicates in an AND-operator in a SARG expression get removed by the SARG builder, evaluation can end up with a NPE. Sub-expressions are typically removed from AND-operators because they aren't SARGable.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.create.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="5581" opendate="2013-10-17 00:00:00" fixdate="2013-12-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement vectorized year/month/day... etc. for string arguments</summary>
      <description>Functions year(), month(), day(), weekofyear(), hour(), minute(), second() need to be implemented for string arguments in vectorized mode. They already work for timestamp arguments.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSecond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMinute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHour.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5582" opendate="2013-10-17 00:00:00" fixdate="2013-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement BETWEEN filter in vectorized mode</summary>
      <description>Implement optimized support for filters of the formcolumn BETWEEN scalar1 AND scalar2in vectorized mode.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5583" opendate="2013-10-17 00:00:00" fixdate="2013-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement support for IN (list-of-constants) filter in vectorized mode</summary>
      <description>Implement optimized, vectorized support for filters of this form:column IN (constant1, ... constantN)</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5586" opendate="2013-10-17 00:00:00" fixdate="2013-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call createSplits with multiple paths if partition specs match on Tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5625" opendate="2013-10-23 00:00:00" fixdate="2013-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix issue with metastore version restriction test.</summary>
      <description>Based on Brock's comments, the change made in HIVE-5403 change the nature of the test.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
    </fixedFiles>
  </bug>
  <bug id="5626" opendate="2013-10-23 00:00:00" fixdate="2013-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable metastore direct SQL for drop/similar queries</summary>
      <description>Metastore direct SQL is currently disabled for any queries running inside external transaction (i.e. all modification queries, like dropping stuff).This was done to keep the strictly performance-optimization behavior when using Postgres, which unlike other RDBMS-es fails the tx on any syntax error; so, if direct SQL is broken there's no way to fall back. So, it is disabled for these cases.It is not as important because drop commands are rare, but we might want to address that. Either by some config setting or by making it work on non-postgres DBs.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5632" opendate="2013-10-23 00:00:00" fixdate="2013-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate splits based on SARGs using stripe statistics in ORC</summary>
      <description>HIVE-5562 provides stripe level statistics in ORC. Stripe level statistics combined with predicate pushdown in ORC (HIVE-4246) can be used to eliminate the stripes (thereby splits) that doesn't satisfy the predicate condition. This can greatly reduce unnecessary reads.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgument.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5653" opendate="2013-10-25 00:00:00" fixdate="2013-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Shuffle Join produces incorrect results</summary>
      <description>Vectorized shuffle join should work out-of-the-box, but it produces empty result set. Investigating.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2009-6-19 00:00:00" fixdate="2009-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc: integrate hive with pentaho report designer</summary>
      <description>Instead of trying to get a complete implementation of jdbc, its probably more useful to pick reporting/analytics software out there and implement the jdbc methods necessary to get them working. This jira is a first attempt at this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2013-10-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>annoying ZK exceptions are annoying</summary>
      <description>when I run tests locally (or on cluster IIRC) there are bunch of ZK-related exceptions in Hive log, such as2013-10-28 09:50:50,851 ERROR zookeeper.ClientCnxn (ClientCnxn.java:processEvent(523)) - Error while calling watcher java.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497) 2013-10-28 09:51:05,747 DEBUG server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1024)) - ignoring exception during input shutdownjava.net.SocketException: Socket is not connected at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:633) at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) at org.apache.zookeeper.server.NIOServerCnxn.closeSock(NIOServerCnxn.java:1020) at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:977) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:347) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224) at java.lang.Thread.run(Thread.java:680)They are annoying when you look for actual problems in logs.Those on DEBUG level should be silenced via log levels for ZK classes by default. Not sure what to do with ERROR level one(s?), I'd need to look if they can be silenced/logged as DEBUG on hive side, or maybe file a bug for ZK...</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.log4j.properties</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">common.src.test.resources.hive-log4j-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j-test.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5699" opendate="2013-10-30 00:00:00" fixdate="2013-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test for vectorized BETWEEN for timestamp inputs</summary>
      <description>See request for this in HIVE-5582</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5729" opendate="2013-11-1 00:00:00" fixdate="2013-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline displays version as ???? after mavenization</summary>
      <description>NO PRECOMMIT TESTSIn Beeline.java, method getApplicationTitle(), it looks to the Beeline class's package to find version information. However, MANIFESTs are not included in Beeline jar after mavenization.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5757" opendate="2013-11-6 00:00:00" fixdate="2013-1-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized support for CASE</summary>
      <description>Implement support for CASE in vectorized mode. The approach is to use the vectorized UDF adaptor internally. A higher-performance version that used VectorExpression subclasses was considered but not done due to complexity. Such a version potentially could be done in the future if it's important enough.This is high priority because CASE is a fairly popular expression.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5760" opendate="2013-11-6 00:00:00" fixdate="2013-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vectorized support for CHAR/VARCHAR data types</summary>
      <description>Add support to allow queries referencing VARCHAR columns and expression results to run efficiently in vectorized mode. This should re-use the code for the STRING type to the extent possible and beneficial. Include unit tests and end-to-end tests. Consider re-using or extending existing end-to-end tests for vectorized string operations.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryFunc.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringScalarCompareColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToStringViaLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.java</file>
    </fixedFiles>
  </bug>
  <bug id="5761" opendate="2013-11-6 00:00:00" fixdate="2013-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized support for the DATE data type</summary>
      <description>Add support to allow queries referencing DATE columns and expression results to run efficiently in vectorized mode. This should re-use the code for the the integer/timestamp types to the extent possible and beneficial. Include unit tests and end-to-end tests. Consider re-using or extending existing end-to-end tests for vectorized integer and/or timestamp operations.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
    </fixedFiles>
  </bug>
  <bug id="5782" opendate="2013-11-8 00:00:00" fixdate="2013-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 should be able to ride out price spikes</summary>
      <description>Price spikes for spot instances cause PTest2 major issues. We should be able to ride them out while providing some minimum level of service.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudComputeService.java</file>
    </fixedFiles>
  </bug>
  <bug id="5795" opendate="2013-11-11 00:00:00" fixdate="2013-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should be able to skip header and footer rows when reading data file for a table</summary>
      <description>Hive should be able to skip header and footer lines when reading data file from table. In this way, user don't need to processing data which generated by other application with a header or footer and directly use the file for table operations.To implement this, the idea is adding new properties in table descriptions to define the number of lines in header and footer and skip them when reading the record from record reader. An DDL example for creating a table with header and footer should be like this:Create external table testtable (name string, message string) row format delimited fields terminated by '\t' lines terminated by '\n' location '/testtable' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="2");</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5812" opendate="2013-11-13 00:00:00" fixdate="2013-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 SSL connection transport binds to loopback address by default</summary>
      <description>The secure socket transport implemented as part of HIVE-5351, binds to loopback address by default. If the bind interface gets used only if its explicitly defined in the hive-site or via environment.This behavior should be same as non-SSL transport.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="5813" opendate="2013-11-13 00:00:00" fixdate="2013-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-way Left outer join fails in vectorized mode</summary>
      <description>with hive.auto.convert.join=true</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5817" opendate="2013-11-13 00:00:00" fixdate="2013-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>column name to index mapping in VectorizationContext is broken</summary>
      <description>Columns coming from different operators may have the same internal names ("_colNN"). There exists a query in the form select b.cb, a.ca from a JOIN b ON ... JOIN x ON ...; (distilled from a more complex query), which runs ok w/o vectorization. With vectorization, it will run ok for most ca, but for some ca it will fail (or can probably return incorrect results). That is because when building column-to-VRG-index map in VectorizationContext, internal column name for ca that the first map join operator adds to the mapping may be the same as internal name for cb that the 2nd one tries to add. 2nd VMJ doesn't add it (see code in ctor), and when it's time for it to output stuff, it retrieves wrong index from the map by name, and then wrong vector from VRG.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="5827" opendate="2013-11-14 00:00:00" fixdate="2013-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect location of logs for failed tests.</summary>
      <description>Extending HIVE-5790 to fix other tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestParse.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="5829" opendate="2013-11-15 00:00:00" fixdate="2013-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite Trim and Pad UDFs based on GenericUDF</summary>
      <description>This JIRA includes following UDFs:1. trim()2. ltrim()3. rtrim()4. lpad()5. rpad()</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5845" opendate="2013-11-18 00:00:00" fixdate="2013-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS failed on vectorized code path</summary>
      <description>Following query fails: create table store_sales_2 stored as orc as select * from alltypesorc;</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="5846" opendate="2013-11-18 00:00:00" fixdate="2013-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze command fails with vectorization on</summary>
      <description>analyze table alltypesorc compute statistics; fails</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5847" opendate="2013-11-18 00:00:00" fixdate="2013-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DatabaseMetadata.getColumns() doesn&amp;#39;t show correct column size for char/varchar/decimal</summary>
      <description>column_size, decimal_digits, num_prec_radix should be set appropriately based on the type qualifiers.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeDescriptor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Type.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="5859" opendate="2013-11-20 00:00:00" fixdate="2013-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create view does not captures inputs</summary>
      <description>For example, CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;should capture "default.tbl_j5jbymsx8e" as input entity for authorization process but currently it's not.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5861" opendate="2013-11-20 00:00:00" fixdate="2013-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception in multi insert statement on Tez</summary>
      <description>Multi insert statements that have multiple group by clauses aren't handled properly in tez.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5867" opendate="2013-11-21 00:00:00" fixdate="2013-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC driver and beeline should support executing an initial SQL script</summary>
      <description>HiveCLI support the .hiverc script that is executed at the start of the session. This is helpful for things like registering UDFs, session specific configs etc.This functionality is missing for beeline and JDBC clients. It would be useful for JDBC driver to support an init script with SQL statements that's automatically executed after connection. The script path can be specified via JDBC connection URL. For example jdbc:hive2://localhost:10000/default;initScript=/home/user1/scripts/init.sqlThis can be added to Beeline's command line option like "-i /home/user1/scripts/init.sql"To help transition from HiveCLI to Beeline, we can keep the default init script as $HOME/.hiverc</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="5877" opendate="2013-11-23 00:00:00" fixdate="2013-12-23 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement vectorized support for IN as boolean-valued expression</summary>
      <description>Implement support for IN as a Boolean-valued expression, e..g.select col1 IN (1, 2, 3) from T;or select col1from Twhere NOT (col1 IN (1, 2, 3));This will also automatically add support for NOT IN because NOT IN is automatically transformed into NOT ( ... IN ... ) by the parser.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
    </fixedFiles>
  </bug>
  <bug id="5895" opendate="2013-11-26 00:00:00" fixdate="2013-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>vectorization handles division by zero differently from normal execution</summary>
      <description>Produces infinity, not NULL</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5897" opendate="2013-11-26 00:00:00" fixdate="2013-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hadoop2 execution environment Milestone 2</summary>
      <description>Follow on to HIVE-5755.List of known issues:hcatalog-pig-adapter and ql need &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;hcatalog core and hbase storage handler needs &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;classifier&gt;tests&lt;/classifier&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-hs&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-server-tests&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;classifier&gt;tests&lt;/classifier&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;hcatalog core needs: &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;beeline needs &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.storage-handlers.hbase.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5901" opendate="2013-11-27 00:00:00" fixdate="2013-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query cancel should stop running MR tasks</summary>
      <description>Currently, query canceling does not stop running MR job immediately.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5946" opendate="2013-12-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5948" opendate="2013-12-4 00:00:00" fixdate="2013-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output file name is random when using Tez with "insert overwrite local directory"</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5954" opendate="2013-12-4 00:00:00" fixdate="2013-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - get_privilege_set should check role hierarchy</summary>
      <description>A role can belong to another role. But get_privilege_set in hive metastore api checks only the privileges of the immediate roles a user belongs to.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.set.show.current.role.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="5955" opendate="2013-12-4 00:00:00" fixdate="2013-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - metastore api support for get_privilege_set api that checks specific role</summary>
      <description>If the user has a set a specific role using 'SET ROLE role', then the authorization check should be done for specific role.The authorization check should not check with all the roles the user belongs to.This would new/different method in metastore api .</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5966" opendate="2013-12-5 00:00:00" fixdate="2013-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix eclipse:eclipse post shim aggregation changes</summary>
      <description>The shim bundle module marks it's deps provided so users of the bundle won't pull in the child dependencies. This causes the eclipse workspace generated by eclipse:eclipse to fail because it only includes the source from the bundle source directory, which is empty.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.assembly.src.assemble.uberjar.xml</file>
      <file type="M">shims.assembly.pom.xml</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5973" opendate="2013-12-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB joins produce incorrect results with multiple partitions and buckets</summary>
      <description>It looks like there is an issue with re-using the output object array in the select operator. When we read rows of the non-big tables, we hold on to the output object in the priority queue. This causes hive to produce incorrect results because all the elements in the priority queue refer to the same object and the join happens on only one of the buckets.output[i] = eval[i].evaluate(row);</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5975" opendate="2013-12-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] templeton mapreduce job failed if provide "define" parameters</summary>
      <description>Trying to submit a mapreduce job through templeton failed:curl -k -u user:pass -d user.name=user -d define=JobName=MRPiJob -d class=pi -d arg=16 -d arg=100 -d jar="hadoop-mapreduce-examples.jar" https://xxx/templeton/v1/mapreduce/jarThe error message is:"Usage: org.apache.hadoop.examples.QuasiMonteCarlo &lt;nMaps&gt; &lt;nSamples&gt; Generic options supported are -conf &lt;configuration file&gt; specify an application configuration file -D &lt;property=value&gt; use value for given property -fs &lt;local|namenode:port&gt; specify a namenode -jt &lt;local|jobtracker:port&gt; specify a job tracker -files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster -libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath. -archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax is bin/hadoop command &amp;#91;genericOptions&amp;#93; &amp;#91;commandOptions&amp;#93;templeton: job failed with exit code 2"Note that if we remove the "define" parameter it works fine.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="5976" opendate="2013-12-6 00:00:00" fixdate="2013-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decouple input formats from STORED as keywords</summary>
      <description>As noted in HIVE-5783, we hard code the input formats mapped to keywords. It'd be nice if there was a registration system so we didn't need to do that.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.duplicate.key.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.union.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.genericFileFormat.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.bad.class.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5978" opendate="2013-12-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollups not supported in vector mode.</summary>
      <description>Rollups are not supported in vector mode, the query should fail to vectorize. A separate jira will be filed to implement rollups in vector mode.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5991" opendate="2013-12-9 00:00:00" fixdate="2013-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC RLEv2 fails with ArrayIndexOutOfBounds exception for PATCHED_BLOB encoding</summary>
      <description>PATCHED_BLOB encoding creates mask with number of bits required for 95th percentile value. If the 95th percentile value requires 32 bits then the mask creation will result in integer overflow.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6011" opendate="2013-12-11 00:00:00" fixdate="2013-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correlation optimizer unit tests are failing on tez</summary>
      <description>Some extra clean-ups in tez branch made this to fail.</description>
      <version>0.13.0</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6012" opendate="2013-12-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>restore backward compatibility of arithmetic operations</summary>
      <description>HIVE-5356 changed the behavior of some of the arithmetic operations, and the change is not backward compatible, as pointed out in this jira commentint / int =&gt; decimalfloat / float =&gt; doublefloat * float =&gt; doublefloat + float =&gt; double</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6045" opendate="2013-12-17 00:00:00" fixdate="2013-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline hivevars is broken for more than one hivevar</summary>
      <description>HIVE-4568 introduced --hivevar flag. But if you specify more than one hivevar, for example beeline --hivevar file1=/user/szehon/file1 --hivevar file2=/user/szehon/file2then the variables during runtime get mangled to evaluate to:file1=/user/szehon/file1&amp;file2=/user/szehon/file2</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6051" opendate="2013-12-17 00:00:00" fixdate="2013-1-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create DecimalColumnVector and a representative VectorExpression for decimal</summary>
      <description>Create a DecimalColumnVector to use as a basis for vectorized decimal operations. Include a representative VectorExpression on decimal (e.g. column-column addition) to demonstrate it's use.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6067" opendate="2013-12-19 00:00:00" fixdate="2013-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized decimal comparison filters</summary>
      <description>Using the new DecimalColumnVector type, implement templates to generate VectorExpression subclasses for Decimal comparison filters (&lt;, &lt;=, &gt;, &gt;=, =, !=). Include scalar-column, column-scalar, and column-column filter cases. Include unit tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6068" opendate="2013-12-19 00:00:00" fixdate="2013-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 client on windows does not handle the non-ascii characters properly</summary>
      <description>When running a select query against a table which contains rows with non-ascii characters HiveServer2 Beeline client returns them wrong. Example:738;Garçu, Le (1995);Drama741;Ghost in the Shell (Kôkaku kidôtai) (1995);Animation|Sci-Ficome out from a HiveServer2 beeline client as:'738' 'Gar?u, Le (1995)' 'Drama''741' 'Ghost in the Shell (K?kaku kid?tai) (1995)' 'Animation|Sci-Fi'</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6095" opendate="2013-12-22 00:00:00" fixdate="2013-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently II</summary>
      <description>This is follow-up of HIVE-3616.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6097" opendate="2013-12-23 00:00:00" fixdate="2013-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sessions on Tez NPE when quitting CLI</summary>
      <description>Reported in HIVE-5148. Code doesn't check whether session exists before attempting to close.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="61" opendate="2008-11-13 00:00:00" fixdate="2008-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6124" opendate="2013-12-31 00:00:00" fixdate="2013-1-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support basic Decimal arithmetic in vector mode (+, -, *)</summary>
      <description>Create support for basic decimal arithmetic (+, -, * but not /, %) based on templates for column-scalar, scalar-column, and column-column operations.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6139" opendate="2014-1-4 00:00:00" fixdate="2014-1-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized decimal division and modulo</summary>
      <description>Support column-scalar, scalar-column, and column-column versions for division and modulo. Include unit tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColAddDecimalColumn.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalarDecimal.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6147" opendate="2014-1-6 00:00:00" fixdate="2014-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support avro data stored in HBase columns</summary>
      <description>Presently, the HBase Hive integration supports querying only primitive data types in columns. It would be nice to be able to store and query Avro objects in HBase columns by making them visible as structs to Hive. This will allow Hive to perform ad hoc analysis of HBase data which can be deeply structured.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseCellMap.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseRowSerializer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ColumnMappings.java</file>
      <file type="M">hbase-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6152" opendate="2014-1-6 00:00:00" fixdate="2014-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert query fails on hdfs federation + viewfs</summary>
      <description>This is because Hive first writes data to /tmp/ and than moves from /tmp to final destination. In federated HDFS recommendation is to mount /tmp on a separate nameservice, which is usually different than /user. Since renames across different mount points are not supported, this fails.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="6156" opendate="2014-1-7 00:00:00" fixdate="2014-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized reader for Date datatype for ORC format.</summary>
      <description>We need to implement vectorized reader for Date datatype for ORC format.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6164" opendate="2014-1-8 00:00:00" fixdate="2014-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build on Windows failed with datanucleus enhancer error "command line is too long"</summary>
      <description>Build hive 0.13 against hadoop 2.0 on Windows always fail:mvn install -Phadoop-2...&amp;#91;ERROR&amp;#93; --------------------&amp;#91;ERROR&amp;#93; Standard error from the DataNucleus tool + org.datanucleus.enhancer.DataNucleusEnhancer :&amp;#91;ERROR&amp;#93; --------------------&amp;#91;ERROR&amp;#93; The command line is too long.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6181" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support grant/revoke on views - parser changes</summary>
      <description>Support grant/revoke statements on views. Includes parser changes.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="6182" opendate="2014-1-10 00:00:00" fixdate="2014-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP Authentication errors need to be more informative</summary>
      <description>There are a host of errors that can happen when logging into an LDAP-enabled Hive-server2 from beeline. But for any error there is only a generic log message:SASL negotiation failurejavax.security.sasl.SaslException: PLAIN auth failed: Error validating LDAP user at org.apache.hadoop.security.SaslPlainServer.evaluateResponse(SaslPlainServer.java:108) at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponsAnd on Beeline side there is only an even more unhelpful message:Error: Invalid URL: jdbc:hive2://localhost:10000/default (state=08S01,code=0)It would be good to print out the underlying error message at least in the log, if not beeline. But today they are swallowed. This is bad because the underlying message is the most important, having the error codes as shown here : LDAP error codeThe beeline seems to throw that exception for any error during connection, authetication or otherwise.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6183" opendate="2014-1-10 00:00:00" fixdate="2014-1-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized type cast from/to decimal(p, s)</summary>
      <description>Add support for all the type supported type casts to/from decimal(p,s) in vectorized mode.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6196" opendate="2014-1-14 00:00:00" fixdate="2014-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect package name for few tests.</summary>
      <description>These are tests which were moved from one dir to another recently.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
    </fixedFiles>
  </bug>
  <bug id="6197" opendate="2014-1-14 00:00:00" fixdate="2014-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently - VI</summary>
      <description>Next in series.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="6211" opendate="2014-1-16 00:00:00" fixdate="2014-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat job status E2E tests fail in presence of other jobs</summary>
      <description>Some job status related system tests in the WebHCat E2E testsuite fail intermittently when other MR jobs are run in the cluster running the tests.The testsuite during verification should improve to handle the above situation.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="6218" opendate="2014-1-16 00:00:00" fixdate="2014-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats for row-count not getting updated with Tez insert + dbclass=counter</summary>
      <description>Inserting data into hive with Tez, the stats on row-count is not getting updated when using the counter dbclass.To reproduce, run "ANALYZE TABLE store_sales COMPUTE STATISTICS;" with tez as the execution engine.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6222" opendate="2014-1-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Vector Group By operator abandon grouping if too many distinct keys</summary>
      <description>Row mode GBY is becoming a pass-through if not enough aggregation occurs on the map side, relying on the shuffle+reduce side to do the work. Have VGBY do the same.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6224" opendate="2014-1-17 00:00:00" fixdate="2014-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unneeded tez dependencies from hive</summary>
      <description>After re-organization of some of the classes in tez, we no longer need to depend on certain packages. Removing these from the shims and from the tests dependencies.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6226" opendate="2014-1-17 00:00:00" fixdate="2014-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It should be possible to get hadoop, hive, and pig version being used by WebHCat</summary>
      <description>Calling /version on WebHCat tells the caller the protocol verison, but there is no way to determine the versions of software being run by the applications that WebHCat spawns. I propose to add an end-point: /version/{module} where module could be pig, hive, or hadoop. The response will then be:{ "module" : _module_name_, "version" : _version_string_}</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug id="6227" opendate="2014-1-18 00:00:00" fixdate="2014-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test JOBS_7 fails</summary>
      <description>WebHCat E2E test JOBS_7 fails while verifying the job status of a TempletonControllerJob and its child pig job. The filter currently is such that only pig jobs are looked at, it should also include TempletonControllerJob.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6228" opendate="2014-1-18 00:00:00" fixdate="2014-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently - VII</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="6229" opendate="2014-1-19 00:00:00" fixdate="2014-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats are missing sometimes (regression from HIVE-5936)</summary>
      <description>if prefix length is smaller than hive.stats.key.prefix.max.length but length of prefix + postfix is bigger than that, stats are missed.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6233" opendate="2014-1-20 00:00:00" fixdate="2014-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JOBS testsuite in WebHCat E2E tests does not work correctly in secure mode</summary>
      <description>JOBS testsuite performs operations with two users test.user.name and test.other.user.name. In Kerberos secure mode it should kinit as the respective user.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6245" opendate="2014-1-21 00:00:00" fixdate="2014-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 creates DBs/Tables with wrong ownership when HMS setugi is true</summary>
      <description>The case with following settings is valid but does not work correctly in current HS2:==hive.server2.authentication=NONE (or LDAP)hive.server2.enable.doAs= truehive.metastore.sasl.enabled=falsehive.metastore.execute.setugi=true==Ideally, HS2 is able to impersonate the logged in user (from Beeline, or JDBC application) and create DBs/Tables with user's ownership.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6255" opendate="2014-1-21 00:00:00" fixdate="2014-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Hive to not pass MRSplitsProto in MRHelpers.createMRInputPayloadWithGrouping</summary>
      <description>TEZ-650 removed this superfluous parameter since splits dont need to be passed to the AM when doing split calculation on the AM. This is needed after Hive builds against TEZ 0.3.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6257" opendate="2014-1-22 00:00:00" fixdate="2014-1-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add more unit tests for high-precision Decimal128 arithmetic</summary>
      <description>Add more unit tests for high-precision Decimal128 arithmetic, with arguments close to or at 38 digit limit. Consider some random stress tests for broader coverage. Coverage is pretty good now (after HIVE-6243) for precision up to about 18. This is to go beyond that.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6258" opendate="2014-1-22 00:00:00" fixdate="2014-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - disallow cycles between roles</summary>
      <description>It should not be possible to have cycles in role relationships.If a grant role statement would end up adding such a cycle, it should result in an error.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="626" opendate="2009-7-10 00:00:00" fixdate="2009-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typecast bug in Join operator</summary>
      <description>There is a type cast error in Join operator. Produced by the following steps:create table zshao_foo (foo_id int, foo_name string, foo_a string, foo_b string,foo_c string, foo_d string) row format delimited fields terminated by ','stored as textfile;create table zshao_bar (bar_id int, bar_0 int, foo_id int, bar_1 int, bar_namestring, bar_a string, bar_b string, bar_c string, bar_d string) row formatdelimited fields terminated by ',' stored as textfile;create table zshao_count (bar_id int, n int) row format delimited fieldsterminated by ',' stored as textfile;Each table has a single row as follows:zshao_foo:1,foo1,a,b,c,dzshao_bar:10,0,1,1,bar10,a,b,c,dzshao_count:10,2load data local inpath 'zshao_foo' overwrite into table zshao_foo;load data local inpath 'zshao_bar' overwrite into table zshao_bar;load data local inpath 'zshao_count' overwrite into table zshao_count;explain extendedselect zshao_foo.foo_name, zshao_bar.bar_name, n from zshao_foo join zshao_bar on zshao_foo.foo_id =zshao_bar.foo_id join zshao_count on zshao_count.bar_id = zshao_bar.bar_id;The case is from David Lerman.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6260" opendate="2014-1-22 00:00:00" fixdate="2014-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compress plan when sending via RPC (Tez)</summary>
      <description>When trying to send plan via RPC it's helpful to compress the payload. That way more potential plans can be sent (size limit).</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="6263" opendate="2014-1-22 00:00:00" fixdate="2014-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid sending input files multiple times on Tez</summary>
      <description>Input paths can be recontructed from the plan. No need to send them in the job conf as well.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6288" opendate="2014-1-23 00:00:00" fixdate="2014-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSCK can be slow when adding partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6313" opendate="2014-1-27 00:00:00" fixdate="2014-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimr tests in hadoop-1 hangs on shutdown</summary>
      <description>It takes minutes after all tests run waiting for all task trackers shutdown. Just shutting down JobTracker after killing pending jobs seemed enough.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  <bug id="6314" opendate="2014-1-27 00:00:00" fixdate="2014-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The logging (progress reporting) is too verbose</summary>
      <description>The progress report is issued every second even when no progress have been made:2014-01-27 10:35:55,209 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 6.68 sec2014-01-27 10:35:56,678 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 6.68 sec2014-01-27 10:35:59,344 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 6.68 sec2014-01-27 10:36:01,268 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 8.67 sec2014-01-27 10:36:03,149 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 8.67 secThis pollutes the logs and the screen, and people do not appreciate it as much as the designers might have thought (How do I limit log verbosity of hive?, controlling the level of verbosity in Hive).It would be nice to be able to control the level of verbosity (but not by the -v switch!): Make sure that the progress report is only issued where there is something new to report; or Remove all the progress messages; or Make sure that progress is reported only every X sec (instead of every 1 second)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="6319" opendate="2014-1-28 00:00:00" fixdate="2014-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert, update, delete functionality needs a compactor</summary>
      <description>In order to keep the number of delta files from spiraling out of control we need a compactor to collect these delta files together, and eventually rewrite the base file when the deltas get large enough.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordIdentifier.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6325" opendate="2014-1-28 00:00:00" fixdate="2014-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable using multiple concurrent sessions in tez</summary>
      <description>We would like to enable multiple concurrent sessions in tez via hive server 2. This will enable users to make efficient use of the cluster when it has been partitioned using yarn queues.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6339" opendate="2014-1-30 00:00:00" fixdate="2014-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement new JDK7 schema management APIs in java.sql.Connection</summary>
      <description>JDK7 has added a few metadata methods in java.sql.Conntion getSchema()setSchema()getCatalog()setCatalog()Currently Hive JDBC just has stub implementation for all these methods throws unsupported exception. This needs to be fixed.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6376" opendate="2014-2-5 00:00:00" fixdate="2014-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable hive to work with tez on secure cluster</summary>
      <description>Need to pass the path objects for Tez to fetch credentials.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6378" opendate="2014-2-5 00:00:00" fixdate="2014-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatClient::createTable() doesn&amp;#39;t allow SerDe class to be specified</summary>
      <description>Recreating the HCATALOG-641 under HIVE, since HCATALOG was moved into HIVE.With respect to HCATALOG-641, a patch was originally provided (but not committed), so this work will consist of simply re-basing the original patch to the current trunk and the latest released version.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hcatalog.api.HCatCreateTableDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="6389" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.</summary>
      <description>RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.When an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:select mymap['1024'] from mytable;and if the mymap column has nulls, then one is treated to the following guttural utterance:2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"mymap":null,"isnull":null} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41) at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524) ... 10 moreA patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.This is handled correctly for Text data, and for RCFiles using ColumnarSerDe.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6392" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive (and HCatalog) don&amp;#39;t allow super-users to add partitions to tables.</summary>
      <description>HDFS allows for users to be added to a "supergroup" (identified by the "dfs.permissions.superusergroup" key in hdfs-site.xml). Users in this group are allowed to modify HDFS contents regardless of the path's ogw permissions.However, Hive's StorageBasedAuthProvider disallows such a superuser from adding partitions to any table that doesn't explicitly grant write permissions to said superuser. This causes the odd scenario where the superuser writes data to a partition-directory (under the table's path), but can't register the appropriate partition.I have a patch that brings the Metastore's behaviour in line with what the HDFS allows.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6393" opendate="2014-2-8 00:00:00" fixdate="2014-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support unqualified column references in Joining conditions</summary>
      <description>Support queries of the form:create table r1(a int);create table r2(b);select a, bfrom r1 join r2 on a = bThis becomes more useful in old style syntax:select a, bfrom r1, r2where a = b</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="640" opendate="2009-7-15 00:00:00" fixdate="2009-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add LazyBinarySerDe to Hive</summary>
      <description>LazyBinarySerDe will serialize the data in binary format while supporting LazyDeserialization.This will be used as the SerDe for value between map and reduce, and also between different map-reduce jobs.This will help improve the performance of Hive a lot.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6406" opendate="2014-2-11 00:00:00" fixdate="2014-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce immutable-table table property and if set, disallow insert-into</summary>
      <description>As part of HIVE-6405's attempt to make HCatalog and Hive behave in similar ways with regards to immutable tables, this is a companion task to introduce the notion of an immutable table, wherein all tables are not immutable by default, and have this be a table property. If this property is set for a table, and we attempt to write to a table that already has data (or a partition), disallow "INSERT INTO" into it from hive(if destination directory is non-empty). This property being set will allow hive to mimic HCatalog's current immutable-table property.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6414" opendate="2014-2-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ParquetInputFormat provides data values that do not match the object inspectors</summary>
      <description>While working on HIVE-5998 I noticed that the ParquetRecordReader returns IntWritable for all 'int like' types, in disaccord with the row object inspectors. I though fine, and I worked my way around it. But I see now that the issue trigger failuers in other places, eg. in aggregates:Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"cint":528534767,"ctinyint":31,"csmallint":4963,"cfloat":31.0,"cdouble":4963.0,"cstring1":"cvLH6Eat2yFsyy7p"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177) ... 8 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:808) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524) ... 9 moreCaused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.get(JavaShortObjectInspector.java:41) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:671) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:631) at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.merge(GenericUDAFMin.java:109) at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.iterate(GenericUDAFMin.java:96) at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:183) at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:641) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:838) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:735) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803) ... 15 moreMy test is (I'm writing a test .q from HIVE-5998, but the repro does not involve vectorization):create table if not exists alltypes_parquet ( cint int, ctinyint tinyint, csmallint smallint, cfloat float, cdouble double, cstring1 string) stored as parquet;insert overwrite table alltypes_parquet select cint, ctinyint, csmallint, cfloat, cdouble, cstring1 from alltypesorc;explain select * from alltypes_parquet limit 10; select * from alltypes_parquet limit 10;explain select ctinyint, max(cint), min(csmallint), count(cstring1), avg(cfloat), stddev_pop(cdouble) from alltypes_parquet group by ctinyint;select ctinyint, max(cint), min(csmallint), count(cstring1), avg(cfloat), stddev_pop(cdouble) from alltypes_parquet group by ctinyint;</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6415" opendate="2014-2-12 00:00:00" fixdate="2014-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow transform clause in sql std authorization mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="6416" opendate="2014-2-12 00:00:00" fixdate="2014-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized mathematical functions for decimal type.</summary>
      <description>Vectorized mathematical functions for decimal type.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.short.regress.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6417" opendate="2014-2-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - new users in admin role config should get added</summary>
      <description>if metastore is started with hive.users.in.admin.role=user1, then user1 is added admin role to metastore.If the value is changed to hive.users.in.admin.role=user2, then user2 should get added to the role in metastore. Right now, if the admin role exists, new users don't get added.A work-around is - user1 adding user2 to the admin role using grant role statement.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="642" opendate="2009-7-16 00:00:00" fixdate="2009-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>udf equivalent to string split</summary>
      <description>It would be very useful to have a function equivalent to string split in java</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6421" opendate="2014-2-12 00:00:00" fixdate="2014-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>abs() should preserve precision/scale of decimal input</summary>
      <description>hive&gt; describe dec1;OKc1 decimal(10,2) None hive&gt; explain select c1, abs(c1) from dec1; ... Select Operator expressions: c1 (type: decimal(10,2)), abs(c1) (type: decimal(38,18))Given that abs() is a GenericUDF it should be possible for the return type precision/scale to match the input precision/scale.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
    </fixedFiles>
  </bug>
  <bug id="644" opendate="2009-7-16 00:00:00" fixdate="2009-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change default size for merging files at the end of the job</summary>
      <description>Currently, the size is 1G and the reducers end up taking a really long time.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6442" opendate="2014-2-17 00:00:00" fixdate="2014-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load_dyn_part1 is flaky on Tez because it doesn&amp;#39;t have the stage re-arranger</summary>
      <description>Need to use the stage re-arranger on tez as well. That will give predictable order of the stages.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6446" opendate="2014-2-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to specify hadoop.bin.path from command line -D</summary>
      <description>the surefire plugin configures hadoop.bin.path as a system property:&lt;hadoop.bin.path&gt;${basedir}/${hive.path.to.root}/testutils/hadoop&lt;/hadoop.bin.path&gt;On Windows testing, this should be: &lt;hadoop.bin.path&gt;${basedir}/${hive.path.to.root}/testutils/hadoop.cmd&lt;/hadoop.bin.path&gt;Additionally, it would be useful to be able to specify the Hadoop CLI location from -D mvn command line.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6453" opendate="2014-2-17 00:00:00" fixdate="2014-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update TezProrcessors to work with Tez API changes (TEZ-668, TEZ-837)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6456" opendate="2014-2-18 00:00:00" fixdate="2014-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Parquet schema evolution</summary>
      <description>In HIVE-5783 we removed schema evolution:https://github.com/Parquet/parquet-mr/pull/297/files#r9824155</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
    </fixedFiles>
  </bug>
  <bug id="6457" opendate="2014-2-18 00:00:00" fixdate="2014-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure Parquet integration has good error messages for data types not supported</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6458" opendate="2014-2-18 00:00:00" fixdate="2014-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schema upgrade scripts for metastore changes related to permanent functions</summary>
      <description>Since HIVE-6330 has metastore changes, there need to be schema upgrade scripts.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.12.0-to-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.12.0-to-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.12.0-to-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.12.0-to-0.13.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="646" opendate="2009-7-16 00:00:00" fixdate="2009-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDFs for conversion between different number bases (conv, hex, bin)</summary>
      <description>Add conv, hex and bin UDFs</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6460" opendate="2014-2-18 00:00:00" fixdate="2014-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need new "show" functionality for transactions</summary>
      <description>With the addition of transactions and compactions for delta files some new "show" commands are required. "show transactions" to show currently open or aborted transactions "show compactions" to show currently waiting or running compactions "show locks" needs to work with the new db style of locks as well.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6461" opendate="2014-2-19 00:00:00" fixdate="2014-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run Release Audit tool, fix missing license issues</summary>
      <description>run mvn apache-rat:check and add apache license in flagged files.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.DataBuilder.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestCompileProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBSubQuery.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.LeadLagBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RevokePrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CompileProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FooterBuffer.java</file>
      <file type="M">pom.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveChar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  <bug id="6466" opendate="2014-2-19 00:00:00" fixdate="2014-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for pluggable authentication modules (PAM) in Hive</summary>
      <description>More on PAM in these articles:http://www.tuxradar.com/content/how-pam-workshttps://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/Pluggable_Authentication_Modules.htmlUsage from JPAM api: http://jpam.sourceforge.net/JPamUserGuide.html#id.s7.1</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.AuthenticationProviderFactory.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6472" opendate="2014-2-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC cancel will not work with current HiveServer2</summary>
      <description>Creating this JIRA to add missing pieces for query cancel capability to JDBC. HIVE-5232 should however fix the core issue. Typical use case is when the client calls Statement#execute in one thread to execute a long running query, and Statement#cancel in another thread to cancel the execution before it is complete.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6474" opendate="2014-2-20 00:00:00" fixdate="2014-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - only db owner should be allowed to create table within a db</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="6475" opendate="2014-2-20 00:00:00" fixdate="2014-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement support for appending to mutable tables in HCatalog</summary>
      <description>Part of HIVE-6405, this is the implementation of the append feature on the HCatalog side. If a table is mutable, we must support being able to append to existing data instead of erroring out as a duplicate publish.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6505" opendate="2014-2-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make stats optimizer more robust in presence of distinct clause</summary>
      <description>Currently it throws exceptions in few cases.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6508" opendate="2014-2-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mismatched results between vector and non-vector mode with decimal field</summary>
      <description>Following query has a little mismatch in result as compared to the non-vector mode.select d_year, i_brand_id, i_brand, sum(ss_ext_sales_price) as sum_aggfrom date_dimjoin store_sales on date_dim.d_date_sk = store_sales.ss_sold_date_skjoin item on store_sales.ss_item_sk = item.i_item_skwhere i_manufact_id = 128 and d_moy = 11group by d_year, i_brand, i_brand_idorder by d_year, sum_agg desc, i_brand_idlimit 100;This query is on tpcds data.The field ss_ext_sales_price is of type decimal(7,2) and everything else is an integer.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6510" opendate="2014-2-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up math based UDFs</summary>
      <description>HIVE-6327, HIVE-6246 and HIVE-6385 touched a lot of the math based UDFs. There are some code inconsistencies and warnings left. This cleans up all the problems I could find.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSqrt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRadians.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog10.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDegrees.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseBitOP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAtan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAsin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAcos.java</file>
    </fixedFiles>
  </bug>
  <bug id="6511" opendate="2014-2-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>casting from decimal to tinyint,smallint, int and bigint generates different result when vectorization is on</summary>
      <description>select dc,cast(dc as int), cast(dc as smallint),cast(dc as tinyint) from vectortab10korc limit 20 generates following result when vectorization is enabled:4619756289662.078125 -1628520834 -16770 1261553532646710.316406 -1245514442 -2762 543367942487288.360352 688127224 -776 -84386447830839.337891 1286221623 12087 55-3234165331139.458008 -54957251 27453 61-488378613475.326172 1247658269 -16099 29-493942492598.691406 -21253559 -19895 733101852523586.039062 886135874 23618 662544105595941.381836 1484956709 -23515 37-3997512403067.0625 1102149509 30597 -123-1183754978977.589355 1655994718 31070 941408783849655.676758 34576568 -26440 -72-2993175106993.426758 417098319 27215 793004723551798.100586 -1753555402 -8650 541103792083527.786133 -14511544 -28088 72469767055288.485352 1615620024 26552 -72-1263700791098.294434 -980406074 12486 -58-4244889766496.484375 -1462078048 30112 -96-3962729491139.782715 1525323068 -27332 60NULL NULL NULL NULLWhen vectorization is disabled, result looks like this:4619756289662.078125 -1628520834 -16770 1261553532646710.316406 -1245514442 -2762 543367942487288.360352 688127224 -776 -84386447830839.337891 1286221623 12087 55-3234165331139.458008 -54957251 27453 61-488378613475.326172 1247658269 -16099 29-493942492598.691406 -21253558 -19894 743101852523586.039062 886135874 23618 662544105595941.381836 1484956709 -23515 37-3997512403067.0625 1102149509 30597 -123-1183754978977.589355 1655994719 31071 951408783849655.676758 34576567 -26441 -73-2993175106993.426758 417098319 27215 793004723551798.100586 -1753555402 -8650 541103792083527.786133 -14511545 -28089 71469767055288.485352 1615620024 26552 -72-1263700791098.294434 -980406074 12486 -58-4244889766496.484375 -1462078048 30112 -96-3962729491139.782715 1525323069 -27331 61NULL NULL NULL NULLThis issue is visible only for certain decimal values. In above example, row 7,11,12, and 15 generates different results.vectortab10korc table schema:t tinyint from deserializer si smallint from deserializer i int from deserializer b bigint from deserializer f float from deserializer d double from deserializer dc decimal(38,18) from deserializer bo boolean from deserializer s string from deserializer s2 string from deserializer ts timestamp from deserializer # Detailed Table Information Database: default Owner: xyz CreateTime: Tue Feb 25 21:54:28 UTC 2014 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://host1.domain.com:8020/apps/hive/warehouse/vectortab10korc Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 10000 rawDataSize 0 totalSize 344748 transient_lastDdlTime 1393365281 # Storage Information SerDe Library: org.apache.hadoop.hive.ql.io.orc.OrcSerde InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: serialization.format 1 Time taken: 0.196 seconds, Fetched: 41 row(s</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6514" opendate="2014-2-27 00:00:00" fixdate="2014-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestExecDriver/HCat Pig tests fails with -Phadoop-2</summary>
      <description>Running TestExecDriver with -Phadoop-2 results in the error below. Looks like the test isn't able to access LocalClientProtocolProvider.java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses. at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:120) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:82) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:75) at org.apache.hadoop.mapred.JobClient.init(JobClient.java:470) at org.apache.hadoop.mapred.JobClient.&lt;init&gt;(JobClient.java:449) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:396) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:739) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'Execution failed with exit status: 1</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6519" opendate="2014-2-27 00:00:00" fixdate="2014-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow optional "as" in subquery definition</summary>
      <description>Allow both:select * from (select * from foo) bar select * from (select * from foo) as bar</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="652" opendate="2009-7-17 00:00:00" fixdate="2009-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>turn off auto-merge for insert local</summary>
      <description>auto-merge should be turned off for insert local</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6521" opendate="2014-2-28 00:00:00" fixdate="2014-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat cannot fetch correct percentComplete for Hive jobs</summary>
      <description>WebHCat E2E test TestHive_7 failed because percentComplete wasn't returned as expected.check_job_percent_complete failed. got percentComplete "map 0% reduce 0%", expected "map 100% reduce 100%"So, there are two problems here. The log parsing is broken for status of percentComplete. In the stderr of the job we see:Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1393486488858_0691, Tracking URL = http://ambari-sec-1393480847-others-2-4.cs1cloud.internal:8088/proxy/application_1393486488858_0691/Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1393486488858_0691Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02014-02-27 18:40:50,166 Stage-1 map = 0%, reduce = 0%2014-02-27 18:40:56,599 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 sec2014-02-27 18:40:57,656 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 sec2014-02-27 18:40:58,706 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 secMapReduce Total cumulative CPU time: 870 msecEnded Job = job_1393486488858_0691MapReduce Jobs Launched: Job 0: Map: 1 Cumulative CPU: 0.87 sec HDFS Read: 305 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 870 msecThe assumption in the code is that the line containing the percent status will end after "reduce = \d+%" but that fails with the above. The last status from Hive job is "map = 100%, reduce = 0%" instead of expected "map = 100%, reduce = 100%".</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6539" opendate="2014-3-3 00:00:00" fixdate="2014-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Couple of issues in fs based stats collection</summary>
      <description>While testing on cluster found couple of bugs: NPE in certain case. map object reuse causing problem</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.stats.only.null.q</file>
      <file type="M">ql.src.test.queries.clientpositive.metadata.only.queries.with.filters.q</file>
      <file type="M">ql.src.test.queries.clientpositive.metadata.only.queries.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="654" opendate="2009-7-17 00:00:00" fixdate="2009-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming the thrift SerDe</summary>
      <description>We recently moved ThriftSerDe from facebook thrift to open source thrift.However the name of the ThriftSerDe class didn't change, which makes it really hard to keep data-code compatibility.We should change the name of the ThriftSerDe as soon as possible, before our users start to use open source thrift with it.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.TReflectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ThriftDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ThriftByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.inputddl8.q</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.create.tbl1.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6543" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync is failing sometimes</summary>
      <description>NO PRECOMMIT TESTSThe test uses "CREATE TABLE NON_EXISTING_TAB (ID STRING) location 'hdfs://localhost:10000/a/b/c'" query for intended fail but it seemed not fail so quickly in testbed. Just making the query worse (replacing hdfs to invalid, etc.) would be enough.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6546" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat job submission for pig with -useHCatalog argument fails on Windows</summary>
      <description>On a one-box windows setup, do the following from a powershell prompt:cmd /c curl.exe -s ` -d user.name=hadoop ` -d arg=-useHCatalog ` -d execute="emp = load '/data/emp/emp_0.dat'; dump emp;" ` -d statusdir="/tmp/webhcat.output01" ` 'http://localhost:50111/templeton/v1/pig' -vThe job fails with error code 7, but it should run. I traced this down to the following. In the job configuration for the TempletonJobController, we have templeton.args set tocmd,/c,call,C:\\hadoop\\\\pig-0.11.0.1.3.0.0-0846/bin/pig.cmd,-D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog",-execute,"emp = load '/data/emp/emp_0.dat'; dump emp;"Notice the = sign before "-useHCatalog". I think this should be a comma.The bad string D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog" gets created in org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows().It happens at line 434: } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; // RIGHT HERE! at iterations i = 37, 38 } }Bug is here: if (prop != null) { if (prop.contains("=")) { // -D__WEBHCAT_TOKEN_FILE_LOCATION__ does not contain equal, so else branch is run and appends ="-useHCatalog", // everything good } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; } } newArgs.add(prop); }One possible fix is to change the string constant org.apache.hcatalog.templeton.tool.TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER to have an "=" sign in it. Or, preProcessForWindows() itself could be changed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6548" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing owner name and type fields in schema script for DBS table</summary>
      <description>HIVE-6386 introduced new columns in DBS table, but those are missing from schema scripts.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="6551" opendate="2014-3-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>group by after join with skew join optimization references invalid task sometimes</summary>
      <description>For example,hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.skewjoin = true;hive&gt; set hive.skewjoin.key = 3;hive&gt; &gt; EXPLAIN FROM &gt; (SELECT src.* FROM src) x &gt; JOIN &gt; (SELECT src.* FROM src) Y &gt; ON (x.key = Y.key) &gt; SELECT sum(hash(Y.key)), sum(hash(Y.value));OKSTAGE DEPENDENCIES: Stage-8 is a root stage Stage-6 depends on stages: Stage-8 Stage-5 depends on stages: Stage-6 , consists of Stage-4, Stage-2 Stage-4 Stage-2 depends on stages: Stage-4, Stage-1 Stage-0 is a root stage...Stage-2 references not-existing Stage-1</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6558" opendate="2014-3-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 Plain SASL authentication broken after hadoop 2.3 upgrade</summary>
      <description>Java only includes Plain SASL client and not server. Hence HiveServer2 includes a Plain SASL server implementation. Now Hadoop has its own Plain SASL server HADOOP-9020 which is part of Hadoop 2.3 release.The two servers use different Sasl callbacks and the servers are registered in java.security.Provider via static code. As a result the HiveServer2 instance could be using Hadoop's Plain SASL server which breaks the authentication.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="656" opendate="2009-7-20 00:00:00" fixdate="2009-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a PMOD (POSITIVE_MOD) function</summary>
      <description>There are a lot of cases people want to get a positive modulo result.For example, people want to bucket the data into 10 buckets. They use the hash code in Hive (based on Java) which can return a negative number. Then they need this POSITIVE_MOD(a, b) to return the results.Otherwise they can still do it but it will be very verbose:((hash(xxx) % 10) + 10) % 10</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6562" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protection from exceptions in ORC predicate evaluation</summary>
      <description>ORC evaluates predicate expressions to select row groups that satisfy predicate condition. There can be exceptions (mostly ClassCastException) when data types of predicate constant and min/max values are different. To avoid this patch catches any such exception and provides a default behaviour i.e; selecting the row group.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6563" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hdfs jar being pulled in when creating a hadoop-2 based hive tar ball</summary>
      <description>Looks like some dependency issue is causing hadoop-hdfs jar to be packaged in the hive tar ball.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6564" opendate="2014-3-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E tests that launch MR jobs fail on check job completion timeout</summary>
      <description>WebHCat E2E tests that fire off an MR job are not correctly being detected as complete so those tests are timing out.The problem is happening because of JSON module available through cpan which returns 1 or 0 instead of true or false.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="6566" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect union-all plan with map-joins on Tez</summary>
      <description>The tez dag is hooked up incorrectly for some union all queries involving map joins. That's quite common and results in either NPE or invalid results.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6568" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized cast of decimal to string and timestamp produces incorrect result.</summary>
      <description>A decimal value 1.23 with scale 5 is represented in string as 1.23000. This behavior is different from HiveDecimal behavior.The difference in cast to timestamp is due to more aggressive rounding in vectorized expression.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.decimal.expressions.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestUnsignedInt128.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="6571" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>query id should be available for logging during query compilation</summary>
      <description>Would be nice to have the query id set during compilation to tie logs together etc.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6575" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select * fails on parquet table with map datatype</summary>
      <description>Create parquet table with map and run select * from parquet_table, returns following exception: FAILED: RuntimeException java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector cannot be cast to org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspectorHowever select &lt;mapcol&gt; from parquet_table seems to work, and thus joins will work.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6576" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sending user.name as a form parameter in POST doesn&amp;#39;t work post HADOOP-10193</summary>
      <description>WebHCat uses AuthFilter to handle authentication. In simple mode that means using PseudoAuthenticationHandler. Prior to HADOOP-10193, the latter handled user.name as form parameter in a POST request. Now it only handles it as a query parameter. to maintain webhcat backwards compat, we need to make WebHCat still extract it from form param. This will be deprecated immediately and removed in 0.15Also, all examples in WebHCat reference manual should be updated to use user.name in query string from current form param (curl -d user.name=foo)</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
    </fixedFiles>
  </bug>
  <bug id="6578" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ORC file footer statistics through StatsProvidingRecordReader interface for analyze command</summary>
      <description>ORC provides file level statistics which can be used in analyze partialscan and noscan cases to compute basic statistics like number of rows, number of files, total file size and raw data size. On the writer side, a new interface was added earlier (StatsProvidingRecordWriter) that exposed stats when writing a table. Similarly, a new interface StatsProvidingRecordReader can be added which when implemented should provide stats that are gathered by the underlying file format.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6580" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.</summary>
      <description>Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="6585" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucket map join fails in presence of _SUCCESS file</summary>
      <description>Reason is missing path filters.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6587" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow specifying additional Hive classpath for Hadoop</summary>
      <description>Allow users to add jars to hive's Hadoop classpath without explicitly modifying their Hadoop classpath</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="6592" opendate="2014-3-8 00:00:00" fixdate="2014-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test abort when pointing to https url of webhdfs</summary>
      <description>WebHCat E2E tests when running against a ssl enabled webhdfs url fails.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="6597" opendate="2014-3-9 00:00:00" fixdate="2014-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E tests doAsTests_6 and doAsTests_7 need to be updated</summary>
      <description>Currently the following WebHCat doAsTests need to be fixed:In doAsTests_6 REST request url needs to be updated and corresponding expected output to reflect the correct intent.doAsTests_7 fails because of the strict error message checking.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.doas.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6601" opendate="2014-3-10 00:00:00" fixdate="2014-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter database commands should support schema synonym keyword</summary>
      <description>It should be possible to use "alter schema" as an alternative to "alter database". But the syntax is not currently supported.alter schema db1 set owner user x; NoViableAltException(215@[])FAILED: ParseException line 1:6 cannot recognize input near 'schema' 'db1' 'set' in alter statement</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="6605" opendate="2014-3-10 00:00:00" fixdate="2014-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not set the environment correctly when running in Tez mode</summary>
      <description>When running in Tez mode, Hive does not correctly set the java.library.path.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="6612" opendate="2014-3-11 00:00:00" fixdate="2014-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misspelling "schemaTool completeted"</summary>
      <description>There is a misspelling of "completed" as "completeted" in the last message from schematool:Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=trueMetastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriverMetastore connection User: hiveuserStarting metastore schema initialization to 0.14.0Initialization script hive-schema-0.14.0.derby.sqlInitialization script completedschemaTool completetedIt is this way even in the wiki: https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="6613" opendate="2014-3-11 00:00:00" fixdate="2014-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Control when spcific Inputs / Outputs are started</summary>
      <description>When running with Tez - a couple of enhancement are possible1) Avoid re-fetching data in case of MapJoins - since the data is likely to be cached after the first run (container re-use for the same query)2) Start Outputs only after required Inputs are ready - specifically useful in case of Reduce - where shuffle requires a large memory, and the Output (if it's a sorted output) also requires a fair amount of memory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="6618" opendate="2014-3-11 00:00:00" fixdate="2014-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>assertion when getting reference key from loader with byte-array mapjoin key</summary>
      <description>java.lang.AssertionError: Should be called after loading tables at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.processRow(MapRecordProcessor.java:205) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:171) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:152)This is because tables may have already been loaded.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="6625" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 running in http mode should support trusted proxy access</summary>
      <description>HIVE-5155 adds trusted proxy access to HiveServer2. This patch a minor change to have it used when running HiveServer2 in http mode. Patch to be applied on top of HIVE-4764 &amp; HIVE-5155.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="6630" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FS based stats collection have issues for list bucketing case</summary>
      <description>We need not to track per directory stats in FS based stats collection mechanism, which other stats collection mechanism need to do.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6633" opendate="2014-3-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pig -useHCatalog with embedded metastore fails to pass command line args to metastore</summary>
      <description>This fails because the embedded metastore can't connect to the database because the command line -D arguments passed to pig are not getting passed to the metastore when the embedded metastore is created. Using hive.metastore.uris set to the empty string causes creation of an embedded metastore.pig -useHCatalog "-Dhive.metastore.uris=" "-Djavax.jdo.option.ConnectionPassword=AzureSQLDBXYZ"The goal is to allow a pig job submitted via WebHCat to specify a metastore to use via job arguments. That is not working because it is not possible to pass Djavax.jdo.option.ConnectionPassword and other necessary arguments to the embedded metastore.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.PigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.HCatLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="6635" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Heartbeats are not being sent when DbLockMgr is used and an operation holds locks</summary>
      <description>The new DbLockManager depends on heartbeats from the client in order to determine that a lock has not timed out. The client is not currently sending those heartbeats.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6636" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>/user/hive is a bad default for HDFS jars path for Tez</summary>
      <description>If user runs hive under the user name that is not "hive", jobs will fail until everyone is granted write access to /user/hive, which is not nice.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="664" opendate="2009-7-21 00:00:00" fixdate="2009-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize UDF split</summary>
      <description>Min Zhou added a comment - 21/Jul/09 07:34 AMIt's very useful for us .some comments: 1. Can you implement it directly with Text ? Avoiding string decoding and encoding would be faster. Of course that trick may lead to another problem, as String.split uses a regular expression for splitting. 2. getDisplayString() always return a string in lowercase.[ Show » ]Min Zhou added a comment - 21/Jul/09 07:34 AM It's very useful for us . some comments: 1. Can you implement it directly with Text ? Avoiding string decoding and encoding would be faster. Of course that trick may lead to another problem, as String.split uses a regular expression for splitting. 2. getDisplayString() always return a string in lowercase.[ Permlink | « Hide ]Namit Jain added a comment - 21/Jul/09 09:22 AMCommitted. Thanks Emil[ Show » ]Namit Jain added a comment - 21/Jul/09 09:22 AM Committed. Thanks Emil[ Permlink | « Hide ]Emil Ibrishimov added a comment - 21/Jul/09 10:48 AMThere are some easy (compromise) ways to optimize split:1. Check if the regex argument actually contains some "regex specific characters" and if it doesn't, do a straightforward split without converting to strings.2. Assume some default value for the second argument (for example - split(str) to be equivalent to split(str, ' ') and optimize for this value3. Have two separate split functions - one that does regex and one that splits around plain text.I think that 1 is a good choice and can be done rather quickly.[ Show » ]Emil Ibrishimov added a comment - 21/Jul/09 10:48 AM There are some easy (compromise) ways to optimize split: 1. Check if the regex argument actually contains some "regex specific characters" and if it doesn't, do a straightforward split without converting to strings. 2. Assume some default value for the second argument (for example - split(str) to be equivalent to split(str, ' ') and optimize for this value 3. Have two separate split functions - one that does regex and one that splits around plain text. I think that 1 is a good choice and can be done rather quickly.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="6641" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimized HashMap keys won&amp;#39;t work correctly with decimals</summary>
      <description>Decimal values with can be equal while having different byte representations (different precision/scale), so comparing bytes is not enough. For a quick fix, we can disable this for decimals</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6647" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump the thrift api version to V7 for HiveServer2</summary>
      <description>HIVE-5155 added new api for delegation token support. Per the convention followed till now, we should update the version to 7. Marking it as blocker for 13. cc prasadm thejas</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6648" opendate="2014-3-13 00:00:00" fixdate="2014-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permissions are not inherited correctly when tables have multiple partition columns</summary>
      <description>Warehouse.mkdirs() always looks at the immediate parent of the path that it creates when determining what permissions to inherit. However, it may have created that parent directory as well, in which case it will have the default permissions and will not have inherited them.This is a problem when performing an INSERT into a table with more than one partition column. E.g., in an empty table:INSERT INTO TABLE tbl PARTITION(p1=1, p2=2) ... A new subdirectory /p1=1/p2=2 will be created, and with permission inheritance (per HIVE-2504) enabled, the intention is presumably for both new directories to inherit the root table dir's permissions. However, mkdirs() will only set the permission of the leaf directory (i.e. /p2=2/), and then only to the permissions of /p1=1/, which was just created.public boolean mkdirs(Path f) throws MetaException { FileSystem fs = null; try { fs = getFs(f); LOG.debug("Creating directory if it doesn't exist: " + f); //Check if the directory already exists. We want to change the permission //to that of the parent directory only for newly created directories. if (this.inheritPerms) { try { return fs.getFileStatus(f).isDir(); } catch (FileNotFoundException ignore) { } } boolean success = fs.mkdirs(f); if (this.inheritPerms &amp;&amp; success) { // Set the permission of parent directory. // HNR: This is the bug - getParent() may refer to a just-created directory. fs.setPermission(f, fs.getFileStatus(f.getParent()).getPermission()); } return success; } catch (IOException e) { closeFs(fs); MetaStoreUtils.logAndThrowMetaException(e); } return false; }</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="665" opendate="2009-7-21 00:00:00" fixdate="2009-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to query hadoop/mapreduce cluster status from hive server</summary>
      <description>Tools/infra around hadoop/hive need to check cluster status in many cases.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.src.gen-py.hive.service.ttypes.py</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive.py</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive-remote</file>
      <file type="M">service.src.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen-php.hive.service.types.php</file>
      <file type="M">service.src.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">service.if.hive.service.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6653" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test JOBS_7 and JOBS_9 fail as profile.url in job details is being returned as null</summary>
      <description>the 2 tests should not be checking profile.url property in the returned JSON since 'url' comes from org.apache.hadoop.mapred.JobProfile class which is marked LimitedPrivate</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6656" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in ORC Timestamp reader returns wrong nanoseconds</summary>
      <description>ORC timestamp writer stores the number of trailing zeros in 3 LSB bits. There is a bug in parsing nanosecond logic that returns incorrect value.Input:1999-01-01 00:00:00.999999999Output: 1999-01-01 00:00:00.463129087The fix for this is parseNanos() should first right shift by 3 and then typecast to int.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6657" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test coverage for Kerberos authentication implementation using Hadoop&amp;#39;s miniKdc</summary>
      <description>Hadoop 2.3 includes miniKdc module. This provides a KDC that can be used by downstream projects to implement unit tests for Kerberos authentication code.Hive has lot of code related to Kerberos and delegation token for authentication, as well as accessing secure hadoop resources. This pretty much has no coverage in the unit tests. We needs to add unit tests using miniKdc module.Note that Hadoop 2.3 doesn't include a secure mini-cluster. Until that is available, we can at least test authentication for components like HiveServer2, Metastore and WebHCat.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6660" opendate="2014-3-14 00:00:00" fixdate="2014-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 running in non-http mode closes server socket for an SSL connection after the 1st request</summary>
      <description>Beeline connection string:!connect jdbc:hive2://&lt;host&gt;:10000/;ssl=true;sslTrustStore=/usr/share/doc/hive-0.13.0.2.1.1.0/examples/files/truststore.jks;trustStorePassword=HiveJdbc vgumashta vgumashta org.apache.hive.jdbc.HiveDriver Error:pool-7-thread-1, handling exception: java.net.SocketTimeoutException: Read timed outpool-7-thread-1, called close()pool-7-thread-1, called closeInternal(true)pool-7-thread-1, SEND TLSv1 ALERT: warning, description = close_notifyPadded plaintext before ENCRYPTION: len = 320000: 01 00 BE 72 AC 10 3B FA 4E 01 A5 DE 9B 14 16 AF ...r..;.N.......0010: 4E DD 7A 29 AD B4 09 09 09 09 09 09 09 09 09 09 N.z)............pool-7-thread-1, WRITE: TLSv1 Alert, length = 32[Raw write]: length = 370000: 15 03 01 00 20 6C 37 82 A8 52 40 DA FB 83 2D CD .... l7..R@...-.0010: 96 9F F0 B7 22 17 E1 04 C1 D1 93 1B C4 39 5A B0 ...."........9Z.0020: A2 3F 5D 7D 2D .?].-pool-7-thread-1, called closeSocket(selfInitiated)pool-7-thread-1, called close()pool-7-thread-1, called closeInternal(true)pool-7-thread-1, called close()pool-7-thread-1, called closeInternal(true)Subsequent queries fail:main, WRITE: TLSv1 Application Data, length = 144main, handling exception: java.net.SocketException: Broken pipe%% Invalidated: [Session-1, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]main, SEND TLSv1 ALERT: fatal, description = unexpected_messagePadded plaintext before ENCRYPTION: len = 320000: 02 0A 52 C3 18 B1 C1 38 DB 3F B6 D1 C5 CA 14 9C ..R....8.?......0010: A5 38 4C 01 31 69 09 09 09 09 09 09 09 09 09 09 .8L.1i..........main, WRITE: TLSv1 Alert, length = 32main, Exception sending alert: java.net.SocketException: Broken pipemain, called closeSocket()Error: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (state=08S01,code=0)java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:226) at org.apache.hive.beeline.Commands.execute(Commands.java:736) at org.apache.hive.beeline.Commands.sql(Commands.java:657) at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:796) at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:659) at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:368) at org.apache.hive.beeline.BeeLine.main(BeeLine.java:351) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:161) at org.apache.thrift.transport.TSaslTransport.flush(TSaslTransport.java:471) at org.apache.thrift.transport.TSaslClientTransport.flush(TSaslClientTransport.java:37) at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:65) at org.apache.hive.service.cli.thrift.TCLIService$Client.send_ExecuteStatement(TCLIService.java:219) at org.apache.hive.service.cli.thrift.TCLIService$Client.ExecuteStatement(TCLIService.java:211) at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:220) ... 11 moreCaused by: java.net.SocketException: Broken pipe at java.net.SocketOutputStream.socketWrite0(Native Method) at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) at java.net.SocketOutputStream.write(SocketOutputStream.java:153) at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:377) at sun.security.ssl.OutputRecord.write(OutputRecord.java:363) at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:830) at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:801) at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:122) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:159) ... 17 moreWorks fine however in http mode using ssl.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6661" opendate="2014-3-14 00:00:00" fixdate="2014-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test TestPig_10 fails (Hadoop 2)</summary>
      <description>enablelog=true is only supported with Hadoop 1. Need to add a flag to skip the test with Hadoop 2</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6666" opendate="2014-3-14 00:00:00" fixdate="2014-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore init scripts should always populate the version information at the end</summary>
      <description>The metastore schema create scripts for 0.13 and 0.14 (current trunk) has multiple other operations after setting the schema version. This is problematic as any failure in those later operations would leave metastore in inconsistent state, and yet with valid version information. The schemaTool depends on the schema version details.Recording the schema version should be the last step in schema initialization script.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.14.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.14.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.14.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="6676" opendate="2014-3-14 00:00:00" fixdate="2014-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcat cli fails to run when running with hive on tez</summary>
      <description>HIVE_CLASSPATH should be added to HADOOP_CLASSPATH before launching hcat CLI</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.bin.hcat</file>
    </fixedFiles>
  </bug>
  <bug id="6686" opendate="2014-3-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat does not honour -Dlog4j.configuration=$WEBHCAT_LOG4J of log4j.properties file on local filesystem.</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.bin.webhcat.server.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6694" opendate="2014-3-18 00:00:00" fixdate="2014-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should provide a way to execute shell command as Hive CLI does</summary>
      <description>Hive CLI allows a user to execute a shell command using ! notation. For instance, !cat myfile.txt. Being able to execute shell command may be important for some users. As a replacement, however, Beeline provides no such capability, possibly because ! notation is reserved for SQLLine commands. It's possible to provide this using a slightly syntactic variation such as !sh cat myfilie.txt.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="670" opendate="2009-7-22 00:00:00" fixdate="2009-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain should show output column names</summary>
      <description>Explain currently only shows the expressions (in which it references the output column names of the last operator).However, it does not show the output column names of the last operator, which makes it hard to debug.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.groupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.selectDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6700" opendate="2014-3-19 00:00:00" fixdate="2014-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In some queries inputs are closed on Tez before the operator pipeline is flushed</summary>
      <description>Group by operators won't flush their last row until operator is closed. In Tez it's possible that the input is already closed at this point.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6701" opendate="2014-3-19 00:00:00" fixdate="2014-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze table compute statistics for decimal columns.</summary>
      <description>Analyze table should compute statistics for decimal columns as well.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="6703" opendate="2014-3-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez should store SHA of the jar when uploading to cache</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6710" opendate="2014-3-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlocks seen in transaction handler using mysql</summary>
      <description>When multiple clients attempt to obtain locks a deadlock on the mysql database occasionally occurs.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6711" opendate="2014-3-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC maps uses getMapSize() from MapOI which is unreliable</summary>
      <description>HIVE-6707 had issues with map size. getMapSize() of LazyMap and LazyBinaryMap does not deserialize the keys and count the number of unique keys. Since getMapSize() may return non-distinct count of keys, the length of maps stored using ORC's map tree writer will not be in sync with actual map size. As a result of this RLE reader will try to read beyond the disk range expecting more map entries and will throw exception.Stack trace will look like:Caused by: java.io.EOFException: Read past end of RLE integer from compressed stream Stream for column 2 kind DATA position: 22059699 length: 22059699 range: 0 offset: 22359014 limit: 22359014 range 0 = 0 to 22059699 uncompressed: 53370 to 53370 at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54) at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:301) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringDictionaryTreeReader.next(RecordReaderImpl.java:1572) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringTreeReader.next(RecordReaderImpl.java:1330) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$MapTreeReader.next(RecordReaderImpl.java:2041) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1772) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2963) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:121)</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6728" opendate="2014-3-21 00:00:00" fixdate="2014-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing file override-container-log4j.properties in Hcatalog</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6732" opendate="2014-3-24 00:00:00" fixdate="2014-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Notes for Hive 0.13</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6733" opendate="2014-3-24 00:00:00" fixdate="2014-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Driver context logs every query in the "warn" level</summary>
      <description>Trivial, just lower the log level on one statement.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="6734" opendate="2014-3-24 00:00:00" fixdate="2014-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL locking too course grained in new db txn manager</summary>
      <description>All DDL operations currently acquire an exclusive lock. This is too course grained, as some operations like alter table add partition shouldn't get an exclusive lock on the entire table.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6735" opendate="2014-3-24 00:00:00" fixdate="2014-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make scalable dynamic partitioning work in vectorized mode</summary>
      <description>HIVE-6455 added support for scalable dynamic partitioning. This is subtask to make HIVE-6455 work with vectorized operators.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6738" opendate="2014-3-25 00:00:00" fixdate="2014-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 secure Thrift/HTTP needs to accept doAs parameter from proxying intermediary</summary>
      <description>See already implemented JIra https://issues.apache.org/jira/browse/HIVE-5155Support secure proxy user access to HiveServer2That fix expects the hive.server2.proxy.user parameter to come in Thrift body.When an intermediary gateway like Apache Knox is authenticating the end client and then proxying the request to HiveServer2, it is not practical for the intermediary like Apache Knox to modify thrift content.Intermediary like Apache Knox should be able to assert doAs in a query parameter. This paradigm is already established by other Hadoop ecosystem components like WebHDFS, WebHCat, Oozie and HBase and Hive needs to be aligned with them.The doAs asserted in query parameter should override if doAs specified in Thrift body.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="6739" opendate="2014-3-25 00:00:00" fixdate="2014-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive HBase query fails on Tez due to missing jars and then due to NPE in getSplits</summary>
      <description>Tez paths in Hive never call configure on the input/output operators, so (among other things, potentially) requisite files never get added to the job</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UnionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="674" opendate="2009-7-22 00:00:00" fixdate="2009-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add UDFs sin/cos</summary>
      <description>Add UDFs for sin/cos</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6744" opendate="2014-3-25 00:00:00" fixdate="2014-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permanent UDF lookup fails when current DB has uppercase letters</summary>
      <description>If defaulting to current DB/schema name for resolving UDF name, the DB name should be lowercased before doing the UDF lookup.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6749" opendate="2014-3-26 00:00:00" fixdate="2014-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn hive.auto.convert.join.use.nonstaged off by default</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sortmerge.mapjoin.mismatch.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6752" opendate="2014-3-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Between and IN expressions don&amp;#39;t work with decimal, date types.</summary>
      <description>Vectorized Between and IN expressions don't work with decimal, date types.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6757" opendate="2014-3-26 00:00:00" fixdate="2014-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated parquet classes from outside of org.apache package</summary>
      <description>Apache shouldn't release projects with files outside of the org.apache namespace.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.parquet.hive.serde.ParquetHiveSerDe.java</file>
      <file type="M">ql.src.java.parquet.hive.MapredParquetOutputFormat.java</file>
      <file type="M">ql.src.java.parquet.hive.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.parquet.hive.DeprecatedParquetOutputFormat.java</file>
      <file type="M">ql.src.java.parquet.hive.DeprecatedParquetInputFormat.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.12.0-to-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.018-HIVE-6757.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.12.0-to-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.018-HIVE-6757.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.12.0-to-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.018-HIVE-6757.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.12.0-to-0.13.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.018-HIVE-6757.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="6760" opendate="2014-3-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scalable dynamic partitioning should bail out properly for list bucketing</summary>
      <description>In case of list bucketing HIVE-6455 looks only at this config "hive.optimize.listbucketing" to bail out. There are cases when this config ("hive.optimize.listbucketing") is not set but list bucketing is enabled using SKEWED BY in CREATE TABLE statement.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6761" opendate="2014-3-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hashcode computation does not use maximum parallelism for scalable dynamic partitioning</summary>
      <description>Hashcode computation for HIVE-6455 should consider all the partitioning columns and bucket number to distribute the rows. The following code for (int i = 0; i &lt; partitionEval.length - 1; i++) {ignores the last partition column thereby generating lesser hashcodes.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6763" opendate="2014-3-27 00:00:00" fixdate="2014-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in http mode might send same kerberos client ticket in case of concurrent requests resulting in server throwing a replay exception</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpKerberosRequestInterceptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6766" opendate="2014-3-27 00:00:00" fixdate="2014-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatLoader always returns Char datatype with maxlength(255) when table format is ORC</summary>
      <description>attached patch containsorg.apache.hive.hcatalog.pig.TestOrcHCatPigStorer#testWriteChar()which shows that char(5) value written to Hive (ORC) table using HCatStorer will come back as char(255) when read with HCatLoader.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InternalUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6767" opendate="2014-3-27 00:00:00" fixdate="2014-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for hadoop-2</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="677" opendate="2009-7-23 00:00:00" fixdate="2009-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>better debugging</summary>
      <description>Currently, we have no idea how many rows are being produced by join/file sink while the process is running.It makes the tasks very difficult to debug - it would be very useful to dump some stats while the process (mapper/reducer) is running</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6771" opendate="2014-3-28 00:00:00" fixdate="2014-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update WebHCat E2E tests now that comments is reported correctly in "describe table" output</summary>
      <description>HIVE-6681 corrected the comments in the describe table output, earlier it would show "from deserializer" in comments.Some WebHCat E2E tests are checking for the string "from deserializer" even overshadowing the actual comments.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.ddl.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6780" opendate="2014-3-29 00:00:00" fixdate="2014-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set tez credential file property along with MR conf property for Tez jobs</summary>
      <description>webhcat should set the additional property - "tez.credentials.path" to the same value as the MapReduce property.WebHCat should always proactively set this tez.credentials.path property to the same value and in the same cases where it is setting the MR equivalent property.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6781" opendate="2014-3-29 00:00:00" fixdate="2014-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC in http mode is using HiveConf - should be removed</summary>
      <description>This change is needed so that in unsecured mode, the jdbc driver does not depend on HiveConf which is derived from hadoop's Configuration class, continue being a thin client.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6782" opendate="2014-3-29 00:00:00" fixdate="2014-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2Concurrency issue when running with tez intermittently, throwing "org.apache.tez.dag.api.SessionNotRunning: Application not running" error</summary>
      <description>HiveServer2 concurrency is failing intermittently when using tez, throwing "org.apache.tez.dag.api.SessionNotRunning: Application not running" error</description>
      <version>None</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6783" opendate="2014-3-29 00:00:00" fixdate="2014-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible schema for maps between parquet-hive and parquet-pig</summary>
      <description>see also in following parquet issue:https://github.com/Parquet/parquet-mr/issues/290The schema written for maps isn't compatible between hive and pig. This means any files written in one cannot be properly read in the other.More specifically, for the same map column c1, parquet-pig generates schema:message pig_schema { optional group c1 (MAP) { repeated group map (MAP_KEY_VALUE) { required binary key (UTF8); optional binary value; } }}while parquet-hive generates schema:message hive_schema { optional group c1 (MAP_KEY_VALUE) { repeated group map { required binary key; optional binary value; } }}</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="6788" opendate="2014-3-29 00:00:00" fixdate="2014-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Abandoned opened transactions not being timed out</summary>
      <description>If a client abandons an open transaction it is never closed. This does not cause any immediate problems (as locks are timed out) but it will eventually lead to high levels of open transactions in the lists that readers need to be aware of when reading tables or partitions.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="679" opendate="2009-7-23 00:00:00" fixdate="2009-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate JDBC driver with SQuirrelSQL for querying</summary>
      <description>Implement the JDBC methods required to support querying and other basic commands using the SQuirrelSQL tool.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6796" opendate="2014-3-31 00:00:00" fixdate="2014-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create/drop roles is case-sensitive whereas &amp;#39;set role&amp;#39; is case insensitive</summary>
      <description>Create/drop role operations should be case insensitive.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.roles.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.set.show.current.role.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.admin.almighty1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.revoke.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.grant.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.grant.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.create.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.role.no.admin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RoleDDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PrincipalDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6797" opendate="2014-3-31 00:00:00" fixdate="2014-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add protection against divide by zero in stats annotation</summary>
      <description>In stats annotation, the denominator computation in join operator is not protected for divide by zero exception. It will be an issue when NDV (count distinct) updated by updateStats() becomes 0. This patch adds protection in updateStats() method to prevent divide-by-zero in downstream operators.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6800" opendate="2014-3-31 00:00:00" fixdate="2014-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 is not passing proxy user setting through hive-site</summary>
      <description>Setting the following in core-site.xml works fine in a secure cluster with hive.server2.allow.user.substitution set to true:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.user1.groups&lt;/name&gt; &lt;value&gt;users&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.user1.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;where user1 will be proxying for user2:!connect jdbc:hive2:/myhostname:10000/;principal=hive/_HOST@EXAMPLE.COM;hive.server2.proxy.user=user2 user1 fakepwd org.apache.hive.jdbc.HiveDriverHowever, setting this in hive-site.xml throws "Failed to validate proxy privilage" exception.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
    </fixedFiles>
  </bug>
  <bug id="6816" opendate="2014-4-2 00:00:00" fixdate="2014-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jar upload path w/o schema is not handled correctly</summary>
      <description>java.io.IOException: java.net.URISyntaxException: Expected scheme name at index 0: :///user/sershe/hive-exec-0.14.0-SNAPSHOT-5a31b3483b29ad46db705a47893898b2b9f5b7ce3c65f0641bbecca2b1201d81.jar at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:304) at org.apache.tez.client.TezSession.submitDAG(TezSession.java:202) at org.apache.tez.client.TezSession.submitDAG(TezSession.java:154) at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:294) at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:147) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1473) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1240) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1058) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:885) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:875) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:687) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: java.net.URISyntaxException: Expected scheme name at index 0: :///user/sershe/hive-exec-0.14.0-SNAPSHOT-5a31b3483b29ad46db705a47893898b2b9f5b7ce3c65f0641bbecca2b1201d81.jar at java.net.URI$Parser.fail(URI.java:2829) at java.net.URI$Parser.failExpecting(URI.java:2835) at java.net.URI$Parser.parse(URI.java:3027) at java.net.URI.&lt;init&gt;(URI.java:753) at org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL(ConverterUtils.java:80) at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:296) ... 22 more</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6817" opendate="2014-4-2 00:00:00" fixdate="2014-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some hadoop2-only tests need diffs to be updated</summary>
      <description>expected output needs updating due to pre/post hook messages from the authorization changes</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.recursive.dir.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6818" opendate="2014-4-2 00:00:00" fixdate="2014-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Array out of bounds when ORC is used with ACID and predicate push down</summary>
      <description>The users gets an ArrayOutOfBoundsException when using ORC, ACID, and predicate push down.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgument.java</file>
    </fixedFiles>
  </bug>
  <bug id="6823" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - database authorization does not check for role ownership</summary>
      <description>A role can own the database, but when the authorization checks are determining the privileges for a user, they are not checking if one of the roles the user belongs to is an owner of the database.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6825" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>custom jars for Hive query should be uploaded to scratch dir per query; and/or versioned</summary>
      <description>Currently the jars are uploaded to either user directory or global, whatever is configured, which is a mess and can cause collisions. We can upload to scratch directory, and/or version. There's a tradeoff between having to upload files every time (for example, for commonly used things like HBase input format) (which is what is done now, into global/user path), and having a mess of one-off custom jars and files, versioned, sitting in .hiveJars.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6826" opendate="2014-4-3 00:00:00" fixdate="2014-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-tez has issues when different partitions work off of different input types</summary>
      <description>create table test (key int, value string) partitioned by (p int) stored as textfile;insert into table test partition (p=1) select * from src limit 10;alter table test set fileformat orc;insert into table test partition (p=2) select * from src limit 10;describe test;select * from test where p=1 and key &gt; 0;select * from test where p=2 and key &gt; 0;select * from test where key &gt; 0;throws a classcast exception</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6828" opendate="2014-4-3 00:00:00" fixdate="2014-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive tez bucket map join conversion interferes with map join conversion</summary>
      <description>The issue is that bucket count is used for checking the scaled down size of the hash tables but is used later on to convert to the map join as well which may be incorrect in cases where the entire hash table does not fit in the specified size.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="6830" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>After major compaction unable to read from partition with MR job</summary>
      <description>After doing a major compaction any attempt to do read the data with an MR job (select count, subsequent compaction) fails with:Caused by: java.lang.IllegalArgumentException: All base directories were ignored, such as hdfs://hdp.example.com:8020/apps/hive/warehouse/purchaselog/ds=201404031016/base_0044000 by 50000:4086:...</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6834" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition optimization bails out after removing file sink operator</summary>
      <description>HIVE-6455 introduced scalable dynamic partitioning optimization that bails out after removing the file sink operator. This causes union_remove_16.q test to fail by removing all the stages in the plan.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6836" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade parquet to 1.4.0</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6837" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 thrift/http mode &amp; binary mode proxy user check fails reporting IP null for client</summary>
      <description>Hive Server running thrift/http with Kerberos security.Kinited user knox attempting to proxy as sam.Beeline connection failed reporting error on hive server logs:Caused by: org.apache.hadoop.security.authorize.AuthorizationException: Unauthorized connection for super-user: knox from IP null</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6840" opendate="2014-4-4 00:00:00" fixdate="2014-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Unordered Output for Bucket Map Joins on Tez</summary>
      <description>Tez 0.4 adds a placeholder UnorderedOutput. Once Hive is changed to use 0.4, it should be possible to make use of this.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6841" opendate="2014-4-4 00:00:00" fixdate="2014-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized execution throws NPE for partitioning columns with __HIVE_DEFAULT_PARTITION__</summary>
      <description>If partitioning columns have _HIVE_DEFAULT_PARTITION_ or null, vectorized execution throws NPE.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="6849" opendate="2014-4-5 00:00:00" fixdate="2014-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden files update for hadoop-2</summary>
      <description>More hadoop-2 related golden files update.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6860" opendate="2014-4-8 00:00:00" fixdate="2014-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Issue with FS based stats collection on Tez</summary>
      <description>Statistics from different tasks got overwritten while running on Tez.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6861" opendate="2014-4-8 00:00:00" fixdate="2014-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>more hadoop2 only golden files to fix</summary>
      <description>More hadoop2 golden files to fix due to HIVE-6643, HIVE-6642, HIVE-6808, HIVE-6144.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.remove.18.q</file>
    </fixedFiles>
  </bug>
  <bug id="6863" opendate="2014-4-8 00:00:00" fixdate="2014-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 binary mode throws exception with PAM</summary>
      <description>Works fine in http mode</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6864" opendate="2014-4-8 00:00:00" fixdate="2014-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 concurrency uses incorrect user information in unsecured mode</summary>
      <description>Concurrent queries create table with wrong ownership</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="6869" opendate="2014-4-8 00:00:00" fixdate="2014-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for tez tests.</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.counter.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.counter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6871" opendate="2014-4-9 00:00:00" fixdate="2014-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build fixes to allow Windows to run TestCliDriver</summary>
      <description>Some of the Java properties have been changed or set differently due to the Mavenization of the Hive build, and it looks like this is causing some issues with the Windows unit tests.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.WindowsPathUtil.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6873" opendate="2014-4-9 00:00:00" fixdate="2014-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DISTINCT clause in aggregates is handled incorrectly by vectorized execution</summary>
      <description>The vectorized aggregates ignore the DISTINCT clause. This cause incorrect results. Due to how GroupByOperatorDesc adds the DISTINCT keys to the overall aggregate keys the vectorized aggregates do account for the extra key, but they do not process the data correctly for the key. the reduce side the aggregates the input from the vectorized map side to results that are only sometimes correct but mostly incorrect. HIVE-4607 tracks the proper fix, but meantime I'm filing a bug to disable vectorized execution if DISTINCT is present. Fix is trivial.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6876" opendate="2014-4-9 00:00:00" fixdate="2014-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logging information should include thread id</summary>
      <description>The multi-threaded nature of hive server and remote metastore makes it difficult to debug issues without enabling thread information. It would be nice to have the thread id in the logs.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.hive-exec-log4j.properties</file>
      <file type="M">common.src.main.resources.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="6880" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHWISessionManager fails with -Phadoop-2</summary>
      <description>Looks like dependencies missing for -Phadoop-2Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec &lt;&lt;&lt; FAILURE! - in org.apache.hadoop.hive.hwi.TestHWISessionManagerwarning(junit.framework.TestSuite$1) Time elapsed: 0.009 sec &lt;&lt;&lt; FAILURE!junit.framework.AssertionFailedError: Exception in constructor: testHiveDriver (java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/TaskAttemptContext at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:171) at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:120) at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:115) at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:80) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&lt;clinit&gt;(HiveConf.java:248) at org.apache.hadoop.hive.conf.HiveConf.&lt;clinit&gt;(HiveConf.java:81) at org.apache.hadoop.hive.hwi.TestHWISessionManager.&lt;init&gt;(TestHWISessionManager.java:46) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at junit.framework.TestSuite.createTest(TestSuite.java:65) at junit.framework.TestSuite.addTestMethod(TestSuite.java:294) at junit.framework.TestSuite.addTestsFromTestCase(TestSuite.java:150) at junit.framework.TestSuite.&lt;init&gt;(TestSuite.java:129) at org.junit.internal.runners.JUnit38ClassRunner.&lt;init&gt;(JUnit38ClassRunner.java:71) at org.junit.internal.builders.JUnit3Builder.runnerForClass(JUnit3Builder.java:14) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.TaskAttemptContext at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ... 28 more) at junit.framework.Assert.fail(Assert.java:50) at junit.framework.TestSuite$1.runTest(TestSuite.java:97)</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6881" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres Upgrade script for hive 0.13 is broken</summary>
      <description>The script added for HIVE-6757 didn't quote the identifiers</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.018-HIVE-6757.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="6882" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make upgrade script schemaTool friendly</summary>
      <description>Current scripts work fine when invoked manually, but fails when invoked via schemaTool.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.015-HIVE-5700.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.015-HIVE-5700.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug id="6888" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive leaks MapWork objects via Utilities::gWorkMap</summary>
      <description>When running multiple queries with hive on a single Application Master, we found that hive leaks a large number of MapWork objects which accumulate in the AM</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="689" opendate="2009-7-24 00:00:00" fixdate="2009-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>dump more memory stat periodically</summary>
      <description>Usually we see out of memory errors for all kinds of reasons - it is very difficult to track what was the reason for that.We should dump more statistics at that point - currently, we dont even know whether we died because the current process was bad orwas it due to some other process on that machine</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6898" opendate="2014-4-11 00:00:00" fixdate="2014-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Functions in hive are failing with java.lang.ClassNotFoundException on Tez</summary>
      <description>CREATE TABLE T1(key int, val STRING) STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;add jar /tmp/testudf.jar;create temporary function square as 'org.apache.hive.udf.UDFSquare';select square(key) from T1 limit 3;Fails with Vertex failed, vertexName=Map 1, vertexId=vertex_1397230190905_0590_1_00, diagnostics=[Task failed, taskId=task_1397230190905_0590_1_00_000000, diagnostics=[AttemptID:attempt_1397230190905_0590_1_00_000000_0 Info:Error: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:163) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307) at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557) at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hive.udf.UDFSquare at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:133) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1636) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isDeterministic(FunctionRegistry.java:1599) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic(ExprNodeGenericFuncEvaluator.java:132) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluatorFactory.java:83) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEval(ExprNodeEvaluatorFactory.java:73) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:59) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121) ... 7 more</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6903" opendate="2014-4-14 00:00:00" fixdate="2014-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of hive.metastore.execute.setugi to true</summary>
      <description>Since its introduction in HIVE-2616 I havent seen any bug reported for it, only grief from users who expect system to work as if this is true by default.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6907" opendate="2014-4-14 00:00:00" fixdate="2014-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 - wrong user gets used for metastore operation with embedded metastore</summary>
      <description>When queries are being run concurrently against HS2, sometimes the wrong user ends performing the metastore action and you get an error like - ..INFO|java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: action WRITE not permitted on path hdfs://example.net:8020/apps/hive/warehouse/tbl_4eeulg9zp4 for user hrt_qa)</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="6909" opendate="2014-4-15 00:00:00" fixdate="2014-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Note for Hive 0.13 RC1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="691" opendate="2009-7-26 00:00:00" fixdate="2009-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>show documentation for user defined functions</summary>
      <description>There are lot of functions and more of them will be added. I think it is cumbersome to update wiki or ask users to go to wiki to find out documentation. instead each user defined function can have an annotation containing documentation and 'show functions' command or similar can show the documentation to users on command line. it will also be easier to update the documentation along with code.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSqrt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSpace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFReverse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRepeat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExpReplace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPosMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFParseUrl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPPositive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPNot.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitXor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitNot.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStdSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVarianceSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAcos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAscii.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAsin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCeil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateDiff.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog10.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLpad.java</file>
    </fixedFiles>
  </bug>
  <bug id="6916" opendate="2014-4-15 00:00:00" fixdate="2014-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export/import inherit permissions from parent directory</summary>
      <description>Export table into an external location and importing into hive, should set the table to have the permission of the parent directory, if the flag hive.warehouse.subdir.inherit.perms is set.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestFolderPermissions.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6917" opendate="2014-4-15 00:00:00" fixdate="2014-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Notes for Hive 0.13 RC2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6919" opendate="2014-4-16 00:00:00" fixdate="2014-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive sql std auth select query fails on partitioned tables</summary>
      <description>analyze table studentparttab30k partition (ds) compute statistics;Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied. Principal [name=hadoopqa, type=USER] does not have following privileges on Object [type=PARTITION, name=null] : [SELECT] (state=42000,code=40000)Sql std auth is supposed to ignore partition level objects for privilege checks, but that is not working as intended.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6921" opendate="2014-4-16 00:00:00" fixdate="2014-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>index creation fails with sql std auth turned on</summary>
      <description></description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6927" opendate="2014-4-17 00:00:00" fixdate="2014-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for MSSQL in schematool</summary>
      <description>Schematool is the preferred way of initializing schema for Hive. Since HIVE-6862 provided the script for MSSQL it would be nice to add the support for it in schematool.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="6928" opendate="2014-4-17 00:00:00" fixdate="2014-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should not chop off "describe extended" results by default</summary>
      <description>By default, beeline truncates long results based on the console width like:+-----------------------------+----------------------------------------------------------------------------------------------------------------------+| col_name | |+-----------------------------+----------------------------------------------------------------------------------------------------------------------+| pat_id | string || score | float || acutes | float || | || Detailed Table Information | Table(tableName:refills, dbName:default, owner:hdadmin, createTime:1393882396, lastAccessTime:0, retention:0, sd:Sto |+-----------------------------+----------------------------------------------------------------------------------------------------------------------+5 rows selected (0.4 seconds)This can be changed by !outputformat, but the default should behave better to give a better experience to the first-time beeline user.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="693" opendate="2009-7-27 00:00:00" fixdate="2009-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a AWS S3 log format deserializer</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6938" opendate="2014-4-21 00:00:00" fixdate="2014-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Support for Parquet Column Rename</summary>
      <description>Parquet was originally introduced without 'replace columns' support in ql. In addition, the default behavior for parquet is to access columns by name as opposed to by index by the Serde. Parquet should allow for either columnar (index based) access or name based access because it can support either.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6945" opendate="2014-4-21 00:00:00" fixdate="2014-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>issues with dropping partitions on Oracle</summary>
      <description>1) Direct SQL is broken on Oracle due to the usage of NUMBER type which is translated by DN into decimal rather than long. This appears to be specific to some cases because it seemed to have worked before (different version of Oracle? JDBC? DN? Maybe depends on whether db was auto-created).2) When partition dropping code falls back to JDO, it creates objects to return, then drops partitions. It appears that dropping makes DN objects invalid. We create metastore partition objects out of DN objects before drop, however the list of partition column values is re-used, rather than copied, into these. DN appears to clear this list during drop, so the returned object becomes invalid and the exception is thrown.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="6957" opendate="2014-4-22 00:00:00" fixdate="2014-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL authorization does not work with HS2 binary mode and Kerberos auth</summary>
      <description>In HiveServer2, when Kerberos auth and binary transport modes are used, the user name that gets passed on to authorization is the long kerberos username.The username that is used in grant/revoke statements tend to be the short usernames.This also fails in authorizing statements that involve URI, as the authorization mode checks the file system permissions for given user. It does not recognize that the given long username actually owns the file or belongs to the group that owns the file.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="6966" opendate="2014-4-23 00:00:00" fixdate="2014-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More fixes for TestCliDriver on Windows</summary>
      <description>This prevents the Test*CliDriver tests from compiling properly.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6967" opendate="2014-4-23 00:00:00" fixdate="2014-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive transaction manager fails when SQLServer is used as an RDBMS</summary>
      <description>When using SQLServer as an RDBMS for the metastore, any transaction or DbLockMgr operations fail with:MetaException(message:Unable to select from transaction database com.microsoft.sqlserver.jdbc.SQLServerException: Line 1: FOR UPDATE clause allowed only for DECLARE CURSOR.The issue is that SQLServer does not support the FOR UPDATE clause in SELECT.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6985" opendate="2014-4-29 00:00:00" fixdate="2014-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - privileges grants to public role not being honored</summary>
      <description>When a privilege is granted to public role, the privilege is supposed to be applicable for all users.However, the privilege check fails for users, even if the have public role in the list of current roles.Note that the issue is only with public role. Grant of privileges of other role are not affected.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="70" opendate="2008-11-18 00:00:00" fixdate="2008-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>provide option to limit standard error output by user scripts</summary>
      <description>runaway user scripts emitting every row to standard error overwhelm our log partitions. We need to provide an option to limit standard error size per task.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="700" opendate="2009-7-28 00:00:00" fixdate="2009-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test error by adding "DROP FUNCTION"</summary>
      <description>Since we added "Show Functions" in HIVE-580, test results will depend on what temporary functions are added to the system.We should add the capability of "DROP FUNCTION", and do that at the end of those "create function" tests to make sure the "show functions" results are deterministic.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.udaf.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.genericudf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FunctionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7000" opendate="2014-5-1 00:00:00" fixdate="2014-8-1 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Several issues with javadoc generation</summary>
      <description>1.Ran 'mvn javadoc:javadoc -Phadoop-2'. Encountered several issues Generated classes are included in the javadoc generation fails in the top level hcatalog folder because its src folder contains no java files.Patch attached to fix these issues.2.Tried mvn javadoc:aggregate -Phadoop-2 cannot get an aggregated javadoc for all of hive tried setting 'aggregate' parameter to true. Didn't workThere are several questions in StackOverflow about multiple project javadoc. Seems like this is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7001" opendate="2014-5-1 00:00:00" fixdate="2014-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fs.permissions.umask-mode is getting unset when Session is started</summary>
      <description>hive&gt; set fs.permissions.umask-mode;fs.permissions.umask-mode=022hive&gt; show tables;OKt1Time taken: 0.301 seconds, Fetched: 1 row(s)hive&gt; set fs.permissions.umask-mode;fs.permissions.umask-mode is undefined</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="7004" opendate="2014-5-1 00:00:00" fixdate="2014-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix more unit test failures on hadoop-2</summary>
      <description>Still a number of precommit failures with hadoop-2, will try to fix some of them.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.map.skew.q</file>
      <file type="M">ql.src.test.queries.clientnegative.stats.partialscan.autogether.q</file>
      <file type="M">ql.src.test.queries.clientnegative.dynamic.partitions.with.whitelist.q</file>
    </fixedFiles>
  </bug>
  <bug id="7005" opendate="2014-5-2 00:00:00" fixdate="2014-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniTez tests have non-deterministic explain plans</summary>
      <description>TestMiniTezCliDriver has a few test failures where there is a diff in the explain plan generated. According to Vikram, the plan generated is correct, but the plan can be generated in a couple of different ways and so sometimes the plan will not diff against the expected output. We should probably come up with a way to validate this explain plan in a reproducible way.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="7008" opendate="2014-5-2 00:00:00" fixdate="2014-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean-up some old dead code</summary>
      <description>There is some code to workaround limitations in historic Hadoop (hadoop-17 &amp; earlier). Lets get rid of those.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestFlatFileInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.RecordTestObj.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.JavaTestObjFlatFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.FlatFileInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="701" opendate="2009-7-28 00:00:00" fixdate="2009-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>lots of reserved keywords in hive</summary>
      <description>There is a problem if we want to use some reserved keywords:for example, creating a function of name left/right ? left/right is already a reserved keyword.The other way around should also be possible - if we want to add a 'show tables status' and some applications already use status as a column name, they should not break</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.archive.partspec3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.show.tables.bad2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.show.tables.bad1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.build.xml</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7017" opendate="2014-5-6 00:00:00" fixdate="2014-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insertion into Parquet tables fails under Tez</summary>
      <description>It seems Parquet tables cannot be written to in Tez mode. CREATE TABLE foo STORED AS PARQUET SELECT ... queries fail with: java.lang.IllegalArgumentException: TaskAttemptId string : task1396892688715_80817_m_000076_3 is not properly formed at org.apache.hadoop.mapreduce.TaskAttemptID.forName(TaskAttemptID.java:201) at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.&lt;init&gt;(ParquetRecordWriterWrapper.java:49)The same queries work fine after setting hive.execution.engine=mr.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="702" opendate="2009-7-29 00:00:00" fixdate="2009-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DROP TEMPORARY FUNCTION should not drop builtin functions</summary>
      <description>Only temporary functions should be dropped. It should error out if the user tries to drop built-in functions.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7024" opendate="2014-5-7 00:00:00" fixdate="2014-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Escape control characters for explain result</summary>
      <description>Comments for columns are now delimited by 0x00, which is binary and make git refuse to make proper diff file.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="7027" opendate="2014-5-7 00:00:00" fixdate="2014-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive job fails when referencing a view that explodes an array</summary>
      <description>For a table created with following DDLCREATE TABLE test_issue (fileid int, infos ARRAY&lt;STRUCT&lt;user:INT&gt;&gt;, test_c STRUCT&lt;user_c:STRUCT&lt;age:INT&gt;&gt;), create a view that lateral view explodes the array column likeCREATE VIEW v_test_issue AS SELECT fileid, i.user, test_c.user_c.age FROM test_issue LATERAL VIEW explode(infos) info AS i; Querying the view such as:SELECT * FROM v_test_issue WHERE age = 25; Will failed with following errors:java.lang.Exception: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)Caused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:695)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 11 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34) ... 16 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 19 moreCaused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:154) ... 24 moreCaused by: java.lang.RuntimeException: cannot find field test_c from [0:_col0, 1:_col5] at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:415) at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:150) at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55) at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:53) at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:53) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:934) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:960) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401) at org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.initializeOp(LateralViewJoinOperator.java:109) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401) at org.apache.hadoop.hive.ql.exec.UDTFOperator.initializeOp(UDTFOperator.java:94) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:401) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:83) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:83) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416) at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:424) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133) ... 24 more</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7033" opendate="2014-5-7 00:00:00" fixdate="2014-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>grant statements should check if the role exists</summary>
      <description>The following grant statement that grants to a role that does not exist succeeds, but it should result in an error.&gt; grant all on t1 to role nosuchrole;</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.role.grant2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.role.grant1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.1.sql.std.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="7035" opendate="2014-5-8 00:00:00" fixdate="2014-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Templeton returns 500 for user errors - when job cannot be found</summary>
      <description>curl -i 'http://localhost:50111/templeton/v1/jobs/job_1399496111138_00011?user.name=ekoifman' should return HTTP Status code 4xx when no such job exists; it currently returns 500.{"error":"org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_201304291205_0015' doesn't exist in RM.\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:247)\r\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:120)\r\n\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:241)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2053)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)\r\n"}NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="7037" opendate="2014-5-8 00:00:00" fixdate="2014-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional tests for transform clauses with Tez</summary>
      <description>Enabling some q tests for Tez wrt to ScriptOperator/Stream/Transform.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7043" opendate="2014-5-9 00:00:00" fixdate="2014-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When using the tez session pool via hive, once sessions time out, all queries go to the default queue</summary>
      <description>When using a tez session pool to run multiple queries, once the sessions time out, we always end up using the default queue to launch queries. The load balancing doesn't work in this case.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="7062" opendate="2014-5-14 00:00:00" fixdate="2014-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Streaming mode in Windowing</summary>
      <description>1. Have the Windowing Table Function support streaming mode.2. Have special handling for Ranking UDAFs.3. Have special handling for Sum/Avg for fixed size Wdws.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7063" opendate="2014-5-14 00:00:00" fixdate="2014-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize for the Top N within a Group use case</summary>
      <description>It is common to rank within a Group/Partition and then only return the Top N entries within each Group.With Streaming mode for Windowing, we should push the post filter on the rank into the Windowing processing as a Limit expression.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7065" opendate="2014-5-14 00:00:00" fixdate="2014-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive jobs in webhcat run in default mr mode even in Hive on Tez setup</summary>
      <description>WebHCat config has templeton.hive.properties to specify Hive config properties that need to be passed to Hive client on node executing a job submitted through WebHCat (hive query, for example).this should include "hive.execution.engine"</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.config.hive.hive-site.xml</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="7066" opendate="2014-5-14 00:00:00" fixdate="2014-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar is missing avro core</summary>
      <description>Running a simple query that reads an Avro table caused the following exception to be thrown on the cluster side:java.lang.RuntimeException: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:365) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:276) at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:254) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:445) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:438) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587) at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:191) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:412) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:394) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190) at org.apache.hadoop.mapred.Child.main(Child.java:249)Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:942) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:850) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:864) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:334) ... 13 moreCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:26) at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:343) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:336) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:56) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:476) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:148) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113) ... 25 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:32) ... 37 moreCaused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumWriter at java.lang.Class.getDeclaredFields0(Native Method) at java.lang.Class.privateGetDeclaredFields(Class.java:2348) at java.lang.Class.getDeclaredFields(Class.java:1779) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.&lt;init&gt;(FieldSerializer.java:109) ... 42 moreCaused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumWriter at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ... 47 moreI took a look at the hive-exec jar and found that the Avro core jar was not included, though avro-mapred is included.I confirmed that Avro core was included in the Hive 0.12 hive-exec jar. Was there a reason why this was removed in trunk? It seems that this would break the AvroSerDe.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7067" opendate="2014-5-15 00:00:00" fixdate="2014-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Min() and Max() on Timestamp and Date columns for ORC returns wrong results</summary>
      <description>min() and max() of timestamp and date columns of ORC table returns wrong results. The reason for that is when ORC creates object inspectors for date and timestamp it uses JAVA primitive objects as opposed to WRITABLE objects. When get() is performed on java primitive objects, a reference to the underlying object is returned whereas when get() is performed on writable objects, a copy of the underlying object is returned. Fix is to change the object inspector creation to return writable objects for timestamp and date.</description>
      <version>None</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7068" opendate="2014-5-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate AccumuloStorageHandler</summary>
      <description>Accumulo is a BigTable-clone which is similar to HBase. Some initial work has been done to support querying an Accumulo table using Hive already. It is not a complete solution as, most notably, the current implementation presently lacks support for INSERTs.I would like to polish up the AccumuloStorageHandler (presently based on 0.10), implement missing basic functionality and compare it to the HBaseStorageHandler (to ensure that we follow the same general usage patterns).I've also been in communication with bfem (the initial author) who expressed interest in working on this again. I hope to coordinate efforts with him.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7080" opendate="2014-5-16 00:00:00" fixdate="2014-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In PTest framework, Add logs URL to the JIRA comment</summary>
      <description>Enhancement request to add the logs url to the JIRA report. Currently it contains URL to the console output and jenkins reports only.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="7092" opendate="2014-5-20 00:00:00" fixdate="2014-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite should not delete the original directory</summary>
      <description>Today the implementation of "insert overwrite" table or partition deletes the entire directory of the table/partition recursively using the HDFS shell (-rmr) and then re-creates it.This makes it get rid of certain user-set attributes of the directory, like permission, owner, group, and will become more important with introduction of HDFS extended-ACL's.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestFolderPermissions.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7099" opendate="2014-5-20 00:00:00" fixdate="2014-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Decimal datatype support for Windowing</summary>
      <description>Decimal datatype is not handled by Windowing</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="711" opendate="2009-7-30 00:00:00" fixdate="2009-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check that GROUP BY works for negative double values</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7111" opendate="2014-5-22 00:00:00" fixdate="2014-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend join transitivity PPD to non-column expressions</summary>
      <description>Join transitive in PPD only supports column expressions, but it's possible to extend this to generic expressions.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7116" opendate="2014-5-22 00:00:00" fixdate="2014-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HDFS FileSystem object cache causes permission issues in creating tmp directories</summary>
      <description>We change permissions of the directory creation to 777 for HiveServer 2 operation and it turns out that because of HDFS caching, it does not reflect once created. We need to use the non-cached version of the API to get a FileSystem object to fix this.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="7117" opendate="2014-5-23 00:00:00" fixdate="2014-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitions not inheriting table permissions after alter rename partition</summary>
      <description>On altering/renaming a partition it must inherit permission of the parent directory, if the flag hive.warehouse.subdir.inherit.perms is set.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7126" opendate="2014-5-27 00:00:00" fixdate="2014-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup build warnings while building hive projects</summary>
      <description>Currently while doing a build of hive projects, warnings like the following show up:mac-swarnim:hive sk018283$ mvn clean install -pl serde -P hadoop-1[INFO] Scanning for projects...[WARNING] [WARNING] Some problems were encountered while building the effective model for org.apache.hive.shims:hive-shims-0.23:jar:0.14.0-SNAPSHOT[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdfs:jar -&gt; duplicate declaration of version ${hadoop-23.version} @ line 110, column 17[WARNING] [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.[WARNING] [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.[WARNING]These warnings should be cleaned up.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7136" opendate="2014-5-28 00:00:00" fixdate="2014-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Hive to read hive scripts from any of the supported file systems in hadoop eco-system</summary>
      <description>Current hive cli assumes that the source file (hive script) is always on the local file system. This patch implements support for reading source files from other file systems in hadoop eco-system (hdfs, s3 etc) as well keeping the default behavior intact to be reading from default filesystem (local) in case scheme is not provided in the url for the source file.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7140" opendate="2014-5-28 00:00:00" fixdate="2014-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump default hive.metastore.client.socket.timeout to 5 minutes</summary>
      <description>The issue is that OOTB clients often face timeouts when using HMS since many operations in the HMS completes are long running (e.g. many operations on a table with many partitions). A few supporting pieces of information: The default value of hive.metastore.client.socket.timeout is 20 seconds. Since the timeout is client only, the server happy continues doing the requested work Clients retry after a small delay to perform the requested work again, often while the server is still trying to complete the original request A few tests have actually increased this value in order to pass reliably.</description>
      <version>0.10.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7143" opendate="2014-5-29 00:00:00" fixdate="2014-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming support in Windowing mode for more UDAFs (min/max, lead/lag, fval/lval)</summary>
      <description>Provided implementations for Streaming for the above fns.Min/Max based on Alg by Daniel Lemire: http://www.archipel.uqam.ca/309/1/webmaximinalgo.pdf</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEnhancer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
    </fixedFiles>
  </bug>
  <bug id="7153" opendate="2014-5-30 00:00:00" fixdate="2014-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveStreaming - Bug in TransactionBatch.toString() method</summary>
      <description>The TransactionBatchImpl.toString() method currently returns :return "TxnIds=[" + txnIds.get(0) + "src/gen/thrift" + txnIds.get(txnIds.size()-1) + "] on endPoint= " + endPt;The "src/gen/thrift" there is a typo and needs to replaced with "..."</description>
      <version>0.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="7155" opendate="2014-5-31 00:00:00" fixdate="2014-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat controller job exceeds container memory limit</summary>
      <description>Submit a Hive query on a large table via WebHCat results in failure because the WebHCat controller job is killed by Yarn since it exceeds the memory limit (set by mapreduce.map.memory.mb, defaults to 1GB): INSERT OVERWRITE TABLE Temp_InjusticeEvents_2014_03_01_00_00 SELECT * from Stage_InjusticeEvents where LogTimestamp &gt; '2014-03-01 00:00:00' and LogTimestamp &lt;= '2014-03-01 01:00:00';We could increase mapreduce.map.memory.mb to solve this problem, but this way we are changing this setting system wise.We need to provide a WebHCat configuration to overwrite mapreduce.map.memory.mb when submitting the controller job.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7169" opendate="2014-6-2 00:00:00" fixdate="2014-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in Http Mode should have a configurable IdleMaxTime timeout</summary>
      <description>Currently, in HiveServer2 we use Jetty Server to start the Http Server. The connector used for this Thrift Http Cli Service has maximum idle time as the default timeout as mentioned in http://grepcode.com/file/repo1.maven.org/maven2/org.eclipse.jetty/jetty-server/7.0.0.v20091005/org/eclipse/jetty/server/AbstractConnector.java#AbstractConnector.0_maxIdleTime.This should be manually configurable using connector.setMaxIdleTime(maxIdleTime);</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7175" opendate="2014-6-4 00:00:00" fixdate="2014-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide password file option to beeline</summary>
      <description>For people connecting to Hive Server 2 with LDAP authentication enabled, in order to batch run commands, we currently have to provide the password openly in the command line. They could use some expect scripting, but I think a valid improvement would be to provide a password file option similar to other CLI commands in hadoop (e.g. sqoop) to be more secure.</description>
      <version>0.13.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="7190" opendate="2014-6-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat launcher task failure can cause two concurent user jobs to run</summary>
      <description>Templeton uses launcher jobs to launch the actual user jobs. Launcher jobs are 1-map jobs (a single task jobs) which kick off the actual user job and monitor it until it finishes. Given that the launcher is a task, like any other MR task, it has a retry policy in case it fails (due to a task crash, tasktracker/nodemanager crash, machine level outage, etc.). Further, when launcher task is retried, it will again launch the same user job, however the previous attempt user job is already running. What this means is that we can have two identical user jobs running in parallel. In case of MRv2, there will be an MRAppMaster and the launcher task, which are subject to failure. In case any of the two fails, another instance of a user job will be launched again in parallel. Above situation is already a bug.Now going further to RM HA, what RM does on failover/restart is that it kills all containers, and it restarts all applications. This means that if our customer had 10 jobs on the cluster (this is 10 launcher jobs and 10 user jobs), on RM failover, all 20 jobs will be restarted, and launcher jobs will queue user jobs again. There are two issues with this design:1. There are possible chances for corruption of job outputs (it would be useful to analyze this scenario more and confirm this statement).2. Cluster resources are spent on jobs redundantlyTo address the issue at least on Yarn (Hadoop 2.0) clusters, webhcat should do the same thing Oozie does in this scenario, and that is to tag all its child jobs with an id, and kill those jobs on task restart before they are kicked off again.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim20S.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SqoopDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7192" opendate="2014-6-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Streaming - Some required settings are not mentioned in the documentation</summary>
      <description>Specifically: hive.support.concurrency on metastore hive.vectorized.execution.enabled for query client</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package.html</file>
    </fixedFiles>
  </bug>
  <bug id="7200" opendate="2014-6-9 00:00:00" fixdate="2014-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline output displays column heading even if --showHeader=false is set</summary>
      <description>A few minor/cosmetic issues with the beeline CLI.1) Tool prints the column headers despite setting the --showHeader to false. This property only seems to affect the subsequent header information that gets printed based on the value of property "headerInterval" (default value is 100).2) When "showHeader" is true &amp; "headerInterval &gt; 0", the header after the first interval gets printed after &lt;headerInterval - 1&gt; rows. The code seems to count the initial header as a row, if you will.3) The table footer(the line that closes the table) does not get printed if the "showHeader" is false. I think the table should get closed irrespective of whether it prints the header or not.0: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val |+------+| t || f || T || F || 0 || 1 |+------+6 rows selected (3.998 seconds)0: jdbc:hive2://localhost:10000&gt; !set headerInterval 20: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val |+------+| t |+------+| val |+------+| f || T |+------+| val |+------+| F || 0 |+------+| val |+------+| 1 |+------+6 rows selected (0.691 seconds)0: jdbc:hive2://localhost:10000&gt; !set showHeader false0: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val |+------+| t || f || T || F || 0 || 1 |6 rows selected (1.728 seconds)</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="7224" opendate="2014-6-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set incremental printing to true by default in Beeline</summary>
      <description>See HIVE-7221.By default beeline tries to buffer the entire output relation before printing it on stdout. This can cause OOM when the output relation is large. However, beeline has the option of incremental prints. We should keep that as the default.</description>
      <version>0.13.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="7226" opendate="2014-6-12 00:00:00" fixdate="2014-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windowing Streaming mode causes NPE for empty partitions</summary>
      <description>Change in HIVE-7062 doesn't handle empty partitions properly. StreamingState is not correctly initialized for empty partition</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7230" opendate="2014-6-13 00:00:00" fixdate="2014-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Eclipse formatter file for Hive coding conventions</summary>
      <description>Eclipse's formatter is a convenient way to clean up formatting for Java code. Currently, there is no Eclipse formatter file checked into Hive's codebase.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7237" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.exec.parallel=true w/ Hive 0.13/Tez causes application to linger forever</summary>
      <description>set hive.exec.parallel=true; will cause the Yarn application instance to lingerforever. set hive.exec.parallel=false, the application goes away as soon as hive query is complete. The underlying table is an ORC store_sales table compressed with SNAPPY.hive.exec.parallel=true;select * from store_sales where ss_ticket_number=5741230 and ss_item_sk=4825The query will run under Tez and finish &lt;&lt; 30 seconds.After 30-40 of these jobs the cluster gets to a point where no jobs will finish.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="7241" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong lock acquired for alter table rename partition</summary>
      <description>Doing an "alter table foo partition (bar='x') rename to partition (bar='y')" acquires a read lock on table foo. It should instead acquire an exclusive lock on partition bar=x.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7242" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table drop partition is acquiring the wrong type of lock</summary>
      <description>Doing an "alter table foo drop partition ('bar=x')" acquired a shared-write lock on partition bar=x. It should be acquiring an exclusive lock in that case.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7246" opendate="2014-6-17 00:00:00" fixdate="2014-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive transaction manager hardwires bonecp as the JDBC pooling implementation</summary>
      <description>Currently TxnManager hardwires BoneCP as the JDBC connection pooling implementation. Instead it should use the same connection pooling that the metastore does.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">hcatalog.streaming.src.test.sit</file>
    </fixedFiles>
  </bug>
  <bug id="7255" opendate="2014-6-19 00:00:00" fixdate="2014-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow partial partition spec in analyze command</summary>
      <description>So that stats collection can happen for multiple partitions through one statement.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.invalid.values.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.incorrect.num.keys.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.incorrect.num.keys.q</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.dp.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="727" opendate="2009-8-6 00:00:00" fixdate="2009-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Server getSchema() returns wrong schema for "Explain" queries</summary>
      <description>The Hive Server's getSchema() function will return a schema with zero fields when executing an "Explain..." query. A quick fix might be to set the default schema to be exactly one column of type string.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7280" opendate="2014-6-24 00:00:00" fixdate="2014-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO V1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveSwapJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushJoinThroughJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveTableScanRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveSortRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.CostBasedOptimizer.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7283" opendate="2014-6-24 00:00:00" fixdate="2014-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: plumb in HepPlanner and FieldTrimmer(ColumnPruner) into Optiq based planning</summary>
      <description>1.HepPlanner initially used for: Predicate Pushdown Transitive Predicate inference Partition Pruning2. Use Optiq's FieldTrimmer for ColumnPrunerTo begin with the rules are copies of Optiq base rules. Once Optiq is refactored to work on Base RelNode classes, the copied rules will be removed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="7298" opendate="2014-6-26 00:00:00" fixdate="2014-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>desc database extended does not show properties of the database</summary>
      <description>HIVE-6386 added owner information to desc, but not updated schema of it.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.database.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.db.owner.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
    </fixedFiles>
  </bug>
  <bug id="7299" opendate="2014-6-26 00:00:00" fixdate="2014-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable metadata only optimization on Tez</summary>
      <description>Enables the metadata only optimization (the one with OneNullRowInputFormat not the query-result-from-stats optimizaton)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="73" opendate="2008-11-19 00:00:00" fixdate="2008-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift Server and Client for Hive</summary>
      <description>Currently the hive cli directly calls the driver code. We need to be able to run a stand alone hive server that multiple clients can connect to. The hive server will allow clients to run queries as well as make meta data calls (by inheriting from the thrift metastore server)</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.input19.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.tableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="7314" opendate="2014-6-30 00:00:00" fixdate="2014-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results of UDF when hive.cache.expr.evaluation is set</summary>
      <description>It seems that the expression caching doesn't work when using UDF inside another UDF or a hive function.For example :tbl has one row : 'a','b'The following query : select concat(custUDF(a),' ', custUDF(b)) from tbl; returns 'a a'seems to cache custUDF(a) and use it for custUDF(b).Same query without the concat works fine.Replacing the concat with another custom UDF also returns 'a a'</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug id="732" opendate="2009-8-6 00:00:00" fixdate="2009-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store intermediate data in binary using LazyBinarySerDe</summary>
      <description>Follow-up on HIVE-640. We should use LazyBinarySerDe in several places in the code to improve the efficiency: value between map-reduce boundary, temporary tables.We should also allow users to create tables stored as binary format.CREATE TABLE xxx (...)ROW FORMAT BINARYSTORED AS SEQUENCEFILE;</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7323" opendate="2014-7-1 00:00:00" fixdate="2014-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type stats in ORC sometimes go stale</summary>
      <description>I cannot make proper test case but sometimes min/max value in date type stats is changed in runtime. Stats for other type contains non-mutable values in it but date type stats contains DateWritable, which of inner value can be changed anytime.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="734" opendate="2009-8-6 00:00:00" fixdate="2009-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>problem in dealing with null</summary>
      <description>This command fails with the following error:hive/bin/hive -e "INSERT OVERWRITE LOCAL DIRECTORY 'abc' select null from zshao_tt"FAILED: Error in semantic analysis:java.lang.RuntimeException: Internal error: Cannot find ObjectInspector for VOIDWhen 'null' is replaced by '' it works.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7346" opendate="2014-7-3 00:00:00" fixdate="2014-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results caused by hive ppd under specific join condition</summary>
      <description>Assuming two tables : t1(id1 string,id2 string) , t2 (id string,d int) t1 contains 1 row : 'a','a't2 contains 1 row : 'a',2The following query : select a.*,b.d d1,c.d d2from t1 a join t2 b on (a.id1=b.id)join t2 c on (a.id2=b.id)where b.d &lt;=1 and c.d&lt;=1 Returns 0 rows as expected because t2.d = 2Wrapping this query, like so : select * from (select a.*,b.d d1,c.d d2from t1 a join t2 b on (a.id1=b.id)join t2 c on (a.id2=b.id)where b.d &lt;=1 and c.d&lt;=1) z where d1&gt;1 or d2&gt;1 Where another filter was add on the columns causes the plan to lack the filter of the "&lt;=1" and return a single row - Wrong Results.The plan is : ABSTRACT SYNTAX TREE: (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME t1) a) (TOK_TABREF (TOK_TABNAME t2) b) (= (. (TOK_TABLE_OR_COL a) id1) (. (TOK_TABLE_OR_COL b) id))) (TOK_TABREF (TOK_TABNAME t2) c) (= (. (TOK_TABLE_OR_COL a) id2) (. (TOK_TABLE_OR_COL b) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) d) d1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) d) d2)) (TOK_WHERE (and (&lt;= (. (TOK_TABLE_OR_COL b) d) 1) (&lt;= (. (TOK_TABLE_OR_COL c) d) 1))))) z)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (&gt; (TOK_TABLE_OR_COL d1) 1) (&gt; (TOK_TABLE_OR_COL d2) 1)))))STAGE DEPENDENCIES: Stage-7 is a root stage Stage-5 depends on stages: Stage-7 Stage-0 is a root stageSTAGE PLANS: Stage: Stage-7 Map Reduce Local Work Alias -&gt; Map Local Tables: z:b Fetch Operator limit: -1 z:c Fetch Operator limit: -1 Alias -&gt; Map Local Operator Tree: z:b TableScan alias: b HashTable Sink Operator condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] Position of Big Table: 0 z:c TableScan alias: c HashTable Sink Operator condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] Position of Big Table: 0 Stage: Stage-5 Map Reduce Alias -&gt; Map Operator Tree: z:a TableScan alias: a Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] outputColumnNames: _col0, _col1, _col4, _col5 Position of Big Table: 0 Filter Operator predicate: expr: (_col1 = _col4) type: boolean Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] outputColumnNames: _col1, _col4, _col5, _col9 Position of Big Table: 0 Filter Operator predicate: expr: ((_col1 &gt; 1) or (_col9 &gt; 1)) type: boolean Select Operator expressions: expr: _col4 type: string expr: _col5 type: string expr: _col1 type: int expr: _col9 type: int outputColumnNames: _col0, _col1, _col2, _col3 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Local Work: Map Reduce Local Work Stage: Stage-0 Fetch Operator limit: -1Setting : hive.optimize.ppd=false Results in the following correct plan : ABSTRACT SYNTAX TREE: (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME t1) a) (TOK_TABREF (TOK_TABNAME t2) b) (= (. (TOK_TABLE_OR_COL a) id1) (. (TOK_TABLE_OR_COL b) id))) (TOK_TABREF (TOK_TABNAME t2) c) (= (. (TOK_TABLE_OR_COL a) id2) (. (TOK_TABLE_OR_COL b) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) d) d1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) d) d2)) (TOK_WHERE (and (&lt;= (. (TOK_TABLE_OR_COL b) d) 1) (&lt;= (. (TOK_TABLE_OR_COL c) d) 1))))) z)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (&gt; (TOK_TABLE_OR_COL d1) 1) (&gt; (TOK_TABLE_OR_COL d2) 1)))))STAGE DEPENDENCIES: Stage-7 is a root stage Stage-5 depends on stages: Stage-7 Stage-0 is a root stageSTAGE PLANS: Stage: Stage-7 Map Reduce Local Work Alias -&gt; Map Local Tables: z:b Fetch Operator limit: -1 z:c Fetch Operator limit: -1 Alias -&gt; Map Local Operator Tree: z:b TableScan alias: b HashTable Sink Operator condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] Position of Big Table: 0 z:c TableScan alias: c HashTable Sink Operator condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] Position of Big Table: 0 Stage: Stage-5 Map Reduce Alias -&gt; Map Operator Tree: z:a TableScan alias: a Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] outputColumnNames: _col0, _col1, _col4, _col5 Position of Big Table: 0 Filter Operator predicate: expr: (_col1 = _col4) type: boolean Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] outputColumnNames: _col1, _col4, _col5, _col9 Position of Big Table: 0 Filter Operator predicate: expr: ((_col1 &lt;= 1) and (_col9 &lt;= 1)) type: boolean Select Operator expressions: expr: _col4 type: string expr: _col5 type: string expr: _col1 type: int expr: _col9 type: int outputColumnNames: _col0, _col1, _col2, _col3 Filter Operator predicate: expr: ((_col2 &gt; 1) or (_col3 &gt; 1)) type: boolean Select Operator expressions: expr: _col0 type: string expr: _col1 type: string expr: _col2 type: int expr: _col3 type: int outputColumnNames: _col0, _col1, _col2, _col3 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Local Work: Map Reduce Local Work Stage: Stage-0 Fetch Operator limit: -1</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="7352" opendate="2014-7-7 00:00:00" fixdate="2014-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries without tables fail under Tez</summary>
      <description>Hive 0.13.0 added support for queries that do not reference tables (such as 'SELECT 1'). These queries fail under Tez:Vertex failed as one or more tasks failed. failedTasks:1]14/07/07 09:54:42 ERROR tez.TezJobMonitor: Vertex failed, vertexName=Map 1, vertexId=vertex_1404652697071_4487_1_00, diagnostics=[Task failed, taskId=task_1404652697071_4487_1_00_000000, diagnostics=[AttemptID:attempt_1404652697071_4487_1_00_000000_0 Info:Error: java.lang.RuntimeException: java.lang.IllegalArgumentException: Can not create a Path from an empty string at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:174) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.&lt;init&gt;(TezGroupedSplitsInputFormat.java:113) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:79) at org.apache.tez.mapreduce.input.MRInput.setupOldRecordReader(MRInput.java:205) at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:362) at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:341) at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:99) at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:68) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:141) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307) at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127) at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:135) at org.apache.hadoop.hive.ql.io.HiveInputFormat$HiveInputSplit.getPath(HiveInputFormat.java:110) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:228) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:171) ... 14 more</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7353" opendate="2014-7-7 00:00:00" fixdate="2014-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 using embedded MetaStore leaks JDOPersistanceManager</summary>
      <description>While using embedded metastore, while creating background threads to run async operations, HiveServer2 ends up creating new instances of JDOPersistanceManager which are cached in JDOPersistanceManagerFactory. Even when the background thread is killed by the thread pool manager, the JDOPersistanceManager are never GCed because they are cached by JDOPersistanceManagerFactory.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7357" opendate="2014-7-7 00:00:00" fixdate="2014-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vectorized support for BINARY data type</summary>
      <description></description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="736" opendate="2009-8-7 00:00:00" fixdate="2009-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MetaStoreUtils get_fields() does not return partition column</summary>
      <description>MetaStoreUtil's get_fields(), which is offered through the Hive Server interface, does not include the partition field in its returned list of fields. This may be related to JIRA-671. At the moment, there is no way for a Hive client to discover a partition column in a table, creating a big issue for anyone who wishes to query a partitioned table.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen-php.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7361" opendate="2014-7-7 00:00:00" fixdate="2014-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>using authorization api for RESET, DFS, ADD, DELETE, COMPILE commands</summary>
      <description>The only way to disable the commands SET, RESET, DFS, ADD, DELETE and COMPILE that is available currently is to use the hive.security.command.whitelist parameter.Some of these commands are disabled using this configuration parameter for security reasons when SQL standard authorization is enabled. However, it gets disabled in all cases.If authorization api is used authorize the use of these commands, it will give authorization implementations the flexibility to allow/disallow these commands based on user privileges.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorize.create.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.temp.table.authorize.create.tbl.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerForTest.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CompileProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DfsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ResetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessController.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveOperationType.java</file>
      <file type="M">ql.src.test.queries.clientnegative.authorization.addjar.q</file>
      <file type="M">ql.src.test.queries.clientnegative.authorization.dfs.q</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addjar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addpartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.default.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.createview.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.macro1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.desc.table.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.dfs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.droppartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.allpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail.nogrant.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insertoverwrite.nodel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noinspriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noselectpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.rename.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.serdeprop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.tab.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.show.parts.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.truncate.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="7364" opendate="2014-7-8 00:00:00" fixdate="2014-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk cannot be built on -Phadoop1 after HIVE-7144</summary>
      <description>Text.copyBytes() is introduced in hadoop-2</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="7365" opendate="2014-7-8 00:00:00" fixdate="2014-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain authorize for auth2 throws exception</summary>
      <description>throws NPE in auth v2.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.view.sqlstd.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7385" opendate="2014-7-10 00:00:00" fixdate="2014-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize for empty relation scans</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7388" opendate="2014-7-11 00:00:00" fixdate="2014-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Remove non-ascii char from comments</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7392" opendate="2014-7-11 00:00:00" fixdate="2014-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Support Columns Stats for Partition Columns</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="7407" opendate="2014-7-15 00:00:00" fixdate="2014-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Handle UDFs generically</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7409" opendate="2014-7-15 00:00:00" fixdate="2014-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add workaround for a deadlock issue of Class.getAnnotation()</summary>
      <description>JDK-7122142 mentions that there is a race condition in getAnnotations. This problem can lead deadlock. The fix on JDK will be merged on jdk8, but hive supports jdk6/jdk7 currently. Therefore, we should add workaround to avoid the issue.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7412" opendate="2014-7-15 00:00:00" fixdate="2014-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>column stats collection throws exception if all values for a column is null</summary>
      <description></description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
    </fixedFiles>
  </bug>
  <bug id="7413" opendate="2014-7-15 00:00:00" fixdate="2014-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fall back to Non-CBO optimizer if CBO fails</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7416" opendate="2014-7-15 00:00:00" fixdate="2014-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>provide context information to authorization checkPrivileges api call</summary>
      <description>Context information such as request ip address, unique string for session, and original sql command string are useful for audit logging from the authorization implementations. Authorization implementations can also choose to log authorization success along with information about what policies matched and the context information.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7422" opendate="2014-7-16 00:00:00" fixdate="2014-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Array out of bounds exception involving ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDouble</summary>
      <description>One of several found by Raj Bains.M/R or Tez.set hive.vectorized.execution.enabled=true;Stack trace:Caused by: java.lang.ArrayIndexOutOfBoundsException: 50 at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.CastLongToDouble.evaluate(CastLongToDouble.java:50) at org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDouble.aggregateInputSelection(VectorUDAFAvgDouble.java:139) at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:121) at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.processBatch(VectorGroupByOperator.java:295) at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(VectorGroupByOperator.java:711) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800) at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:139) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:43)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7424" opendate="2014-7-16 00:00:00" fixdate="2014-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveException: Error evaluating concat(concat(&amp;#39; &amp;#39;, str2), &amp;#39; &amp;#39;) in ql.exec.vector.VectorSelectOperator.processOp</summary>
      <description>One of several found by Raj Bains.M/R or Tez.set hive.vectorized.execution.enabled=true;Stack trace:Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating concat(concat(' ', str2), ' ') at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:127) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:43)</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7429" opendate="2014-7-16 00:00:00" fixdate="2014-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set replication for archive called before file exists</summary>
      <description>The call to set replication is called prior to uploading the archive file to hdfs, which does not throw an error, but the replication never gets set.This has a significant impact on large jobs (especially hash joins) due to too many tasks hitting the data nodes.</description>
      <version>0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="743" opendate="2009-8-8 00:00:00" fixdate="2009-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>let user specify serde for custom scripts</summary>
      <description>Splitting up https://issues.apache.org/jira/browse/HIVE-708 into this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="750" opendate="2009-8-12 00:00:00" fixdate="2009-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>new partitionpruner does not work with test mode</summary>
      <description>set hive.test.mode=true;the new partition pruner does not work</description>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7563" opendate="2014-7-31 00:00:00" fixdate="2014-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader should be released from LogFactory</summary>
      <description>NO PRECOMMIT TESTSLogFactory uses ClassLoader as a key in map, which makes the classloader impossible to be unloaded. LogFactory.release() should be called explicitly.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7565" opendate="2014-7-31 00:00:00" fixdate="2014-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Fix exception in Greedy Join reordering Algo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
    </fixedFiles>
  </bug>
  <bug id="7567" opendate="2014-7-31 00:00:00" fixdate="2014-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support automatic calculating reduce task number [Spark Branch]</summary>
      <description>Hive have its own machenism to calculate reduce task number, we need to implement it on spark job.NO PRECOMMIT TESTS. This is for spark-branch only.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7575" opendate="2014-7-31 00:00:00" fixdate="2014-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTables thrift call is very slow</summary>
      <description>The GetTables thrift call takes a long time when the number of table is large.With around 5000 tables, the call takes around 80 seconds compared to a "Show Tables" query on the same HiveServer2 instance which takes 3-7 seconds.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="7612" opendate="2014-8-5 00:00:00" fixdate="2014-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Add link to parent vertex to mapjoin in explain</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="7615" opendate="2014-8-5 00:00:00" fixdate="2014-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should have an option for user to see the query progress</summary>
      <description>When executing query in Beeline, user should have a option to see the progress through the outputs.Beeline could use the API introduced in HIVE-4629 to get and display the logs to the client.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7625" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: stats for Partitioned tables are not read correctly.</summary>
      <description>The wrong call is being made to read stats for Partitioned tables.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="7626" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add jar through CLI did not loaded by Spark executor[Spark Branck]</summary>
      <description>Add customized jar through CLI, still get ClassNotFound exception.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="7627" opendate="2014-8-6 00:00:00" fixdate="2014-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FSStatsPublisher does fit into Spark multi-thread task mode[Spark Branch]</summary>
      <description>Hive table statistic failed on FSStatsPublisher mode, with the following exception in Spark executor side:14/08/05 16:46:24 WARN hdfs.DFSClient: DataStreamer Exceptionjava.io.FileNotFoundException: ID mismatch. Request id and saved id: 20277 , 20278 for file /tmp/hive-root/8833d172-1edd-4508-86db-fdd7a1b0af17/hive_2014-08-05_16-46-03_013_6279446857294757772-1/-ext-10000/tmpstats-0 at org.apache.hadoop.hdfs.server.namenode.INodeId.checkId(INodeId.java:53) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2952) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2754) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2662) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1442) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1261) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): ID mismatch. Request id and saved id: 20277 , 20278 for file /tmp/hive-root/8833d172-1edd-4508-86db-fdd7a1b0af17/hive_2014-08-05_16-46-03_013_6279446857294757772-1/-ext-10000/tmpstats-0 at org.apache.hadoop.hdfs.server.namenode.INodeId.checkId(INodeId.java:53) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2952) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2754) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2662) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007) at org.apache.hadoop.ipc.Client.call(Client.java:1410) at org.apache.hadoop.ipc.Client.call(Client.java:1363) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206) at com.sun.proxy.$Proxy19.addBlock(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103) at com.sun.proxy.$Proxy19.addBlock(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1439) ... 2 more</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7628" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: log Plan coming out of each phase in Optiq Planning</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7634" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration.getPassword() if available to eliminate passwords from hive-site.xml</summary>
      <description>HADOOP-10607 provides a Configuration.getPassword() API that allows passwords to be retrieved from a configured credential provider, while also being able to fall back to the HiveConf setting if no provider is set up. Hive should use this API for versions of Hadoop that support this API. This would give users the ability to remove the passwords from their Hive configuration files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7658" opendate="2014-8-8 00:00:00" fixdate="2014-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive search order for hive-site.xml when using --config option</summary>
      <description>When using the hive cli, the tool appears to favour a hive-site.xml file in the current working directory even if the --config option is used with a valid directory containing a hive-site.xml file.I would have expected the directory specified with --config to take precedence in the CLASSPATH search order.Here's an example -/home/spurija/hive-site.xml =&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/example1&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;/tmp/hive/hive-site.xml =&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/example2&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;-bash-4.1$ diff /home/spurija/hive-site.xml /tmp/hive/hive-site.xml23c23&lt; &lt;value&gt;/tmp/example1&lt;/value&gt;&amp;#8212;&gt; &lt;value&gt;/tmp/example2&lt;/value&gt;{ check the value of scratchdir, should be example 1 }-bash-4.1$ pwd/home/spurija-bash-4.1$ hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example1{ run with a specified config, check the value of scratchdir, should be example2 … still reported as example1 }-bash-4.1$ pwd/home/spurija-bash-4.1$ hive --config /tmp/hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example1{ remove the local config, check the value of scratchdir, should be example2 … now correct }-bash-4.1$ pwd/home/spurija-bash-4.1$ rm hive-site.xml-bash-4.1$ hive --config /tmp/hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example2Is this expected behavior or should it use the directory supplied with --config as the preferred configuration?</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="7659" opendate="2014-8-8 00:00:00" fixdate="2014-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary sort in query plan [Spark Branch]</summary>
      <description>For hive on spark.Currently we rely on the sort order in RS to decide whether we need a sortByKey transformation. However a simple group by query will also have the sort order set to '+'.Consider the query: select key from table group by key. The RS in the map work will have sort order set to '+', thus requiring a sortByKey shuffle.To avoid the unnecessary sort, we should either use another way to decide if there has to be a sort shuffle, or we should set the sort order only when sort is really needed.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7700" opendate="2014-8-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>authorization api - HivePrivilegeObject for permanent function should have database name set</summary>
      <description>The HivePrivilegeObject for permanent function should have databasename set, and the functionname should be without the db part.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.local.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Entity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="7715" opendate="2014-8-13 00:00:00" fixdate="2014-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Support Union All</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>cbo-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.TraitsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveProjectRel.java</file>
    </fixedFiles>
  </bug>
  <bug id="776" opendate="2009-8-20 00:00:00" fixdate="2009-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make div as infix operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.divider.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.divider.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7788" opendate="2014-8-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate plans for insert, update, and delete</summary>
      <description>Insert plans needs to be generated differently for ACID tables, plus we need to be able to generate plans in the semantic analyzer for update and delete.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.invalid.cast.from.binary.1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7790" opendate="2014-8-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update privileges to check for update and delete</summary>
      <description>In the new SQLStdAuth scheme, we need to add UPDATE and DELETE as operations and add ability check for them.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="7791" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (1) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. alter_merge_orc.q,\ alter_merge_stats_orc.q,\ auto_join0.q,\ auto_join1.q,\ bucket2.q,\ bucket3.q,\ bucket4.q,\ count.q,\ create_merge_compressed.q,\ cross_join.q,\ cross_product_check_1.q,\ cross_product_check_2.q,\ ctas.q,\custom_input_output_format.q,\ disable_merge_for_bucketing.q,\ dynpart_sort_opt_vectorization.q,\ dynpart_sort_optimization.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7792" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (2) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on.limit_pushdown.q,\ load_dyn_part1.q,\ load_dyn_part2.q,\ load_dyn_part3.q,\ mapjoin_mapjoin.q,\ mapreduce1.q,\ mapreduce2.q,\ merge1.q,\ merge2.q,\ metadata_only_queries.q,\ optimize_nullscan.q,\ orc_analyze.q,\ orc_merge1.q,\ orc_merge2.q,\ orc_merge3.q,\ orc_merge4.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7793" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (3) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. ptf.q,\ sample1.q,\ script_env_var1.q,\ script_env_var2.q,\ script_pipe.q,\ scriptfile1.q,\ stats_counter.q,\ stats_counter_partitioned.q,\ stats_noscan_1.q,\ subquery_exists.q,\ subquery_in.q,\ temp_table.q,\ transform1.q,\ transform2.q,\ transform_ppr1.q,\ transform_ppr2.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7794" opendate="2014-8-20 00:00:00" fixdate="2014-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (4) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. vector_cast_constant.q,\ vector_data_types.q,\ vector_decimal_aggregate.q,\ vector_left_outer_join.q,\ vector_string_concat.q,\ vectorization_12.q,\ vectorization_13.q,\ vectorization_14.q,\ vectorization_15.q,\ vectorization_9.q,\ vectorization_part_project.q,\ vectorization_short_regress.q,\ vectorized_mapjoin.q,\ vectorized_nested_mapjoin.q,\ vectorized_ptf.q,\ vectorized_shufflejoin.q,\ vectorized_timestamp_funcs.q</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7795" opendate="2014-8-20 00:00:00" fixdate="2014-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ptf.q and ptf_streaming.q.[Spark Branch]</summary>
      <description>ptf.q and ptf_streaming.q contains join queries, we should enable these qtests in milestone2.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="780" opendate="2009-8-20 00:00:00" fixdate="2009-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error messages for wrong argument length in terminatePartial()/merge() UDAF calls</summary>
      <description>This is a simple fix to make sure if the UDAF's argument length of terminatePartial()/merge() UDAF calls have a problem.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7848" opendate="2014-8-22 00:00:00" fixdate="2014-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refresh SparkContext when spark configuration changes [Spark Branch]</summary>
      <description>Recreate the spark client if spark configurations are updated (through set command).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7864" opendate="2014-8-23 00:00:00" fixdate="2014-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Query fails if it refers only partitioning column</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="7876" opendate="2014-8-26 00:00:00" fixdate="2014-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>further improve the columns stats update speed for all the partitions of a table</summary>
      <description>The previous solution HIVE-7736 is not enough for the case when there are too many columns/partitions.The user will encounter org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed outWe try to remove more of transaction overhead</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="7885" opendate="2014-8-26 00:00:00" fixdate="2014-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLIServer.openSessionWithImpersonation logs as if it were openSession</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="7949" opendate="2014-9-3 00:00:00" fixdate="2014-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table LIKE command doesn&amp;#39;t set new owner</summary>
      <description>'Create table like' command doesn't set the current user as owner of new table, instead new table owner is same as source table owner.This is a regression from 0.12</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7950" opendate="2014-9-3 00:00:00" fixdate="2014-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StorageHandler resources aren&amp;#39;t added to Tez Session if already Session is already Open</summary>
      <description>Was trying to run some queries using the AccumuloStorageHandler when using the Tez execution engine. Some things that classes which were added to tmpjars weren't making it into the container. When a Tez Session is already open, as is the normal case when simply using the `hive` command, the resources aren't added.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="7951" opendate="2014-9-3 00:00:00" fixdate="2014-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>InputFormats implementing (Job)Configurable should not be cached</summary>
      <description>Currently, initial configuration instance is shared to all following input formats, which should not be like that.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7957" opendate="2014-9-3 00:00:00" fixdate="2014-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit event version handling in dynamic partition pruning on Tez</summary>
      <description>Once TEZ-1447 is resolved, we should be able to simplify the handing of event versions.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
    </fixedFiles>
  </bug>
  <bug id="796" opendate="2009-8-25 00:00:00" fixdate="2009-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RCFile results missing columns from UNION ALL</summary>
      <description>create table tt(a int, b string, c string) row format serde "org.apache.hadoop.hive.serde2.column.ColumnarSerDe" stored as RCFile;load data: 1 b c 2 e f 3 i jselect * from ( select b as cola from tt union all select c as cola from tt) s;results: NULL b NULL e NULL i</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7960" opendate="2014-9-3 00:00:00" fixdate="2014-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 2.5</summary>
      <description>Tracking JIRA for upgrading to 2.5</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
    </fixedFiles>
  </bug>
  <bug id="7998" opendate="2014-9-5 00:00:00" fixdate="2014-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance JDBC Driver to not require class specification</summary>
      <description>The hotspot VM offers a way to avoid having to specify the driver class explicitly when using the JDBC driver. The DriverManager methods getConnection and getDrivers have been enhanced to support the Java Standard Edition Service Provider mechanism. JDBC 4.0 Drivers must include the file META-INF/services/java.sql.Driver. This file contains the name of the JDBC drivers implementation of java.sql.Driver. For example, to load the my.sql.Driver class, the META-INF/services/java.sql.Driver file would contain the entry: `my.sql.Driver`Applications no longer need to explicitly load JDBC drivers using Class.forName(). Existing programs which currently load JDBC drivers using Class.forName() will continue to work without modification.via http://docs.oracle.com/javase/7/docs/api/java/sql/DriverManager.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8008" opendate="2014-9-5 00:00:00" fixdate="2014-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE while reading null decimal value</summary>
      <description>Say you have this table dec_test:dec decimal(10,0) If the table has a row that is 9999999999.5, and if we doselect * from dec_test;it will crash with NPE:2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerExceptionjava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137) ... 12 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39) at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87) ... 19 more</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="8083" opendate="2014-9-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Authorization DDLs should not enforce hive identifier syntax for user or group</summary>
      <description>The compiler expects principals (user, group and role) as hive identifiers for authorization DDLs. The user and group are entities that belong to external namespace and we can't expect those to follow hive identifier syntax rules. For example, a userid or group can contain '-' which is not allowed by compiler.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="8084" opendate="2014-9-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle casting for parameterized type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="8085" opendate="2014-9-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats optimizer should not use Description annotation to figure out function mapping (because FunctionRegistry doesn&amp;#39;t)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8086" opendate="2014-9-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Disable Trivial Project Removal Rule, Fix Result Schema</summary>
      <description>Disable trivial project rule till Optiq-407 gets fixed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
    </fixedFiles>
  </bug>
  <bug id="8087" opendate="2014-9-13 00:00:00" fixdate="2014-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Bug in constant conversion for Date type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="8099" opendate="2014-9-15 00:00:00" fixdate="2014-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IN operator for partition column fails when the partition column type is DATE</summary>
      <description>Test table DLL:CREATE TABLE testTbl(col1 string) PARTITIONED BY (date_prt date);Following query used to work fine in Hive 0.12 as the constant types are 'string' and partition column type is considered as 'string' throughout the planning and optimization (including partition pruning).SELECT * FROM testTbl WHERE date_prt IN ('2014-08-09', '2014-08-08'); In trunk the above query fails with:Line 1:33 Wrong arguments ''2014-08-08'': The arguments for IN should be the same type! Types are: {date IN (string, string)}HIVE-6642 changed the SemanticAnalyzer.java to consider partition type given in table definition instead of hardcoded 'string' type. (Modified Hive 0.12 code). So changed the query as follows to go past the above error:SELECT * FROM testTbl WHERE date_prt IN (CAST('2014-08-09' AS DATE), CAST('2014-08-08' AS DATE)); Now query goes past the error in SemanticAnalyzer, but hits the same issue (default 'string' type for partition columns) in partition pruning optimization. (Realted code here).14/09/14 20:07:20 ERROR ql.Driver: FAILED: SemanticException MetaException(message:The arguments for IN should be the same type! Types are: {string IN (date, date)})We need to change partition pruning code to consider the partition column as the type given in table definition.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MockPartitionExpressionForMetastore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="810" opendate="2009-8-31 00:00:00" fixdate="2009-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>annotation for function for JDBC</summary>
      <description>After committing 645, I realized a problem - the function does not have annotation, and therefore the describe will fail.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.queries.clientpositive.dboutput.q</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8114" opendate="2014-9-16 00:00:00" fixdate="2014-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type resolution for udf arguments of Decimal Type results in error</summary>
      <description>select log (2, 10.5BD) from src;results in exception.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="8115" opendate="2014-9-16 00:00:00" fixdate="2014-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive select query hang when fields contain map</summary>
      <description>Attached the repro of the issue. When creating an table loading the data attached, all hive query with hangs even just select * from the table.repro steps:1. run createTable.hql2. hadoop fs -put data /data3. LOAD DATA INPATH '/data' OVERWRITE INTO TABLE testtable;4. SELECT * FROM testtable;</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="8117" opendate="2014-9-16 00:00:00" fixdate="2014-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Let cbo handle field expression for join conditions</summary>
      <description>Was disabled, but can be enabled.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.JoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="812" opendate="2009-9-3 00:00:00" fixdate="2009-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive compiled with "ant package" should work with hadoop 0.17</summary>
      <description>The goal of shims is to make Hive compiled with "ant package" to work with any version of hadoop.However currently it is not working with hadoop 0.17 for a minor issue:org.apache.hadoop.io.Text.compareTo was inherited (and NOT overriden) from org.apache.hadoop.io.BinaryComparable.compareTo in hadoop 0.18 and above. When compiling Hive by default, we will compile it against hadoop 0.19.0. Java compiler will automatically replace org.apache.hadoop.io.Text.compareTo with org.apache.hadoop.io.BinaryComparable.compareTo in some places, and that caused the problem.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.19.java.org.apache.hadoop.hive.shims.Hadoop19Shims.java</file>
      <file type="M">shims.src.0.18.java.org.apache.hadoop.hive.shims.Hadoop18Shims.java</file>
      <file type="M">shims.src.0.17.java.org.apache.hadoop.hive.shims.Hadoop17Shims.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.UDAFTestMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFMax.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8121" opendate="2014-9-16 00:00:00" fixdate="2014-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create micro-benchmarks for ParquetSerde and evaluate performance</summary>
      <description>These benchmarks should not execute queries but test only the ParquetSerde code to ensure we are as efficient as possible. The output of this JIRA is:1) Benchmark tool exists2) We create new tasks under HIVE-8120 to track the improvements required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8124" opendate="2014-9-16 00:00:00" fixdate="2014-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move local_mapred_error_cache.q to minimr</summary>
      <description>local_mapred_error_cache.q gets stuck sometimes. Looks like an issue with local runner. The test works fine in minimr.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8239" opendate="2014-9-23 00:00:00" fixdate="2014-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSSQL upgrade schema scripts does not map Java long datatype columns correctly for transaction related tables</summary>
      <description>In Transaction related tables, Java long column fields are mapped to int which results in failure as shown:2014-09-23 18:08:00,030 DEBUG txn.TxnHandler (TxnHandler.java:lock(1243)) - Going to execute update &lt;insert into HIVE_LOCKS (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, hl_table, hl_partition, hl_lock_state, hl_lock_type, hl_last_heartbeat, hl_user, hl_host) values (28, 1,0, 'default', null, null, 'w', 'r', 1411495679547, 'hadoopqa', 'onprem-sqoop1')&gt;2014-09-23 18:08:00,033 DEBUG txn.TxnHandler (TxnHandler.java:lock(406)) - Going to rollback2014-09-23 18:08:00,045 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - org.apache.thrift.TException: MetaException(message:Unable to update transaction database com.microsoft.sqlserver.jdbc.SQLServerException: Arithmetic overflow error converting expression to data type int. at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:197) at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:246) at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:83) at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1488) at com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQLServerStatement.java:775) at com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute(SQLServerStatement.java:676) at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615) at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeUpdate(SQLServerStatement.java:633) at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497) at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1244) at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:403) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5255) ...In this query one of the column HL_LAST_HEARTBEAT defined as int datatype in HIVE_LOCKS is trying to take in a long value (1411495679547) and throws the error. We should use bigint as column type instead.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.13.0-to-0.14.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.14.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="8245" opendate="2014-9-24 00:00:00" fixdate="2014-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect table read entities at same time as view read entities</summary>
      <description></description>
      <version>0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8246" opendate="2014-9-24 00:00:00" fixdate="2014-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in http-kerberos mode is restrictive on client usernames</summary>
      <description>Unable to use client usernames of the format:username/host@REALMusername@FOREIGN_REALMThe following works fine:username@REALM</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8250" opendate="2014-9-25 00:00:00" fixdate="2014-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncating table doesnt invalidate stats</summary>
      <description></description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8298" opendate="2014-9-29 00:00:00" fixdate="2014-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results for n-way join when join expressions are not in same order across joins</summary>
      <description>select * from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;is minimal query which reproduces it</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8313" opendate="2014-9-30 00:00:00" fixdate="2014-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize evaluation for ExprNodeConstantEvaluator and ExprNodeNullEvaluator</summary>
      <description>Consider the following query:SELECT foo, bar, goo, idFROM myTableWHERE id IN ( 'A', 'B', 'C', 'D', ... , 'ZZZZZZ' );One finds that when the IN clause has several thousand elements (and the table has several million rows), the query above takes orders-of-magnitude longer to run on Hive 0.12 than say Hive 0.10.I have a possibly incomplete fix.</description>
      <version>0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8428" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesnt remove filters involving casts</summary>
      <description>e.g.,select key,value from srcpart where hr = cast(11 as double);</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pcr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.vectorization.ppd.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.decimal.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="8429" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add records in/out counters</summary>
      <description>We don't do counters for input/output records right now. That would help for debugging though (if it can be done with minimal overhead).</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8435" opendate="2014-10-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add identity project remover optimization</summary>
      <description></description>
      <version>0.9.0,0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8436" opendate="2014-10-12 00:00:00" fixdate="2014-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify SparkWork to split works with multiple child works [Spark Branch]</summary>
      <description>Based on the design doc, we need to split the operator tree of a work in SparkWork if the work is connected to multiple child works. The way splitting the operator tree is performed by cloning the original work and removing unwanted branches in the operator tree. Please refer to the design doc for details.This process should be done right before we generate SparkPlan. We should have a utility method that takes the orignal SparkWork and return a modified SparkWork.This process should also keep the information about the original work and its clones. Such information will be needed during SparkPlan generation (HIVE-8437).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkTableScanProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMultiInsertionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMergeTaskProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8460" opendate="2014-10-14 00:00:00" fixdate="2014-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC SARG literal creation for double from float may lead to wrong evaluation of SARG</summary>
      <description>e.g., expression like d = 0.22 where 0.22 is of float type, gets converted to double d = 0.219999986 in SARG creation. This will cause erroneous evaluation of SARG expression.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.decimal.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="8528" opendate="2014-10-20 00:00:00" fixdate="2014-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add remote Spark client to Hive [Spark Branch]</summary>
      <description>For the time being, at least, we've decided to build the Spark client (see SPARK-3215) inside Hive. This task tracks merging the ongoing work into the Spark branch.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8539" opendate="2014-10-21 00:00:00" fixdate="2014-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable collect table statistics based on SparkCounter[Spark Branch]</summary>
      <description>Hive support collect table statistics based on Counters/TezCounters in MR/Tez mode, we should enable this in Spark mode as well.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.varchar.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.char.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.decimal.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.lazy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.enforce.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.create.merge.compressed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin6.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">data.conf.spark.hive-site.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.counter.SparkCounters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SimpleSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="854" opendate="2009-9-24 00:00:00" fixdate="2009-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>post execute hook needed</summary>
      <description>Sometimes pre execute hooks are not enough - it would be useful to have post execution hooks too</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.rc.seq.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.script.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unhex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.minute.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.divider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.cos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.asin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.acos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tablestatus.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.showparts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.with.semi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullinput.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart.err.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertexternal1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.cb.delim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input16.cc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.neg.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.function.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.struct.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tablestatus.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.txt.seq.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.add.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.arraymapstruct.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.format.q.out</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.moveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
      <file type="M">ql.src.test.results.clientnegative.altern1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.udaf.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.deletejar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.describe.xpath1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.describe.xpath2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.describe.xpath3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.describe.xpath4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.native.udf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.external1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.external2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fetchtask.ioexception.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8611" opendate="2014-10-27 00:00:00" fixdate="2014-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>grant/revoke syntax should support additional objects for authorization plugins</summary>
      <description>The authorization framework supports URI and global objects. The SQL syntax however doesn't allow granting privileges on these objects. We should allow the compiler to parse these so that it can be handled by authorization plugins.</description>
      <version>0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="8627" opendate="2014-10-28 00:00:00" fixdate="2014-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute stats on a table from impala caused the table to be corrupted</summary>
      <description>Use impala 2.0 to connect to hive-0.13 Metastore.From impala, run the following queries:create table voter1(voter_id int,name string,age tinyint, registrationstring,contributions decimal(5,2),voterzone smallint,create_time timestamp) rowformat delimited fields terminated by '\t';load data inpath '/tmp/votertab' into table voter1;After this, can successfully select from table voter 1.Execute the following from impala shell:&gt; compute stats voter1;After this, got the following error selecting from table voter1:&gt; select * from voter1 limit 5;Query: select * from voter1 limit 5ERROR: AnalysisException: Failed to load metadata for table: default.voter1CAUSED BY: TableLoadingException: Failed to load metadata for table: voter1CAUSED BY: TTransportException: java.net.SocketException: Broken pipeCAUSED BY: SocketException: Broken pipeBelow is the exception found in Hive log:org.apache.thrift.protocol.TProtocolException: Cannot write a TUnion with no set value!at org.apache.thrift.TUnion$TUnionStandardScheme.write(TUnion.java:240)at org.apache.thrift.TUnion$TUnionStandardScheme.write(TUnion.java:213)at org.apache.thrift.TUnion.write(TUnion.java:152)at org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj$ColumnStatisticsObjStandardScheme.write(ColumnStatisticsObj.java:550)at org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj$ColumnStatisticsObjStandardScheme.write(ColumnStatisticsObj.java:488)at org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.write(ColumnStatisticsObj.java:414)at org.apache.hadoop.hive.metastore.api.TableStatsResult$TableStatsResultStandardScheme.write(TableStatsResult.java:388)at org.apache.hadoop.hive.metastore.api.TableStatsResult$TableStatsResultStandardScheme.write(TableStatsResult.java:338)at org.apache.hadoop.hive.metastore.api.TableStatsResult.write(TableStatsResult.java:288)at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_statistics_req_result$get_table_statistics_req_resultStandardScheme.write(ThriftHiveMetastore.java)at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_statistics_req_result$get_table_statistics_req_resultStandardScheme.write(ThriftHiveMetastore.java)at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_statistics_req_result.write(ThriftHiveMetastore.java)at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)at org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:48)at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)at java.lang.Thread.run(Thread.java:745)</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="8649" opendate="2014-10-29 00:00:00" fixdate="2014-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase level of parallelism in reduce phase [Spark Branch]</summary>
      <description>We calculate the number of reducers based on the same code for MapReduce. However, reducers are vastly cheaper in Spark and it's generally recommended we have many more reducers than in MR.Sandy Ryza who works on Spark has some ideas about a heuristic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="865" opendate="2009-10-1 00:00:00" fixdate="2009-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin: memory leak for same key with very large number of values</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8705" opendate="2014-11-3 00:00:00" fixdate="2014-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support using pre-authenticated subject in kerberized HiveServer2 HTTP mode</summary>
      <description>HIVE-6486 provided a patch to utilize pre-authenticated subject (someone who has programmatically done a JAAS login and is not doing kinit before connecting to HiveServer2 using the JDBC driver). However, that feature was only for the binary mode code path. We need to have a similar feature when the driver-server communicate using http transport.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpKerberosRequestInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="8714" opendate="2014-11-3 00:00:00" fixdate="2014-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDatabase reports direct SQL error when database is missing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="8715" opendate="2014-11-3 00:00:00" fixdate="2014-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 14 upgrade scripts can fail for statistics if database was created using auto-create</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.019-HIVE-7784.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.020-HIVE-7784.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.019-HIVE-7784.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.002-HIVE-7784.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="8732" opendate="2014-11-4 00:00:00" fixdate="2014-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC string statistics are not merged correctly</summary>
      <description>Currently ORC's string statistics do not merge correctly causing incorrect maximum values.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8746" opendate="2014-11-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC timestamp columns are sensitive to daylight savings time</summary>
      <description>Hive uses Java's Timestamp class to manipulate timestamp columns. Unfortunately the textual parsing in Timestamp is done in local time and the internal storage is in UTC.ORC mostly side steps this issue by storing the difference between the time and a base time also in local and storing that difference in the file. Reading the file between timezones will mostly work correctly "2014-01-01 12:34:56" will read correctly in every timezone.However, when moving between timezones with different daylight saving it creates trouble. In particular, moving from a computer in PST to UTC will read "2014-06-06 12:34:56" as "2014-06-06 11:34:56".</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.static.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8760" opendate="2014-11-6 00:00:00" fixdate="2014-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass a copy of HiveConf to hooks</summary>
      <description>because hadoop's Configuration is not thread-safe</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="8847" opendate="2014-11-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bugs in jenkins scripts</summary>
      <description>1) Incorrect help message in process_jira function2) Spark builds do not work3) Build "profiles" (which map to a properties file) are hard coded4) A JIRA is required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8870" opendate="2014-11-14 00:00:00" fixdate="2014-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>errors when selecting a struct field within an array from ORC based tables</summary>
      <description>When using ORC as storage for a table, we get errors on selecting a struct field within an array. These errors do not appear with default format.CREATE TABLE `foobar_orc`( `uid` bigint, `elements` array&lt;struct&lt;elementid:bigint,foo:struct&lt;bar:string&gt;&gt;&gt;)STORED AS ORC;When selecting from this empty table, we get a direct NPE within the Hive CLI:SELECT elements.elementIdFROM foobar_orc;-- FAILED: RuntimeException java.lang.NullPointerExceptionA more real-world query produces a RuntimeException / NullPointerException in the mapper:SELECT uid, element.elementIdFROM foobar_orcLATERAL VIEW EXPLODE(elements) e AS element;Error: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)[...]Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61)[...]FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskBoth queries run fine on a non-orc table:CREATE TABLE `foobar`( `uid` bigint, `elements` array&lt;struct&lt;elementid:bigint,foo:struct&lt;bar:string&gt;&gt;&gt;); SELECT elements.elementIdFROM foobar;-- OK-- Time taken: 0.225 secondsSELECT uid, element.elementIdFROM foobarLATERAL VIEW EXPLODE(elements) e AS element;-- Total MapReduce CPU Time Spent: 1 seconds 920 msec-- OK-- Time taken: 25.905 seconds</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
    </fixedFiles>
  </bug>
  <bug id="8898" opendate="2014-11-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HIVE-8874 once HBASE-12493 is fixed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="8961" opendate="2014-11-25 00:00:00" fixdate="2014-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary dependency collection task [Spark Branch]</summary>
      <description>Seems some dependency collection task we add for move task is unnecessary.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.varchar.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.char.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="9048" opendate="2014-12-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build failed on hadoop-1 after HIVE-8828.</summary>
      <description>HIVE-8828 introduce org.apache.hadoop.tools.HadoopArchives which included in hadoop-tools(in hadoop-1)/hadoop-archives(in hadoop-2), while hadoop-tools is not added into hadoop-1 dependency. This lead to the following compile error:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[175,30] error: package org.apache.hadoop.tools does not exist[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,8] error: cannot find symbol[ERROR] class DDLTask[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,33] error: cannot find symbol[ERROR] -&gt; [Help 1]</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9059" opendate="2014-12-10 00:00:00" fixdate="2014-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove wrappers for SparkJobInfo and SparkStageInfo [Spark Branch]</summary>
      <description>SPARK-4567 is resolved. We can remove the wrappers we added to solve the serailization issues.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkStageInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkJobInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9062" opendate="2014-12-10 00:00:00" fixdate="2014-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain plan doesn&amp;#39;t print join keys for Tez shuffle join</summary>
      <description>For map join, it already prints the keys, but not for shuffle join.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CommonMergeJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="9121" opendate="2014-12-16 00:00:00" fixdate="2014-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable beeline query progress information for Spark job[Spark Branch]</summary>
      <description>We could not get query progress information in Beeline as SparkJobMonitor is filtered out of operation log.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="9226" opendate="2014-12-30 00:00:00" fixdate="2014-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline interweaves the query result and query log sometimes</summary>
      <description>In most case, Beeline output the query log during execution and output the result at last. However, sometimes there are logs output after result, although the query has been done. This might make users a little confused.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="9234" opendate="2014-12-30 00:00:00" fixdate="2014-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks FileSystem objects in FileSystem.CACHE</summary>
      <description>Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true), but creating new FileSystem objects is expensive.</description>
      <version>0.12.0,0.12.1,0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
    </fixedFiles>
  </bug>
  <bug id="9513" opendate="2015-1-29 00:00:00" fixdate="2015-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9514" opendate="2015-1-29 00:00:00" fixdate="2015-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool is broken in hive 1.0.0</summary>
      <description>Schematool gives following error - bin/schematool -dbType derby -initSchemaStarting metastore schema initialization to 1.0org.apache.hadoop.hive.metastore.HiveMetaException: Unknown version specified for initialization: 1.0Metastore schema hasn't changed from 0.14.0 to 1.0.0. So there is no need for new .sql files for 1.0.0. However, schematool needs to be made aware of the metastore schema equivalence.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="9599" opendate="2015-2-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove derby, datanucleus and other not related to jdbc client classes from hive-jdbc-standalone.jar</summary>
      <description>Looks like the following packages (included to hive-jdbc-standalone.jar) are not used when jdbc client opens jdbc connection and runs queries:antlr/antlr/actions/cpp/antlr/actions/csharp/antlr/actions/java/antlr/actions/python/antlr/ASdebug/antlr/build/antlr/collections/antlr/collections/impl/antlr/debug/antlr/debug/misc/antlr/preprocessor/com/google/gson/com/google/gson/annotations/com/google/gson/internal/com/google/gson/internal/bind/com/google/gson/reflect/com/google/gson/stream/com/google/inject/com/google/inject/binder/com/google/inject/internal/com/google/inject/internal/asm/com/google/inject/internal/cglib/core/com/google/inject/internal/cglib/proxy/com/google/inject/internal/cglib/reflect/com/google/inject/internal/util/com/google/inject/matcher/com/google/inject/name/com/google/inject/servlet/com/google/inject/spi/com/google/inject/util/com/jamesmurty/utils/com/jcraft/jsch/com/jcraft/jsch/jce/com/jcraft/jsch/jcraft/com/jcraft/jsch/jgss/com/jolbox/bonecp/com/jolbox/bonecp/hooks/com/jolbox/bonecp/proxy/com/sun/activation/registries/com/sun/activation/viewers/com/sun/istack/com/sun/istack/localization/com/sun/istack/logging/com/sun/mail/handlers/com/sun/mail/iap/com/sun/mail/imap/com/sun/mail/imap/protocol/com/sun/mail/mbox/com/sun/mail/pop3/com/sun/mail/smtp/com/sun/mail/util/com/sun/xml/bind/com/sun/xml/bind/annotation/com/sun/xml/bind/api/com/sun/xml/bind/api/impl/com/sun/xml/bind/marshaller/com/sun/xml/bind/unmarshaller/com/sun/xml/bind/util/com/sun/xml/bind/v2/com/sun/xml/bind/v2/bytecode/com/sun/xml/bind/v2/model/annotation/com/sun/xml/bind/v2/model/core/com/sun/xml/bind/v2/model/impl/com/sun/xml/bind/v2/model/nav/com/sun/xml/bind/v2/model/runtime/com/sun/xml/bind/v2/runtime/com/sun/xml/bind/v2/runtime/output/com/sun/xml/bind/v2/runtime/property/com/sun/xml/bind/v2/runtime/reflect/com/sun/xml/bind/v2/runtime/reflect/opt/com/sun/xml/bind/v2/runtime/unmarshaller/com/sun/xml/bind/v2/schemagen/com/sun/xml/bind/v2/schemagen/episode/com/sun/xml/bind/v2/schemagen/xmlschema/com/sun/xml/bind/v2/util/com/sun/xml/txw2/com/sun/xml/txw2/annotation/com/sun/xml/txw2/output/com/thoughtworks/paranamer/contribs/mx/javax/activation/javax/annotation/javax/annotation/concurrent/javax/annotation/meta/javax/annotation/security/javax/el/javax/inject/javax/jdo/javax/jdo/annotations/javax/jdo/datastore/javax/jdo/identity/javax/jdo/listener/javax/jdo/metadata/javax/jdo/spi/javax/mail/javax/mail/event/javax/mail/internet/javax/mail/search/javax/mail/util/javax/security/auth/message/javax/security/auth/message/callback/javax/security/auth/message/config/javax/security/auth/message/module/javax/servlet/javax/servlet/http/javax/servlet/jsp/javax/servlet/jsp/el/javax/servlet/jsp/tagext/javax/transaction/javax/transaction/xa/javax/xml/bind/javax/xml/bind/annotation/javax/xml/bind/annotation/adapters/javax/xml/bind/attachment/javax/xml/bind/helpers/javax/xml/bind/util/javax/xml/stream/javax/xml/stream/events/javax/xml/stream/util/jline/jline/console/jline/console/completer/jline/console/history/jline/console/internal/jline/internal/net/iharder/base64/org/aopalliance/aop/org/aopalliance/intercept/org/apache/commons/beanutils/org/apache/commons/beanutils/converters/org/apache/commons/beanutils/expression/org/apache/commons/beanutils/locale/org/apache/commons/beanutils/locale/converters/org/apache/commons/cli/org/apache/commons/codec/org/apache/commons/codec/binary/org/apache/commons/codec/digest/org/apache/commons/codec/language/org/apache/commons/codec/net/org/apache/commons/collections/org/apache/commons/collections/bag/org/apache/commons/collections/bidimap/org/apache/commons/collections/buffer/org/apache/commons/collections/collection/org/apache/commons/collections/comparators/org/apache/commons/collections/functors/org/apache/commons/collections/iterators/org/apache/commons/collections/keyvalue/org/apache/commons/collections/list/org/apache/commons/collections/map/org/apache/commons/collections/set/org/apache/commons/configuration/org/apache/commons/configuration/beanutils/org/apache/commons/configuration/event/org/apache/commons/configuration/interpol/org/apache/commons/configuration/plist/org/apache/commons/configuration/reloading/org/apache/commons/configuration/tree/org/apache/commons/configuration/tree/xpath/org/apache/commons/configuration/web/org/apache/commons/dbcp/org/apache/commons/dbcp/cpdsadapter/org/apache/commons/dbcp/datasources/org/apache/commons/dbcp/managed/org/apache/commons/digester/org/apache/commons/digester/parser/org/apache/commons/digester/plugins/org/apache/commons/digester/plugins/strategies/org/apache/commons/digester/substitution/org/apache/commons/digester/xmlrules/org/apache/commons/el/org/apache/commons/el/parser/org/apache/commons/httpclient/org/apache/commons/httpclient/auth/org/apache/commons/httpclient/contrib/proxy/org/apache/commons/httpclient/cookie/org/apache/commons/httpclient/methods/org/apache/commons/httpclient/methods/multipart/org/apache/commons/httpclient/params/org/apache/commons/httpclient/protocol/org/apache/commons/httpclient/util/org/apache/commons/io/org/apache/commons/io/comparator/org/apache/commons/io/filefilter/org/apache/commons/io/input/org/apache/commons/io/monitor/org/apache/commons/io/output/org/apache/commons/jocl/org/apache/commons/math3/org/apache/commons/math3/analysis/org/apache/commons/math3/analysis/differentiation/org/apache/commons/math3/analysis/function/org/apache/commons/math3/analysis/integration/org/apache/commons/math3/analysis/integration/gauss/org/apache/commons/math3/analysis/interpolation/org/apache/commons/math3/analysis/polynomials/org/apache/commons/math3/analysis/solvers/org/apache/commons/math3/complex/org/apache/commons/math3/dfp/org/apache/commons/math3/distribution/org/apache/commons/math3/exception/org/apache/commons/math3/exception/util/org/apache/commons/math3/filter/org/apache/commons/math3/fitting/org/apache/commons/math3/fraction/org/apache/commons/math3/genetics/org/apache/commons/math3/geometry/org/apache/commons/math3/geometry/euclidean/oned/org/apache/commons/math3/geometry/euclidean/threed/org/apache/commons/math3/geometry/euclidean/twod/org/apache/commons/math3/geometry/partitioning/org/apache/commons/math3/geometry/partitioning/utilities/org/apache/commons/math3/linear/org/apache/commons/math3/ode/org/apache/commons/math3/ode/events/org/apache/commons/math3/ode/nonstiff/org/apache/commons/math3/ode/sampling/org/apache/commons/math3/optim/org/apache/commons/math3/optimization/org/apache/commons/math3/optimization/direct/org/apache/commons/math3/optimization/fitting/org/apache/commons/math3/optimization/general/org/apache/commons/math3/optimization/linear/org/apache/commons/math3/optimization/univariate/org/apache/commons/math3/optim/linear/org/apache/commons/math3/optim/nonlinear/scalar/org/apache/commons/math3/optim/nonlinear/scalar/gradient/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/org/apache/commons/math3/optim/nonlinear/vector/org/apache/commons/math3/optim/nonlinear/vector/jacobian/org/apache/commons/math3/optim/univariate/org/apache/commons/math3/random/org/apache/commons/math3/special/org/apache/commons/math3/stat/org/apache/commons/math3/stat/clustering/org/apache/commons/math3/stat/correlation/org/apache/commons/math3/stat/descriptive/org/apache/commons/math3/stat/descriptive/moment/org/apache/commons/math3/stat/descriptive/rank/org/apache/commons/math3/stat/descriptive/summary/org/apache/commons/math3/stat/inference/org/apache/commons/math3/stat/ranking/org/apache/commons/math3/stat/regression/org/apache/commons/math3/transform/org/apache/commons/math3/util/org/apache/commons/net/org/apache/commons/net/bsd/org/apache/commons/net/chargen/org/apache/commons/net/daytime/org/apache/commons/net/discard/org/apache/commons/net/echo/org/apache/commons/net/finger/org/apache/commons/net/ftp/org/apache/commons/net/ftp/parser/org/apache/commons/net/imap/org/apache/commons/net/io/org/apache/commons/net/nntp/org/apache/commons/net/ntp/org/apache/commons/net/pop3/org/apache/commons/net/smtp/org/apache/commons/net/telnet/org/apache/commons/net/tftp/org/apache/commons/net/time/org/apache/commons/net/util/org/apache/commons/net/whois/org/apache/commons/pool/org/apache/commons/pool/impl/org/apache/curator/org/apache/curator/drivers/org/apache/curator/ensemble/org/apache/curator/ensemble/exhibitor/org/apache/curator/ensemble/fixed/org/apache/curator/framework/org/apache/curator/framework/api/org/apache/curator/framework/api/transaction/org/apache/curator/framework/imps/org/apache/curator/framework/listen/org/apache/curator/framework/recipes/org/apache/curator/framework/recipes/atomic/org/apache/curator/framework/recipes/barriers/org/apache/curator/framework/recipes/cache/org/apache/curator/framework/recipes/leader/org/apache/curator/framework/recipes/locks/org/apache/curator/framework/recipes/nodes/org/apache/curator/framework/recipes/queue/org/apache/curator/framework/recipes/shared/org/apache/curator/framework/state/org/apache/curator/retry/org/apache/curator/utils/org/apache/derby/agg/org/apache/derby/authentication/org/apache/derby/catalog/org/apache/derby/catalog/types/org/apache/derby/database/org/apache/derby/diag/org/apache/derby/iapi/db/org/apache/derby/iapi/error/org/apache/derby/iapi/jdbc/org/apache/derby/iapi/reference/org/apache/derby/iapi/security/org/apache/derby/iapi/services/org/apache/derby/iapi/services/cache/org/apache/derby/iapi/services/classfile/org/apache/derby/iapi/services/compiler/org/apache/derby/iapi/services/context/org/apache/derby/iapi/services/crypto/org/apache/derby/iapi/services/daemon/org/apache/derby/iapi/services/diag/org/apache/derby/iapi/services/i18n/org/apache/derby/iapi/services/info/org/apache/derby/iapi/services/io/org/apache/derby/iapi/services/jmx/org/apache/derby/iapi/services/loader/org/apache/derby/iapi/services/locks/org/apache/derby/iapi/services/memory/org/apache/derby/iapi/services/monitor/org/apache/derby/iapi/services/property/org/apache/derby/iapi/services/stream/org/apache/derby/iapi/services/timer/org/apache/derby/iapi/services/uuid/org/apache/derby/iapi/sql/org/apache/derby/iapi/sql/compile/org/apache/derby/iapi/sql/conn/org/apache/derby/iapi/sql/depend/org/apache/derby/iapi/sql/dictionary/org/apache/derby/iapi/sql/execute/org/apache/derby/iapi/sql/execute/xplain/org/apache/derby/iapi/store/access/org/apache/derby/iapi/store/access/conglomerate/org/apache/derby/iapi/store/access/xa/org/apache/derby/iapi/store/raw/org/apache/derby/iapi/store/raw/data/org/apache/derby/iapi/store/raw/log/org/apache/derby/iapi/store/raw/xact/org/apache/derby/iapi/store/replication/master/org/apache/derby/iapi/store/replication/slave/org/apache/derby/iapi/tools/i18n/org/apache/derby/iapi/transaction/org/apache/derby/iapi/types/org/apache/derby/iapi/util/org/apache/derby/impl/db/org/apache/derby/impl/io/org/apache/derby/impl/io/vfmem/org/apache/derby/impl/jdbc/org/apache/derby/impl/jdbc/authentication/org/apache/derby/impl/load/org/apache/derby/impl/services/bytecode/org/apache/derby/impl/services/cache/org/apache/derby/impl/services/daemon/org/apache/derby/impl/services/jce/org/apache/derby/impl/services/jmx/org/apache/derby/impl/services/jmxnone/org/apache/derby/impl/services/locks/org/apache/derby/impl/services/monitor/org/apache/derby/impl/services/reflect/org/apache/derby/impl/services/stream/org/apache/derby/impl/services/timer/org/apache/derby/impl/services/uuid/org/apache/derby/impl/sql/org/apache/derby/impl/sql/catalog/org/apache/derby/impl/sql/compile/org/apache/derby/impl/sql/conn/org/apache/derby/impl/sql/depend/org/apache/derby/impl/sql/execute/org/apache/derby/impl/sql/execute/rts/org/apache/derby/impl/sql/execute/xplain/org/apache/derby/impl/store/access/org/apache/derby/impl/store/access/btree/org/apache/derby/impl/store/access/btree/index/org/apache/derby/impl/store/access/conglomerate/org/apache/derby/impl/store/access/heap/org/apache/derby/impl/store/access/sort/org/apache/derby/impl/store/raw/org/apache/derby/impl/store/raw/data/org/apache/derby/impl/store/raw/log/org/apache/derby/impl/store/raw/xact/org/apache/derby/impl/store/replication/org/apache/derby/impl/store/replication/buffer/org/apache/derby/impl/store/replication/master/org/apache/derby/impl/store/replication/net/org/apache/derby/impl/store/replication/slave/org/apache/derby/impl/tools/sysinfo/org/apache/derby/io/org/apache/derby/jdbc/org/apache/derby/mbeans/org/apache/derby/osgi/org/apache/derby/security/org/apache/derby/shared/common/error/org/apache/derby/shared/common/reference/org/apache/derby/tools/org/apache/derby/vti/org/apache/directory/api/asn1/org/apache/directory/api/asn1/util/org/apache/directory/api/util/org/apache/directory/api/util/exception/org/apache/directory/server/i18n/org/apache/directory/server/kerberos/changepwd/exceptions/org/apache/directory/server/kerberos/changepwd/io/org/apache/directory/server/kerberos/changepwd/messages/org/apache/directory/server/kerberos/protocol/codec/org/apache/directory/server/kerberos/shared/crypto/checksum/org/apache/directory/server/kerberos/shared/crypto/encryption/org/apache/directory/server/kerberos/shared/keytab/org/apache/directory/server/kerberos/shared/replay/org/apache/directory/server/kerberos/shared/store/org/apache/directory/shared/kerberos/org/apache/directory/shared/kerberos/codec/org/apache/directory/shared/kerberos/codec/actions/org/apache/directory/shared/kerberos/codec/adAndOr/org/apache/directory/shared/kerberos/codec/adAndOr/actions/org/apache/directory/shared/kerberos/codec/adIfRelevant/org/apache/directory/shared/kerberos/codec/adKdcIssued/org/apache/directory/shared/kerberos/codec/adKdcIssued/actions/org/apache/directory/shared/kerberos/codec/adMandatoryForKdc/org/apache/directory/shared/kerberos/codec/apRep/org/apache/directory/shared/kerberos/codec/apRep/actions/org/apache/directory/shared/kerberos/codec/apReq/org/apache/directory/shared/kerberos/codec/apReq/actions/org/apache/directory/shared/kerberos/codec/asRep/org/apache/directory/shared/kerberos/codec/asRep/actions/org/apache/directory/shared/kerberos/codec/asReq/org/apache/directory/shared/kerberos/codec/asReq/actions/org/apache/directory/shared/kerberos/codec/authenticator/org/apache/directory/shared/kerberos/codec/authenticator/actions/org/apache/directory/shared/kerberos/codec/authorizationData/org/apache/directory/shared/kerberos/codec/authorizationData/actions/org/apache/directory/shared/kerberos/codec/changePwdData/org/apache/directory/shared/kerberos/codec/changePwdData/actions/org/apache/directory/shared/kerberos/codec/checksum/org/apache/directory/shared/kerberos/codec/checksum/actions/org/apache/directory/shared/kerberos/codec/encApRepPart/org/apache/directory/shared/kerberos/codec/encApRepPart/actions/org/apache/directory/shared/kerberos/codec/encAsRepPart/org/apache/directory/shared/kerberos/codec/encAsRepPart/actions/org/apache/directory/shared/kerberos/codec/EncKdcRepPart/org/apache/directory/shared/kerberos/codec/EncKdcRepPart/actions/org/apache/directory/shared/kerberos/codec/encKrbCredPart/org/apache/directory/shared/kerberos/codec/encKrbCredPart/actions/org/apache/directory/shared/kerberos/codec/encKrbPrivPart/org/apache/directory/shared/kerberos/codec/encKrbPrivPart/actions/org/apache/directory/shared/kerberos/codec/encryptedData/org/apache/directory/shared/kerberos/codec/encryptedData/actions/org/apache/directory/shared/kerberos/codec/encryptionKey/org/apache/directory/shared/kerberos/codec/encryptionKey/actions/org/apache/directory/shared/kerberos/codec/encTgsRepPart/org/apache/directory/shared/kerberos/codec/encTgsRepPart/actions/org/apache/directory/shared/kerberos/codec/encTicketPart/org/apache/directory/shared/kerberos/codec/encTicketPart/actions/org/apache/directory/shared/kerberos/codec/etypeInfo/org/apache/directory/shared/kerberos/codec/etypeInfo2/org/apache/directory/shared/kerberos/codec/etypeInfo2/actions/org/apache/directory/shared/kerberos/codec/etypeInfo2Entry/org/apache/directory/shared/kerberos/codec/etypeInfo2Entry/actions/org/apache/directory/shared/kerberos/codec/etypeInfo/actions/org/apache/directory/shared/kerberos/codec/etypeInfoEntry/org/apache/directory/shared/kerberos/codec/etypeInfoEntry/actions/org/apache/directory/shared/kerberos/codec/hostAddress/org/apache/directory/shared/kerberos/codec/hostAddress/actions/org/apache/directory/shared/kerberos/codec/hostAddresses/org/apache/directory/shared/kerberos/codec/hostAddresses/actions/org/apache/directory/shared/kerberos/codec/kdcRep/org/apache/directory/shared/kerberos/codec/kdcRep/actions/org/apache/directory/shared/kerberos/codec/kdcReq/org/apache/directory/shared/kerberos/codec/kdcReq/actions/org/apache/directory/shared/kerberos/codec/kdcReqBody/org/apache/directory/shared/kerberos/codec/kdcReqBody/actions/org/apache/directory/shared/kerberos/codec/krbCred/org/apache/directory/shared/kerberos/codec/krbCred/actions/org/apache/directory/shared/kerberos/codec/krbCredInfo/org/apache/directory/shared/kerberos/codec/krbCredInfo/actions/org/apache/directory/shared/kerberos/codec/krbError/org/apache/directory/shared/kerberos/codec/krbError/actions/org/apache/directory/shared/kerberos/codec/krbPriv/org/apache/directory/shared/kerberos/codec/krbPriv/actions/org/apache/directory/shared/kerberos/codec/krbSafe/org/apache/directory/shared/kerberos/codec/krbSafe/actions/org/apache/directory/shared/kerberos/codec/krbSafeBody/org/apache/directory/shared/kerberos/codec/krbSafeBody/actions/org/apache/directory/shared/kerberos/codec/lastReq/org/apache/directory/shared/kerberos/codec/lastReq/actions/org/apache/directory/shared/kerberos/codec/methodData/org/apache/directory/shared/kerberos/codec/methodData/actions/org/apache/directory/shared/kerberos/codec/options/org/apache/directory/shared/kerberos/codec/padata/org/apache/directory/shared/kerberos/codec/padata/actions/org/apache/directory/shared/kerberos/codec/paEncTimestamp/org/apache/directory/shared/kerberos/codec/paEncTsEnc/org/apache/directory/shared/kerberos/codec/paEncTsEnc/actions/org/apache/directory/shared/kerberos/codec/principalName/org/apache/directory/shared/kerberos/codec/principalName/actions/org/apache/directory/shared/kerberos/codec/tgsRep/org/apache/directory/shared/kerberos/codec/tgsRep/actions/org/apache/directory/shared/kerberos/codec/tgsReq/org/apache/directory/shared/kerberos/codec/tgsReq/actions/org/apache/directory/shared/kerberos/codec/ticket/org/apache/directory/shared/kerberos/codec/ticket/actions/org/apache/directory/shared/kerberos/codec/transitedEncoding/org/apache/directory/shared/kerberos/codec/transitedEncoding/actions/org/apache/directory/shared/kerberos/codec/typedData/org/apache/directory/shared/kerberos/codec/typedData/actions/org/apache/directory/shared/kerberos/codec/types/org/apache/directory/shared/kerberos/components/org/apache/directory/shared/kerberos/crypto/checksum/org/apache/directory/shared/kerberos/exceptions/org/apache/directory/shared/kerberos/flags/org/apache/directory/shared/kerberos/messages/org/apache/hadoop/hive/metastore/org/apache/hadoop/hive/metastore/api/org/apache/hadoop/hive/metastore/events/org/apache/hadoop/hive/metastore/hooks/org/apache/hadoop/hive/metastore/model/org/apache/hadoop/hive/metastore/parser/org/apache/hadoop/hive/metastore/partition/spec/org/apache/hadoop/hive/metastore/tools/org/apache/hadoop/hive/metastore/txn/org/apache/hadoop/hive/schshim/org/apache/jasper/org/apache/jasper/compiler/org/apache/jasper/compiler/tagplugin/org/apache/jasper/runtime/org/apache/jasper/security/org/apache/jasper/servlet/org/apache/jasper/tagplugins/jstl/org/apache/jasper/tagplugins/jstl/core/org/apache/jasper/util/org/apache/jasper/xmlparser/org/apache/jute/org/apache/jute/compiler/org/apache/jute/compiler/generated/org/apache/zookeeper/org/apache/zookeeper/client/org/apache/zookeeper/common/org/apache/zookeeper/data/org/apache/zookeeper/jmx/org/apache/zookeeper/proto/org/apache/zookeeper/server/org/apache/zookeeper/server/auth/org/apache/zookeeper/server/persistence/org/apache/zookeeper/server/quorum/org/apache/zookeeper/server/quorum/flexible/org/apache/zookeeper/server/upgrade/org/apache/zookeeper/server/util/org/apache/zookeeper/txn/org/apache/zookeeper/version/org/apache/zookeeper/version/util/org/codehaus/jackson/jaxrs/org/codehaus/jackson/xc/org/codehaus/jettison/org/codehaus/jettison/badgerfish/org/codehaus/jettison/json/org/codehaus/jettison/mapped/org/codehaus/jettison/util/org/datanucleus/org/datanucleus/api/org/datanucleus/api/jdo/org/datanucleus/api/jdo/exceptions/org/datanucleus/api/jdo/metadata/org/datanucleus/api/jdo/query/org/datanucleus/api/jdo/state/org/datanucleus/asm/org/datanucleus/cache/org/datanucleus/enhancer/org/datanucleus/enhancer/jdo/org/datanucleus/enhancer/jdo/method/org/datanucleus/enhancer/spi/org/datanucleus/exceptions/org/datanucleus/flush/org/datanucleus/identity/org/datanucleus/management/org/datanucleus/management/jmx/org/datanucleus/metadata/org/datanucleus/metadata/annotations/org/datanucleus/metadata/xml/org/datanucleus/plugin/org/datanucleus/properties/org/datanucleus/query/org/datanucleus/query/cache/org/datanucleus/query/compiler/org/datanucleus/query/evaluator/org/datanucleus/query/evaluator/memory/org/datanucleus/query/expression/org/datanucleus/query/node/org/datanucleus/query/symbol/org/datanucleus/query/typesafe/org/datanucleus/state/org/datanucleus/store/org/datanucleus/store/autostart/org/datanucleus/store/connection/org/datanucleus/store/encryption/org/datanucleus/store/exceptions/org/datanucleus/store/federation/org/datanucleus/store/fieldmanager/org/datanucleus/store/objectvaluegenerator/org/datanucleus/store/query/org/datanucleus/store/query/cache/org/datanucleus/store/rdbms/org/datanucleus/store/rdbms/adapter/org/datanucleus/store/rdbms/autostart/org/datanucleus/store/rdbms/connectionpool/org/datanucleus/store/rdbms/datasource/org/datanucleus/store/rdbms/datasource/dbcp/org/datanucleus/store/rdbms/datasource/dbcp/cpdsadapter/org/datanucleus/store/rdbms/datasource/dbcp/datasources/org/datanucleus/store/rdbms/datasource/dbcp/jocl/org/datanucleus/store/rdbms/datasource/dbcp/managed/org/datanucleus/store/rdbms/datasource/dbcp/pool/org/datanucleus/store/rdbms/datasource/dbcp/pool/impl/org/datanucleus/store/rdbms/exceptions/org/datanucleus/store/rdbms/fieldmanager/org/datanucleus/store/rdbms/identifier/org/datanucleus/store/rdbms/key/org/datanucleus/store/rdbms/mapping/org/datanucleus/store/rdbms/mapping/datastore/org/datanucleus/store/rdbms/mapping/java/org/datanucleus/store/rdbms/mapping/oracle/org/datanucleus/store/rdbms/query/org/datanucleus/store/rdbms/request/org/datanucleus/store/rdbms/schema/org/datanucleus/store/rdbms/scostore/org/datanucleus/store/rdbms/sql/org/datanucleus/store/rdbms/sql/expression/org/datanucleus/store/rdbms/sql/method/org/datanucleus/store/rdbms/sql/operation/org/datanucleus/store/rdbms/table/org/datanucleus/store/rdbms/valuegenerator/org/datanucleus/store/schema/org/datanucleus/store/schema/naming/org/datanucleus/store/schema/table/org/datanucleus/store/scostore/org/datanucleus/store/types/org/datanucleus/store/types/backed/org/datanucleus/store/types/converters/org/datanucleus/store/types/simple/org/datanucleus/store/valuegenerator/org/datanucleus/transaction/org/datanucleus/transaction/jta/org/datanucleus/util/org/datanucleus/validation/org/fusesource/hawtjni/runtime/org/fusesource/jansi/org/fusesource/jansi/internal/org/fusesource/leveldbjni/org/fusesource/leveldbjni/internal/org/htrace/org/htrace/impl/org/htrace/wrappers/org/iq80/leveldb/org/jboss/netty/bootstrap/org/jboss/netty/buffer/org/jboss/netty/channel/org/jboss/netty/channel/group/org/jboss/netty/channel/local/org/jboss/netty/channel/socket/org/jboss/netty/channel/socket/http/org/jboss/netty/channel/socket/nio/org/jboss/netty/channel/socket/oio/org/jboss/netty/container/microcontainer/org/jboss/netty/container/osgi/org/jboss/netty/container/spring/org/jboss/netty/handler/codec/org/jboss/netty/handler/codec/base64/org/jboss/netty/handler/codec/compression/org/jboss/netty/handler/codec/embedder/org/jboss/netty/handler/codec/frame/org/jboss/netty/handler/codec/http/org/jboss/netty/handler/codec/http/multipart/org/jboss/netty/handler/codec/http/websocket/org/jboss/netty/handler/codec/http/websocketx/org/jboss/netty/handler/codec/marshalling/org/jboss/netty/handler/codec/oneone/org/jboss/netty/handler/codec/protobuf/org/jboss/netty/handler/codec/replay/org/jboss/netty/handler/codec/rtsp/org/jboss/netty/handler/codec/serialization/org/jboss/netty/handler/codec/socks/org/jboss/netty/handler/codec/spdy/org/jboss/netty/handler/codec/string/org/jboss/netty/handler/execution/org/jboss/netty/handler/ipfilter/org/jboss/netty/handler/logging/org/jboss/netty/handler/queue/org/jboss/netty/handler/ssl/org/jboss/netty/handler/stream/org/jboss/netty/handler/timeout/org/jboss/netty/handler/traffic/org/jboss/netty/logging/org/jboss/netty/util/org/jboss/netty/util/internal/org/jboss/netty/util/internal/jzlib/org/jets3t/service/org/jets3t/service/acl/org/jets3t/service/acl/gs/org/jets3t/service/impl/rest/org/jets3t/service/impl/rest/httpclient/org/jets3t/service/io/org/jets3t/service/model/org/jets3t/service/model/cloudfront/org/jets3t/service/model/container/org/jets3t/service/multi/org/jets3t/service/multi/event/org/jets3t/service/multi/s3/org/jets3t/service/multithread/org/jets3t/service/mx/org/jets3t/service/security/org/jets3t/service/utils/org/jets3t/service/utils/gatekeeper/org/jets3t/service/utils/oauth/org/jets3t/service/utils/signedurl/org/mortbay/component/org/mortbay/io/org/mortbay/io/bio/org/mortbay/io/nio/org/mortbay/jetty/org/mortbay/jetty/bio/org/mortbay/jetty/deployer/org/mortbay/jetty/handler/org/mortbay/jetty/nio/org/mortbay/jetty/security/org/mortbay/jetty/servlet/org/mortbay/jetty/webapp/org/mortbay/log/org/mortbay/resource/org/mortbay/servlet/org/mortbay/servlet/jetty/org/mortbay/thread/org/mortbay/util/org/mortbay/util/ajax/org/mortbay/xml/org/objectweb/asm/org/objectweb/asm/commons/org/objectweb/asm/signature/org/objectweb/asm/tree/org/xerial/snappy/org/znerd/xmlenc/org/znerd/xmlenc/sax/schema/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9659" opendate="2015-2-12 00:00:00" fixdate="2015-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Error while trying to create table container&amp;#39; occurs during hive query case execution when hive.optimize.skewjoin set to &amp;#39;true&amp;#39; [Spark Branch]</summary>
      <description>We found that 'Error while trying to create table container' occurs during Big-Bench Q12 case execution when hive.optimize.skewjoin set to 'true'.If hive.optimize.skewjoin set to 'false', the case could pass.How to reproduce:1. set hive.optimize.skewjoin=true;2. Run BigBench case Q12 and it will fail. Check the executor log (e.g. /usr/lib/spark/work/app-XXXX/2/stderr) and you will found error 'Error while trying to create table container' in the log and also a NullPointerException near the end of the log.(a) Detail error message for 'Error while trying to create table container':15/02/12 01:29:49 ERROR SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Error while trying to create table containerorg.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Error while trying to create table container at org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.load(HashTableLoader.java:118) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:193) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:219) at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1051) at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055) at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055) at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:486) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:47) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:98) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:217) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:65) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error while trying to create table container at org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.load(MapJoinTableContainerSerDe.java:158) at org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.load(HashTableLoader.java:115) ... 21 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error, not a directory: hdfs://bhx1:8020/tmp/hive/root/d22ef465-bff5-4edb-a822-0a9f1c25b66c/hive_2015-02-12_01-28-10_008_6897031694580088767-1/-mr-10009/HashTable-Stage-6/MapJoin-mapfile01--.hashtable at org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.load(MapJoinTableContainerSerDe.java:106) ... 22 more15/02/12 01:29:49 INFO SparkRecordHandler: maximum memory = 4093902848015/02/12 01:29:49 INFO PerfLogger: &lt;PERFLOG method=SparkInitializeOperators from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler&gt;(b) Detail error message for NullPointerException:5/02/12 01:29:50 ERROR MapJoinOperator: Unexpected exception: nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.MapJoinOperator.setMapJoinKey(MapJoinOperator.java:227) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:271) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:47) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:98) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:217) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:65) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)15/02/12 01:29:50 INFO Executor: Executor is trying to kill task 144.2 in stage 3.0 (TID 1500)15/02/12 01:29:50 INFO MapOperator: Initializing Self MAP[1800]15/02/12 01:29:50 ERROR SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"wcs_click_date_sk":37793,"wcs_click_time_sk":null,"wcs_sales_sk":null,"wcs_item_sk":51402,"wcs_web_page_sk":null,"wcs_user_sk":2541920}org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"wcs_click_date_sk":37793,"wcs_click_time_sk":null,"wcs_sales_sk":null,"wcs_item_sk":51402,"wcs_web_page_sk":null,"wcs_user_sk":2541920} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:47) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:98) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:217) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:65) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) ... 14 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.MapJoinOperator.setMapJoinKey(MapJoinOperator.java:227) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:271) ... 20 more15/02/12 01:29:50 INFO MapOperator: MAP[1797]: records read - 115/02/12 01:29:50 INFO Executor: Executor is trying to kill task 96.3 in stage 3.0 (TID 1515)15/02/12 01:29:50 INFO PerfLogger: &lt;PERFLOG method=SparkInitializeOperators from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler&gt;15/02/12 01:29:50 INFO MapOperator: Initialization Done 1800 MAP15/02/12 01:29:50 INFO SparkRecordHandler: processing 1 rows: used memory = 1202378261615/02/12 01:29:50 ERROR Executor: Exception in task 16.2 in stage 3.0 (TID 1488)java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"wcs_click_date_sk":37793,"wcs_click_time_sk":null,"wcs_sales_sk":null,"wcs_item_sk":51402,"wcs_web_page_sk":null,"wcs_user_sk":2541920} at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:153) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:47) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:98) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:217) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:65) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"wcs_click_date_sk":37793,"wcs_click_time_sk":null,"wcs_sales_sk":null,"wcs_item_sk":51402,"wcs_web_page_sk":null,"wcs_user_sk":2541920} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141) ... 13 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) ... 14 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.MapJoinOperator.setMapJoinKey(MapJoinOperator.java:227) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:271) ... 20 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkMapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.PreOrderWalker.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="966" opendate="2009-12-2 00:00:00" fixdate="2009-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive command line should output log messages in 24-hour format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9661" opendate="2015-2-12 00:00:00" fixdate="2015-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refine debug log with schema information for the method of creating session directories</summary>
      <description>For a session, the scratch directory can be either a local path or a hdfs scratch path. The method name createRootHDFSDir is quite confusing. So add the schema information to the debug log for the troubleshooting need.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="9892" opendate="2015-3-7 00:00:00" fixdate="2015-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>various MSSQL upgrade scripts don&amp;#39;t work</summary>
      <description>Issue with GO statement when run through schematool - it results in syntax error. the create if not exists logic for PART_COL_STATS wasn't workingNO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.005-HIVE-9296.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.004-HIVE-8550.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.002-HIVE-7784.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.pre-0-upgrade-0.12.0-to-0.13.0.mssql.sql</file>
    </fixedFiles>
  </bug>
</bugrepository>
