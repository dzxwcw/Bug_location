<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="2268" opendate="2011-3-3 00:00:00" fixdate="2011-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CQL-enabled stress.java</summary>
      <description>It would be great if stress.java had a CQL mode. For making the inevitable RPC-&gt;CQL comparisons, but also as a basis for measuring optimizations, and spotting performance regressions.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.util.Operation.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.StressAction.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.Session.java</file>
    </fixedFiles>
  </bug>
  <bug id="3220" opendate="2011-9-16 00:00:00" fixdate="2011-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add describe_ring to cli</summary>
      <description>Lately I have found the describe_ring feature was needed to debug/analyze issue, but the cli does not have this available.So just in case it is useful, please see the attached patch.here is the sample output:[default@unknown] help;......decr Decrements a counter column.describe ring Describe the token range information.describe cluster Describe the cluster configuration.......[default@unknown] help describe ring;describe ring &lt;keyspace&gt;;Describes the token range settings for the named keyspace.Required Parameters:- keyspace: Name of the keyspace to describe the token range.Examples:describe ring &lt;keyspace&gt;; - Describes the token range settings for the named keyspace.[default@unknown] describe ring Keyspace3;TokenRange: TokenRange(start_token:9739248273232290250409572410247679660, end_token:9739248273232290250409572410247679660, endpoints:[192.168.0.125], rpc_endpoints:[192.168.0.125], endpoint_details:[EndpointDetails(host:192.168.0.125, port:9160, datacenter:168)])[default@unknown] describe ring fooks;Keyspace with name 'fooks' wasn't found, , please, authorize to one of the keyspaces first.[default@unknown] describe ring;Syntax error at position 13: mismatched input ';' expecting set null</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeProbe.java</file>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
      <file type="M">src.java.org.apache.cassandra.thrift.CassandraServer.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageServiceMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.resources.org.apache.cassandra.cli.CliHelp.yaml</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.Cli.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3337" opendate="2011-10-8 00:00:00" fixdate="2011-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a way to obliterate nodes from the ring</summary>
      <description>Sometimes you just want a token gone: no re-replication, nothing, just excise it.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.gms.GossiperMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
    </fixedFiles>
  </bug>
  <bug id="3419" opendate="2011-10-28 00:00:00" fixdate="2011-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CQL queries should allow CF names to be qualified by keyspace, part 2</summary>
      <description>See CASSANDRA-3130. The same treatment should be applied to the other related CQL commands dealing with column family names, viz: INSERT UPDATE DELETE TRUNCATEAlso, if the intent is to make it possible to go without USE entirely (I'm not sure if it is): CREATE COLUMNFAMILY CREATE INDEX DROP COLUMNFAMILY ALTER COLUMNFAMILY DROP INDEX (support keyspacename.indexname?)</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql.UpdateStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.QueryProcessor.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.DeleteStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.Cql.g</file>
      <file type="M">src.java.org.apache.cassandra.cql.AbstractModification.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3422" opendate="2011-10-29 00:00:00" fixdate="2011-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can create a Column Family with comparator CounterColumnType which is subsequently unusable</summary>
      <description>It's probably the case that this shouldn't be allowed at all but one is currently allowed to create a Column Family with comparator CounterColumnType which then appears unusable.CREATE COLUMNFAMILY comparator_cf_counter (id text PRIMARY KEY) WITH comparator=CounterColumnType FailsUPDATE comparator_cf_counter SET 1=1 + 1 WHERE id='test_key'Error =&gt; invalid operation for non commutative columnfamily comparator_cf_counter</description>
      <version>0.8.9,1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql.CreateColumnFamilyStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3457" opendate="2011-11-4 00:00:00" fixdate="2011-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make cqlsh look for a suitable python version</summary>
      <description>On RHEL 5, which I guess we still want to support, the default "python" in the path is still 2.4. cqlsh does use a fair number of python features introduced in 2.5, like collections.defaultdict, functools.partial, generators. We can require RHEL 5 users to install a later python from EPEL, but we'd have to call it as 'python2.5', or 'python2.6', etc.So rather than take the time to vet everything against python2.4, we may want to make a wrapper script for cqlsh that checks for the existence of python2.7, 2.6, and 2.5, and calls the appropriate one to run the real cqlsh.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.cqlsh</file>
    </fixedFiles>
  </bug>
  <bug id="3489" opendate="2011-11-13 00:00:00" fixdate="2011-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EncryptionOptions should be instantiated</summary>
      <description>As the title says, otherwise you get an NPE when the options are missing from the yaml. It's included in my second patch on CASSANDRA-3045 and is a one line fix.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.net.MessagingService.java</file>
      <file type="M">src.java.org.apache.cassandra.config.Config.java</file>
    </fixedFiles>
  </bug>
  <bug id="3540" opendate="2011-11-29 00:00:00" fixdate="2011-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong check of partitioner for secondary indexes</summary>
      <description>CASSANDRA-3407 doesn't handle the fact that secondary indexes have a specific partitioner (LocalPartitioner). This result in the following error when starting nodes in 1.0.4:java.lang.RuntimeException: Cannot open /var/lib/cassandra/data/Index/AttractionLocationCategoryDateIdx.AttractionLocationCategoryDateIdx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.LocalPartitioner</description>
      <version>1.0.6</version>
      <fixedVersion>Feature/2iIndex</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.io.sstable.SSTableReaderTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.io.sstable.SSTableMetadataSerializerTest.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableWriter.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableMetadata.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3541" opendate="2011-11-29 00:00:00" fixdate="2011-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support timeuuid column names</summary>
      <description>Real-world Cassandra applications often use "wide rows" to denormalize queries into. Most often, this means they do a lot of appending to existing rows, with few overwrites. An easy way to add this to Stress would be to allow specifying timeuuid column names (which will be inherently sequential, or nearly so). For forwards-compatibility, we could add a --comparator option that only supports the existing Ascii type and the proposed UUID type, to start with.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.CqlInserter.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.util.Operation.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.Session.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.Inserter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3551" opendate="2011-12-1 00:00:00" fixdate="2011-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timeout exception for quorum reads after upgrade from 1.0.2 to 1.0.5</summary>
      <description>I upgraded from 1.0.2 to 1.0.5. For some column families always got TimeoutException. I turned on debug and increase rpc_timeout to 1 minute, but still got timeout. I believe it is bug on 1.0.5.ConsistencyLevel is QUORUM, replicate factor is 3. Here are partial logs. DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,717 StorageProxy.java (line 813) RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/999/99999], max_keys=1024}DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,718 StorageProxy.java (line 1012) restricted ranges for query &amp;#91;PROD/US/000/0,PROD/US/999/99999&amp;#93; are [&amp;#91;PROD/US/000/0,PROD/US/300/~&amp;#93;, (PROD/US/300/~,PROD/US/600/], (PROD/US/600/,PROD/US/999/99999]]DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 157) ReplicationFactor 3DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 33) PROD/US/300/~DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/300/~ 10.92.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/600/~ 10.72.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/999/~ 10.8.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,723 VoxeoStrategy.java (line 157) ReplicationFactor 3DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,724 ReadCallback.java (line 77) Blockfor/repair is 2/false; setting up requests to /10.92.208.103,/10.72.208.103DEBUG &amp;#91;WRITE-/10.92.208.103&amp;#93; 2011-12-01 22:25:39,725 OutboundTcpConnection.java (line 206) attempting to connect to /10.92.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.92.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.72.208.103DEBUG &amp;#91;WRITE-/10.8.208.103&amp;#93; 2011-12-01 22:25:39,727 OutboundTcpConnection.java (line 206) attempting to connect to /10.8.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,727 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.8.208.103DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 0 of 1024: active:false:1@1322777621601000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 1 of 1024: name:false:4@1322777621601000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 2 of 1024: providerData:false:2283@1321549067179000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 3 of 1024: providerID:false:1@1322777621601000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 4 of 1024: timestamp:false:13@1322777621601000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 5 of 1024: vendorData:false:2364@1322777621601000DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,733 ColumnFamilyStore.java (line 1331) scanned DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)DEBUG &amp;#91;ReadStage:1&amp;#93; 2011-12-01 22:25:39,733 RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=Row(key=DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31), cf=ColumnFamily(dataProvider [active:false:1@1322777621601000,name:false:4@1322777621601000,providerData:false:2283@1321549067179000,providerID:false:1@1322777621601000,timestamp:false:13@1322777621601000,vendorData:false:2364@1322777621601000,]))} to 72@/10.72.208.103DEBUG &amp;#91;RequestResponseStage:1&amp;#93; 2011-12-01 22:25:39,734 ResponseVerbHandler.java (line 44) Processing response on a callback from 72@/10.72.208.103DEBUG &amp;#91;RequestResponseStage:2&amp;#93; 2011-12-01 22:25:39,887 ResponseVerbHandler.java (line 44) Processing response on a callback from 71@/10.92.208.103DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,889 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: active:false:1@1322777621601000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: name:false:4@1322777621601000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 2 of 2147483647: providerData:false:2283@1321549067179000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 3 of 2147483647: providerID:false:1@1322777621601000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 4 of 2147483647: timestamp:false:13@1322777621601000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,891 SliceQueryFilter.java (line 123) collecting 5 of 2147483647: vendorData:false:2364@1322777621601000DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:25:39,892 StorageProxy.java (line 867) range slices read DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)DEBUG &amp;#91;RequestResponseStage:3&amp;#93; 2011-12-01 22:25:39,936 ResponseVerbHandler.java (line 44) Processing response on a callback from 73@/10.8.208.103DEBUG &amp;#91;ScheduledTasks:1&amp;#93; 2011-12-01 22:26:19,788 LoadBroadcaster.java (line 86) Disseminating load info ...DEBUG &amp;#91;pool-2-thread-1&amp;#93; 2011-12-01 22:26:39,904 StorageProxy.java (line 874) Range slice timeout: java.util.concurrent.TimeoutException: Operation timed out.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.ReadRepairVerbHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3556" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool info reports inaccurate datacenter/rack for localhost</summary>
      <description>The datacenter &amp; rack information provided by 'nodetool info' is incorrect when using 'nodetool -h localhost info'. This is because the IP address passed to the EndpointSnitch to determine the datacenter &amp; rack is sourced from the host parameter provided to nodetool and not the actual endpoint address used in the ring.</description>
      <version>0.8.9,1.0.6</version>
      <fixedVersion>Tool/nodetool</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeProbe.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3558" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compression chunk_length_kb is not correctly returned for thrift/avro</summary>
      <description>CASSANDRA-3492 fixed the interpretation of chunk_length_kb as a size in bytes but infortunately forgot to convert it back to kb when returning it for thrift/avro. In particular, this means that a describe cf would return things like chunk_length_kb: 65535.I'm afraid that because migration uses Avro this is kind of a problem. One may have to issue an 'update column family' with the right chunk_length_kb to be sure to be in a safe place.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.compress.CompressionParameters.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3563" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Packaging should increase vm.max_map_count to accommodate leveled compaction</summary>
      <description>As the title says, leveled can create a lot of files and you can run into an IOError trying to mmap all of them.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">debian.rules</file>
      <file type="M">debian.cassandra.postinst</file>
      <file type="M">debian.cassandra.install</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3584" opendate="2011-12-7 00:00:00" fixdate="2011-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for 0.0.0.0 is incorrect</summary>
      <description>As noted by Jake in the comments to CASSANDRA-3214, we are using == for a String comparison.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.ColumnFamilyInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3586" opendate="2011-12-7 00:00:00" fixdate="2011-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new SHOW command in CQL shell to list column families</summary>
      <description>add command SHOW COLUMN FAMILIES to list all column families in a current keyspace by name.</description>
      <version>None</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.cqlsh</file>
    </fixedFiles>
  </bug>
  <bug id="3587" opendate="2011-12-7 00:00:00" fixdate="2011-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DESC alias for DESCRIBE keyword in CQL shell</summary>
      <description>Allow DESC to be used instead of full DESCRIBE keyword. MySQL and Oracle users are used to DESC.</description>
      <version>None</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.cqlsh</file>
    </fixedFiles>
  </bug>
  <bug id="3588" opendate="2011-12-7 00:00:00" fixdate="2011-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cqlsh: HELP for DELETE_USING and DELETE_COLUMNS broken</summary>
      <description>type "HELP DELETE_USING" or "HELP DELETE_COLUMNS" in cqlsh. both of those are referred to by the HELP index, by tab-completion after "HELP", and in the help text for DELETE.nothing is shown but a new command prompt.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.cqlsh</file>
    </fixedFiles>
  </bug>
  <bug id="3591" opendate="2011-12-8 00:00:00" fixdate="2011-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>a better exception than "Keyspace names must be case-insensitively unique"</summary>
      <description>The exception "Keyspace names must be case-insensitively unique" does not give the implied names.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.thrift.ThriftValidation.java</file>
    </fixedFiles>
  </bug>
  <bug id="3596" opendate="2011-12-8 00:00:00" fixdate="2011-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cqlsh: DESCRIBE output for a columnfamily does not work as input to same C* instance</summary>
      <description>The DESCRIBE COLUMNFAMILY cqlsh command produces output that is intended to be usable as valid CQL (at least, when given to another Cassandra instance of the same version). But the output yields errors when run:cqlsh&gt; USE blah;cqlsh:blah&gt; CREATE COLUMNFAMILY cf1 (c1 int PRIMARY KEY, c2 varchar);cqlsh:blah&gt; DESCRIBE COLUMNFAMILY cf1;CREATE COLUMNFAMILY cf1 ( c1 int PRIMARY KEY, c2 text) WITH comment='' AND comparator=text AND row_cache_provider='ConcurrentLinkedHashCacheProvider' AND key_cache_size=200000.000000 AND row_cache_size=0.000000 AND read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND default_validation=text AND min_compaction_threshold=4 AND max_compaction_threshold=32 AND row_cache_save_period_in_seconds=0 AND key_cache_save_period_in_seconds=14400 AND replication_on_write=True;cqlsh:blah&gt; CREATE COLUMNFAMILY cf1 ( ... c1 int PRIMARY KEY, ... c2 text ... ) WITH ... comment='' AND ... comparator=text AND ... row_cache_provider='ConcurrentLinkedHashCacheProvider' AND ... key_cache_size=200000.000000 AND ... row_cache_size=0.000000 AND ... read_repair_chance=0.100000 AND ... gc_grace_seconds=864000 AND ... default_validation=text AND ... min_compaction_threshold=4 AND ... max_compaction_threshold=32 AND ... row_cache_save_period_in_seconds=0 AND ... key_cache_save_period_in_seconds=14400 AND ... replication_on_write=True;Bad Request: replication_on_write is not a valid keyword argument for CREATE COLUMNFAMILYSo it needs to do a better job of determining which CF attributes are valid for which C* versions.</description>
      <version>1.0.6</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.cqlhandling.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3598" opendate="2011-12-8 00:00:00" fixdate="2011-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Index Scan&amp;#39;s will span across multiple DC&amp;#39;s</summary>
      <description>Looks like we send requests to all the nodes provided by StorageService.instance.getLiveNaturalEndpoints(keyspace, range.right);We dont filter it based on blockedFor (Consistency levels).In a multi DC setup this will cause unnecessary load on the other DC. And even within a DC we might query more nodes than needed.</description>
      <version>0.7.4,0.8.9,1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageProxy.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3601" opendate="2011-12-9 00:00:00" fixdate="2011-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_count NullPointerException with counters</summary>
      <description>get_count doesn't currently work for counter columns or super counter columns. The fix seems to be pretty simple.</description>
      <version>1.0.6</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.thrift.CassandraServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3602" opendate="2011-12-9 00:00:00" fixdate="2011-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove test/distributed</summary>
      <description>Now that we've shifted focus to the new ccm-based distributed tests (https://github.com/riptano/cassandra-dtest) I think it's time to remove the now long-neglected distributed test cruft from our tree.</description>
      <version>1.0.6,1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.distributed.README.txt</file>
      <file type="M">test.distributed.org.apache.cassandra.utils.KeyPair.java</file>
      <file type="M">test.distributed.org.apache.cassandra.utils.BlobUtils.java</file>
      <file type="M">test.distributed.org.apache.cassandra.TestBase.java</file>
      <file type="M">test.distributed.org.apache.cassandra.MutationTest.java</file>
      <file type="M">test.distributed.org.apache.cassandra.MovementTest.java</file>
      <file type="M">test.distributed.org.apache.cassandra.CountersTest.java</file>
      <file type="M">test.distributed.org.apache.cassandra.CassandraServiceController.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
