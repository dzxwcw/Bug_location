<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="3916" opendate="2012-2-15 00:00:00" fixdate="2012-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not bind the storage_port if internode_encryption = all</summary>
      <description>We are highly security conscious and having additional clear text ports open are undesirable.I have modified locally to get around but it seems that this is a very trivial fix to only bind the clear text storage_port if the internode_encryption is not all. If all is selected then no clear text communication should be permitted.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.net.MessagingService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4549" opendate="2012-8-16 00:00:00" fixdate="2012-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the pig examples to include more recent pig/cassandra features</summary>
      <description>Now that there is support for a variety of Cassandra features from Pig (esp 1.1+), it would great to have some of them in the examples so that people can see how to use them.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">examples.pig.example-script.pig</file>
    </fixedFiles>
  </bug>
  <bug id="5084" opendate="2012-12-21 00:00:00" fixdate="2012-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra should expose connected client state via JMX</summary>
      <description>There is currently no good way to determine or estimate how many clients are connected to a cassandra node without using netstat or (if using sync thrift server) counting threads. There is also no way to understand what state any given connection is in. People regularly come into #cassandra/cassandra-user@ and ask how to get the equivalent of a MySQL "SHOW FULL PROCESSLIST."While I understand that feature parity with SHOW FULL PROCESSLIST/information_schema.processlist is unlikely, even a few basic metrics like "number of connected clients" or "number of active clients" would greatly help with this operational information need.</description>
      <version>1.2.11</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.transport.Server.java</file>
      <file type="M">src.java.org.apache.cassandra.thrift.ThriftSessionManager.java</file>
      <file type="M">src.java.org.apache.cassandra.thrift.CassandraServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5695" opendate="2013-6-24 00:00:00" fixdate="2013-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert pig smoke tests into real PigUnit tests</summary>
      <description>Currently, we have some ghetto pig tests in examples/pig/test, but there's currently no way to continuously integrate these since a human needs to check that the output isn't wrong, not just that the tests ran successfully. We've had garbled output problems in the past, so it would be nice to formalize our tests to catch this. PigUnit appears to be a good choice for this.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.conf.log4j-junit.properties</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.CqlStorage.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5732" opendate="2013-7-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not query secondary index</summary>
      <description>Noticed after taking a column family that already existed and assigning to an IntegerType index_type:KEYS and the caching was already set to 'ALL' that the prepared statement do not return rows neither did it throw an exception. Here is the sequence.1. Starting state query running with caching off for a Column Family with the query using the secondary index for te WHERE clause.2, Set Column Family caching to ALL using Cassandra-CLI and update CQL. Cassandra-cli Describe shows column family caching set to ALL3. Rerun query and it works.4. Restart Cassandra and run query and no rows returned. Cassandra-cli Describe shows column family caching set to ALL5. Set Column Family caching to NONE using Cassandra-cli and update CQL. Rerun query and no rows returned. Cassandra-cli Describe for column family shows caching set to NONE.6. Restart Cassandra. Rerun query and it is working again. We are now back to the starting state.Best Regards,-Tony</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>Feature/2iIndex</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5752" opendate="2013-7-12 00:00:00" fixdate="2013-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift tables are not supported from CqlPagingInputFormat</summary>
      <description>CqlPagingInputFormat inspects the system schema to generate the WHERE clauses needed to page "wide rows," but for a classic Thrift table there are no entries for the "default" column names of key, column1, column2, ..., value so CPIF breaks.</description>
      <version>1.2.11</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.cql3.CqlRecordWriter.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5865" opendate="2013-8-8 00:00:00" fixdate="2013-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when you mistakenly set listen_address to 0.0.0.0</summary>
      <description>It's clearly stated that setting listen_address to 0.0.0.0 is always wrong. But if you mistakenly do it anyway you end up with an NPE on 1.2.8 while it's not the case on 2.0.0-rc1. See bellow: INFO 16:34:43,598 JOINING: waiting for ring information INFO 16:34:44,505 Handshaking version with /127.0.0.1 INFO 16:34:44,533 Handshaking version with /0.0.0.0 INFO 16:35:13,626 JOINING: schema complete, ready to bootstrap INFO 16:35:13,631 JOINING: getting bootstrap tokenERROR 16:35:13,633 Exception encountered during startupjava.lang.RuntimeException: No other nodes seen! Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed. Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster. Usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154) at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135) at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115) at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666) at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554) at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451) at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348) at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447) at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)java.lang.RuntimeException: No other nodes seen! Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed. Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster. Usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154) at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135) at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115) at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666) at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554) at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451) at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348) at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447) at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)Exception encountered during startup: No other nodes seen! Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed. Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster. Usually, this can be solved by giving all nodes the same seed list.ERROR 16:35:13,668 Exception in thread Thread[StorageServiceShutdownHook,5,main]java.lang.NullPointerException at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321) at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370) at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88) at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:519) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) at java.lang.Thread.run(Thread.java:724)</description>
      <version>1.2.11</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5889" opendate="2013-8-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add tombstone metrics to cfstats or cfhistograms</summary>
      <description>/cc pmcfadin</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStoreMBean.java</file>
    </fixedFiles>
  </bug>
  <bug id="5916" opendate="2013-8-21 00:00:00" fixdate="2013-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>gossip and tokenMetadata get hostId out of sync on failed replace_node with the same IP address</summary>
      <description>If you try to replace_node an existing, live hostId, it will error out. However if you're using an existing IP to do this (as in, you chose the wrong uuid to replace on accident) then the newly generated hostId wipes out the old one in TMD, and when you do try to replace it replace_node will complain it does not exist. Examination of gossipinfo still shows the old hostId, however now you can't replace it either.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.GossipDigestSynVerbHandler.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.GossipDigestAckVerbHandler.java</file>
      <file type="M">src.java.org.apache.cassandra.db.SystemTable.java</file>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
      <file type="M">NEWS.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5954" opendate="2013-8-30 00:00:00" fixdate="2013-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make nodetool ring print an error message and suggest nodetool status when vnodes are enabled</summary>
      <description></description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>Tool/nodetool</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
    </fixedFiles>
  </bug>
  <bug id="5980" opendate="2013-9-5 00:00:00" fixdate="2013-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow cache keys-to-save to be dynamically set</summary>
      <description>We have the option {key,row}_cache_keys_to_save so that operators can chose a balance between "can't handle load on cold start" and "takes too long to reboot". However, this can currently only be set at startup. This means that once it has been determined that X is a bad value for keys_to_save another badness inducing restart is required to change X.</description>
      <version>1.2.11,2.0.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.resources.org.apache.cassandra.tools.NodeToolHelp.yaml</file>
      <file type="M">src.java.org.apache.cassandra.tools.NodeProbe.java</file>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
      <file type="M">src.java.org.apache.cassandra.service.CacheServiceMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.service.CacheService.java</file>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6001" opendate="2013-9-10 00:00:00" fixdate="2013-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Tracing line for index/seq scan queries</summary>
      <description>Tracing should show the user why a specific index was selected. Something likeIndex mean cardinality is {users_by_lastname: 100, users_by_state: 2000, users_by_birthdate: 1000}. Scanning with users_by_lastname.</description>
      <version>1.2.11</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.index.keys.KeysSearcher.java</file>
      <file type="M">src.java.org.apache.cassandra.db.index.composites.CompositesSearcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6037" opendate="2013-9-17 00:00:00" fixdate="2013-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parens around WHERE condition break query</summary>
      <description>SELECT * FROM user WHERE (key=&lt;UUID&gt;);Bad Request: line 1:25 no viable alternative at input '('SELECT * FROM user WHERE key=&lt;UUID&gt;; &amp;#8211; No parens&amp;#8211; Normal outputThe example provided is minimal, bug was discovered with AND logic on indexed columns.Parens-enclosed conditions is good SQL and so is produced by database abstraction layers in complex queries to avoid operation precedence problems.Fixing this at application side is no option - this will open the can of logic bugs.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.Cql.g</file>
    </fixedFiles>
  </bug>
  <bug id="6042" opendate="2013-9-17 00:00:00" fixdate="2013-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add WARN when there are a lot of tombstones in a query</summary>
      <description>Now that we count the number of tombstones hit (so it can go in tracing), can we pick some threshold (or make it configurable with 0 being don't warn), and spit out a warning saying "Just went through 10000 tombstones in partition XYZ".Right now if you are having GC problems because some row got a bunch of tombstones, you can turn on server side tracing, and hope the bad query gets in there, or you can keep making heap dumps, dig through them, and hope you catch the query in there.I have seen code problems at multiple places causing this same issue (some code causing way more tombstones than it should, for just one row). And it is a PITA+Luck to debug it right now.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageServiceMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.metrics.ColumnFamilyMetrics.java</file>
      <file type="M">src.java.org.apache.cassandra.db.filter.SliceQueryFilter.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStoreMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
      <file type="M">src.java.org.apache.cassandra.config.Config.java</file>
      <file type="M">conf.cassandra.yaml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6068" opendate="2013-9-19 00:00:00" fixdate="2013-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support login/password auth in cassandra-stress</summary>
      <description>Support login/password auth in cassandra-stress</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.Session.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6076" opendate="2013-9-22 00:00:00" fixdate="2013-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement HELP COMPOSITE_PRIMARY_KEYS</summary>
      <description>HELP COMPOSITE_PRIMARY_KEYS is referenced from HELP CREATE_TABLE, but doesn't exist. added.</description>
      <version>1.2.11</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.helptopics.py</file>
    </fixedFiles>
  </bug>
  <bug id="6107" opendate="2013-9-27 00:00:00" fixdate="2013-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CQL3 Batch statement memory leak</summary>
      <description>We are doing large volume insert/update tests on a CASS via CQL3. Using 4GB heap, after roughly 750,000 updates create/update 75,000 row keys, we run out of heap, and it never dissipates, and we begin getting this infamous error which many people seem to be encountering:WARN &amp;#91;ScheduledTasks:1&amp;#93; 2013-09-26 16:17:10,752 GCInspector.java (line 142) Heap is 0.9383457210434385 full. You may need to reduce memtable and/or cache sizes. Cassandra will now flush up to the two largest memtables to free up memory. Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically INFO &amp;#91;ScheduledTasks:1&amp;#93; 2013-09-26 16:17:10,753 StorageService.java (line 3614) Unable to reduce heap usage since there are no dirty column families8 and 12 GB heaps appear to delay the problem by roughly proportionate amounts of 75,000 - 100,000 rowkeys per 4GB. Each run of 50,000 row key creations sees the heap grow and never shrink again. We have attempted to no effect: removing all secondary indexes to see if that alleviates overuse of bloom filters adjusted parameters for compaction throughput adjusted memtable flush thresholds and other parametersBy examining heapdumps, it seems apparent that the problem is perpetual retention of CQL3 BATCH statements. We have even tried dropping the keyspaces after the updates and the CQL3 statement are still visible in the heapdump, and after many many many CMS GC runs. G1 also showed this issue.The 750,000 statements are broken into batches of roughly 200 statements.</description>
      <version>1.2.11</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.QueryProcessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6122" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the auth default super user/default user check</summary>
      <description>Optimize the auth default super user/default user check by checking for the 'cassandra' user first, and only if that fails, doing the range query.For people following our docs (don't drop 'cassandra', just strip its superuser status and change the password), this will always mean performing just one get.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.auth.PasswordAuthenticator.java</file>
      <file type="M">src.java.org.apache.cassandra.auth.Auth.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6128" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more data mappings for Pig</summary>
      <description>We need add more data mappings for DecimalType InetAddressType LexicalUUIDType TimeUUIDType UUIDTypeExisting implementation throws exception for those data type</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.CqlStorage.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="6152" opendate="2013-10-6 00:00:00" fixdate="2013-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assertion error in 2.0.1 at db.ColumnSerializer.serialize(ColumnSerializer.java:56)</summary>
      <description>ERROR [COMMIT-LOG-WRITER] 2013-10-06 12:12:36,845 CassandraDaemon.java (line 185) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]java.lang.AssertionError at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:56) at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:77) at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:268) at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:229) at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:352) at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:48) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) at java.lang.Thread.run(Thread.java:722)</description>
      <version>1.2.11</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.UpdateParameters.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.QueryProcessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6160" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw errror when attempting to create a secondary index against counter</summary>
      <description>Using CQL you can create a secondary index against a counter which is then non-functional. cqlsh:test&gt; create table test2 (col1 int, col2 counter, primary key (col1)) ;cqlsh:test&gt; create index dodgy on test2(col2) ;cqlsh:test&gt; update test2 set col2 = col2 + 0 where col1 = 1 ;cqlsh:test&gt; select * from test2 ; col1 | col2------+------ 1 | 0cqlsh:t7088&gt; select * from test2 where col2 = 0 ;We should return an error to let users know they are in unsupported territory.</description>
      <version>1.2.11</version>
      <fixedVersion>Feature/2iIndex,Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.statements.CreateIndexStatement.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6164" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch CFS histograms to biased sampling</summary>
      <description></description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.metrics.ColumnFamilyMetrics.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStoreMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6183" opendate="2013-10-11 00:00:00" fixdate="2013-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip mutations that pass CRC but fail to deserialize</summary>
      <description>We've had a couple reports of CL replay failure that appear to be caused by dropping and recreating the same table with a different schema, e.g. CASSANDRA-5905. While CASSANDRA-5202 is the "right" fix for this, it's too involved for 2.0 let alone 1.2, so we need a stopgap until then.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.commitlog.CommitLogReplayer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6191" opendate="2013-10-13 00:00:00" fixdate="2013-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a warning for small sstable size setting in LCS</summary>
      <description>Not sure "bug" is the right description, because I can't say for sure that the large number of SSTables is the cause of the memory issues. I'll share my research so far:Under high read-load with a very large number of compressed SSTables (caused by the initial default 5mb sstable_size in LCS) it seems memory is exhausted, without any room for GC to fix this. It tries to GC but doesn't reclaim much.The node first hits the "emergency valves" flushing all memtables, then reducing caches. And finally logs 0.99+ heap usages and hangs with GC failure or crashes with OutOfMemoryError.I've taken a heapdump and started analysis to find out what's wrong. The memory seems to be used by the byte[] backing the HeapByteBuffer in the "compressed" field of org.apache.cassandra.io.compress.CompressedRandomAccessReader. The byte[] are generally 65536 byes in size, matching the block-size of the compression.Looking further in the heap-dump I can see that these readers are part of the pool in org.apache.cassandra.io.util.CompressedPoolingSegmentedFile. Which is linked to the "dfile" field of org.apache.cassandra.io.sstable.SSTableReader. The dump-file lists 45248 instances of CompressedRandomAccessReader.Is this intended to go this way? Is there a leak somewhere? Or should there be an alternative strategy and/or warning for cases where a node is trying to read far too many SSTables?EDIT:Searching through the code I found that PoolingSegmentedFile keeps a pool of RandomAccessReader for re-use. While the CompressedRandomAccessReader allocates a ByteBuffer in it's constructor and (to make things worse) enlarges it if it's reasing a large chunk. This (sometimes enlarged) ByteBuffer is then kept alive because it becomes part of the CompressedRandomAccessReader which is in turn kept alive as part of the pool in the PoolingSegmentedFile.</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.compaction.LeveledCompactionStrategy.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6197" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create more Pig unit tests for type mappings</summary>
      <description>We need add more Pig unit testing for data type mappings including collections</description>
      <version>1.2.11,2.0.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.pig.CqlStorage.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
