<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="1278" opendate="2010-7-13 00:00:00" fixdate="2010-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make bulk loading into Cassandra less crappy, more pluggable</summary>
      <description>Currently bulk loading into Cassandra is a black art. People are either directed to just do it responsibly with thrift or a higher level client, or they have to explore the contrib/bmt example - http://wiki.apache.org/cassandra/BinaryMemtable That contrib module requires delving into the code to find out how it works and then applying it to the given problem. Using either method, the user also needs to keep in mind that overloading the cluster is possible - which will hopefully be addressed in CASSANDRA-685This improvement would be to create a contrib module or set of documents dealing with bulk loading. Perhaps it could include code in the Core to make it more pluggable for external clients of different types.It is just that this is something that many that are new to Cassandra need to do - bulk load their data into Cassandra.</description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.utils.BloomFilter.java</file>
      <file type="M">src.java.org.apache.cassandra.streaming.StreamOutSession.java</file>
      <file type="M">src.java.org.apache.cassandra.streaming.OperationType.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageServiceMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2231" opendate="2011-2-23 00:00:00" fixdate="2011-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CompositeType comparer to the comparers provided in org.apache.cassandra.db.marshal</summary>
      <description>CompositeType is a custom comparer that makes it possible to create comparable composite values out of the basic types that Cassandra currently supports, such as Long, UUID, etc. This is very useful in both the creation of custom inverted indexes using columns in a skinny row, where each column name is a composite value, and also when using Cassandra's built-in secondary index support, where it can be used to encode the values in the columns that Cassandra indexes. One scenario for the usage of these is documented here: http://www.anuff.com/2010/07/secondary-indexes-in-cassandra.html. Source for contribution is attached and has been previously maintained on github here: https://github.com/edanuff/CassandraCompositeType</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.SchemaLoader.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.TypeParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2267" opendate="2011-3-3 00:00:00" fixdate="2011-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stress.java daemon mode</summary>
      <description>stress.java uses a JVM, but there is no way to warm it up, which skews results. It would be nice if there was some sort of daemon mode so the JVM could stay hot, and even better if there was a way to control the daemon programmatically, perhaps via JMX.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.util.Operation.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.Stress.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.Session.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.Reader.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.RangeSlicer.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.MultiGetter.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.Inserter.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.IndexedRangeSlicer.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.CounterGetter.java</file>
      <file type="M">tools.stress.src.org.apache.cassandra.stress.operations.CounterAdder.java</file>
    </fixedFiles>
  </bug>
  <bug id="2355" opendate="2011-3-18 00:00:00" fixdate="2011-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow parameters for comparator</summary>
      <description>Being able to provide arguments to a comparator, to parametrize its behavior, can be useful.For example, a generic ReverseComparator is trivial to write, but without the ability to provide arguments at the comparator constriction, it cannot be used.CASSANDRA-2231 provides another use case for this.This ticket proposes to introduce a parser for comparator definition that supports optional parameters and to introduce a generic ReverseType as a simple example of this.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.marshal.ReversedType.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.FBUtilities.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.AbstractType.java</file>
      <file type="M">src.java.org.apache.cassandra.config.ColumnDefinition.java</file>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2473" opendate="2011-4-14 00:00:00" fixdate="2011-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CQL support for counters</summary>
      <description></description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.system.test.cql.py</file>
      <file type="M">src.java.org.apache.cassandra.cql.UpdateStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.QueryProcessor.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.DeleteStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.CreateColumnFamilyStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.Cql.g</file>
      <file type="M">src.java.org.apache.cassandra.cql.BatchStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.AbstractModification.java</file>
      <file type="M">drivers.py.cql.marshal.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2530" opendate="2011-4-21 00:00:00" fixdate="2011-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional AbstractType data type definitions to enrich CQL</summary>
      <description>Provide 5 additional Datatypes: ByteType, DateType, BooleanType, FloatType, DoubleType.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql.Term.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.CreateColumnFamilyStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.Cql.g</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.cassandra.bat</file>
    </fixedFiles>
  </bug>
  <bug id="2531" opendate="2011-4-21 00:00:00" fixdate="2011-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>There should be an easy way to see what DC and rack a node belongs to</summary>
      <description>It would be especially nice to see them in nodetool ring, but I would settle for having it listed in nodetool info.</description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeProbe.java</file>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
    </fixedFiles>
  </bug>
  <bug id="2545" opendate="2011-4-23 00:00:00" fixdate="2011-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CQL: cqlsh error running batch update commands</summary>
      <description>CQL Test Case//TEST CASE #1BEGIN BATCHUPDATE users SET gender = 'm', birth_year = '1981' WHERE KEY = 'user1';UPDATE users SET gender = 'm', birth_year = '1982' WHERE KEY = 'user2';UPDATE users SET gender = 'm', birth_year = '1983' WHERE KEY = 'user3';APPLY BATCH //TEST CASE #2BEGIN BATCH USING CONSISTENCY ZEROUPDATE users SET state = 'TX' WHERE KEY = 'user1';UPDATE users SET state = 'TX' WHERE KEY = 'user2';UPDATE users SET state = 'TX' WHERE KEY = 'user3';APPLY BATCH //ERRORBad Request: line 0:-1 mismatched input '&lt;EOF&gt;' expecting K_APPLYTest SetupCREATE COLUMNFAMILY users ( KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint);INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');Documented SyntaxBEGIN BATCH &amp;#91;USING &lt;CONSISTENCY&gt;&amp;#93;UPDATE CF1 SET name1 = value1, name2 = value2 WHERE KEY = keyname1;UPDATE CF1 SET name3 = value3 WHERE KEY = keyname2;UPDATE CF2 SET name4 = value4, name5 = value5 WHERE KEY = keyname3;APPLY BATCH</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">drivers.py.cqlsh</file>
    </fixedFiles>
  </bug>
  <bug id="2583" opendate="2011-4-29 00:00:00" fixdate="2011-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize batches containing multiple updates to the same rows</summary>
      <description>Currently we turn batches into one RowMutation per statement. This is suboptimal if we have multiple statements operating on the same row. This could happen either with generated code, or deliberately to allow different options to different columns in the row, e.g. for TTLs (CASSANDRA-2476).</description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql.UpdateStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.DeleteStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.BatchStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.AbstractModification.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2616" opendate="2011-5-6 00:00:00" fixdate="2011-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "DROP INDEX" command to CLI</summary>
      <description></description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cli.CliTest.java</file>
      <file type="M">src.resources.org.apache.cassandra.cli.CliHelp.yaml</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.Cli.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2631" opendate="2011-5-10 00:00:00" fixdate="2011-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replaying a commitlog entry from a dropped keyspace will cause an error</summary>
      <description></description>
      <version>0.7.6,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.commitlog.CommitLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2637" opendate="2011-5-11 00:00:00" fixdate="2011-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bloom filter true positives not counted unless key cache is enabled</summary>
      <description></description>
      <version>0.7.6,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2644" opendate="2011-5-12 00:00:00" fixdate="2011-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make bootstrap retry</summary>
      <description>We ran into a situation where we had rpc_timeout set to 1 second, and the node needing to compute the token took over a second (1.6 seconds). The bootstrapping node hangs forever without getting a token because the expiring map removes it before the reply comes back.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.dht.BootStrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2660" opendate="2011-5-17 00:00:00" fixdate="2011-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BRAF.sync() bug can cause massive commit log write magnification</summary>
      <description>This was discovered, fixed and tested on 0.7.5. Cursory examination shows it should still be an issue on trunk/0.8. If people otherwise agree with the patch I can rebase if necessary.Problem:BRAF.flush() is actually broken in the sense that it cannot be called without close co-operation with the caller. rebuffer() does the co-op by adjusting bufferOffset and validateBufferBytes appropriately, by sync() doesn't. This means sync() is broken, and sync() is used by the commit log.The attached patch moves the bufferOffset/validateBufferBytes handling out into resetBuffer() and has both flush() and rebuffer() call that. This makes sync() safe.What happened was that for batched commit log mode, every time sync() was called the data buffered so far would get written to the OS and fsync():ed. But until rebuffer() is called for other reasons as part of the write path, all subsequent sync():s would result in the very same data (plus whatever was written since last time) being re-written and fsync():ed again. So first you write+fsync N bytes, then N+N1, then N+N1+N2... (each N being a batch), until at some point you trigger a rebuffer() and it starts all over again.The result is that you see a lot more writes to the commit log than are in fact written to the BRAF. And these writes translate into actual real writes to the underlying storage device due to fsync(). We had crazy numbers where we saw spikes upwards of 80 mb/second where the actual throughput was more like ~ 1 mb second of data to the commit log.(One can make a possibly weak argument that it is also functionally incorrect as I can imagine implementations where re-writing the same blocks does copy-on-write in such a way that you're not necessarily guaranteed to see before-or-after data, particularly in case of partial page writes. However that's probably not a practical issue.)Worthy of noting is that this probably causes added difficulties in fsync() latencies since the average fsync() will contain a lot more data. Depending on I/O scheduler and underlying device characteristics, the extra writes may not have a detrimental effect, but I think it's pretty easy to point to cases where it will be detrimental - in particular if the commit log is on a non-battery backed drive. Even with a nice battery backed RAID with the commit log on, the size of the writes probably contributes to difficulty in making the write requests propagate down without being starved by reads (but this is speculation, not tested, other than that I've observed commit log writer starvation that seemed excessive).This isn't the first subtle BRAF bug. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)</description>
      <version>0.7.7,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.util.BufferedRandomAccessFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2668" opendate="2011-5-19 00:00:00" fixdate="2011-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t perform HH to client-mode nodes</summary>
      <description></description>
      <version>0.7.7,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.EndpointState.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2680" opendate="2011-5-21 00:00:00" fixdate="2011-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>range scan doesn&amp;#39;t repair missing rows</summary>
      <description>Range scans do not do digest queries but they do compare all the replicas they receive and repair any discrepancies in the background. (Thus, to get comparable behavior to normal read repair, CL.ALL must be used.)The bug is that currently, replicas that omit a row entirely will be ignored and that row will not be sent to them.</description>
      <version>0.7.7,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.RangeSliceResponseResolver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2697" opendate="2011-5-24 00:00:00" fixdate="2011-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"insufficient space to compact even the two smallest files, aborting"</summary>
      <description>When I explicitly send compaction command "compact" with nodetool, Cassandra outputs error message above. This message is shown even when there is only one file and no need for compaction. I believe this is confusing for users, and recommend that some alternative message should be shown for such case.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.CompactionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2702" opendate="2011-5-24 00:00:00" fixdate="2011-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Row Cache Provider support</summary>
      <description></description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cache.CacheProviderTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.SerializingCacheProvider.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.SerializingCache.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.IRowCacheProvider.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.ConcurrentLinkedHashCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="2718" opendate="2011-5-27 00:00:00" fixdate="2011-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in SSTableWriter when no ReplayPosition availible</summary>
      <description>The following NPE occurs when durable_writes is set to falseERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]java.lang.RuntimeException: java.lang.NullPointerException at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:619)Caused by: java.lang.NullPointerException at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127) at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209) at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187) at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173) at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253) at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49) at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30) ... 3 more</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2721" opendate="2011-5-30 00:00:00" fixdate="2011-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool statusthrift exception while node starts up</summary>
      <description>We noticed when calling nodetool statusthrift, while a node is starting up, it throws an exception. I think the proper behavior should be just return false, instead of throwing an exception if RPC server hasn't started yet. That way this stack trace won't have to be thrown in nodetool:Exception in thread "main" java.lang.IllegalStateException: No configured RPC daemon</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
    </fixedFiles>
  </bug>
  <bug id="2722" opendate="2011-5-30 00:00:00" fixdate="2011-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool statusthrift</summary>
      <description>Provide the status of thrift server, if it's running or not.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeProbe.java</file>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
    </fixedFiles>
  </bug>
  <bug id="2723" opendate="2011-5-31 00:00:00" fixdate="2011-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rows that don&amp;#39;t exist get cached</summary>
      <description>We noticed that rows that don't exist were getting cached anyway. We end up storing an empty CF in cache.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2731" opendate="2011-6-1 00:00:00" fixdate="2011-9-1 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Impelement in-house file caching.</summary>
      <description>Implement FileCache, CachedRandomAccessFile (to replace BufferedRandomAccessFile) and RadixTree (to play role of the backend cache storage) classes.FileCache class with be responsible for storing/retrieving data from Radix Tree and also flushing of the dirty pages to the disk, page management such as adding new pages, utilizing old/unused pages.CRAF Linux only features (via JNI):1). O_DIRECT for both read/write operations.2). AIO's lio_listio write operation batching.Provide possibility to migrate hot data directly from Memtable to CRAF cache to keep live-reads data always hot in memory. To minimise compaction effects CRAF should provide a way to by-pass a caching data if it does not already exists. Provide a way to make pointers in the cache which will be useful to minimize impact on performance when a single column is distributed among multiple SSTable files (except counter columns).Use jemalloc (http://www.canonware.com/jemalloc/) for cache memory management.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.index.SecondaryIndexManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2733" opendate="2011-6-1 00:00:00" fixdate="2011-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool ring with EC2Snitch, NPE checking for the zone and dc</summary>
      <description>Existing EC2Snitch... compare is done via == instead of equals() while comparing the IP's... (endpoint == FBUtilities.getLocalAddress())It is ok to compare the Object Address as most of the code uses FBU.getLocalAddress() and it returns the same object everywhere... but it breaks nodetool ring.</description>
      <version>0.7.7,0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.locator.Ec2Snitch.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2743" opendate="2011-6-6 00:00:00" fixdate="2011-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add TABLE as a CQL alias for COLUMNFAMILY</summary>
      <description>This will make it easier on tools that support multiple databases like SQLAlchemy.</description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.system.test.cql.py</file>
      <file type="M">src.java.org.apache.cassandra.cql.Cql.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2756" opendate="2011-6-9 00:00:00" fixdate="2011-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make CQL CREATE KS/CF not return until the operation is complete</summary>
      <description>this would be more in line with the principle of least surprise, and would avoid the need for everyone who does programmatic schema manipulation to re-invent logic like CASSANDRA-2649.CliClient.validateSchemaIsSettled is a good template for the logic we want.</description>
      <version>0.8.1</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql.QueryProcessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2758" opendate="2011-6-10 00:00:00" fixdate="2011-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool repair never finishes. Loops forever through merkle trees?</summary>
      <description>I am not sure all steps here is needed, but as part of testing something else, I set upnode1: initial_token: 1node2: initial_token: 5Then:create keyspace myks with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' with strategy_options = [{ replication_factor:2 }];use myks;create column family test with comparator = AsciiType and column_metadata=[ {column_name: 'up_', validation_class: LongType, index_type: 0}, {column_name: 'del_', validation_class: LongType, index_type: 0} ] and keys_cached = 100000 and rows_cached = 10000 and min_compaction_threshold = 2;quit;Doing nodetool repair after this gets both nodes busy looping forever.A quick look at one node in eclipse makes me guess its having fun spinning through merkle trees, but I have to admit I have not look at it for a long time.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.utils.MerkleTree.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2759" opendate="2011-6-10 00:00:00" fixdate="2011-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrub could lose increments and replicate that loss</summary>
      <description>If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2781" opendate="2011-6-16 00:00:00" fixdate="2011-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>regression: exposing cache size through MBean</summary>
      <description>Looks like it was part of CASSANDRA-1969. A method called size, as opposed to getSize, won't be exposed through jmx.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.NodeCmd.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.InstrumentingCacheMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.InstrumentingCache.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2788" opendate="2011-6-17 00:00:00" fixdate="2011-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add startup option renew the NodeId (for counters)</summary>
      <description>If an sstable of a counter column family is corrupted, the only safe solution a user have right now is to: Remove the NodeId System table to force the node to regenerate a new NodeId (and thus stop incrementing on it's previous, corrupted, subcount) Remove all the sstables for that column family on that node (this is important because otherwise the node will never get "repaired" for it's previous subcount)This is far from being ideal, but I think this is the price we pay for avoiding the read-before-write. In any case, the first step (remove the NodeId system table) happens to remove the list of the old NodeId this node has, which could prevent us for merging the other potential previous nodeId. This is ok but sub-optimal. This ticket proposes to add a new startup flag to make the node renew it's NodeId, thus replacing this first.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2801" opendate="2011-6-21 00:00:00" fixdate="2011-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tombstone are not purged when the row is in only one sstable</summary>
      <description>We messed up the refactor of compactionController. It echoes rows if they are present in only one sstable, even if they could be purged.</description>
      <version>0.8.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionController.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
