<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="10015" opendate="2015-8-7 00:00:00" fixdate="2015-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create tool to debug why expired sstables are not getting dropped</summary>
      <description>Sometimes fully expired sstables are not getting dropped, and it is a real pain to manually find out why.A tool that outputs which sstable blocks (by having older data than the newest tombstone in an expired sstable) expired ones would save a lot of time.</description>
      <version>2.0.17,2.1.9,2.2.1,3.0beta1</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.compaction.TTLExpiryTest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10238" opendate="2015-9-1 00:00:00" fixdate="2015-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidating racks violates the RF contract</summary>
      <description>I have only tested this on 2.0 so far, but I suspect it will affect multiple versions.Repro: create a datacenter with rf&gt;1 create more than one rack in this datacenter consolidate these racks into 1 getendpoints will reveal the RF in practice is 1, even though other tools will report the original RF that was setRestarting Cassandra will resolve this.</description>
      <version>2.0.17,2.1.10,2.2.2,3.0.0rc1</version>
      <fixedVersion>Legacy/Coordination</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.locator.TokenMetadataTest.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.locator.YamlFileNetworkTopologySnitch.java</file>
      <file type="M">src.java.org.apache.cassandra.locator.TokenMetadata.java</file>
      <file type="M">src.java.org.apache.cassandra.locator.PropertyFileSnitch.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10330" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Gossipinfo could return more useful information</summary>
      <description>For instance, the version for each state, which can be useful for diagnosing the reason for any missing states. Also instead of just omitting the TOKENS state, let's indicate whether the state was actually present or not.</description>
      <version>2.0.17,2.1.10,2.2.2,3.0.0rc1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.gms.FailureDetector.java</file>
    </fixedFiles>
  </bug>
  <bug id="10366" opendate="2015-9-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added gossip states can shadow older unseen states</summary>
      <description>In CASSANDRA-6135 we added cloneWithHigherVersion to ensure that if another thread added states to gossip while we were notifying we would increase our version to ensure the existing states wouldn't get shadowed. This however, was not entirely perfect since it's possible that after the clone, but before the addition another thread will insert an even newer state, thus shadowing the others. A common case (of this rare one) is when STATUS and TOKENS are added a bit later in SS.setGossipTokens, where something in another thread injects a new state (likely SEVERITY) just before the addition after the clone.</description>
      <version>2.0.17,2.1.10,2.2.2,3.0.0rc1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9382" opendate="2015-5-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot file descriptors not getting purged (possible fd leak)</summary>
      <description>OpsCenter has the repair service which does a lot of small range repairs. Each repair would generate a snapshot as per normal. The cluster was showing a steady increase in disk space over the course of a couple of days and the only way to workaround the issue was to restart the node.Upon some further inspection it was seen that a lsof output of the cassandra process was still showing file descriptors for snapshots that no longer existed on the file system. For example:ava 5822 cassandra DEL REG 202,32 7359833 /media/ephemeral1/cassandra/data/somekeyspace/table1/snapshots/669a3a30-f3d3-11e4-bec6-3f6c4fb06498/somekeyspace-table1-jb-897689-Data.dbWe also took a heapdump which basically showed the same thing, lots of references to these file handles. We checked the logs for any errors especially relating to repairs that might have failed but there was nothing observedThe repair service logs in OpsCenter showed also that all repairs (1000s of them) had completed successfully, again showing that there was no repair issue.I have not yet been able to reproduce the issue locally on a test box. The cluster that this original issue appeared on was a production cluster with the following spec:cassandra_versions: 2.0.14.352cluster_cores : 8, cluster_instance_types : i2.2xlargecluster_os : Amazon linux amd64 node count: 4node java version: Oracle Java 1.7.0_51</description>
      <version>2.0.17,2.1.9,2.2.1,3.0alpha1</version>
      <fixedVersion>Legacy/LocalWrite-ReadPaths</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9385" opendate="2015-5-14 00:00:00" fixdate="2015-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cqlsh autocomplete does not work for DTCS min_threshold</summary>
      <description>Confirmed that min_threshold is a valid parameter for DTCS:create TABLE test1 (key text, col1 text, primary key (key)) WITH compaction = {'class': 'DateTieredCompactionStrategy', 'min_threshold': '4' };But the min_threshold does not appear in the tab auto complete options.</description>
      <version>2.0.17,2.1.8,2.2.0rc2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.cql3handling.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9527" opendate="2015-6-1 00:00:00" fixdate="2015-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot create secondary index on a table WITH COMPACT STORAGE</summary>
      <description>In CASSANDRA-8156 it is said that secondary indexes are not allowed on clustering columns in COMPACT tables.However, I found that it is not possible to create a secondary index on the value column in a COMPACT table:CREATE TABLE t ( a INT, b INT, c INT, PRIMARY KEY (a, b)) WITH COMPACT STORAGE;CREATE INDEX ON t (c);Bad Request: Secondary indexes are not supported on PRIMARY KEY columns in COMPACT STORAGE tables</description>
      <version>2.0.17,2.1.8,2.2.0rc2</version>
      <fixedVersion>Feature/2iIndex,Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.statements.CreateIndexStatement.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9565" opendate="2015-6-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;WITH WITH&amp;#39; in alter keyspace statements causes NPE</summary>
      <description>Running any of these statements:ALTER KEYSPACE WITH WITH DURABLE_WRITES = true;ALTER KEYSPACE ks WITH WITH DURABLE_WRITES = true;CREATE KEYSPACE WITH WITH DURABLE_WRITES = true;CREATE KEYSPACE ks WITH WITH DURABLE_WRITES = true;Fails, raising a SyntaxException and giving a NullPointerException as the reason for failure. This happens in all versions I tried, including 2.0.15, 2.1.5, and HEAD on cassandra-2.0, cassandra-2.1, cassandra-2.2, and trunk.EDIT: A dtest is here, but it would probably be more appropriate to test with unit tests.</description>
      <version>2.0.17,2.1.7,2.2.0rc2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cql3.Cql.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9603" opendate="2015-6-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose private listen_address in system.local</summary>
      <description>We had some hopes CASSANDRA-9436 would add it, yet it added rpc_address instead of both rpc_address and listen_address. We really need listen_address here, because we need to get information on the private IP C* binds to. Knowing this we could better match Spark nodes to C* nodes and process data locally in environments where rpc_address != listen_address like EC2. See, Spark does not know rpc addresses nor it has a concept of broadcast address. It only knows the hostname / IP its workers bind to. In case of cloud environments, these are private IPs. Now if we give Spark a set of C* nodes identified by rpc_addresses, Spark doesn't recognize them as belonging to the same cluster. It treats them as "remote" nodes and has no idea where to send tasks optimally. Current situation (example):Spark worker nodes: &amp;#91;10.0.0.1, 10.0.0.2, 10.0.0.3&amp;#93;C* nodes: &amp;#91;10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com&amp;#93;What the application knows about the cluster: &amp;#91;node1.blah.ec2.com, node2.blah.ec2.com, node3.blah.ec2.com&amp;#93;What the application sends to Spark for execution: Task1 - please execute on node1.blah.ec2.com Task2 - please execute on node2.blah.ec2.com Task3 - please execute on node3.blah.ec2.comHow Spark understands it: "I have no idea what node1.blah.ec2.com is, let's assign Task1 it to a random node" Expected:Spark worker nodes: &amp;#91;10.0.0.1, 10.0.0.2, 10.0.0.3&amp;#93;C* nodes: &amp;#91;10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com&amp;#93;What the application knows about the cluster: &amp;#91;10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com&amp;#93;What the application sends to Spark for execution: Task1 - please execute on node1.blah.ec2.com or 10.0.0.1 Task2 - please execute on node2.blah.ec2.com or 10.0.0.2 Task3 - please execute on node3.blah.ec2.com or 10.0.0.3How Spark understands it: "10.0.0.1? - I have a worker on that node, lets put Task 1 there"</description>
      <version>2.0.17,2.1.8,2.2.0rc2</version>
      <fixedVersion>Legacy/DistributedMetadata</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.SystemKeyspace.java</file>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9631" opendate="2015-6-22 00:00:00" fixdate="2015-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary required filtering for query on indexed clustering key</summary>
      <description>Let's create and populate a simple table composed of one partition key a, two clustering keys b &amp; c, and one secondary index on a standard column e:$ cqlsh 127.0.0.1Connected to test21 at 127.0.0.1:9160.[cqlsh 4.1.1 | Cassandra 2.1.6-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]Use HELP for help.cqlsh&gt; CREATE KEYSPACE test WITH REPLICATION={'class': 'SimpleStrategy', 'replication_factor': 3};cqlsh&gt; CREATE TABLE test.table1 ( ... a int, ... b int, ... c int, ... d int, ... e int, ... PRIMARY KEY (a, b, c) ... );cqlsh&gt; CREATE INDEX table1_e ON test.table1 (e);cqlsh&gt; INSERT INTO test.table1 (a, b, c, d, e) VALUES (1, 1, 1, 1, 1);(...)cqlsh&gt; SELECT * FROM test.table1; a | b | c | d | e---+---+---+---+--- 1 | 1 | 1 | 1 | 1 1 | 1 | 2 | 2 | 2 1 | 1 | 3 | 3 | 3 1 | 2 | 1 | 1 | 3 1 | 3 | 1 | 1 | 1 2 | 4 | 1 | 1 | 1(6 rows)With such a schema, I am allowed to query on the indexed column without filtering by providing the first two elements of the primary key:cqlsh&gt; SELECT * FROM test.table1 WHERE a=1 AND b=1 AND e=3; a | b | c | d | e---+---+---+---+--- 1 | 1 | 3 | 3 | 3(1 rows)Let's now introduce an index on the first clustering key:cqlsh&gt; CREATE INDEX table1_b ON test.table1 (b);Now, I expect the same query as above to work without filtering, but it's not:cqlsh&gt; SELECT * FROM test.table1 WHERE a=1 AND b=1 AND e=3;Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERINGI think this is a bug on the way secondary indexes are accounted for when checking for unfiltered queries.</description>
      <version>2.0.17,2.1.8,2.2.0rc2</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cql3.SingleColumnRelationTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.cql3.MultiColumnRelationTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.statements.SelectStatement.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9682" opendate="2015-6-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>setting log4j.logger.org.apache.cassandra=DEBUG causes keyspace username/password to show up in system.log</summary>
      <description>if using a third party log aggregator (which many cloud users use), this causes db credentials to be reproduced offsite, which has potential to be security issue. I would prefer the ability to disable the logging of this information while still setting log4j.logger.org.apache.cassandra=DEBUGexample system.log entry:DEBUG [Native-Transport-Requests:373] 2015-06-21 07:52:44,595 Message.java (line 326) Responding: AUTHENTICATE org.apache.cassandra.auth.PasswordAuthenticator, v=1DEBUG [Native-Transport-Requests:384] 2015-06-21 07:52:44,597 Message.java (line 319) Received: CREDENTIALS {username=redacted, password=redacted}, v=1</description>
      <version>2.0.17,2.1.9,2.2.0,3.0alpha1</version>
      <fixedVersion>Legacy/Observability</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.transport.messages.CredentialsMessage.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9740" opendate="2015-7-6 00:00:00" fixdate="2015-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t transition from write survey to normal mode</summary>
      <description>When attempting a transition a node from write survey to normal mode, the "nodetool join" invocation fails as the node is already "joined" (even though it's only bootstrapped).Attached is a trivial patch to fix. Note: I'm more concerned with ensuring correctness here rather than resolving an obscure bug.</description>
      <version>2.0.17,2.1.9,2.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9765" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>checkForEndpointCollision fails for legitimate collisions</summary>
      <description>Since CASSANDRA-7939, checkForEndpointCollision no longer catches a legitimate collision. Without CASSANDRA-7939, wiping a node and starting it again fails with 'A node with address %s already exists', but with it the node happily enters joining state, potentially streaming from the wrong place and violating consistency.</description>
      <version>2.0.17,2.1.9,2.2.1,3.0alpha1</version>
      <fixedVersion>Legacy/DistributedMetadata</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
    </fixedFiles>
  </bug>
  <bug id="9793" opendate="2015-7-13 00:00:00" fixdate="2015-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log when messages are dropped due to cross_node_timeout</summary>
      <description>When a node has clock skew and cross node timeouts are enabled, there's no indication that the messages were dropped due to the cross timeout, just that messages were dropped. This can errantly lead you down a path of troubleshooting a load shedding situation when really you just have clock drift on one node. This is also not simple to troubleshoot, since you have to determine that this node will answer requests, but other nodes won't answer requests from it. If the problem goes away on a reboot (and the machine does one-shot time sync, not continuous) it becomes even harder to detect because you're left with a weird piece of evidence such as "it's fine after a reboot, but comes back in about X days every time."It would help tremendously if there were a log message indicating how many messages (don't need them broken down by type) were eagerly dropped due to the cross node timeout.</description>
      <version>2.0.17,2.1.9,2.2.1</version>
      <fixedVersion>Legacy/StreamingandMessaging</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.net.MessageDeliveryTask.java</file>
      <file type="M">src.java.org.apache.cassandra.net.IncomingTcpConnection.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.cassandra.net.MessagingService.java</file>
    </fixedFiles>
  </bug>
  <bug id="9959" opendate="2015-8-3 00:00:00" fixdate="2015-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expected bloom filter size should not be an int</summary>
      <description>We cast the expected number of rows in scrub and cleanup to an int. Seems to have been this way since 0.7 days. Patch here: https://github.com/krummas/cassandra/commits/marcuse/dontcastscrubcleanup</description>
      <version>2.0.17,2.1.9,2.2.1,3.0beta1</version>
      <fixedVersion>Local/Compaction</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.compaction.Scrubber.java</file>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
