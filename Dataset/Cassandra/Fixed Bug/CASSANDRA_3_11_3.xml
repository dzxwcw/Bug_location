<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="11163" opendate="2016-2-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Summaries are needlessly rebuilt when the BF FP ratio is changed</summary>
      <description>This is from trunk, but I also saw this happen on 2.0:Before:root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/total 221460drwxr-xr-x 2 root root 4096 Feb 11 23:34 backups-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-6-big-TOC.txt-rw-r--r-- 1 root root 26518 Feb 11 23:50 ma-6-big-Summary.db-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-6-big-Statistics.db-rw-r--r-- 1 root root 2607705 Feb 11 23:50 ma-6-big-Index.db-rw-r--r-- 1 root root 192440 Feb 11 23:50 ma-6-big-Filter.db-rw-r--r-- 1 root root 10 Feb 11 23:50 ma-6-big-Digest.crc32-rw-r--r-- 1 root root 35212125 Feb 11 23:50 ma-6-big-Data.db-rw-r--r-- 1 root root 2156 Feb 11 23:50 ma-6-big-CRC.db-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-7-big-TOC.txt-rw-r--r-- 1 root root 26518 Feb 11 23:50 ma-7-big-Summary.db-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-7-big-Statistics.db-rw-r--r-- 1 root root 2607614 Feb 11 23:50 ma-7-big-Index.db-rw-r--r-- 1 root root 192432 Feb 11 23:50 ma-7-big-Filter.db-rw-r--r-- 1 root root 9 Feb 11 23:50 ma-7-big-Digest.crc32-rw-r--r-- 1 root root 35190400 Feb 11 23:50 ma-7-big-Data.db-rw-r--r-- 1 root root 2152 Feb 11 23:50 ma-7-big-CRC.db-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-5-big-TOC.txt-rw-r--r-- 1 root root 104178 Feb 11 23:50 ma-5-big-Summary.db-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-5-big-Statistics.db-rw-r--r-- 1 root root 10289077 Feb 11 23:50 ma-5-big-Index.db-rw-r--r-- 1 root root 757384 Feb 11 23:50 ma-5-big-Filter.db-rw-r--r-- 1 root root 9 Feb 11 23:50 ma-5-big-Digest.crc32-rw-r--r-- 1 root root 139201355 Feb 11 23:50 ma-5-big-Data.db-rw-r--r-- 1 root root 8508 Feb 11 23:50 ma-5-big-CRC.dbroot@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-Summary.db5fca154fc790f7cfa37e8ad6d1c7552cBF ratio changed, node restarted:root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/total 242168drwxr-xr-x 2 root root 4096 Feb 11 23:34 backups-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-6-big-TOC.txt-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-6-big-Statistics.db-rw-r--r-- 1 root root 2607705 Feb 11 23:50 ma-6-big-Index.db-rw-r--r-- 1 root root 192440 Feb 11 23:50 ma-6-big-Filter.db-rw-r--r-- 1 root root 10 Feb 11 23:50 ma-6-big-Digest.crc32-rw-r--r-- 1 root root 35212125 Feb 11 23:50 ma-6-big-Data.db-rw-r--r-- 1 root root 2156 Feb 11 23:50 ma-6-big-CRC.db-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-7-big-TOC.txt-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-7-big-Statistics.db-rw-r--r-- 1 root root 2607614 Feb 11 23:50 ma-7-big-Index.db-rw-r--r-- 1 root root 192432 Feb 11 23:50 ma-7-big-Filter.db-rw-r--r-- 1 root root 9 Feb 11 23:50 ma-7-big-Digest.crc32-rw-r--r-- 1 root root 35190400 Feb 11 23:50 ma-7-big-Data.db-rw-r--r-- 1 root root 2152 Feb 11 23:50 ma-7-big-CRC.db-rw-r--r-- 1 root root 80 Feb 11 23:50 ma-5-big-TOC.txt-rw-r--r-- 1 root root 10264 Feb 11 23:50 ma-5-big-Statistics.db-rw-r--r-- 1 root root 10289077 Feb 11 23:50 ma-5-big-Index.db-rw-r--r-- 1 root root 757384 Feb 11 23:50 ma-5-big-Filter.db-rw-r--r-- 1 root root 9 Feb 11 23:50 ma-5-big-Digest.crc32-rw-r--r-- 1 root root 139201355 Feb 11 23:50 ma-5-big-Data.db-rw-r--r-- 1 root root 8508 Feb 11 23:50 ma-5-big-CRC.db-rw-r--r-- 1 root root 80 Feb 12 00:03 ma-8-big-TOC.txt-rw-r--r-- 1 root root 14902 Feb 12 00:03 ma-8-big-Summary.db-rw-r--r-- 1 root root 10264 Feb 12 00:03 ma-8-big-Statistics.db-rw-r--r-- 1 root root 1458631 Feb 12 00:03 ma-8-big-Index.db-rw-r--r-- 1 root root 10808 Feb 12 00:03 ma-8-big-Filter.db-rw-r--r-- 1 root root 10 Feb 12 00:03 ma-8-big-Digest.crc32-rw-r--r-- 1 root root 19660275 Feb 12 00:03 ma-8-big-Data.db-rw-r--r-- 1 root root 1204 Feb 12 00:03 ma-8-big-CRC.db-rw-r--r-- 1 root root 26518 Feb 12 00:04 ma-7-big-Summary.db-rw-r--r-- 1 root root 26518 Feb 12 00:04 ma-6-big-Summary.db-rw-r--r-- 1 root root 104178 Feb 12 00:04 ma-5-big-Summary.dbroot@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-Summary.db 5fca154fc790f7cfa37e8ad6d1c7552c This hurts startup time and appears to do nothing useful whatsoever.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/Core</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.io.sstable.SSTableReaderTest.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.format.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">NEWS.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12743" opendate="2016-10-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assertion error while running compaction</summary>
      <description>While running compaction I run into an error sometimes :nodetool compacterror: null-- StackTrace --java.lang.AssertionError at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.&lt;init&gt;(CompressionMetadata.java:463) at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:228) at org.apache.cassandra.io.util.CompressedSegmentedFile.createMappedSegments(CompressedSegmentedFile.java:80) at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.&lt;init&gt;(CompressedPoolingSegmentedFile.java:38) at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:101) at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:198) at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:315) at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:171) at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:116) at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.append(DefaultCompactionWriter.java:64) at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:184) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:599) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Why is that happening?Is there anyway to provide more details (e.g. which SSTable cannot be compacted)?We are using Cassandra 2.2.7</description>
      <version>2.2.13,3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Local/Compaction</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.io.util.SequentialWriterTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.io.compress.CompressedSequentialWriterTest.java</file>
      <file type="M">src.java.org.apache.cassandra.io.util.SequentialWriter.java</file>
      <file type="M">src.java.org.apache.cassandra.io.compress.CompressedSequentialWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13851" opendate="2017-9-7 00:00:00" fixdate="2017-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow existing nodes to use all peers in shadow round</summary>
      <description>In CASSANDRA-10134 we made collision checks necessary on every startup. A side-effect was introduced that then requires a nodes seeds to be contacted on every startup. Prior to this change an existing node could start up regardless whether it could contact a seed node or not (because checkForEndpointCollision() was only called for bootstrapping nodes). Now if a nodes seeds are removed/deleted/fail it will no longer be able to start up until live seeds are configured (or itself is made a seed), even though it already knows about the rest of the ring. This is inconvenient for operators and has the potential to cause some nasty surprises and increase downtime.One solution would be to use all a nodes existing peers as seeds in the shadow round. Not a Gossip guru though so not sure of implications.</description>
      <version>3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Local/StartupandShutdown</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.gms.Gossiper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13891" opendate="2017-9-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fromJson(null) throws java.lang.NullPointerException on Cassandra</summary>
      <description>Basically, fromJson throws a java.lang.NullPointerException when NULL is passed, instead of just returning a NULL itself. Say I create a UDT and a table as follows:create type type1(id int,name text);create table table1(id int,t FROZEN&lt;type1&gt;,primary key (id));And then try and insert a row as such:insert into table1 (id, t) VALUES (1, fromJson(null));I get the error: java.lang.NullPointerExceptionThis works as expected: insert into table1 (id, t) VALUES (1, null);Programmatically, one does not always know when a UDT will be null, hence me expecting fromJson to just return NULL.</description>
      <version>2.2.13,3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cql3.validation.entities.JsonTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.functions.FunctionCall.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14170" opendate="2018-1-17 00:00:00" fixdate="2018-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loss of digits when doing CAST from varint/bigint to decimal</summary>
      <description>Cast functions from numeric types to decimal type are implemented as conversion to double first and then from double to decimal: https://github.com/apache/cassandra/compare/trunk...blerer:10310-3.0#diff-6aa4a8f76df6c30c5bb4026b8c9251eeR80.This can cause loss of digits for big values stored in varint or bigint. It is probably unexpected because decimal can store such values precisely.Examples:cqlsh&gt; CREATE TABLE cast_bigint_test(k int PRIMARY KEY, bigint_clmn bigint); cqlsh&gt; INSERT INTO cast_bigint_test(k, decimal_clmn) VALUES(2, 9223372036854775807); cqlsh&gt; SELECT CAST(bigint_clmn AS decimal) FROM cast_bigint_test; cast(bigint_clmn as decimal) ------------------------------ 9.223372036854776E+18 (1 rows)cqlsh&gt; CREATE TABLE cast_varint_test (k int PRIMARY KEY, varint_clmn varint); cqlsh&gt; INSERT INTO cast_varint_test(k, varint_clmn) values(2, 1234567890123456789); cqlsh&gt; SELECT CAST(varint_clmn AS decimal) FROM cast_varint_test; cast(varint_clmn as decimal) ------------------------------1.23456789012345677E+18 (1 rows) </description>
      <version>3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cql3.functions.CastFctsTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.functions.CastFcts.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14194" opendate="2018-1-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chain commit log marker potential performance regression in batch commit mode</summary>
      <description>CASSANDRA-13987 modified how the commit log sync thread works. I noticed that cql3.ViewTest and ViewBuilderTaskTest have been timing out, but only in CircleCI. When I jstack in CircleCI what I see is that the commit log writer thread is parked and the threads trying to append to the commit log are blocked waiting on it.I tested the commit before 13987 and it passed in CircleCI and then I tested with 13987 and it timed out. I suspect this may be a general performance regression and not just a CircleCI issue.And this is with write barriers disabled (sync thread doesn't actually sync) so it wasn't blocked in the filesystem.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/Core,Legacy/Testing</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.commitlog.CommitLogSegment.java</file>
      <file type="M">src.java.org.apache.cassandra.db.commitlog.AbstractCommitLogService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14202" opendate="2018-1-30 00:00:00" fixdate="2018-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assertion error on sstable open during startup should invoke disk failure policy</summary>
      <description>We should catch all exceptions when opening sstables on startup and invoke the disk failure policy</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.sstable.format.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.CorruptSSTableException.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14209" opendate="2018-2-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>group by select queries query results differ when using select * vs select fields</summary>
      <description>I get two different out with these 2 queries.  The only difference between the 2 queries is that one does ‘select *’ and other does ‘select specific fields’ without any aggregate functions.I am using Apache Cassandra 3.10.Consistency level set to LOCAL_QUORUM.cassandra@cqlsh&gt; select * from wp.position where account_id = 'user_1';{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}---------------------------------------------------------------------------------+--------------------------------{{ user_1 | AMZN | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}(2 rows)cassandra@cqlsh&gt; select * from wp.position where account_id = 'user_1' group by security_id;{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}---------------------------------------------------------------------------------+--------------------------------{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}(1 rows)cassandra@cqlsh&gt; select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ;{{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}---------------------------------------------------+--------------------------------{{ user_1 | AMZN | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}}(1 rows)Table Description:CREATE TABLE wp.position ({{ account_id text,}}{{ security_id text,}}{{ counter bigint,}}{{ avg_exec_price double,}}{{ pending_quantity double,}}{{ quantity double,}}{{ transaction_id uuid,}}{{ update_time timestamp,}}{{ PRIMARY KEY (account_id, security_id, counter)}}) WITH CLUSTERING ORDER BY (security_id ASC, counter DESC) </description>
      <version>3.11.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cql3.validation.operations.SelectGroupByTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.cql3.validation.entities.JsonTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.statements.SelectStatement.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.selection.Selection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14210" opendate="2018-2-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize SSTables upgrade task scheduling</summary>
      <description>When starting the SSTable-rewrite process by running nodetool upgradesstables --jobs N, with N &gt; 1, not all of the provided N slots are used.For example, we were testing with concurrent_compactors=5 and N=4. What we observed both for version 2.2 and 3.0, is that initially all 4 provided slots are used for "Upgrade sstables" compactions, but later when some of the 4 tasks are finished, no new tasks are scheduled immediately. It takes the last of the 4 tasks to finish before new 4 tasks would be scheduled. This happens on every node we've observed.This doesn't utilize available resources to the full extent allowed by the --jobs N parameter. In the field, on a cluster of 12 nodes with 4-5 TiB data each, we've seen that the whole process was taking more than 7 days, instead of estimated 1.5-2 days (provided there would be close to full N slots utilization).Instead, new tasks should be scheduled as soon as there is a free compaction slot.Additionally, starting from the biggest SSTables could further reduce the total time required for the whole process to finish on any given node.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Local/Compaction</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.utils.FBUtilitiesTest.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.FBUtilities.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.format.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14245" opendate="2018-2-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SELECT JSON prints null on empty strings</summary>
      <description>SELECT JSON reports an empty string as null. Example:cqlsh:unittest&gt; create table test(id INT, name TEXT, PRIMARY KEY(id));cqlsh:unittest&gt; insert into test (id, name) VALUES (1, 'Foo');cqlsh:unittest&gt; insert into test (id, name) VALUES (2, '');cqlsh:unittest&gt; insert into test (id, name) VALUES (3, null);cqlsh:unittest&gt; select * from test;id | name----+------  1 |  Foo  2 |       3 | null(3 rows)cqlsh:unittest&gt; select JSON * from test;[json]--------------------------{"id": 1, "name": "Foo"}{"id": 2, "name": null}{"id": 3, "name": null}(3 rows) This even happens, if the string is part of the Primary Key, which makes the generated string not insertable. cqlsh:unittest&gt; create table test2 (id INT, name TEXT, age INT, PRIMARY KEY(id, name));cqlsh:unittest&gt; insert into test2 (id, name, age) VALUES (1, '', 42);cqlsh:unittest&gt; select JSON * from test2;[json]------------------------------------{"id": 1, "name": null, "age": 42}(1 rows)cqlsh:unittest&gt; insert into test2 JSON '{"id": 1, "name": null, "age": 42}';InvalidRequest: Error from server: code=2200 [Invalid query] message="Invalid null value in condition for column name" On an older version of Cassandra (3.0.8) does not have this problem.</description>
      <version>3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cql3.validation.entities.JsonTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cql3.selection.Selection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14247" opendate="2018-2-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SASI tokenizer for simple delimiter based entries</summary>
      <description>Currently SASI offers only two tokenizer options: NonTokenizerAnalyser StandardAnalyzerThe latter is built upon Snowball, powerful for human languages but overkill for simple tokenization.A simple tokenizer is proposed here. The need for this arose as a workaround of CASSANDRA-11182, and to avoid the disk usage explosion when having to resort to CONTAINS. See https://github.com/openzipkin/zipkin/issues/1861Example use of this would be:CREATE CUSTOM INDEX span_annotation_query_idx ON zipkin2.span (annotation_query) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = { 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.DelimiterAnalyzer', 'delimiter': '░', 'case_sensitive': 'true', 'mode': 'prefix', 'analyzed': 'true'};Original credit for this work goes to https://github.com/zuochangan</description>
      <version>3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Feature/SASI</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14251" opendate="2018-2-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>View replica is not written to pending endpoint when base replica is also view replica</summary>
      <description>From the dev list:There's an optimization that when we're lucky enough that the paired view replica is the same as this base replica, mutateMV doesn't use the normal view-mutation-sending code (wrapViewBatchResponseHandler) and just writes the mutation locally. In particular, in this case we do NOT write to the pending node (unless I'm missing something). But, sometimes all replicas will be paired with themselves - this can happen for example when number of nodes is equal to RF, or when the base and view table have the same partition keys (but different clustering keys). In this case, it seems the pending node will not be written at all...This was a regression from CASSANDRA-13069 and the original behavior should be restored.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Feature/MaterializedViews</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageProxy.java</file>
      <file type="M">NEWS.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14310" opendate="2018-3-13 00:00:00" fixdate="2018-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow nodetool refresh before cfs is opened</summary>
      <description>There is a potential deadlock in during startup if nodetool refresh is called while sstables are being opened. We should not allow refresh to be called before everything is initialized.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14411" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables</summary>
      <description>There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are &amp;#91;a, b&amp;#93;.</description>
      <version>2.2.13,3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Consistency/Repair</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.compaction.AntiCompactionTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14423" opendate="2018-4-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSTables stop being compacted</summary>
      <description>So seeing a problem in 3.11.0 where SSTables are being lost from the view and not being included in compactions/as candidates for compaction. It seems to get progressively worse until there's only 1-2 SSTables in the view which happen to be the most recent SSTables and thus compactions completely stop for that table.The SSTables seem to still be included in reads, just not compactions.The issue can be fixed by restarting C*, as it will reload all SSTables into the view, but this is only a temporary fix. User defined/major compactions still work - not clear if they include the result back in the view but is not a good work around.This also results in a discrepancy between SSTable count and SSTables in levels for any table using LCS.Keyspace : xxxRead Count: 57761088Read Latency: 0.10527088681224288 ms.Write Count: 2513164Write Latency: 0.018211106398149903 ms.Pending Flushes: 0Table: xxxSSTable count: 10SSTables in each level: [2, 0, 0, 0, 0, 0, 0, 0, 0]Space used (live): 894498746Space used (total): 894498746Space used by snapshots (total): 0Off heap memory used (total): 11576197SSTable Compression Ratio: 0.6956629530569777Number of keys (estimate): 3562207Memtable cell count: 0Memtable data size: 0Memtable off heap memory used: 0Memtable switch count: 87Local read count: 57761088Local read latency: 0.108 msLocal write count: 2513164Local write latency: NaN msPending flushes: 0Percent repaired: 86.33Bloom filter false positives: 43Bloom filter false ratio: 0.00000Bloom filter space used: 8046104Bloom filter off heap memory used: 8046024Index summary off heap memory used: 3449005Compression metadata off heap memory used: 81168Compacted partition minimum bytes: 104Compacted partition maximum bytes: 5722Compacted partition mean bytes: 175Average live cells per slice (last five minutes): 1.0Maximum live cells per slice (last five minutes): 1Average tombstones per slice (last five minutes): 1.0Maximum tombstones per slice (last five minutes): 1Dropped Mutations: 0Also for STCS we've confirmed that SSTable count will be different to the number of SSTables reported in the Compaction Bucket's. In the below example there's only 3 SSTables in a single bucket - no more are listed for this table. Compaction thresholds haven't been modified for this table and it's a very basic KV schema.Keyspace : yyy Read Count: 30485 Read Latency: 0.06708991307200263 ms. Write Count: 57044 Write Latency: 0.02204061776873992 ms. Pending Flushes: 0 Table: yyy SSTable count: 19 Space used (live): 18195482 Space used (total): 18195482 Space used by snapshots (total): 0 Off heap memory used (total): 747376 SSTable Compression Ratio: 0.7607394576769735 Number of keys (estimate): 116074 Memtable cell count: 0 Memtable data size: 0 Memtable off heap memory used: 0 Memtable switch count: 39 Local read count: 30485 Local read latency: NaN ms Local write count: 57044 Local write latency: NaN ms Pending flushes: 0 Percent repaired: 79.76 Bloom filter false positives: 0 Bloom filter false ratio: 0.00000 Bloom filter space used: 690912 Bloom filter off heap memory used: 690760 Index summary off heap memory used: 54736 Compression metadata off heap memory used: 1880 Compacted partition minimum bytes: 73 Compacted partition maximum bytes: 124 Compacted partition mean bytes: 96 Average live cells per slice (last five minutes): NaN Maximum live cells per slice (last five minutes): 0 Average tombstones per slice (last five minutes): NaN Maximum tombstones per slice (last five minutes): 0 Dropped Mutations: 0 Apr 27 03:10:39 cassandra[9263]: TRACE o.a.c.d.c.SizeTieredCompactionStrategy Compaction buckets are [[BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67168-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67167-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67166-big-Data.db')]]Also for every LCS table we're seeing the following warning being spammed (seems to be in line with anticompaction spam):Apr 26 21:30:09 cassandra[9263]: WARN  o.a.c.d.c.LeveledCompactionStrategy Live sstable /var/lib/cassandra/data/xxx/xxx-8c3ef9e0e3fc11e6868a8fd39a64fd59/mc-79024-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.This is a vnodes cluster with 256 tokens per node, and the only thing that seems like it could be causing issues is anticompactions.CASSANDRA-14079 might be related but doesn't quite describe the same issue, and in this case we're using only a single disk for data. Have yet to reproduce but figured worth reporting here first.</description>
      <version>2.2.13,3.0.17,3.11.3</version>
      <fixedVersion>Local/Compaction</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.compaction.AntiCompactionTest.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.cassandra.db.compaction.CompactionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14428" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run ant eclipse-warnings in circleci</summary>
      <description>We should run ant eclipse-warnings in circle-ci</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>CI</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.circleci.config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="14447" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup StartupClusterConnectivityChecker and PING Verb</summary>
      <description>This is a followup to CASSANDRA-13993. After an internal review, iamaleksey had some concerns wrt to the VERB choices as was committed; this was discussed on CASSANDRA-13993, after commit. Further, he pointed out (and provided) some optimizations for StartupClusterConnectivityChecker itself. While testing some of the proposed changes, I also discovered a small bug where the timeout for waiting for the response to the PING message was dramtically shorter than the overall timeout that is configured for StartupClusterConnectivityChecker.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.net.MessagingService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14451" opendate="2018-5-17 00:00:00" fixdate="2018-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Infinity ms Commit Log Sync</summary>
      <description>Its giving commit log sync warnings where there were apparently zero syncs and therefore gives "Infinityms" as the average durationWARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:11:14,294 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 74.40ms WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:16:57,844 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 198.69ms WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:24:46,325 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 264.11ms WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:29:46,393 NoSpamLogger.java:94 - Out of 32 commit log syncs over the past 268.84s with, average duration of 17.56ms, 1 have exceeded the configured commit interval by an average of 173.66ms</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.commitlog.AbstractCommitLogServiceTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.commitlog.AbstractCommitLogService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14475" opendate="2018-5-29 00:00:00" fixdate="2018-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nodetool - Occasional high CPU on large, CPU capable machines</summary>
      <description>Periodically calling nodetool every 5 min results in increased CPU usage by nodetool only on a machine with 32 physical cores (64 vCPUs) according to our monitoring:Investigation and testing has shown that it is related to running with default number of parallel GC threads which is 43 on this particular machine. We see a System.gc() according to flight recorder but no real evidence from where it comes from. The nodetool call in question is simply gathering e.g. the version with "nodetool version".After explicitly setting the number of parallel GC threads to 1, the high CPU is entirely gone (see chart above), without impacting nodetool being executed successfully. 1 parallel GC thread should be sufficient for nodetool anyway I think.</description>
      <version>3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Tool/nodetool</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.nodetool</file>
    </fixedFiles>
  </bug>
  <bug id="14513" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reverse order queries in presence of range tombstones may cause permanent data loss</summary>
      <description>Slice queries in descending sort order can create oversized artificial range tombstones. At CL &gt; ONE, read repair can propagate these tombstones to all replicas, wiping out vast data ranges that they mistakenly cover.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/Core,Legacy/CQL,Legacy/LocalWrite-ReadPaths</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.columniterator.SSTableReversedIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.db.columniterator.AbstractSSTableIterator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14515" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Short read protection in presence of almost-purgeable range tombstones may cause permanent data loss</summary>
      <description>Because read responses don't necessarily close their open RT bounds, it's possible to lose data during short read protection, if a closing bound is compacted away between two adjacent reads from a node.</description>
      <version>3.0.17,3.11.3,4.0-alpha1,4.0</version>
      <fixedVersion>Consistency/Coordination,Local/SSTable</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.DataResolver.java</file>
      <file type="M">src.java.org.apache.cassandra.db.rows.UnfilteredRowIterators.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ReadCommand.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
