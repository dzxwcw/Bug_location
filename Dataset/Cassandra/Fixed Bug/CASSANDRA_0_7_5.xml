<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="CASSANDRA">
  <bug id="1618" opendate="2010-10-13 00:00:00" fixdate="2010-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying a slice predicate for pig queries to filter out columns</summary>
      <description>Currently the Cassandra/Pig loadfunc hardcodes the slice predicate to get all columns in a particular row. It would be nice to allow the user to configure the slice predicate.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.ConfigHelper.java</file>
      <file type="M">contrib.pig.src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
      <file type="M">contrib.pig.README.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1761" opendate="2010-11-19 00:00:00" fixdate="2010-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexes: Auto-generating the CFname may collide with user-generated names</summary>
      <description>column_families: - name: CF comparator: BytesType column_metadata: - name: foo index_name: 626172 index_type: KEYS - name: bar index_type: KEYSAuto-generated versus user-supplied names collide in the YAML above. The code:cfname = parentCf + "." + (info.getIndexName() == null ? FBUtilities.bytesToHex(info.name) : info.getIndexName())From the first ColumnDefinition, we create cfname = "CF.626172" (from the fail clause of the ternany, user-supplied name)From the second ColumnDefinition, we create cfname = "CF.626172" (from the pass clause of the ternary, we generate the name)They're in hex form. This is possible, but fairly unlikely that someone will do this.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.system.test.thrift.server.py</file>
      <file type="M">src.java.org.apache.cassandra.thrift.ThriftValidation.java</file>
      <file type="M">src.java.org.apache.cassandra.thrift.CassandraServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2080" opendate="2011-1-31 00:00:00" fixdate="2011-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to release of Whirr 0.3.0</summary>
      <description>Whirr 0.3.0 has been released.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
      <file type="M">build.properties.default</file>
    </fixedFiles>
  </bug>
  <bug id="2088" opendate="2011-2-1 00:00:00" fixdate="2011-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up after failed (repair) streaming operation</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.streaming.IncomingStreamReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="2227" opendate="2011-2-23 00:00:00" fixdate="2011-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add cache loading to row/key cache tests</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.RowCacheTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.db.KeyCacheTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.CleanupHelper.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.cache.InstrumentedCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="2258" opendate="2011-3-1 00:00:00" fixdate="2011-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>service.SerializationsTest failes under cobertura</summary>
      <description>ant codecoverage -Dtest.name=SerializationsTest gives [junit] Testcase: testTreeResponseRead(org.apache.cassandra.service.SerializationsTest): Caused an ERROR [junit] java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124 [junit] java.lang.RuntimeException: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124 [junit] at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:634) [junit] at org.apache.cassandra.service.SerializationsTest.testTreeResponseRead(SerializationsTest.java:90) [junit] Caused by: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124 [junit] at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:562) [junit] at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1582) [junit] at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946) [junit] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946) [junit] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946) [junit] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946) [junit] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946) [junit] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870) [junit] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752) [junit] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328) [junit] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350) [junit] at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:630)</description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Testing</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2279" opendate="2011-3-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tombstones not collected post-repair</summary>
      <description>The keys would only show up in sstables2json and look like this:(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am ===&gt; /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 2011-02-23 07:24:43,710 INFO &amp;#91;org.apache.cassandra.config.DatabaseDescriptor&amp;#93; - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 2011-02-23 07:24:43,972 INFO &amp;#91;org.apache.cassandra.io.SSTableReader&amp;#93; - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db { "waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b": [] } (root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am ===&gt;The steps that I took to reproduce it were:Create a keyspace, column family, and a keyDelete the key on Node 1 using the cli (del cf&amp;#91;&amp;#39;key&amp;#39;&amp;#93;Flush Repair on a cluster with more than 1 nodeWait GCSeconds CompactAnd the empty row would appear on Node 2However, when I was able to get rid of the empty rows, I was following these steps on a single machine: Create a keyspace, column family, and a keyDelete the keyFlushSample write (writing to some temporary key)Deleting the attribute to that temporary key (not the entire key)FlushCompactor these steps:Create a keyspace, column family, and a keyDelete the keyFlush Wait GCsecondsCompact</description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.RowIterationTest.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.ReducingIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.db.RowIteratorFactory.java</file>
      <file type="M">src.java.org.apache.cassandra.db.columniterator.IColumnIterator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2283" opendate="2011-3-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming Old Format Data Fails in 0.7.3 after upgrade from 0.6.8</summary>
      <description>After successfully upgrading a 0.6.8 ring to 0.7.3, we needed to bootstrap in a new node relatively quickly. When starting the new node with an assigned token in auto bootstrap mode, we see the following exceptions on the new node:INFO &amp;#91;main&amp;#93; 2011-03-07 10:37:32,671 StorageService.java (line 505) Joining: sleeping 30000 ms for pending range setup INFO &amp;#91;main&amp;#93; 2011-03-07 10:38:02,679 StorageService.java (line 505) Bootstrapping INFO &amp;#91;HintedHandoff:1&amp;#93; 2011-03-07 10:38:02,899 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.211.14.200 INFO &amp;#91;HintedHandoff:1&amp;#93; 2011-03-07 10:38:02,900 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.211.14.200 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:04,924 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-1 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:05,390 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-2 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:05,768 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-1 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:06,389 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-2 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:06,581 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-3ERROR &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:07,056 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread&amp;#91;CompactionExecutor:1,1,main&amp;#93;java.io.EOFException at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65) at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:08,480 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-5 INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:08,582 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-1ERROR &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:08,635 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread&amp;#91;CompactionExecutor:1,1,main&amp;#93;java.io.EOFException at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65) at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)ERROR &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:08,666 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread&amp;#91;CompactionExecutor:1,1,main&amp;#93;java.io.EOFException at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65) at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923) at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) INFO &amp;#91;CompactionExecutor:1&amp;#93; 2011-03-07 10:38:08,855 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-4Two attempts to bootstrap in the new node both exhibited this behavior. On the node owning the tokens being migrated, stream activity is visible but doesn't update any progress I think due to the issues on the receiving host.Lastly, just case it's relevant, we had an EC2 node die underneath us during the upgrade so not all nodes were drained. This didn't affect the upgrade but I wanted to note it her to be thorough.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.streaming.SerializationsTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.streaming.BootstrapTest.java</file>
      <file type="M">src.java.org.apache.cassandra.streaming.StreamIn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2290" opendate="2011-3-8 00:00:00" fixdate="2011-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repair hangs if one of the neighbor is dead</summary>
      <description>Repair don't cope well with dead/dying neighbors. There is 2 problems: Repair don't check if a node is dead before sending a TreeRequest; this is easily fixable. If a neighbor dies mid-repair, the repair will also hang forever.The second point is not easy to deal with. The best approach is probably CASSANDRA-1740 however. That is, if we add a way to query the state of a repair, and that this query correctly check all neighbors and also add a way to cancel a repair, this would probably be enough.</description>
      <version>0.7.5,0.8beta1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.service.AntiEntropyServiceTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.service.AntiEntropyServiceTestAbstract.java</file>
      <file type="M">src.java.org.apache.cassandra.service.AntiEntropyService.java</file>
    </fixedFiles>
  </bug>
  <bug id="2305" opendate="2011-3-10 00:00:00" fixdate="2011-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tombstoned rows not purged from cache after gcgraceseconds</summary>
      <description>From email to list:I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):insert row X with timestamp Tdelete row X with timestamp T+1force flush + compactioninsert row X with timestamp TMy understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.CompactionsPurgeTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.io.CompactionIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.io.CompactionController.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2318" opendate="2011-3-12 00:00:00" fixdate="2011-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid seeking when sstable2json exports the entire file</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.tools.SSTableExport.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2320" opendate="2011-3-13 00:00:00" fixdate="2011-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dropping an index leaves index in Built state in system table</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.ColumnFamilyStoreTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2323" opendate="2011-3-14 00:00:00" fixdate="2011-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stress.java should not allow arbitrary arguments</summary>
      <description>This doesn't seem like a big deal, until you accidentally insert a space between a dash and it's flag, and it's at the point where the line wraps in your terminal.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stress.src.org.apache.cassandra.contrib.stress.Session.java</file>
    </fixedFiles>
  </bug>
  <bug id="2326" opendate="2011-3-14 00:00:00" fixdate="2011-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stress.java indexed range slicing is broken</summary>
      <description>I probably broke it when I fixed the build that CASSANDRA-2312 broke. Now it compiles, but never works.</description>
      <version>0.7.5,0.8beta1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stress.src.org.apache.cassandra.contrib.stress.util.Operation.java</file>
      <file type="M">contrib.stress.src.org.apache.cassandra.contrib.stress.Session.java</file>
      <file type="M">contrib.stress.src.org.apache.cassandra.contrib.stress.operations.IndexedRangeSlicer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2330" opendate="2011-3-15 00:00:00" fixdate="2011-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queue indexes for flush before the parent</summary>
      <description>Secondary indexes flush when the parent does. This has an unfortunate side effect: a single CF flushing with a single secondary index fills the flush queue and blocks further writes until the first one completes. A simple but naive optimization here would be to queue the indexes before the parent since they are generally going to be smaller, and thus flush faster, reducing the amount of time writes are blocked.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2331" opendate="2011-3-15 00:00:00" fixdate="2011-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow a Job Configuration to control the consistency level with which reads and writes occur for mapreduce jobs.</summary>
      <description>Allow a Job Configuration to control the consistency level with which reads and writes occur for mapreduce jobs. This would be useful if we were doing statistical rollups and wanted to fail if consistency isn't met (W + R &lt;= N as described by http://wiki.apache.org/cassandra/ArchitectureOverview). An example of a situation where this might happen would be if a set of cassandra nodes became partitioned from the other nodes, but were still accepting some writes with ConsistencyLevel 1/2 (in the case that writes were local to the partition). Then statistical rollup jobs might succeed without those accepted writes being included, and this would not be desirable.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.hadoop.ConfigHelper.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.ColumnFamilyRecordReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2333" opendate="2011-3-15 00:00:00" fixdate="2011-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up thread pool and queue sizes</summary>
      <description>Most of Cassandra assumes that ThreadPoolExecutor handles tasks by starting with Core threads, adding threads up to Max as tasks arrive, then queuing any additional. This is not correct: If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing. If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread. If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.CASSANDRA-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple CFs flushing (or a single one with indexes) then you will start blocking writes.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.concurrent.DebuggableThreadPoolExecutorTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
      <file type="M">src.java.org.apache.cassandra.config.Config.java</file>
      <file type="M">src.java.org.apache.cassandra.concurrent.StageManager.java</file>
      <file type="M">src.java.org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor.java</file>
      <file type="M">src.java.org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutorMBean.java</file>
      <file type="M">src.java.org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor.java</file>
      <file type="M">src.java.org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.java</file>
      <file type="M">conf.cassandra.yaml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2334" opendate="2011-3-15 00:00:00" fixdate="2011-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>validateSchemaAgreement() should raise something besides InvalidRequestException</summary>
      <description>ThriftValidation.validateSchemaAgreement() throws InvalidRequestException when schemas disagree. Since this is propagated to the client, a more specific exception would relay more context to clients.</description>
      <version>None</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">drivers.java.src.org.apache.cassandra.cql.jdbc.Connection.java</file>
      <file type="M">drivers.java.src.org.apache.cassandra.cql.jdbc.CassandraStatement.java</file>
      <file type="M">drivers.java.src.org.apache.cassandra.cql.jdbc.CassandraConnection.java</file>
      <file type="M">interface.cassandra.thrift</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
      <file type="M">drivers.py.cql.connection.py</file>
      <file type="M">interface.thrift.gen-java.org.apache.cassandra.thrift.ConsistencyLevel.java</file>
      <file type="M">interface.thrift.gen-java.org.apache.cassandra.thrift.Cassandra.java</file>
      <file type="M">src.java.org.apache.cassandra.thrift.CassandraServer.java</file>
      <file type="M">src.java.org.apache.cassandra.cql.QueryProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2343" opendate="2011-3-16 00:00:00" fixdate="2011-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow configured ports to be overridden with system properties</summary>
      <description>For automated testing, it would be nice to be able to override the storage/rpc ports in config, rather than rewrite the yaml.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.config.DatabaseDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2345" opendate="2011-3-16 00:00:00" fixdate="2011-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI: Units on show keyspaces output</summary>
      <description>Memtable thresholds don't give units/designations:Memtable thresholds: 0.0375/8/60By comparison, cache info fully qualifies the numbers.Key cache size / save period: 0.01/14400</description>
      <version>0.7.5,0.8beta1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2349" opendate="2011-3-17 00:00:00" fixdate="2011-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expring columns can expire between the two phase of LazilyCompactedRow.</summary>
      <description>LazilyCompactedRow reads the columns to compact twice. First to create the index, bloom filter and calculate the data size, and then another phase to actually write the columns. But a column can expire between those two phase, which will result in a bad data size in the sstable (and a possibly corrupted row index).</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.util.ColumnSortedMap.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableIdentityIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.db.SuperColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnSerializer.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamily.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2354" opendate="2011-3-18 00:00:00" fixdate="2011-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI should allow users to chose consistency level</summary>
      <description>Currently the CLI runs at CL.ONE. If the CLI can operate at other levels it makes it easier to test multi-dc and failure scenarios</description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cli.CliUserHelp.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliCompleter.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.Cli.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2358" opendate="2011-3-20 00:00:00" fixdate="2011-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI doesn&amp;#39;t handle inserting negative integers</summary>
      <description>The CLI raises a syntax error when trying to insert negative integers:[default@Keyspace1] set StandardInteger['key'][-12] = 'val';Syntax error at position 28: mismatched character '1' expecting '-'</description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.cli.CliTest.java</file>
      <file type="M">src.java.org.apache.cassandra.cli.Cli.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2361" opendate="2011-3-21 00:00:00" fixdate="2011-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AES depends on java serialization</summary>
      <description>0.8 should be able to run in the same cluster as 0.7. AES uses java serialization which means that Token serialization stands a good chance of being brittle. This needs to be fixed.1. place a hard-coded serialVersionUID for Token in 0.7.5.2. have AES use ICompactSerializer in place of ObjectInputStream in 0.8.This would be a good opportunity to audit the code for imprudent uses of Object&amp;#91;Input|Output&amp;#93;Stream and get those cleaned up. Also, it will exercise the versioning code a bit.</description>
      <version>0.7.5,0.8beta1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.service.AntiEntropyServiceTestAbstract.java</file>
      <file type="M">test.unit.org.apache.cassandra.service.AntiEntropyServiceTest.java</file>
      <file type="M">test.unit.org.apache.cassandra.io.CompactSerializerTest.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.MerkleTree.java</file>
      <file type="M">src.java.org.apache.cassandra.service.AntiEntropyService.java</file>
      <file type="M">src.java.org.apache.cassandra.dht.StringToken.java</file>
      <file type="M">src.java.org.apache.cassandra.dht.LocalToken.java</file>
      <file type="M">src.java.org.apache.cassandra.dht.BytesToken.java</file>
      <file type="M">src.java.org.apache.cassandra.dht.BigIntegerToken.java</file>
    </fixedFiles>
  </bug>
  <bug id="2376" opendate="2011-3-24 00:00:00" fixdate="2011-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Both name an index iterators cast block offset to int</summary>
      <description>This means that performing random access to the end of a large row will not work.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.utils.ByteBufferUtil.java</file>
      <file type="M">src.java.org.apache.cassandra.io.util.FileUtils.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTable.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.IndexHelper.java</file>
      <file type="M">src.java.org.apache.cassandra.hadoop.streaming.AvroOutputReader.java</file>
      <file type="M">src.java.org.apache.cassandra.db.columniterator.SSTableNamesIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.db.columniterator.IndexedSliceReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2382" opendate="2011-3-25 00:00:00" fixdate="2011-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>statistics component not fsynced</summary>
      <description>The statistics file is prone to getting lost during a hard reset since it is not fsynced like the other sstable components.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.io.CompactSerializerTest.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.EstimatedHistogram.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2387" opendate="2011-3-25 00:00:00" fixdate="2011-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible for pig to understand packed data</summary>
      <description>Packed values are throwing off pig. This ticket is to make it so pig can interpret packed values. Originally we thought we could just use a loadcaster. However, the only way we know how we can do it now is to get the schema through thrift and essentially perform the function of the loadcaster in the getNext method.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.utils.FBUtilities.java</file>
      <file type="M">src.java.org.apache.cassandra.utils.ByteBufferUtil.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.UTF8Type.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.TimeUUIDType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.LongType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.LocalByPartionerType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.LexicalUUIDType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.IntegerType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.BytesType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.AsciiType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.AbstractType.java</file>
      <file type="M">contrib.pig.src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2406" opendate="2011-3-31 00:00:00" fixdate="2011-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secondary index and index expression problems</summary>
      <description>When I iteratively get data with secondary index and index clause, result of data acquired by consistency level "one" is different from the one by consistency level "quorum". The one by consistecy level "one" is correct result. But the one by consistecy level "quorum" is incorrect and is dropped by Cassandra. You can reproduce the bug by executing attached programs. 1. Start Cassandra cluster. It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner. Initial tokens of those nodes are &amp;#91;"31", "32", "33"&amp;#93;. 2. Create keyspace and column family, according to "create_table.cli", 3. Execute "secondary_index_insertv2.py", inserting a few hundred columns to cluster 4. Execute "secondary_index_checkv2.py" and get data with secondary index and index clause iteratively. "secondary_index_insertv2.py" and "secondary_index_checkv2.py" require pycassa.You will be able to execute 4th "secondary_index_checkv2.py" script with following option so that you get data with consistency level "one". % python "secondary_index_checkv2.py" -oneOn the other hand, to acquire data with consistency level "quorum", you will need to use following option. % python "secondary_index_checkv2.py" -quorumYou can check that result of data acquired by consistency level "one" is different from one by consistency level "quorum".</description>
      <version>0.7.5</version>
      <fixedVersion>Feature/2iIndex</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.ColumnFamilyStoreTest.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2413" opendate="2011-4-1 00:00:00" fixdate="2011-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce default memtable size</summary>
      <description>I'm going to wimp out on targeting CASSANDRA-2006 for 0.7.5 so to mitigate OOMing by newcomers let's reduce the default memtable size &amp;#8211; what we have now predates indexes, which can dramatically increase memory requirements.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2414" opendate="2011-4-1 00:00:00" fixdate="2011-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IntegerType isn&amp;#39;t noted in cli</summary>
      <description>The cli's help doesn't list all comparator types.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cli.CliUserHelp.java</file>
    </fixedFiles>
  </bug>
  <bug id="2416" opendate="2011-4-5 00:00:00" fixdate="2011-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException in CacheWriter.saveCache()</summary>
      <description>I've seen NullPointerException of CacheWriter in our cluster (replication 3).ERROR &amp;#91;CompactionExecutor:1&amp;#93; 2011-04-05 09:57:42,968 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread&amp;#91;CompactionExecutor:1,1,main&amp;#93;java.lang.RuntimeException: java.lang.NullPointerException at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)Caused by: java.lang.NullPointerException at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:275) at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:84) at org.apache.cassandra.db.CompactionManager$10.runMayThrow(CompactionManager.java:960) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30) ... 6 more</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2418" opendate="2011-4-5 00:00:00" fixdate="2011-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>default gc log settings overwrite previous log</summary>
      <description>For those spoiled by nice rolling and appending syslogs log4js etc the JVM gc log can be jarring: # GC logging options -- uncomment to enable# JVM_OPTS="$JVM_OPTS -XX:+PrintGCDetails"# JVM_OPTS="$JVM_OPTS -XX:+PrintGCTimeStamps"# JVM_OPTS="$JVM_OPTS -XX:+PrintClassHistogram"# JVM_OPTS="$JVM_OPTS -XX:+PrintTenuringDistribution"# JVM_OPTS="$JVM_OPTS -XX:+PrintGCApplicationStoppedTime"# JVM_OPTS="$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log" Will result in gc.log with days of data being overwritten on restart, which leads to sad faces.The simplest change would be along these lines: GC_LOG_TS=`date +%s`JVM_OPTS="$JVM_OPTS -XX:+PrintGCDetails"JVM_OPTS="$JVM_OPTS -XX:+PrintGCTimeStamps"JVM_OPTS="$JVM_OPTS -XX:+PrintClassHistogram"JVM_OPTS="$JVM_OPTS -XX:+PrintTenuringDistribution"JVM_OPTS="$JVM_OPTS -XX:+PrintGCApplicationStoppedTime"JVM_OPTS="$JVM_OPTS -Xloggc:/var/log/cassandra/gc-$GC_LOG_TS.log" There are probably prettier approaches.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.cassandra-env.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2421" opendate="2011-4-5 00:00:00" fixdate="2011-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make sure we get the output schema when we output to cassandra from pig</summary>
      <description>see summary</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.pig.src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2425" opendate="2011-4-6 00:00:00" fixdate="2011-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid unnecessary copies in range/index scans</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageProxy.java</file>
      <file type="M">src.java.org.apache.cassandra.service.RangeSliceResponseResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2428" opendate="2011-4-6 00:00:00" fixdate="2011-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running cleanup on a node with join_ring=false removes all data</summary>
      <description>If you need to bring up a node with join_ring=false for operator maintenance, and this node already has data, it will end up removing the data on the node. We noticed this when we were calling cleanup on a specific CF.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.CompactionManager.java</file>
      <file type="M">test.unit.org.apache.cassandra.db.CleanupTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2431" opendate="2011-4-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try harder to close scanners after a failed compaction</summary>
      <description>Forked from 2191.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.util.FileUtils.java</file>
      <file type="M">src.java.org.apache.cassandra.io.CompactionIterator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2435" opendate="2011-4-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>auto bootstrap happened on already bootstrapped nodes</summary>
      <description>I believe the following was observed on 0.7.2. I meant to dig deeper, but never had the time, and now I want to at least file this even if I don't have extremely helpful information.A piece of background is that we consciously made the decision to have the default configuration on nodes have auto_bootstrap set to true. The logic was that if one accidentally were to start a new node, we'd rather have it join with data than join without data and cause bogus read results in the cluster.We executed this policy (by way of having the puppet managed config have auto_bootstrap set to true).On one of our clusters with 5 nodes, we did some moves. All looked well; the moves completed. For unrelated reasons, we wanted to restart nodes after they had been moved. When we did, three of the 5, specifically those 3 that were NOT seed nodes, initiated a bootstrap procedure! Before the moves the cluster had been running for several days at least.The logs indicated the automatic token selection, and they joined the ring under a new automatically selected token.Presumably, this violated consistency but at the time there was no live traffic to the cluster and we didn't confirm (put traffic on it after repair+cleanup).I did look a little bit at the code in light of this but didn't see anything obvious, so I don't really know what the likely culprit is.A potential complication was that seed nodes were moved without using the correct procedure of de-seeding them first. This was clearly wrong, but it is not obvious to me that it would cause other nodes to incorrectly bootstrap since a node should never bootstrap more than once if the local system tables say it's been bootstrapped.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2442" opendate="2011-4-8 00:00:00" fixdate="2011-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For &amp;#39;describe keyspace ks&amp;#39; - it should output the default_validation_class for each column family</summary>
      <description>Even when a default_validation_class is set for a column family, describe keyspace doesn't output its value. It would be nice to see the value to verify that it is what it was set to.</description>
      <version>0.7.5</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2451" opendate="2011-4-11 00:00:00" fixdate="2011-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make clean compactions cleanup the row cache</summary>
      <description>We uselessly keep in cache keys that are cleanup, which is not a big deal because they will get expunged eventually but there is no point in wasting the memory in the meantime.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.CompactionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2460" opendate="2011-4-12 00:00:00" fixdate="2011-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make scrub validate deserialized columns</summary>
      <description>Right now, scrub deserialize the columns but don't validate the fields, and such there is a number of errors it could fix (or at least corrupted rows it could skip) but don't.This ticket proposes to handle those errors.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.CounterColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.context.CounterContext.java</file>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableIdentityIterator.java</file>
      <file type="M">src.java.org.apache.cassandra.db.SuperColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.IColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ExpiringColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.DeletedColumn.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamily.java</file>
      <file type="M">src.java.org.apache.cassandra.db.Column.java</file>
    </fixedFiles>
  </bug>
  <bug id="2463" opendate="2011-4-12 00:00:00" fixdate="2011-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers</summary>
      <description>Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate &amp;#91;n&amp;#93; contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the "release valve" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.Here's the base 0.7.4 release:http://cl.ly/413g2K06121z252e2t10Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.Here's the 0.7.4 release with this patch applied:http://cl.ly/050I1g26401B1X0w3s1fThis graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.io.sstable.SSTableWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2465" opendate="2011-4-13 00:00:00" fixdate="2011-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pig load/storefunc loads only one schema and BytesType validation class needs fix</summary>
      <description>With a recent optimization, it appears that the Pig load/store func gets only one schema from Cassandra and tries to apply it to all CFs in the pig script. Also, the BytesType validation tries to cast the object in putNext as a DataByteArray and wrap it as a ByteBuffer. Instead it should just call objToBB which should take care of it.</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.pig.src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2502" opendate="2011-4-18 00:00:00" fixdate="2011-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable cache saving on system CFs</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="2512" opendate="2011-4-19 00:00:00" fixdate="2011-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Updating a column&amp;#39;s validation class from AsciiType to UTF8Type does not actually work</summary>
      <description>Please note this is reproducible on both Cassandra 0.74 and the April 18th trunk build.Reproduction Stepscreate column family users with comparator = UTF8Typeand column_metadata = [{column_name: password, validation_class: UTF8Type},{column_name: gender, validation_class: AsciiType}];update column family users with comparator = UTF8Typeand column_metadata = [{column_name: password, validation_class: UTF8Type}{column_name: gender, validation_class: UTF8Type}];Before &amp; After quitting cassandra-cli: Notice the validation class for the gender client still shows AsciiType[default@demo] describe keyspace demo;Keyspace: demo: Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy Options: [datacenter1:1] Column Families: ColumnFamily: users Key Validation Class: org.apache.cassandra.db.marshal.BytesType Default column value validator: org.apache.cassandra.db.marshal.BytesType Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type Row cache size / save period in seconds: 0.0/0 Key cache size / save period in seconds: 200000.0/14400 Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes) GC grace seconds: 864000 Compaction min/max thresholds: 4/32 Read repair chance: 1.0 Replicate on write: false Built indexes: [] Column Metadata: Column Name: gender Validation Class: org.apache.cassandra.db.marshal.AsciiType Column Name: password Validation Class: org.apache.cassandra.db.marshal.UTF8Type</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.db.ColumnFamilyStoreTest.java</file>
      <file type="M">src.java.org.apache.cassandra.service.StorageService.java</file>
      <file type="M">src.java.org.apache.cassandra.db.ColumnFamilyStore.java</file>
      <file type="M">src.java.org.apache.cassandra.config.ColumnDefinition.java</file>
      <file type="M">src.java.org.apache.cassandra.config.CFMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="2513" opendate="2011-4-20 00:00:00" fixdate="2011-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>r/m unnecessary AbstractType getInstance methods</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.utils.FBUtilities.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.UTF8Type.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.TimeUUIDType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.LongType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.LexicalUUIDType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.IntegerType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.BytesType.java</file>
      <file type="M">src.java.org.apache.cassandra.db.marshal.AsciiType.java</file>
      <file type="M">contrib.pig.src.java.org.apache.cassandra.hadoop.pig.CassandraStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2514" opendate="2011-4-20 00:00:00" fixdate="2011-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren&amp;#39;t sufficient live nodes</summary>
      <description>We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace definition:&lt;snip&gt;keyspaces: name: KeyspaceMetadata replica_placement_strategy: org.apache.cassandra.locator.NetworkTopologyStrategy strategy_options: DC1 : 2 DC2 : 2 replication_factor: 4&lt;/snip&gt;I shutdown all except one node and waited for the live node to recognize that other nodes are dead. Following is the nodetool ring output on the live node:Address Status State Load Owns Token 169579575332184635438912517119426957796 10.17.221.19 Down Normal ? 29.20% 49117425183422571410176530597442406739 10.17.221.17 Up Normal 81.64 KB 4.41% 56615248844645582918169246064691229930 10.16.80.54 Down Normal ? 21.13% 92563519227261352488017033924602789201 10.17.221.18 Down Normal ? 45.27% 169579575332184635438912517119426957796 I expect UnavailableException when I send batch_mutate request to node that is up. However, it returned TimeOutException:TimedOutException() at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493) at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916) at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)Following is the cassandra-topology.properties Cassandra Node IP=Data Center:Rack10.17.221.17=DC1:RAC110.17.221.19=DC1:RAC210.17.221.18=DC2:RAC110.16.80.54=DC2:RAC2</description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.service.WriteResponseHandler.java</file>
      <file type="M">src.java.org.apache.cassandra.service.DatacenterWriteResponseHandler.java</file>
      <file type="M">src.java.org.apache.cassandra.service.DatacenterSyncWriteResponseHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2516" opendate="2011-4-20 00:00:00" fixdate="2011-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow LOCAL_QUORUM, EACH_QUORUM CLs to work w/ any Strategy class</summary>
      <description></description>
      <version>0.7.5</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.locator.NetworkTopologyStrategy.java</file>
      <file type="M">src.java.org.apache.cassandra.locator.AbstractReplicationStrategy.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
