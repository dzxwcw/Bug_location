<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="CASSANDRA">
  
  
  <bug fixdate="2016-10-28 01:00:00" id="12720" opendate="2016-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permissions on aggregate functions are not removed on drop</summary>
      <description>When a user defined aggregate is dropped, either directly or when it's enclosing keyspace is dropped, permissions granted on it are not cleaned up.</description>
      <version>2.2.9,3.0.10,3.10</version>
      <fixedVersion>Legacy/DistributedMetadata</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.auth.AuthMigrationListener.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-10 01:00:00" id="12765" opendate="2016-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSTable ignored incorrectly with partition level tombstone</summary>
      <description>CREATE TABLE test.payload( bucket_id TEXT, name TEXT, data TEXT, PRIMARY KEY (bucket_id, name));insert into test.payload (bucket_id, name, data) values ('8772618c9009cf8f5a5e0c18', 'test', 'hello');Flush nodes (nodetool flush)insert into test.payload (bucket_id, name, data) values ('8772618c9009cf8f5a5e0c19', 'test2', 'hello');delete from test.payload where bucket_id = '8772618c9009cf8f5a5e0c18';Flush nodes (nodetool flush)select * from test.payload where bucket_id = '8772618c9009cf8f5a5e0c18' and name = 'test';Expected 0 rows but get 1 row back.</description>
      <version>2.1.17,2.2.9,3.0.10,3.10,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/LocalWrite-ReadPaths</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.cassandra.db.CollationController.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-31 01:00:00" id="12863" opendate="2016-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cqlsh COPY FROM cannot parse timestamp in partition key if table contains a counter value</summary>
      <description>This sample table:CREATE TABLE test (columnname text, day timestamp, israndom boolean, columnvalue text, counter counter, PRIMARY KEY ((columnname, day, israndom), columnvalue));with this sample data:origins|2016-10-01 00:00:00+0000|False|ACTUAL|6origins|2016-10-01 00:00:00+0000|False|ADGMOB|4origins|2016-10-01 00:00:00+0000|False|ANONPM|4origins|2016-10-01 00:00:00+0000|False|CSRT2L|76origins|2016-10-01 00:00:00+0000|False|DIAGOP|18origins|2016-10-01 00:00:00+0000|False|E-SOFT|17origins|2016-10-01 00:00:00+0000|False|E-TASK|10when imported withCOPY ks.test FROM 'test.csv' WITH DELIMITER = '|';will generate a parse error:Failed to import 7 rows: ParseError - can't interpret u"'2016-10-01 00:00:00+0000'" as a date with this format: %Y-%m-%d %H:%M:%S%z, given up without retriesThe problem is that when a counter value is present, we don't use prepared statements and so we typically don't convert values unless they are part of the partition key. We also add quotes for certain types, such as timestamps. The problem is that we do not remove such quotes before parsing the partition key values, therefore ending up with a parse error.</description>
      <version>2.2.9,3.0.10,3.10</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.copyutil.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-3 01:00:00" id="12876" opendate="2016-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Negative mean write latency</summary>
      <description>The mean write latency returned by JMX turns negative every 30 minutes. As the attached screenshots show, the value turns negative every 30 minutes after the startup of the node.We did not experience this behavior in 2.1.16.</description>
      <version>2.2.9,3.0.11,3.11.0,4.0-alpha1,4.0</version>
      <fixedVersion>Legacy/Observability</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoirTest.java</file>
      <file type="M">src.java.org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-14 01:00:00" id="12909" opendate="2016-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cqlsh copy cannot parse strings when counters are present</summary>
      <description>We get parse error Failed to import 1 rows: ParseError - argument for 's' must be a string when using the following table and data:CREATE TABLE ks.test ( object_id ascii, user_id timeuuid, counter_id ascii, count counter, PRIMARY KEY ((object_id, user_id), counter_id))EVT:be3bd2d0-a68d-11e6-90d4-1b2a65b8a28a,f7ce3ac0-a66e-11e6-b58e-4e29450fd577,SA,2The problem is this line here, strings are serialized as unicode rather than ordinary strings but only for non-prepared statements (unsure why).</description>
      <version>2.2.9,3.0.11,3.10</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.copyutil.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-27 01:00:00" id="12959" opendate="2016-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>copy from csv import wrong values with udt having set when fields are not specified in correct order in csv</summary>
      <description>create KEYSPACE test WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};CREATE TYPE test.my_udt ( first_field text, second_field frozen&lt;set&lt;text&gt;&gt;);CREATE TABLE test.test ( key text, value my_udt, PRIMARY KEY (key));The following works as expected : INSERT INTO test.test (key , value ) VALUES ( 'key1', {second_field: {'test1', 'test2'}, first_field: 'first_field'}); key | value-----+--------------------------------------------------------------- key1 | {first_field: 'first_field', second_field: {'test1', 'test2'}}but when inserted using a .csv the result is wrong:"key1","{second_field: {'test1', 'test2'}, first_field: 'first_field'}"COPY test.test FROM '~/test.csv'; key | value-----+--------------------------------------------------------------------- key1 | {first_field: '{''test1'', ''test2''}', second_field: {'irst_fiel'}}it works as expected if the keys are in order: "key1","{first_field: 'first_field', second_field: {'test1', 'test2'}}")</description>
      <version>2.1.17,2.2.9,3.0.11,3.10</version>
      <fixedVersion>Legacy/Tools</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pylib.cqlshlib.copyutil.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-7 01:00:00" id="13017" opendate="2016-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DISTINCT queries on partition keys and static column might not return all the results</summary>
      <description>In 2.1 and 2.2, a DISTINCT query on partition keys and static columns might not return all the data if some rows have no data and the static columns have also no values.The problem can be reproduced using the Java driver with the following code: session = cluster.connect(); session.execute("CREATE KEYSPACE IF NOT EXISTS test WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor' : '1'}"); session.execute("USE test"); session.execute("DROP TABLE IF EXISTS test"); session.execute("CREATE TABLE test (pk int, c int, v int, s int static, primary key(pk, c))"); PreparedStatement prepare = session.prepare("INSERT INTO test (pk, c, v, s) VALUES (?, ?, ?, ?)"); for (int i = 0; i &lt; 10; i++) for (int j = 0; j &lt; 1; j++) session.execute(prepare.bind(i, j, null, null)); for (Row row : session.execute(new SimpleStatement("SELECT DISTINCT token(pk), pk, s FROM test").setFetchSize(2))) { System.out.println(row); }</description>
      <version>2.1.17,2.2.9</version>
      <fixedVersion>Legacy/CQL</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test.unit.org.apache.cassandra.service.QueryPagerTest.java</file>
      <file type="M">src.java.org.apache.cassandra.service.pager.RangeSliceQueryPager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>