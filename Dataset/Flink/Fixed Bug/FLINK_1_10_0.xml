<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10918" opendate="2018-11-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incremental Keyed state with RocksDB throws cannot create directory error in windows</summary>
      <description>Facing error while enabling keyed state with RocksDBBackend with checkpointing to a local windows directory Caused by: org.rocksdb.RocksDBException: Failed to create dir: /c:/tmp/data/job_dbe01128760d4d5cb90809cd94c2a936_op_StreamMap_b5c8d46f3e7b141acf271f12622e752b__3_8__uuid_45c1f62b-a198-44f5-add5-7683079b03f8/chk-1.tmp: Invalid argument                at org.rocksdb.Checkpoint.createCheckpoint(Native Method)                at org.rocksdb.Checkpoint.createCheckpoint(Checkpoint.java:51)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$RocksDBIncrementalSnapshotOperation.takeSnapshot(RocksDBKeyedStateBackend.java:2549)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$IncrementalSnapshotStrategy.performSnapshot(RocksDBKeyedStateBackend.java:2008)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:498)                at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:406)                ... 13 more  </description>
      <version>1.6.2,1.9.2,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotTransformerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SnapshotDirectoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SnapshotDirectory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DirectoryStateHandle.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10938" opendate="2018-11-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add e2e test for natively running Flink session cluster on Kubernetes</summary>
      <description>Add E2E tests to verify Flink on K8s integration</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10939" opendate="2018-11-19 00:00:00" fixdate="2018-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documents for natively running Flink session cluster on k8s</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="1094" opendate="2014-9-9 00:00:00" fixdate="2014-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify input split assignments</summary>
      <description>The input split assigner is currently one shared instances across jobs and inputs. The mapping between splits, split type, and assigner is very inflexible, as it requires changing an internal class in the runtime project.The following changes will make it simpler and more efficient: Attach the split assigner to the job vertex (the ExecutionJobVertex) for the task that consumes the input splits. Move the input split assigner interfaces into the core project, as well as the simple base implementations of the assigners (default and locality aware) Let input split producers (such as input formats) create their own assigners.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.LocalInstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.DefaultInstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.LocatableInputSplitList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.LocatableInputSplitAssigner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.InputSplitWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.InputSplitTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.InputSplitManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.InputSplitAssigner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.file.FileInputSplitList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.file.FileInputSplitAssigner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.splitassigner.DefaultInputSplitAssigner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.LocatableInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.InputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.GenericInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileChannelWrapper.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.BlockLocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="11074" opendate="2018-12-5 00:00:00" fixdate="2018-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the harness test to make it possible test with state backend</summary>
      <description>Currently, the harness test can only test without state backend. If you use a DataView in the accumulator of the aggregate function, the DataView is a java object and held in heap, not replaced with StateMapView/StateListView which values are actually held in the state backend. We should improve the harness test to make it possible to test with state backend. Otherwise, issues such as FLINK-10674 could have never been found. With this harness test available, we could test the built-in aggregate functions which use the DataView more fine grained.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CollectAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11136" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13185" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Bump Calcite dependency to 1.20.0 in sql parser &amp; flink planner</summary>
      <description>blink planner had upgraded calcite version to 1.20.0 (before version is 1.19.0), and blink planner will support DDL in FLINK-1.9 which depends on flink-sql-parser. so calcite version in flink-sql-parser should also be upgrade to 1.20.0.walterddr, FLINK-11935 will not be fixed in this issue, because supporting DDL in blink planner is blocked by this.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.RetractionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.BasicOperatorTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilderFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13277" opendate="2019-7-15 00:00:00" fixdate="2019-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add documentation of Hive source/sink</summary>
      <description>add documentation of Hive source/sink in batch/connector.mdits corresponding Chinese one is FLINK-13278cc xuefuz lirui Terry1897</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="13299" opendate="2019-7-17 00:00:00" fixdate="2019-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python failed on Travis</summary>
      <description>Log: https://api.travis-ci.com/v3/job/216620643/log.txtError:___________________________________ summary ____________________________________ERROR: py27: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/2.7/bin/python2.7 py27 (exited with code 1) py33: commands succeeded ERROR: py34: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/3.4/bin/python3.4 py34 (exited with code 100) py35: commands succeeded py36: commands succeeded py37: commands succeeded ============tox checks... &amp;#91;FAILED&amp;#93;============ PYTHON exited with EXIT CODE: 1. Trying to KILL watchdog (12896). ./tools/travis_watchdog.sh: line 229: 12896 Terminated watchdog</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13314" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner</summary>
      <description>Correct resultType of the following PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner:Minus/plus/Div/Mul/Ceil/Floor/Round </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="13315" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port wmstrategies to api-java-bridge</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1333" opendate="2014-12-16 00:00:00" fixdate="2014-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getter/Setter recognition for POJO fields with generics is not working</summary>
      <description>Fields likeprivate List&lt;Contributors&gt; contributors;Are not recognized correctly, even if they have getters and setters.Workaround: make them public.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13335" opendate="2019-7-19 00:00:00" fixdate="2019-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bring the SQL CREATE TABLE DDL closer to FLIP-37</summary>
      <description>At a first glance it does not seem that the newly introduced DDL is compliant with FLIP-37. We should ensure consistent behavior esp. also for corner cases.Update:This brings the SQL DDL closer to FLIP-37. However, there are a couple of known limitations. Currently unsupported features: INTERVAL ROW with comments ANY NULL NOT NULL/NULL for top-level types ignoring collation/charset VARCHAR without length (=VARCHAR(1)) TIMESTAMP WITH TIME ZONE user-defined types data types in non-DDL parts (e.g. CAST(f AS STRING))</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.SqlRowType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.SqlMapType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="13339" opendate="2019-7-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an implementation of pipeline&amp;#39;s api</summary>
      <description>Add an implement PipelineStage, Estimator, Transformer, Model. Add MLSession to hold the execution environment and others session shared variable. Add AlgoOperator for the implementation of algorithms. Add BatchOperator and StreamOperator based on AlgoOperator Add TableSourceBatchOp and TableSourceStreamOp</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13353" opendate="2019-7-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2 args constructor in REPLACE expression</summary>
      <description>Replace definition in stringExpression.scala has another constructor with 2 arguments.According to source code, the args' meaning are str, begin. And it call other constructor with 3 args adding the 3rd arg which is the length of str.But its expectTypes is (String, String, String), but actually is (String, int, int).So I think the 2 args defined constructor means search and replacement is "" default, not begin and length of str. </description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13354" opendate="2019-7-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to use blink planner</summary>
      <description>Add documentation for how to use different planner “Overview”: add pom dependency “Concepts &amp; Common API”: add description about how to use different planner in code</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="13355" opendate="2019-7-22 00:00:00" fixdate="2019-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Temporal Table Join in blink planner</summary>
      <description>Add documentation for Temporal Table Join in blink planner “Streaming Concepts / Temporal Tables”: introduce concepts of temporal table in blink planner and the difference and sameness to flink planner temporal table “Joins in Continuous Queries”: how to use temporal join in bink planner “SQL”: join with temporal table in SQL</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="13356" opendate="2019-7-22 00:00:00" fixdate="2019-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for TopN and Deduplication in blink planner</summary>
      <description>Add documentation for TopN in blink planner“SQL”: how to write TopN in SQL and some tips</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="13359" opendate="2019-7-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DDL introduction</summary>
      <description>Add documentation for DDL introduction “Concepts &amp; Common API”: Add a section to describe how to execute DDL on TableEnvironment. “SQL Client”: Add a section and example in SQL CLI page too?</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="13453" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump shade plugin to 3.2.1</summary>
      <description>The shade plugin fails with an IllegalArgumentException when run on Java 11.</description>
      <version>None</version>
      <fixedVersion>shaded-9.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13454" opendate="2019-7-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp jaxb dependencies</summary>
      <description>The japicmp plugins fails with a ClassNotFoundExceptions with the currently defined jaxb dependencies when run on java 11.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13455" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move jdk.tools exclusions out of dependency management</summary>
      <description>Defining exclusions via dependencyManagement is a bit unreliable, since the shade-plugin ignores them during dependency resolution  if they are defined for transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13456" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lombok to 1.16.22</summary>
      <description>Compiling the tests for flink-core fails with an ErrorDuringInitialization due to lombok,</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13473" opendate="2019-7-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GroupWindowed FlatAggregate support to stream Table API(blink planner), i.e, align with flink planner</summary>
      <description>Add GroupWindowed FlatAggregate support to stream Table API(blink planner), i.e, align with flink planner.The API looks like: TableAggregateFunction tableAggFunc = new MyTableAggregateFunction(); tableEnv.registerFunction("tableAggFunc", tableAggFunc); windowGroupedTable .flatAggregate("tableAggFunc(a, b) as (x, y, z)") .select("key, window.start, x, y, z")The detail can be found in Flip-29</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorContractTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.PanedWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.GeneralWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.NamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.GroupWindowTableAggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13488" opendate="2019-7-30 00:00:00" fixdate="2019-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python fails to build on Travis due to PackagesNotFoundError</summary>
      <description>https://api.travis-ci.org/v3/job/564925115/log.txtinstall conda ... [SUCCESS]install miniconda... [SUCCESS]installing python environment...installing python2.7...install python2.7... [SUCCESS]installing python3.3...PackagesNotFoundError: The following packages are not available from current channels: - python=3.3Current channels: - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1349" opendate="2015-1-5 00:00:00" fixdate="2015-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanups of recently added Akka code</summary>
      <description>There are a few remaining cleanup: There are still various config constants relating to the old RPC There is still a bit of unused code and utilities from the old RPC The Writable/IOReadableWritable serialzation buffer start size should be smaller (most messages are rather small) For message logging, make system calls (timestamps) only in debug mode Add serial version UIDs to all serializable classes</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManagerMessages.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.TaskManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.CustomCompensatableDanglingPageRankWithCombiner.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.CustomCompensatableDanglingPageRank.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.serialization.WritableSerializer.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.serialization.IOReadableWritableSerializer.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.ActorLogMessages.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SocketOutputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SocketIOWithTimeout.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SocketInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.NetUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.WebInfoServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.SubSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.RemoteReceiver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.ConnectionInfoLookupResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.DistributedFileStatus.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.GateDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ChannelDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobKey.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.accumulators.AccumulatorEvent.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.Path.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-addons.flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13494" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner changes source parallelism which causes stream SQL e2e test fails</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.nodes.resource.ExecNodeResourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.resource.ExecNodeResourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testGetStatsFromCatalog.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.FinalParallelismSetterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.MockNodeTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.NodeResource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.NodeResourceUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.FinalParallelismSetter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ParallelismProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStage.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13495" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink-planner should support decimal precision to table source</summary>
      <description>Now there is an exception when use DataTypes.DECIMAL(5, 2) to table source when use blink-planner.Some conversions between DataType and TypeInfo loose precision information.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13504" opendate="2019-7-31 00:00:00" fixdate="2019-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoSuchFieldError when executing DDL via tEnv.sqlUpdate in application project</summary>
      <description>When we create a quickstart project to try flink 1.9/1.10, a NoSuchFieldError is thrown.The dependencies (the flink 1.0 is installed locally for commit 70fe6aa747ad021bbb8dd8cdc0beecc863f010be, flink 1.9 has the same problem): &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;The program code:package com.github.wuchong;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;public class DDLTest { public static void main(String[] args) { EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.sqlUpdate("CREATE TABLE MyTable (\n" + " a int, \n" + " b bigint, \n" + " c varchar \n" + ")\n comment 'table comment'\n" + "partitioned by (b)\n" + "with (\n" + " connector = 'csv', \n" + " csv.path = '/tmp/path'\n" + ")"); }}The exception:Exception in thread "main" java.lang.NoSuchFieldError: names at org.apache.flink.sql.parser.ddl.SqlCreateTable.fullTableName(SqlCreateTable.java:326) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:140) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:86) at org.apache.flink.table.planner.StreamPlanner.parse(StreamPlanner.scala:115) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at com.github.wuchong.DDLTest.main(DDLTest.java:29)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13509" opendate="2019-7-31 00:00:00" fixdate="2019-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forbidden `IS NOT DISTINCT FROM `(or an expanded version) in LookupJoin</summary>
      <description>Example1:`SELECT T.id, T.len, T.content, D.name FROM T JOIN userTable for system_time as of T.proctime AS D ON T.id = D.id OR (T.id is null and D.id is null)`Example2:"SELECT T.id, T.len, T.content, D.name FROM T JOIN userTable for system_time as of T.proctime AS D ON T.id IS NOT DISTINCT FROM D.id"In 1.9 version, we can simply throw exception in compile phase for the above sql and add proper support in FLINK-13648.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13518" opendate="2019-7-31 00:00:00" fixdate="2019-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive tests</summary>
      <description>Hive straight up doesn't support Java 11 (or anything above Java 8 really), so we might as well disable all tests on Java 11.15:49:57.131 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.208 s - in org.apache.flink.batch.connectors.hive.HiveTableFactoryTestSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.15/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.15:50:11.610 [INFO] Running org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.625 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.627 [ERROR] org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest Time elapsed: 0.007 s &lt;&lt;&lt; ERROR!java.lang.IllegalStateException: Failed to create HiveServer :Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.RuntimeException: Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13523" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct arithmetic function&amp;#39;s semantic for Blink planner</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.MiniBatchGroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ReturnTypeInference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.agg.TestLongAvgFunc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13529" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct agg function&amp;#39;s semantic for Blink planner</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlFirstLastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlIncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13545" opendate="2019-8-2 00:00:00" fixdate="2019-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin</summary>
      <description>run tpcds 14.a on blink planner, an exception will thrownjava.lang.ArrayIndexOutOfBoundsException: 84 at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564) at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80) at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284) at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)the reason is JoinToMultiJoinRule should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by SemiJoin which is not matched JoinToMultiJoinRule.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13547" opendate="2019-8-2 00:00:00" fixdate="2019-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct string function&amp;#39;s semantic for Blink planner</summary>
      <description>Currently many string builtin functions in blink planner follow hive/spark semantics, which should keep compatible with old planner. And some non-standard functions(Blink planner intros) should be removed. concat/concat_ws function (null treatment) substring function (follow calcite/flink) from_base64 should return string not binary intro truncate function to blink planner uuid should be no-argument (remove the one-argument version) length/jsonvalue/keyvalue/substr (non-standard function should be removed) md5/sha1/sha2/sha224/sha256/sha384/sha512(remove the two-arguments version) ascii (operand type should beSqlTypeFamily.CHARACTER)</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryStringTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryStringUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13550" opendate="2019-8-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for CPU FlameGraphs in web UI</summary>
      <description>For a better insight into a running job, it would be useful to have ability to render a CPU flame graph for a particular job vertex.Flink already has a stack-trace sampling mechanism in-place, so it should be straightforward to implement.This should be done by implementing a new endpoint in REST API, which would sample the stack-trace the same way as current BackPressureTracker does, only with a different sampling rate and length of sampling.Here is a little demo of the feature.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.tsconfig.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.index.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoSamplesRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.JobVertexStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.TaskThreadInfoResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.rest.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="13563" opendate="2019-8-3 00:00:00" fixdate="2019-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TumblingGroupWindow should implement toString method</summary>
      <description>@Test def testAllEventTimeTumblingGroupWindowOverTime(): Unit = { val util = streamTestUtil() val table = util.addDataStream[(Long, Int, String)]( "T1", 'long, 'int, 'string, 'rowtime.rowtime) val windowedTable = table .window(Tumble over 5.millis on 'rowtime as 'w) .groupBy('w) .select('int.count) util.verifyPlan(windowedTable) }currently, it's physical plan is HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])we know nothing about the TumblingGroupWindow except its name. the expected plan isHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.logical.groupWindows.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13564" opendate="2019-8-3 00:00:00" fixdate="2019-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</summary>
      <description>just as FLINK-11017, blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13567" opendate="2019-8-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed on Travis</summary>
      <description>The Avro Confluent Schema Registry nightly end-to-end test failed on Travis with[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directoryhttps://api.travis-ci.org/v3/job/567273939/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13587" opendate="2019-8-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some transformation names are not set in blink planner</summary>
      <description>Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses "LookupJoin" directly which loses a lot of informatoion.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ProjectPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkPruneEmptyRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testGetStatsFromCatalog.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.RelDisplayNameWriterImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.Expand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13633" opendate="2019-8-7 00:00:00" fixdate="2019-9-7 01:00:00" resolution="Done">
    <buginformation>
      <summary>Move submittedJobGraph and completedCheckpoint to cluster-id subdirectory of high-availability storage</summary>
      <description>Currently, if we enable the high-availability, the ha storage directory structure is stored as below. The submittedJobGraph and completedCheckpoint are directly stored under the ha storage path. It is reasonable when the flink cluster finished normally. However, when the Yarn application is failed or killed, the submittedJobGraph and completedCheckpoint will exist there forever. Even we could not know which flink cluster(Yarn application) they belongs to. So i suggest to move them into application subdirectory. Some external tools could be used to clean up these residual files.Also, we need to do best effort clean-up before the flink cluster finishes. Current ha storage directory structure└── &lt;high-availability.storageDir&gt;    ├── submittedJobGraph ├ ├ &lt;jobgraph1&gt;(random named) ├ ├ &lt;jobgraph2&gt;(random named)    ├── completedCheckpoint ├ ├ &lt;checkpoint1&gt;(random named) ├ ├ &lt;checkpoint2&gt;(random named) ├ ├ &lt;checkpoint3&gt;(random named)    ├── &lt;high-availability.cluster-id&gt;    ├── blob    ├── &lt;blob1&gt;(named as [no_job|job_&lt;job-id&gt;]/blob_&lt;blob-key&gt;) The new ha storage directory structure└── &lt;high-availability.storageDir&gt;    ├── &lt;high-availability.cluster-id&gt; ├── submittedJobGraph ├ ├ &lt;jobgraph1&gt;(random named) ├ ├ &lt;jobgraph2&gt;(random named) ├── completedCheckpoint ├ ├ &lt;checkpoint1&gt;(random named) ├ ├ &lt;checkpoint2&gt;(random named) ├ ├ &lt;checkpoint1&gt;(random named) ├── blob ├── &lt;blob1&gt;(named as [no_job|job_&lt;job-id&gt;]/blob_&lt;blob-key&gt;) </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="13634" opendate="2019-8-7 00:00:00" fixdate="2019-11-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Hadoop compression format for use with StreamingFileSink</summary>
      <description>I have developed a CompressFileWriter base on BulkWriter to compress data using hadoop compressor</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="13663" opendate="2019-8-9 00:00:00" fixdate="2019-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test for modern Kafka failed on Travis</summary>
      <description>The SQL Client end-to-end test for modern Kafka failed on Travis because it could not download https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz.Maybe we could add a similar retry logic as with the Kinesis end-to-end test FLINK-13599.https://api.travis-ci.org/v3/job/569262834/log.txthttps://api.travis-ci.org/v3/job/569262828/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13667" opendate="2019-8-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Add the utility class for the Table</summary>
      <description>Add the utility class for the Table the operations on column name the operations on column type</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.TableUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.TableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1367" opendate="2015-1-7 00:00:00" fixdate="2015-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add field aggregations to Streaming Scala api</summary>
      <description>Field aggregations are missing from the streaming scala api for case classes.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13704" opendate="2019-8-13 00:00:00" fixdate="2019-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-H end-to-end test (Blink planner) fails on Travis</summary>
      <description>The TPC-H end-to-end test fails on Travis with the following problem:Running query #22...Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:129) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)[FAIL] Test script contains errors.Checking for errors...org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL update statement. at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:539) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:432) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:367) at org.apache.flink.table.client.cli.CliClient.callInsertInto(CliClient.java:496) at org.apache.flink.table.client.cli.CliClient.lambda$submitUpdate$0(CliClient.java:231) at java.util.Optional.map(Optional.java:215) at org.apache.flink.table.client.cli.CliClient.submitUpdate(CliClient.java:228) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:127) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 13, column 10 to line 13, column 30: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:125) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:82) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:154) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:89) at org.apache.flink.table.planner.delegation.PlannerBase.parse(PlannerBase.scala:130) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.sqlUpdate(StreamTableEnvironmentImpl.java:299) at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$applyUpdate$12(LocalExecutor.java:531) at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216) at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:529) ... 9 moreCaused by: org.apache.calcite.runtime.CalciteContextException: From line 13, column 10 to line 13, column 30: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:809) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4807) at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1762) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:273) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:215) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5566) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5553) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1680) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1665) at org.apache.calcite.sql.type.InferTypes.lambda$static$0(InferTypes.java:46) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1854) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereOrOn(SqlValidatorImpl.java:4006) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:3998) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3368) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3111) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3093) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3365) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:932) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:639) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:122) ... 18 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 51 morehttps://api.travis-ci.org/v3/job/570757857/log.txthttps://api.travis-ci.org/v3/job/570757863/log.txthttps://api.travis-ci.org/v3/job/570757869/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13723" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use liquid-c for faster doc generation</summary>
      <description>Jekyll requires liquid and only optionally uses liquid-c if available. The latter uses natively-compiled code and reduces generation time by ~5% for me.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug id="13725" opendate="2019-8-14 00:00:00" fixdate="2019-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use sassc for faster doc generation</summary>
      <description>Jekyll requires sass but can optionally also use a C-based implementation provided by sassc. Although we do not use sass directly, there may be some indirect use inside jekyll. It doesn't seem to hurt to upgrade here.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug id="13729" opendate="2019-8-14 00:00:00" fixdate="2019-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update website generation dependencies</summary>
      <description>The website generation dependencies are quite old. By upgrading some of them we get improvements like a much nicer code highlighting and prepare for the jekyll update of FLINK-13726 and FLINK-13727.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug id="13745" opendate="2019-8-16 00:00:00" fixdate="2019-3-16 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13746" opendate="2019-8-16 00:00:00" fixdate="2019-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch (v2.3.5) sink end-to-end test fails on Travis</summary>
      <description>The Elasticsearch (v2.3.5) sink end-to-end test fails on Travis because it logs contain the following line:INFO org.elasticsearch.plugins - [Terror] modules [], plugins [], sites []Due to this, the error check is triggered.https://api.travis-ci.org/v3/job/572255901/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1375" opendate="2015-1-8 00:00:00" fixdate="2015-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove incubator references &amp; rename to new project urls</summary>
      <description>We need to change all urls and remove the "incubator" string from it.Also, the website doesn't need the incubator disclaimer.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.DISCLAIMER</file>
    </fixedFiles>
  </bug>
  <bug id="13759" opendate="2019-8-18 00:00:00" fixdate="2019-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>All builds for master branch are failed during compile stage</summary>
      <description>Here is an instance: https://api.travis-ci.org/v3/job/572950228/log.txtThere is an error in the log.==============================================================================find: ‘flink-connectors/flink-connector-elasticsearch/target/flink-connector-elasticsearch*.jar’: No such file or directory==============================================================================Previous build failure detected, skipping cache setup.==============================================================================The flink-connector-elasticsearch is not exist. But recent commits didn't modify this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13760" opendate="2019-8-18 00:00:00" fixdate="2019-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hardcode Scala version dependency in hive connector</summary>
      <description>FLINK-13688 introduced a flink-test-utils dependency in flink-connector-hive. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13841" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Hive version support to all 1.2 and 2.3 versions</summary>
      <description>This is to support all 1.2 (1.2.0, 1.2.1, 1.2.2) and 2.3 (2.3.0-5) versions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="13845" opendate="2019-8-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop all the content of removed "Checkpointed" interface</summary>
      <description>From FLINK-7461, we have already removed the backward compatibility before Flink-1.1 and the deprecated Checkpointed interface has been totally removed. However, we still have many contents including java docs, documentation talked about this non-existing interface. I think it's time to remove these contents now.</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.ListCheckpointed.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="13853" opendate="2019-8-26 00:00:00" fixdate="2019-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA (file, async) end-to-end test failed on Travis</summary>
      <description>Running HA (file, async) end-to-end test failed on Travis:https://api.travis-ci.org/v3/job/576002743/log.txthttps://api.travis-ci.org/v3/job/576002736/log.txthttps://api.travis-ci.org/v3/job/576002730/log.txthttps://api.travis-ci.org/v3/job/576002724/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ha.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13884" opendate="2019-8-28 00:00:00" fixdate="2019-9-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Set default restart-strategy delay to 1s</summary>
      <description>As agreed in FLIP-62, we should set the default delay of all restart strategies to "1 s".</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.NoOrFixedIfCheckpointingEnabledRestartStrategyFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.FailureRateRestartStrategy.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">docs..includes.generated.restart.strategy.configuration.html</file>
      <file type="M">docs..includes.generated.fixed.delay.restart.strategy.configuration.html</file>
      <file type="M">docs..includes.generated.failure.rate.restart.strategy.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="13891" opendate="2019-8-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment flink-shaded version</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13937" opendate="2019-9-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix wrong hive dependency version in documentation</summary>
      <description>There is a wrong maven dependency in the hive connector's documentation.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13942" opendate="2019-9-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Overview page for Getting Started section</summary>
      <description>The Getting Started section provide different types of tutorials that target users with different interests and backgrounds.We should add a brief overview page that describes the different tutorials such that users easily find the material that they need to get started with Flink.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13943" opendate="2019-9-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide api to convert flink table to java List (blink planner)</summary>
      <description>It would be nice to convert flink table to java List so that I can do other data manipulation in client side after execution flink job. For flink planner, I can convert flink table to DataSet and use DataSet#collect, but for blink planner, there's no such api.EDIT from FLINK-14807:Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. Other apis such as Table#head, Table#print is also helpful.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13969" opendate="2019-9-5 00:00:00" fixdate="2019-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test fails on Travis</summary>
      <description>The Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test fails on Travis because its log contains an exceptionorg.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 16 for operator ArtificalKeyedStateMapper_Avro -&gt; ArtificalOperatorStateMapper (2/4). Failure reason: Checkpoint was declined. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1302) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1236) at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:892) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:797) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:728) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88) at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:118) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:48) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:144) at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:277) at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:147) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:404) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: Cannot register Closeable, registry is already closed. Closing argument. at org.apache.flink.util.AbstractCloseableRegistry.registerCloseable(AbstractCloseableRegistry.java:85) at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.&lt;init&gt;(AsyncSnapshotCallable.java:122) at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.&lt;init&gt;(AsyncSnapshotCallable.java:110) at org.apache.flink.runtime.state.AsyncSnapshotCallable.toAsyncSnapshotFutureTask(AsyncSnapshotCallable.java:104) at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.doSnapshot(RocksIncrementalSnapshotStrategy.java:170) at org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.snapshot(RocksDBSnapshotStrategyBase.java:126) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:439) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:411) ... 17 morehttps://api.travis-ci.org/v3/job/580915660/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1397" opendate="2015-1-13 00:00:00" fixdate="2015-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager web interface log file access doesn&amp;#39;t work on YARN</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.LogfileInfoServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="13978" opendate="2019-9-5 00:00:00" fixdate="2019-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Azure Pipelines as a CI tool for Flink</summary>
      <description>See ML discussion: https://lists.apache.org/thread.html/b90aa518fcabce94f8e1de4132f46120fae613db6e95a2705f1bd1ea@%3Cdev.flink.apache.org%3E We want to try out Azure Pipelines for the following reasons: more mature system (compared to travis) 10 parallel, 6 hrs builds for open source ability to add custom machines (See also INFRA-17030) </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="13979" opendate="2019-9-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate new streamfilesink docs to chinese</summary>
      <description>The StreamFileSink docs have been reworked as part of FLINK-13842</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13981" opendate="2019-9-6 00:00:00" fixdate="2019-9-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce a switch for enabling the new task executor memory configurations</summary>
      <description>Introduce a temporal config option as a switch between the current / new task executor memory configuration code paths. This allows us to implement and test the new code paths without affect the existing code paths and behaviors.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="1399" opendate="2015-1-13 00:00:00" fixdate="2015-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for registering Serializers with Kryo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.KryoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="14009" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cron jobs broken due to verifying incorrect NOTICE-binary file</summary>
      <description>With FLINK-13968 we introduced an automatic NOTICE-binary file check. However, since we don't use the correct NOTICE-binary file (FLINK-14008) for Scala 2.12 it fails currently our cron jobs.I suggest to only enable the automatic NOTICE-binary files for Scala 2.11 until FLINK-14008 has been fixed.</description>
      <version>1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14010" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher &amp; JobManagers don&amp;#39;t give up leadership when AM is shut down</summary>
      <description>In YARN deployment scenario, YARN RM possibly launches a new AM for the job even if the previous AM does not terminated, for example, when AMRM heartbeat timeout. This is a common case that RM will send a shutdown request to the previous AM and expect the AM shutdown properly.However, currently in YARNResourceManager, we handle this request in onShutdownRequest which simply close the YARNResourceManager but not Dispatcher and JobManagers. Thus, Dispatcher and JobManager launched in new AM cannot be granted leadership properly. Visually,on previous AM: Dispatcher leader, JM leaderson new AM: ResourceManager leadersince on client side or in per-job mode, JobManager address and port are configured as the new AM, the whole cluster goes into an unrecoverable inconsistent status: client all queries the dispatcher on new AM who is now the leader. Briefly, Dispatcher and JobManagers on previous AM do not give up their leadership properly.</description>
      <version>1.7.2,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14014" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce PythonScalarFunctionRunner to handle the communication with Python worker for Python ScalarFunction execution</summary>
      <description>PythonScalarFunctionRunner is responsible for Python ScalarFunction execution and it only handles the Python ScalarFunction execution and nothing else. So its logic should be very simple, forwarding an input element to Python worker and fetching the execution results back: Internally, it uses Apache Beam’s portability for Python UDF execution and this is transparent for the caller of PythonScalarFunctionRunner By default, each runner will startup a separate Python worker The Python worker can run in a docker, a separate process or even an non-managed external service. It has the ability to execute multiple Python ScalarFunctions It also supports chained Python ScalarFunctions</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.projectsetup.dependencies.zh.md</file>
      <file type="M">docs.dev.projectsetup.dependencies.md</file>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14016" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce RelNodes FlinkLogicalPythonScalarFunctionExec and DataStreamPythonScalarFunctionExec which are containers for Python PythonScalarFunctions</summary>
      <description>Dedicated RelNodes such as FlinkLogicalPythonScalarFunctionExec and DataStreamPythonScalarFunctionExec should be introduced for Python ScalarFunction execution. These nodes exists as containers for Python ScalarFunctions which could be executed in a batch and then we can employ PythonScalarFunctionOperator for Python ScalarFunction execution.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.StreamOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.Optimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionDefinition.java</file>
    </fixedFiles>
  </bug>
  <bug id="14017" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support to start up Python worker in process mode</summary>
      <description>We employ Apache Beam's portability frameowork for the Python UDF execution. However, there is only a golang implementation for the boot script to start up SDK harness in Beam. It’s used by both the Python SDK harness and the Go SDK harness. This is not a problem for Beam. However, it’s indeed a problem for Flink as it indicates that the whole stack of Beam’s Go SDK harness will be depended if we use the golang implementation of the boot script. We want to avoid this by adding a Python boot script.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="14021" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to push down the Python ScalarFunctions contained in the join condition of Correlate node</summary>
      <description>The Python ScalarFunctions contained in the join condition of Correlate node should be extracted to make sure the TableFunction works well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14022" opendate="2019-9-9 00:00:00" fixdate="2019-11-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add validation check for places where Python ScalarFunction cannot be used</summary>
      <description>Currently, there are places where Python ScalarFunction could not be used, for example: Python UDF could not be used in MatchRecognize Python UDFs could not be used in Join condition which take the columns from both the left table and the right table as inputsWe should add validation check for places where it’s not supported.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.Optimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14023" opendate="2019-9-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support accessing job parameters in Python user-defined functions</summary>
      <description>Currently, it’s possible to access job parameters in the Java user-defined functions. It could be used to define the behavior according to job parameters. It should also be supported for Python user-defined functions.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.EmbeddedPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.ProtoUtils.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.table.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.metrics.tests.test.metric.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="14026" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Manage the resource of Python worker properly</summary>
      <description>For a Flink Table API &amp; SQL job, if it uses Python user-defined functions, the Java operator will launch separate Python process for Python user-defined function execution. We should make sure that the resources used by the Python process are managed by Flink’s resource management framework.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14027" opendate="2019-9-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for Python user-defined functions</summary>
      <description>We should add documentation about how to use Python user-defined functions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.dev.table.udfs.zh.md</file>
      <file type="M">docs.dev.table.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="14029" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flink&amp;#39;s Mesos scheduling behavior to reject all expired offers</summary>
      <description>While digging into why our Flink jobs weren't being scheduled on our internal Mesos setup we noticed that we were hitting Mesos quota limits tied to the way we've set up the Fenzo (https://github.com/Netflix/Fenzo/) library defaults in the Flink project. Behavior we noticed was that we got a bunch of offers from our Mesos master (50+) out of which only 1 or 2 of them were super skewed and took up a huge chunk of our disk resource quota. Thanks to this we were not sent any new / different offers (as our usage at the time + resource offers reached our Mesos disk quota). As the Flink / Fenzo Mesos scheduling code was not using the 1-2 skewed disk offers they end up expiring. The way we've set up the Fenzo scheduler is to use the default values on when to expire unused offers (120s) and maximum number of unused offer leases at a time (4). Unfortunately as we have a considerable number of outstanding expired offers (50+) we end up in a situation where we reject only 4 or so every 2 mins and we never get around to rejecting the super skewed disk ones which are stopping us from scheduling our Flink job. Thanks to this we end up in a situation where our job is waiting to be scheduled for more than an hour. An option to work around this is to reject all expired offers at 2 minute expiry time rather than hold on to them. This will allow Mesos to send alternate offers that might be scheduled by Fenzo.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.TaskSchedulerBuilder.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1403" opendate="2015-1-14 00:00:00" fixdate="2015-1-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Streaming api doesn&amp;#39;t support output file named with "file://" prefix</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.SinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.RichSinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="14062" opendate="2019-9-12 00:00:00" fixdate="2019-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set managed memory fractions according to slot sharing groups</summary>
      <description>For operators with specified ResourceSpecs, calculate fractions according to operators ResourceSpecs For operators with unknown ResourceSpecs, calculate fractions according to number of operators using managed memoryThis step should not introduce any behavior changes.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14063" opendate="2019-9-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators use fractions to decide how many managed memory to allocate</summary>
      <description>Operators allocate memory segments with the amount returned by MemoryManager#computeNumberOfPages. Operators reserve memory with the amount returned by MemoryManager#computeMemorySize. This step activates the new fraction based managed memory.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.SortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1411" opendate="2015-1-16 00:00:00" fixdate="2015-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanVisualizer is not working</summary>
      <description>In the current master, the PlanVisualizer is no longer working. The reason is that the resources folder containing the web resources has been moved to the flink-runtime and flink-clients jar. Maybe we should pick up FLINK-1317 and make the PlanVisualizer accessible through the flink website.</description>
      <version>None</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.tools.planVisualizer.html</file>
      <file type="M">flink-dist.src.main.assemblies.yarn.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14117" opendate="2019-9-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate changes on index page to Chinese</summary>
      <description>The changes of commit ee0d6fdf0604d74bd1cf9a6eb9cf5338ac1aa4f9 on the documentation index page should be translated to Chinese.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="14128" opendate="2019-9-19 00:00:00" fixdate="2019-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the description of restart strategy customization</summary>
      <description>Restart strategy customization was not a public interface since it was not documented.Since existing RestartStrategy implementation will not be supported with the new scheduler introduced in FLINK-10429, we'd better not mark restart strategy customization a public interface.This way we' need to remove the documentation of restart strategy customization which was recently added in FLINK-13898.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs..includes.generated.restart.strategy.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14131" opendate="2019-9-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support configurable failover strategy for scheduler NG</summary>
      <description>FLINK-10429 introduces new version failover strategies for scheduler NG.There are 2 failover strategies and can be more in the future.Users should be able to choose proper strategies for their jobs via configuration.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14134" opendate="2019-9-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce LimitableTableSource to optimize limit</summary>
      <description>SQL: select *from t1 limit 1Now source will scan full table, if we can introduce LimitableTableSource, let source know the limit line, source can just read one row is OK.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14135" opendate="2019-9-20 00:00:00" fixdate="2019-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce vectorized orc InputFormat for blink runtime</summary>
      <description>VectorizedOrcInputFormat is introduced to read orc data in batches.When returning each row of data, instead of actually retrieving each field, we use BaseRow's abstraction to return a Columnar Row-like view.This will greatly improve the downstream filtered scenarios, so that there is no need to access redundant fields on the filtered data.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.PlanUtil.scala</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcTableSourceTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcTableSource.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcRowInputFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTablePartition.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableInputSplit.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14139" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix potential memory leak of rest server when using session/standalone cluster</summary>
      <description>Flink's rest server uses netty decoder for http request processing and file uploading. However io.netty.handler.codec.http.multipart.DiskAttribute and io.netty.handler.codec.http.multipart.DiskFileUpload class of netty would register some temp files, including post chunks and upload file chunks, to java.io.DeleteOnExitHook which has a potential of memory leak, because the registered file names will never be deleted before the cluster stops. Most of the time, this is not a big problem, however we use Flink session cluster a long running service for ad-hoc SQL query, this problem gets worse.In fact, Flink handles the clean up of temp files through org.apache.flink.util.ShutdownHookUtil (though not including the post chunks), so it is no need to register these files to java.io.DeleteOnExitHook.</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14150" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary __pycache__ directories appears in pyflink.zip</summary>
      <description>It seems we are packaging _pycache_ directories into pyflink.zip. These directories contain bytecode cache files that are automatically generated by python3. We should remove them from the python source code folder before packaging.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14156" opendate="2019-9-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute/run processing timer triggers taking into account operator level mailbox loops</summary>
      <description>With FLINK-12481, the timer triggers are executed by the mailbox thread and passed to the mailbox with the maximum priority.In case of operators that use mailbox.yield() (introduced in FLINK-13248), current approach may execute timer triggers that belong to an upstream operator. Such timer trigger, may potentially call processElement|Watermark() which eventually would come back to the current operator. This situation may be similar to FLINK-13063.To avoid this, the proposal is to set mailbox letters priorities of timer triggers with the priority of the operator that the trigger belongs to.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskOperatorTimerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14157" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily remove S3 StreamingFileSink end-to-end test</summary>
      <description>This issue temporarily disables the failing test for Java 11 so that we can have a green travis build, until a proper solution is found. In addition, it removes the relocations for Java 8 so that the production code works for Java 8, the main java version Flink supports.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14158" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update Mesos configs to add leaseOfferExpiration and declinedOfferRefuse durations</summary>
      <description>While debugging some Flink on Mesos scheduling issues (tied to our use of Mesos quotas) we end up getting skewed offers that are useless fairly often. As we are not rejecting these offers fast enough and as we are not telling Mesos to not re-send for a long enough period, we end up not being able to schedule our job for upwards of an hour (~30 Mesos containers). The Fenzo default is to reject expired and unused Mesos offers after 120s, this can be overridden using their TaskScheduler builder. Additionally, Mesos allows us to override the time for which it won't re-send offers (default is 5s). We found that updating to reject more aggressively (every 1s instead of 120s) and keeping rejected offers away for longer (60s instead of 5s) dramatically increases our chances of scheduling our jobs on Mesos.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.TaskSchedulerBuilder.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14164" opendate="2019-9-23 00:00:00" fixdate="2019-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric to show failover count regarding fine grained recovery</summary>
      <description>Previously Flink uses restart all strategy to recover jobs from failures. And the metric "fullRestart" is used to show the count of failovers.However, with fine grained recovery introduced in 1.9.0, the "fullRestart" metric only reveals how many times the entire graph has been restarted, not including the number of fine grained failure recoveries.As many users want to build their job alerting based on failovers, I'd propose to add such a new metric numberOfRestarts which also respects fine grained recoveries. The metric should be a Gauge.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.metrics.NumberOfFullRestartsGauge.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14178" opendate="2019-9-24 00:00:00" fixdate="2019-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven-shade-plugin 3.2.1 doesn&amp;#39;t work on ARM for Flink</summary>
      <description>recently, maven-shade-plugin is bumped from 3.0.0 to 3.2.1 by the commit. While with my test locally on ARM, The Flink build process will be jammed. After debugging, I found there is an infinite loop.Downgrade maven-shade-plugin to 3.1.0 can solve this problem.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">docs.dev.projectsetup.dependencies.zh.md</file>
      <file type="M">docs.dev.projectsetup.dependencies.md</file>
    </fixedFiles>
  </bug>
  <bug id="14183" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove unnecessary scala Duration usages in flink-runtime</summary>
      <description>This ticket is to remove all usages of scala Duration/FiniteDuration in flink-runtime, except for those usages for Akka components (in AkkaUtils, AkkaRpcActor and ActorSystemScheduledExecutorAdapter).</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ZooKeeperHADispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.LeaderRetrievalUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.LeaderRetrievalHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14195" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly connectors-jdk11 fails because of missing jaxb classes</summary>
      <description>As titled, https://api.travis-ci.org/v3/job/588652149/log.txt is one example. We could see below errors from the log message:23:48:12.419 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 5.117 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)23:48:12.419 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.205 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14205" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Distinguish duplicate job submissions from other job submission errors</summary>
      <description>In order to better handle duplicate job submissions, I propose to add a new exception type DuplicateJobSubmissionException which inherits from JobSubmissionException and which is returned if the submitted job is a duplicate.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="14227" opendate="2019-9-26 00:00:00" fixdate="2019-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Razorpay to Chinese Powered By page</summary>
      <description>Razorpay was added to the English Powered By page with commit: 87a034140e97be42616e1a3dbe58e4f7a014e560.It should be added to the Chinese Powered By (and index.html) page as well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="14237" opendate="2019-9-26 00:00:00" fixdate="2019-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to rename shipped Flink jar</summary>
      <description>Currently, when we ship Flink jar configured by -yj, we always rename it as flink.jar. It seems a redundant operation since we can always use the exact name of the real jar. It also causes some confusion to our users who should not be required to know about Flink internal implementation that they configure a specific Flink jar(said flink-private-version-suffix.jar) but cannot find it on YARN container, because it is now flink.jar.CC trohrmann</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14246" opendate="2019-9-27 00:00:00" fixdate="2019-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotate all MiniCluster tests in flink-runtime with AlsoRunWithSchedulerNG</summary>
      <description>This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-runtime, so that we can know breaking changes in time when further improving the new generation scheduler. We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobExecutionITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleOrUpdateConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.BlobsCleanupITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14247" opendate="2019-9-27 00:00:00" fixdate="2019-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use NoResourceAvailableException to wrap TimeoutException on slot allocation (Scheduler NG)</summary>
      <description>This makes the error to be more user friendly.It also helps MiniClusterITCases to pass.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14254" opendate="2019-9-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FileSystemOutputFormat for batch</summary>
      <description>Introduce FileSystemOutputFormat to support all table file system connector with partition support in batch mode.FileSystemOutputFormat use PartitionWriter to write: DynamicPartitionWriter GroupedPartitionWriter NonPartitionWriterFileSystemOutputFormat use FileCommitter to commit temporary files. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14257" opendate="2019-9-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate csv to FileSystemTableFactory</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14258" opendate="2019-9-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate file system connector to streaming sink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionPathUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14262" opendate="2019-9-27 00:00:00" fixdate="2019-11-27 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>support referencing function with fully/partially qualified names in SQL</summary>
      <description>Flink should support referencing function with fully/partially qualified names in SQLSELECT &lt;cat&gt;.&lt;db&gt;.&lt;func&gt;(col) FROM TSELECT &lt;db&gt;.&lt;func&gt;(col) FROM TThis work depends on the completion of other subtasks in FLINK-14090</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.FunctionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionIdentifier.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.CallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.UnresolvedCallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.PartitionPrunerTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.HiveTableSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.HiveScalarSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ScalarFunctionConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkContextImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="14264" opendate="2019-9-28 00:00:00" fixdate="2019-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose CheckpointBackend in checkpoint config RestAPI</summary>
      <description>Currently, we can get checkpoint config from rest api&amp;#91;1&amp;#93;, the response contains the information as below timeout min_pause max_concurrent externalizationBut did not contain the type of CheckpointBackend, but in some scenarios, we want to get the CheckpointBackend type from Rest, this issue wants to add the simple name of the CheckpointBackend in the checkpoints/config rest with key }}{{checkpoint_backend, so the response will contain the information such as below timeout min_pause max_concurrent checkpoint_backend  externalization  &amp;#91;1&amp;#93; https://ci.apache.org/projects/flink/flink-docs-release-1.9/monitoring/rest_api.html#jobs-jobid-checkpoints-config</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.ExecutionGraphCacheTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="14265" opendate="2019-9-28 00:00:00" fixdate="2019-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t use ContinuousFileReaderOperator to support multiple paths</summary>
      <description>Now, blink planner use ContinuousFileReaderOperator to support InputFormat, but ContinuousFileReaderOperator not support multiple paths.If read partitioned source, after partition pruning, we need let InputFormat to read multiple partitions which are multiple paths.We can use InputFormatSourceFunction directly to support InputFormat.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14266" opendate="2019-9-28 00:00:00" fixdate="2019-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Row Csv InputFormat</summary>
      <description>Now, we have an old CSV, but that is not standard CSV support. we should support the RFC-compliant CSV format for table/sql.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="14272" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Blink planner for Python UDF</summary>
      <description>Currently, the Python UDF only works in the legacy planner, we should also support it in the Blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="14276" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala quickstart project does not compile on Java9+</summary>
      <description>The Quickstarts Scala nightly end-to-end test fails on Travis when running the e2e - misc - jdk11 profile. The failure cause is19:32:57.344 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider19:32:57.344 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)19:32:57.344 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.344 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)19:32:57.344 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.345 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)19:32:57.345 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)19:32:57.345 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.345 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)19:32:57.345 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.345 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.345 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)19:32:57.346 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)19:32:57.346 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)19:32:57.346 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)19:32:57.346 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)19:32:57.346 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)19:32:57.346 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)19:32:57.346 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)19:32:57.346 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)19:32:57.346 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)19:32:57.346 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)19:32:57.346 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)19:32:57.347 [INFO] at scala.tools.nsc.Main.main(Main.scala)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)19:32:57.347 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:566)19:32:57.347 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)19:32:57.347 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)19:32:57.347 [INFO] java.lang.reflect.InvocationTargetException19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)19:32:57.347 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)19:32:57.347 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:566)19:32:57.348 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)19:32:57.348 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)19:32:57.348 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider19:32:57.348 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)19:32:57.348 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.349 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)19:32:57.349 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)19:32:57.350 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)19:32:57.350 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)19:32:57.350 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.351 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)19:32:57.351 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)19:32:57.351 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)19:32:57.352 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)19:32:57.352 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)19:32:57.352 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)19:32:57.352 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)19:32:57.352 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)19:32:57.352 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)19:32:57.352 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)19:32:57.352 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)19:32:57.352 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)19:32:57.352 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)19:32:57.353 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)19:32:57.353 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)19:32:57.353 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)19:32:57.353 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)19:32:57.353 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)19:32:57.353 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)19:32:57.353 [INFO] at scala.tools.nsc.Main.main(Main.scala)19:32:57.353 [INFO] ... 6 morehttps://api.travis-ci.org/v3/job/590390311/log.txtThe issue might be a Java 11 problem and might be fixable with passing -nobootcp to the scala compiler plugin.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14282" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Simplify DispatcherResourceManagerComponent</summary>
      <description>With the completion of the FLINK-14281 it is now possible to encapsulate the shutdown logic of the MiniDispatcher within the DispatcherRunner. Consequently, it is no longer necessary to have separate DispatcherResourceManagerComponent implementations. I suggest to remove the special case implementations.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.StandaloneDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="14283" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update Kinesis consumer documentation for watermarks and event time alignment</summary>
      <description>Periodic per shard watermarking and event time alignment have been added over past releases but the doc has not been updated.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="14285" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Simplify Dispatcher factories by removing generics</summary>
      <description>The Dispatcher factories can be simplified by removing the generics. For better maintainability of the code base, I propose to do this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.SessionDispatcherWithUUIDFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.component.TestingDefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.SessionDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.StandaloneDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.MiniDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14299" opendate="2019-9-30 00:00:00" fixdate="2019-10-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Factor status and system metrics out of JobManagerMetricGroup</summary>
      <description>At the moment, we use the JobManagerMetricGroup to not only register Dispatcher specific metrics but also process specific metrics such as CPU, threads, memory, etc. Due to this fact, it is not possible to close the JobManagerMetricGroup when the life time of the Dispatcher terminates. In order to do this, I suggest to introduce a new ProcessMetricGroup which is used to register the process specific metrics. In order to guarantee backwards compatibility, I suggest to use the same scope format as SCOPE_NAMING_JM and then appending .Status.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="14304" opendate="2019-10-1 00:00:00" fixdate="2019-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid task starvation with mailbox</summary>
      <description>Currently, all mails are always prioritized over regular input, which makes sense in most cases. However, it's easy to devise an operator that gets into starvation: each mail enqueues a new mail.This ticket implements a simple extension in the mailbox processor: instead of draining the mailbox one-by-one, fetch all mails from the mailbox and run them one-by-one before running the default action. Only then, fetch all mails again and repeat.So we execute all mails that are available at the start of this loop but no mails that are added in the meantime.Special attention needs to be directed towards yield to downstream, such that it doesn't process mails outside of the current batch.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.Mailbox.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14305" opendate="2019-10-1 00:00:00" fixdate="2019-10-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Move ownership of JobManagerMetricGroup to Dispatcher</summary>
      <description>With FLINK-14303 and FLINK-14299, it is now possible to move the ownership of the JobManagerMetricGroup into the Dispatcher. This makes the lifespan of the metric group shorter and the lifecycle management easier.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.SessionDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.JobDispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14310" opendate="2019-10-2 00:00:00" fixdate="2019-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get ExecutionVertexID from ExecutionVertex rather than creating new instances</summary>
      <description>ExecutionVertexID is now added as a field to ExecutionVertex.Many components, however, are still creating ExecutionVertexID from ExecutionVertex by themselves. This may lead to more memory consumption of JM. It also slows down the ExecutionVertexID equality check.We should change them to use the field ExecutionVertex#executionVertexID directly.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphToInputsLocationsRetrieverAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.adapter.DefaultFailoverTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.AdaptedRestartPipelinedRegionStrategyNG.java</file>
    </fixedFiles>
  </bug>
  <bug id="14311" opendate="2019-10-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed on Travis</summary>
      <description>The Streaming File Sink end-to-end test fails on Travis because it does not produce output for 10 minutes.https://api.travis-ci.org/v3/job/591992274/log.txt</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14312" opendate="2019-10-2 00:00:00" fixdate="2019-11-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support building logical pipelined regions from JobGraph</summary>
      <description>Logical pipelined region partitioning is needed by FLINK-14060 to determine JobVertex slot sharing group.We can leverage PipelinedRegionComputeUtil#computePipelinedRegions to do this by adapting JobGraph to a base Topology.With changes from FLINK-14453, we can build LogicalPipelinedRegions from JobGraph by:1. Introduce LogicalTopology which extends Topology2. Implement DefaultLogicalTopology as an adapter of JobGraph to LogicalTopology3. Add DefaultLogicalTopology#getLogicalPipelinedRegions to return the logical pipelined regions of a JobGraph</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertexID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.IntermediateDataSetID.java</file>
    </fixedFiles>
  </bug>
  <bug id="14318" opendate="2019-10-4 00:00:00" fixdate="2019-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDK11 build stalls during shading</summary>
      <description>JDK11 build stalls during shading.Travis stage: e2d - misc - jdk11https://travis-ci.org/apache/flink/builds/593022581?utm_source=slack&amp;utm_medium=notificationhttps://api.travis-ci.org/v3/job/593022629/log.txtRelevant excerpt from logs:01:53:43.889 [INFO] ------------------------------------------------------------------------01:53:43.889 [INFO] Building flink-metrics-reporter-prometheus-test 1.10-SNAPSHOT01:53:43.889 [INFO] ------------------------------------------------------------------------...01:53:44.508 [INFO] Including org.apache.flink:force-shading:jar:1.10-SNAPSHOT in the shaded jar.01:53:44.508 [INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.01:53:44.508 [INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.01:53:44.508 [INFO] No artifact matching filter io.netty:netty01:53:44.522 [INFO] Replacing original artifact with shaded artifact.01:53:44.523 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded.jar01:53:44.524 [INFO] Replacing original test artifact with shaded test artifact.01:53:44.524 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-tests.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded-tests.jar01:53:44.524 [INFO] Dependency-reduced POM written at: /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/dependency-reduced-pom.xmlNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.nightly.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14321" opendate="2019-10-5 00:00:00" fixdate="2019-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse watermark statement in SQL DDL</summary>
      <description>Support to parse watermark syntax in SQL DDL. This can implemented in flink-sql-parser module. The watermark syntax is as following:WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;We should also do some validation during parsing, for example, whether the referenced rowtime field exist. We should also support to reference a nested field as the rowtime field.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="14337" opendate="2019-10-7 00:00:00" fixdate="2019-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistoryServer does not handle NPE on corruped archives properly</summary>
      <description>The HistoryServerTest.testHistoryServerIntegration failed on Travis with[ERROR] testHistoryServerIntegration[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest) Time elapsed: 10.667 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;3&gt; but was:&lt;2&gt;https://api.travis-ci.org/v3/job/594533358/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.history.FsJobArchivist.java</file>
    </fixedFiles>
  </bug>
  <bug id="14365" opendate="2019-10-11 00:00:00" fixdate="2019-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotate MiniCluster tests in core modules with AlsoRunWithSchedulerNG</summary>
      <description>This task is to annotate MiniCluster tests with AlsoRunWithSchedulerNG in flink core modules, so that we can know breaking changes in time when further improving the new generation scheduler.Core modules are the basic flink modules as defined in MODULES_CORE in flink/travis/stage.sh.MODULES_CORE="\flink-annotations,\flink-test-utils-parent/flink-test-utils,\flink-state-backends/flink-statebackend-rocksdb,\flink-clients,\flink-core,\flink-java,\flink-optimizer,\flink-runtime,\flink-runtime-web,\flink-scala,\flink-streaming-java,\flink-streaming-scala,\flink-metrics,\flink-metrics/flink-metrics-core"Note that the test bases in flink-test-utils will not be annotated in this task, since it enables MiniCluster tests in flink-tests and other non-core modules.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14366" opendate="2019-10-11 00:00:00" fixdate="2019-10-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Annotate MiniCluster tests in flink-tests with AlsoRunWithSchedulerNG</summary>
      <description>This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-tests, so that we can know breaking changes in time when further improving the new generation scheduler.We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.The tickets for failed tests should be linked in this task.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.failing.TaskFailureITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.java.org.apache.flink.streaming.scala.api.TextOutputFormatITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.java.org.apache.flink.streaming.scala.api.CsvOutputFormatITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BigUserProgramJobSubmitITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BackPressureITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.SchedulingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NetworkStackThroughputITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NettyEpollITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.IPv6HostnamesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.FileBufferReaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.SimpleRecoveryFixedDelayRestartStrategyITBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.SimpleRecoveryFailureRateStrategyITBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.CustomDistributionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.MiscellaneousIssuesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.CustomSerializationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.StreamingScalabilityAndLatency.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.failing.JobSubmissionFailsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.client.JobRetrievalITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeAllWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorErrorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.JobManagerMetricsITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14378" opendate="2019-10-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup rocksDB lib folder if fail to load library</summary>
      <description>This improvement is inspired due to some of our machines need some time to load the rocksDB library. When some other unrecoverable exceptions continue to happen and the process to load library would be interrupted which cause the rocksdb-lib folder created but not cleaned up. As the job continues to failover, the rocksdb-lib folder would be created more and more. We even come across that machine was running out of inodes!Details could refer to current implementation</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitResetTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="14393" opendate="2019-10-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to enable/disable cancel job in web ui</summary>
      <description>add the option to enable/disable cancel job in web uiwhen disabled, user can not cancel a job through the web ui</description>
      <version>1.10.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.RestHandlerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.DashboardConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.web.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="14395" opendate="2019-10-15 00:00:00" fixdate="2019-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor ES 7 connectors to make them keep consistency with es 6 connectors</summary>
      <description>In FLINK-13025, the table-specific code of es7 connector has been split into flink-sql-connector-elasticsearch7. We need to revert those changes and make it work the same way as the ES6 connector.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.sql.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.sql.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactory.java</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.sql.connectors.elasticsearch7.Elasticsearch7UpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14397" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to run Hive UDTF with array arguments</summary>
      <description>Tried to call org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2 (in hive-contrib) with query: "select x,y from foo, lateral table(hiveudtf(arr)) as T(x,y)". Failed with exception:java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Ljava.lang.Integer;</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="14413" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade-plugin ApacheNoticeResourceTransformer uses platform-dependent encoding</summary>
      <description>Some NOTICE files contain quotes that, at least on my system, result in some encoding errors when generating the binary licensing. One example can be found here; the closing quotes would be replaced with a question mark.This is due to the ApacheNoticeResourceTransformer using the platform encoding.</description>
      <version>shaded-8.0,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>shaded-9.0,1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14416" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Module interface and ModuleManager</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="14417" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop CoreModule to provide Flink built-in functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14418" opendate="2019-10-16 00:00:00" fixdate="2019-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HiveModule to provide Hive built-in functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
    </fixedFiles>
  </bug>
  <bug id="14419" opendate="2019-10-16 00:00:00" fixdate="2019-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ModuleFactory, ModuleDescriptor, ModuleValidator for factory discovery service</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.ModuleConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14452" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Keep only one execution topology in scheduler</summary>
      <description>Currently there are 3 failover topology instances created, 2 permanently kept in JM. 2 scheduling topology instances created permanently kept in JM. It a waste of computation to build the topologies and memory to keep these topologies. Which may be a significant issue when the job scale is large.With FLINK-14450 and FLINK-14451, the SchedulingTopology and FailoverTopology are able to share one default implementation. We can change the scheduler to create and keep only one such an execution topology instance to reduce the cost to build and host execution topologies.More details see FLINK-14330 and the design doc.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.AdaptedRestartPipelinedRegionStrategyNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="14468" opendate="2019-10-20 00:00:00" fixdate="2019-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kubernetes docs</summary>
      <description>Two minor improvements to documented Kubernetes resource definitions: avoid referencing deprecated extensions/v1beta1/Deployment run unprivileged</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="14505" opendate="2019-10-23 00:00:00" fixdate="2019-1-23 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>SQL Client end-to-end test for Kafka 0.10 nightly end-to-end test failed on travis</summary>
      <description>The SQL Client end-to-end test for Kafka 0.10 nightly end-to-end test failed on Travis with[FAIL] 'SQL Client end-to-end test for Kafka 0.10' failed after 0 minutes and 37 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 26336) is running anymore on travis-job-2c704099-0645-4182-942d-3fb5c2e10e54.No standalonesession daemon to stop on host travis-job-2c704099-0645-4182-942d-3fb5c2e10e54.https://api.travis-ci.org/v3/job/600710614/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.kafka.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14511" opendate="2019-10-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checking YARN queues should add "root" prefix</summary>
      <description>As we all know, all queues in the YARN cluster are children of the "root" queue. While submitting an application to "root.product" queue with -qu product parameter, the client logs that "The specified queue 'product' does not exist. Available queues....". But this queue is exist and we can still submit application to YARN cluster, which is confusing for users. So I think that when checking queues should add "root." prefix to the queue name.List&lt;QueueInfo&gt; queues = yarnClient.getAllQueues();if (queues.size() &gt; 0 &amp;&amp; this.yarnQueue != null) { // check only if there are queues configured in yarn and for this session. boolean queueFound = false; for (QueueInfo queue : queues) { if (queue.getQueueName().equals(this.yarnQueue) { queueFound = true; break; } } if (!queueFound) { String queueNames = ""; for (QueueInfo queue : queues) { queueNames += queue.getQueueName() + ", "; } LOG.warn("The specified queue '" + this.yarnQueue + "' does not exist. " + "Available queues: " + queueNames); }</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14524" opendate="2019-10-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL JDBC sink generates invalid SQL in upsert mode</summary>
      <description>The "upsert" query generated for the PostgreSQL dialect is missing a closing parenthesis in the ON CONFLICT clause, causing the INSERT statement to error out with the error ERROR o.a.f.s.runtime.tasks.StreamTask - Error during disposal of stream operator.java.lang.RuntimeException: Writing records to JDBC failed.{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.checkFlushException(JDBCUpsertOutputFormat.java:135)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.close(JDBCUpsertOutputFormat.java:184)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.close(JDBCUpsertSinkFunction.java:61)}}{{ at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)}}{{ at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)}}{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:585)}}{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:484)}}{{ at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)}}{{ at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)}}{{ at java.lang.Thread.run(Thread.java:748)}}Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO "public.temperature"("id", "timestamp", "temperature") VALUES ('sensor_17', '2019-10-25 00:39:10-05', 20.27573964210997) ON CONFLICT ("id", "timestamp" DO UPDATE SET "id"=EXCLUDED."id", "timestamp"=EXCLUDED."timestamp", "temperature"=EXCLUDED."temperature" was aborted: ERROR: syntax error at or near "DO"{{ Position: 119 Call getNextException to see other errors in the batch.}}{{ at org.postgresql.jdbc.BatchResultHandler.handleCompletion(BatchResultHandler.java:163)}}{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:838)}}{{ at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)}}{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingUpsertStatement.internalExecuteBatch(UpsertWriter.java:177)}}{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:117)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.flush(JDBCUpsertOutputFormat.java:159)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.lambda$open$0(JDBCUpsertOutputFormat.java:124)}}{{ at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)}}{{ at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)}}{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)}}{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)}}{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}{{ ... 1 common frames omitted}}Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near "DO"{{ Position: 119}}{{ at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)}}{{ at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)}}{{ at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)}}{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)}}{{ ... 12 common frames omitted}}</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
    </fixedFiles>
  </bug>
  <bug id="14556" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the package of cloud pickle</summary>
      <description>Currently the package structure of cloud pickle is as following:cloudpickle-1.2.2/cloudpickle-1.2.2/cloudpickle/cloudpickle-1.2.2/cloudpickle/__init__.py cloudpickle-1.2.2/cloudpickle/cloudpickle.py cloudpickle-1.2.2/cloudpickle/cloudpickle_fast.py cloudpickle-1.2.2/LICENSEIt should be:cloudpickle/ cloudpickle/__init__.py  cloudpickle/cloudpickle.py  cloudpickle/cloudpickle_fast.py  cloudpickle/LICENSEOtherwise, the following error will be thrown when running in a standalone cluster :"ImportError: No module named cloudpickle". </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.lib.cloudpickle-1.2.2-src.zip</file>
    </fixedFiles>
  </bug>
  <bug id="14557" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Clean up the package of py4j</summary>
      <description>Currently it contains a directory __MACOSX in the Py4j package. It's useless and should be removed. </description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.lib.py4j-0.10.8.1-src.zip</file>
    </fixedFiles>
  </bug>
  <bug id="14558" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the ClassNotFoundException issue for run python job in standalone mode</summary>
      <description>java.lang.ClassNotFoundException: org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator will be thrown when running a Python UDF job in a standalone cluster.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14561" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t write FLINK_PLUGINS_DIR ENV variable to Flink configuration</summary>
      <description>With FLINK-12143 we introduced the plugin mechanism. As part of this feature, we now write the FLINK_PLUGINS_DIR environment variable to the Flink Configuration we use for the cluster components. This is problematic, because we also use this Configuration to start new processes (Yarn and Mesos TaskExecutors). If the Configuration contains a configured FLINK_PLUGINS_DIR which differs from the one used by the newly created process, then this leads to problems.In order to solve this problem, I suggest to not write env variables which are intended for local usage within the started process into the Configuration. Instead we should directly read the environment variable at the required site similar to what we do with the env variable FLINK_LIB_DIR.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlayTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlay.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="14573" opendate="2019-10-30 00:00:00" fixdate="2019-12-30 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support time data types in Python user-defined functions</summary>
      <description>This jira is a sub-task of FLINK-14388. In this jira, only time types are dedicated to be supported for python UDF.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="14578" opendate="2019-10-30 00:00:00" fixdate="2019-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load/unloadModule() should throw RuntimeException rather than checked exception</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.exceptions.ModuleNotFoundException.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.exceptions.ModuleAlreadyExistException.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14579" opendate="2019-10-31 00:00:00" fixdate="2019-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable SQL CLI to configure modules via yaml config</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="1458" opendate="2015-1-27 00:00:00" fixdate="2015-1-27 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Interfaces and abstract classes are not valid types</summary>
      <description>I don't know whether this is by design or is a bug, but I am having trouble working with DataSet and traits in scala which is a major limitation. A simple example is shown below. Compile time warning is 'Type Main.SimpleTrait has no fields that are visible from Scala Type analysis. Falling back to Java Type Analysis...'Run time error is 'Interfaces and abstract classes are not valid types: interface Main$SimpleTrait'Regards, John val env = ExecutionEnvironment.getExecutionEnvironment trait SimpleTrait { def contains(x: String): Boolean } class SimpleClass extends SimpleTrait { def contains(x: String) = true } val data: DataSet&amp;#91;Double&amp;#93; = env.fromElements(1.0, 2.0, 3.0, 4.0) def f(data: DataSet&amp;#91;Double&amp;#93;): DataSet&amp;#91;SimpleTrait&amp;#93; = { data.mapPartition(iterator =&gt; { Iterator(new SimpleClass) }) } val g = f(data) g.print() env.execute("Simple example")</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.TraversableSerializerTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.ScalaSpecialTypesSerializerTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14580" opendate="2019-10-31 00:00:00" fixdate="2019-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HiveModuleFactory, HiveModuleDescriptor, and HiveModuleValidator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
    </fixedFiles>
  </bug>
  <bug id="14584" opendate="2019-10-31 00:00:00" fixdate="2019-12-31 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support complex data types in Python user-defined functions</summary>
      <description>This jira is a sub-task of FLINK-14388. In this jira, complex data types which include ArrayType, DecimalType, MapType and MultisetType are dedicated to be supported for python UDF.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="14588" opendate="2019-10-31 00:00:00" fixdate="2019-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 1.0.0 and 1.0.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV110.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14595" opendate="2019-11-4 00:00:00" fixdate="2019-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move flink-orc to flink-formats from flink-connectors</summary>
      <description>We already have the parent model of formats. we have put other formats(flink-avro, flink-json, flink-parquet, flink-json, flink-csv, flink-sequence-file) to flink-formats. flink-orc is a format too. So we can move it to flink-formats. In theory, there should be no compatibility problem, only the parent model needs to be changed, and no other changes are needed.  Discuss thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-flink-orc-to-flink-formats-from-flink-connectors-td34438.htmlVote thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Move-flink-orc-to-flink-formats-from-flink-connectors-td34496.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-timetypes.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-repeating.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-nestedlist.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-nested.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-flat.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-decimal.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.test-data-composites-with-nulls.orc</file>
      <file type="M">flink-connectors.flink-orc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-orc.src.test.java.org.apache.flink.orc.util.OrcTestFileGenerator.java</file>
      <file type="M">flink-connectors.flink-orc.src.test.java.org.apache.flink.orc.OrcTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-orc.src.test.java.org.apache.flink.orc.OrcTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-orc.src.test.java.org.apache.flink.orc.OrcRowInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-orc.src.test.java.org.apache.flink.orc.OrcBatchReaderTest.java</file>
      <file type="M">flink-connectors.flink-orc.src.main.java.org.apache.flink.orc.OrcTableSource.java</file>
      <file type="M">flink-connectors.flink-orc.src.main.java.org.apache.flink.orc.OrcRowInputFormat.java</file>
      <file type="M">flink-connectors.flink-orc.src.main.java.org.apache.flink.orc.OrcBatchReader.java</file>
      <file type="M">flink-connectors.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14613" opendate="2019-11-5 00:00:00" fixdate="2019-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add validation check when applying UDF to tempral table key in Temporal Table Join condition</summary>
      <description>In Temporal Table Join, We don't support using  UDF in tempral table join key. For we can't analyze LookupKeys  when call is an expression. When users use like this, the program run normally,  and the result will be wrong. So we should add validation to prevent it.The SQL as following:INSERT INTO ASELECT B.amount, B.currency, C.amount, C.product FROM B join C FOR SYSTEM_TIME AS OF B.proctime on B.amount = cancat(C.amount, 'r') and C.product = '1'</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14641" opendate="2019-11-7 00:00:00" fixdate="2019-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix description of metric `fullRestarts`</summary>
      <description>The metric `fullRestarts` counts both full restarts and fine grained restarts since 1.9.2.We should update the metric description doc accordingly.We need to pointing out the the metric counts full restarts in 1.9.1 or earlier versions, and turned to count all kinds of restarts since 1.9.2.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="14652" opendate="2019-11-7 00:00:00" fixdate="2019-4-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Refactor checkpointing related parts into one place on task side</summary>
      <description>As suggested by sewen within review for PR-8693, it would be worthy to refactor all checkpointing parts into a single place on task side.This issue focus on refactoring these parts.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1466" opendate="2015-1-31 00:00:00" fixdate="2015-2-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add InputFormat to read HCatalog tables</summary>
      <description>HCatalog is a metadata repository and InputFormat to make Hive tables accessible to other frameworks such as Pig.Adding support for HCatalog would give access to Hive managed data.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hcatalog.src.main.scala.org.apache.flink.hcatalog.scala.HCatInputFormat.scala</file>
      <file type="M">flink-staging.flink-hcatalog.src.main.java.org.apache.flink.hcatalog.java.HCatInputFormat.java</file>
      <file type="M">flink-staging.flink-hcatalog.src.main.java.org.apache.flink.hcatalog.HCatInputFormatBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14660" opendate="2019-11-7 00:00:00" fixdate="2019-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add &amp;#39;SHOW MODULES&amp;#39; sql command</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="14673" opendate="2019-11-8 00:00:00" fixdate="2019-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shouldn&amp;#39;t expect HMS client to throw NoSuchObjectException for non-existing function</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14758" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Executor-related interfaces and utilities</summary>
      <description>This is issue only targets at introducing the interfaces without (yet) using them. It will add the Executor interface, the ExecutorFactory and the ExecutorServiceLoader which will be able to "discover" the registered ExecutorFactories.In addition, it will add the env.execute() (both batch and streaming) implementations as described in [FLIP-73|https://cwiki.apache.org/confluence/display/FLINK/FLIP-73%3A+Introducing+Executors+for+job+submission], but this is not going to affect anything as the current environments override the execute() method.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="14780" opendate="2019-11-14 00:00:00" fixdate="2019-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid leaking instance of DefaultScheduler before object is constructed</summary>
      <description>An instance of DefaultScheduler may leak to a metric reporter thread before the instance of the object is fully constructed. This can lead to an NPE (see below.https://api.travis-ci.org/v3/job/611597698/log.txtjava.lang.NullPointerException at org.apache.flink.runtime.scheduler.DefaultScheduler.getNumberOfRestarts(DefaultScheduler.java:156) at org.apache.flink.metrics.slf4j.Slf4jReporter.tryReport(Slf4jReporter.java:114) at org.apache.flink.metrics.slf4j.Slf4jReporter.report(Slf4jReporter.java:80) at org.apache.flink.runtime.metrics.MetricRegistryImpl$ReporterTask.run(MetricRegistryImpl.java:436) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14781" opendate="2019-11-14 00:00:00" fixdate="2019-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ZH] clarify that a RocksDB dependency in pom.xml may not be needed</summary>
      <description>The English version was clarified with respect when and how to add the maven dependencies via https://github.com/apache/flink/commit/d36ce5ff77fae2b01b8fbe8e5c15d610de8ed9f5. The Chinese version still needs that update</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="14796" opendate="2019-11-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about limitations of different Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="14809" opendate="2019-11-15 00:00:00" fixdate="2019-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataStreamAllroundTestProgram does not run because return types cannot be determined</summary>
      <description>2019-11-14 19:34:55,185 ERROR org.apache.flink.client.cli.CliFrontend - Error while running the command.org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: The return type of function 'main(DataStreamAllroundTestProgram.java:182)' could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the 'ResultTypeQueryable' interface. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:336) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:206) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:173) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:747) at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:282) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:219) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1011) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1084) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1084)Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The return type of function 'main(DataStreamAllroundTestProgram.java:182)' could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the 'ResultTypeQueryable' interface. at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:412) at org.apache.flink.streaming.api.datastream.DataStream.addSink(DataStream.java:1296) at org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.main(DataStreamAllroundTestProgram.java:185) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:322) ... 12 moreCaused by: org.apache.flink.api.common.functions.InvalidTypesException: Input mismatch: Generic type 'org.apache.flink.streaming.tests.Event' or a subclass of it expected but was 'org.apache.flink.streaming.tests.Event'. at org.apache.flink.api.java.typeutils.TypeExtractor.validateInputType(TypeExtractor.java:1298) at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:585) at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196) at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:634) at org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.main(DataStreamAllroundTestProgram.java:182) ... 17 moreCaused by: org.apache.flink.api.common.functions.InvalidTypesException: Generic type 'org.apache.flink.streaming.tests.Event' or a subclass of it expected but was 'org.apache.flink.streaming.tests.Event'. at org.apache.flink.api.java.typeutils.TypeExtractor.validateInfo(TypeExtractor.java:1481) at org.apache.flink.api.java.typeutils.TypeExtractor.validateInfo(TypeExtractor.java:1491) at org.apache.flink.api.java.typeutils.TypeExtractor.validateInputType(TypeExtractor.java:1295) ... 21 moreThis happens in multiple nightlies and jepsen runs. Examplehttps://api.travis-ci.org/v3/job/611848582/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.ProcessFunctionTestHarnessesTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
    </fixedFiles>
  </bug>
  <bug id="14811" opendate="2019-11-15 00:00:00" fixdate="2019-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Java Streams with for-loops in vertex input checking</summary>
      <description>Vertex input checking is invoked in lazily triggered scheduling by a FINISHED vertex state update RPC or a scheduleOrUpdateConsumers RPC. Java Streams is used in it, but it should be avoided since it is performance critical code. See ref &amp;#91;1&amp;#93; and &amp;#91;2&amp;#93;.We should refactor these Java Streams to improve the performance, for both legacy scheduler and NG schedulers.cc Stephan Ewen Gary Yao&amp;#91;1&amp;#93; flink code style guide&amp;#91;2&amp;#93; discussion &amp; performance test which compares the performance.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.InputDependencyConstraintChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="14813" opendate="2019-11-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the new mechanism implemented in FLINK-14472 as a "is back-pressured" metric</summary>
      <description>{   "id": "0.isBackPressured",  "value": "true" }</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.BackPressureGauge.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleIOOwnerContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.NettyShuffleMetricFactory.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="14814" opendate="2019-11-15 00:00:00" fixdate="2019-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the vertex that produces the backpressure source in the job</summary>
      <description>By checking the status of output and input buffer pools exposed via FLINK-14815 (output buffer empty, input buffer full) it is possible to display which node is a source of the back pressure. This information could be displayed/accessible in the Web Frontend.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
    </fixedFiles>
  </bug>
  <bug id="14822" opendate="2019-11-15 00:00:00" fixdate="2019-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable &amp;#39;Streaming File Sink end-to-end test&amp;#39; to pass with new DefaultScheduler</summary>
      <description>The tests fails because we exhaust the number of restarts (3). The reason is that the new scheduler may re-schedule tasks faster – we start counting down the restart back-off time as soon as we triggered task cancellation, however the legacy scheduler will only start counting down after the task cancellation is finished. Thus, re-scheduled tasks may be deployed into a TM that was killed, and therefore increase the number of restarts multiple times. The speed of the TM loss detection depends on heartbeat.interval and heartbeat.timeout. These settings are by default 10s and 50s respectively. The problem can even be reproduced with the legacy scheduler on the current master by setting heartbeat.timeout to a high value, such as 180000.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-file-sink-test.src.main.java.StreamingFileSinkProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="14824" opendate="2019-11-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve schema derivation for CSV and JSON formats</summary>
      <description>Currently, one also needs to define a schema for formats in DDL or decriptor API. In this ticket, we will make derive schema as the default behavior, and keep compatibility with previous version to still support explicitly declare a format schema.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.OldCsvTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.CsvTableSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsvValidator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsv.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.table.descriptors.JsonTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.JsonValidator.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.Json.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.table.descriptors.CsvTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.table.descriptors.CsvValidator.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.table.descriptors.Csv.java</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="14826" opendate="2019-11-15 00:00:00" fixdate="2019-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable &amp;#39;Streaming bucketing end-to-end test&amp;#39; to pass with new DefaultScheduler</summary>
      <description>The tests fails because we exhaust the number of restarts (3). The reason is that the new scheduler may re-schedule tasks faster – we start counting down the restart back-off time as soon as we triggered task cancellation, however the legacy scheduler will only start counting down after the task cancellation is finished. Thus, re-scheduled tasks may be deployed into a TM that was killed, and therefore increase the number of restarts multiple times. The speed of the TM loss detection depends on heartbeat.interval and heartbeat.timeout. These settings are by default 10s and 50s respectively. The problem can even be reproduced with the legacy scheduler on the current master by setting heartbeat.timeout to a high value, such as 180000.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-bucketing-sink-test.src.main.java.org.apache.flink.streaming.tests.BucketingSinkTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="14834" opendate="2019-11-17 00:00:00" fixdate="2019-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN on Docker test fails on Travis</summary>
      <description>https://api.travis-ci.org/v3/job/612782888/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.travis.splits.split.container.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14841" opendate="2019-11-18 00:00:00" fixdate="2019-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add create and drop function DDL in parser</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="14842" opendate="2019-11-18 00:00:00" fixdate="2019-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add logging for loaded modules and functions</summary>
      <description>to help users identify which module provides that function</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14891" opendate="2019-11-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PythonScalarFunctionOperator should be chained with upstream operators by default</summary>
      <description>Currently the default chaining strategy for PythonScalarFunctionOperator is not set and it's HEAD by default. We should set the default value as ALWAYS.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14894" opendate="2019-11-21 00:00:00" fixdate="2019-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HybridOffHeapUnsafeMemorySegmentTest#testByteBufferWrap failed on Travis</summary>
      <description>HybridOffHeapUnsafeMemorySegmentTest&gt;MemorySegmentTestBase.testByteBufferWrapping:2465 expected:&lt;992288337&gt; but was:&lt;196608&gt;https://api.travis-ci.com/v3/job/258950527/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.MemorySegmentChecksTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HybridMemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="14939" opendate="2019-11-25 00:00:00" fixdate="2019-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase fails due to distDir property not being set</summary>
      <description>https://api.travis-ci.org/v3/job/616462870/log.txt08:12:34.965 [INFO] -------------------------------------------------------08:12:34.965 [INFO] T E S T S08:12:34.965 [INFO] -------------------------------------------------------08:12:35.868 [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase08:12:35.893 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 0.02 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase08:12:35.893 [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:35.893 [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.001 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:35.893 [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.001 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; . at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:71)08:12:36.233 [INFO] 08:12:36.233 [INFO] Results:08:12:36.233 [INFO] 08:12:36.233 [ERROR] Failures: 08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [ERROR] StreamingKafkaITCase.&lt;init&gt;:71 The distDir property was not set. You can set it when running maven via -DdistDir=&lt;path&gt; .08:12:36.233 [INFO] 08:12:36.233 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 008:12:36.233 [INFO]</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14940" opendate="2019-11-25 00:00:00" fixdate="2019-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis build passes despite Test failures</summary>
      <description>Build https://travis-ci.org/apache/flink/jobs/616462870 is green despite the presence of Test failures.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14965" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CatalogTableStatistics UNKNOWN should be consistent with TableStats UNKNOWN</summary>
      <description>UNKNOWN stats in` org.apache.flink.table.catalog.statspublic class CatalogTableStatistics { public static final CatalogTableStatistics UNKNOWN = new CatalogTableStatistics(0, 0, 0, 0);`and `org.apache.flink.table.plan.statspublic final class TableStats { public static final TableStats UNKNOWN = new TableStats(-1, new HashMap&lt;&gt;());`are not consistent which will cause some cbo unexpect behavior</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14968" opendate="2019-11-27 00:00:00" fixdate="2019-11-27 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Kerberized YARN on Docker test (custom fs plugin) fails on Travis</summary>
      <description>This change made the test flaky: https://github.com/apache/flink/commit/749965348170e4608ff2a23c9617f67b8c341df5. It changes the job to have two sources instead of one which, under normal circumstances, requires too many slots to run and therefore the job will fail.The setup of this test is very intricate, we configure YARN to have two NodeManagers with 2500mb memory each: https://github.com/apache/flink/blob/413a77157caf25dbbfb8b0caaf2c9e12c7374d98/flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/config/yarn-site.xml#L39. We run the job with parallelism 3 and configure Flink to use 1000mb as TaskManager memory and 1000mb of JobManager memory. This means that the job fits into the YARN memory budget but more TaskManagers would not fit. We also don't simply increase the YARN resources because we want the Flink job to use TMs on different NMs because we had a bug where Kerberos config file shipping was not working correctly but the bug was not materialising if all TMs where on the same NM.https://api.travis-ci.org/v3/job/612782888/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14972" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement RemoteExecutor as a new Executor</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.RemoteStreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-clients.src.main.resources.META-INF.services.org.apache.flink.core.execution.ExecutorFactory</file>
    </fixedFiles>
  </bug>
  <bug id="14976" opendate="2019-11-27 00:00:00" fixdate="2019-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra Connector leaks Semaphore on Throwable; hangs on close</summary>
      <description>This issue was mostly fixed in FLINK-13059; unfortunately, the fix only caught Exception so any non-Exception Throwable can still cause the issue of leaking semaphores.</description>
      <version>1.8.2,1.9.1,1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14978" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce constraint class hierarchy required for primary keys</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="14984" opendate="2019-11-28 00:00:00" fixdate="2019-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old WebUI</summary>
      <description>Following the discussion on the ML, remove the old WebUI.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-310x150.png</file>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.stdout.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.log.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.taskmanager.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.taskmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.submit.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.running-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.timeline.vertex.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.timeline.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.properties.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.watermarks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.taskmanagers.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.summary.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.statistics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.operator.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.history.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.details.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.counts.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoints.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.checkpoint-history.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node.accumulators.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.watermarks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.checkpoints.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.backpressure.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.node-list.accumulators.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.plan.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.exceptions.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.job.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobs.completed-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.stdout.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.log.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.partials.jobmanager.config.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.js.hs.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.index.hs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.safari-pinned-tab.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-70x70.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-310x310.png</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.@angular</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-drag-and-drop-list</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-moment</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.angular-ui-router</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ant-design-palettes</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.bootstrap</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.core-js</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.d3</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.dagre</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.dagre-d3</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ev-emitter</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.font-awesome</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.graphlib</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.imagesloaded</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.jquery</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.lodash</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.moment</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.moment-duration-format</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.monaco-editor</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.ng-zorro-antd</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.qtip2</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.rxjs</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.split</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.tinycolor2</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.tslib</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.licenses.LICENSE.zone</file>
      <file type="M">flink-runtime-web.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-runtime-web.web-dashboard.angular.json</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.eot</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.ttf</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.woff</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.fontawesome-webfont.woff2</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.fonts.FontAwesome.otf</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.android-chrome-192x192.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.android-chrome-512x512.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.apple-touch-icon.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.browserconfig.xml</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon-16x16.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon-32x32.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.favicon.ico</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.flink-logo.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.grips.horizontal.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.grips.vertical.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.manifest.json</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-144x144.png</file>
      <file type="M">flink-runtime-web.web-dashboard.old-version.images.mstile-150x150.png</file>
    </fixedFiles>
  </bug>
  <bug id="15006" opendate="2019-12-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to close shuffle when dynamic partition inserting</summary>
      <description>When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.We can have an option to close shuffle in such cases:‘connector.sink.shuffle-by-partition.enable’ = ...</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15008" opendate="2019-12-2 00:00:00" fixdate="2019-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests in flink-yarn-tests fail with ClassNotFoundException (JDK11)</summary>
      <description>1) Error injecting constructor, java.lang.NoClassDefFoundError: javax/activation/DataSource at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.&lt;init&gt;(JAXBContextResolver.java:41) at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:51) while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver1 error at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:987) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1013) at com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:332) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.&lt;init&gt;(IoCProviderFactory.java:179) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:100) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:93) at com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153) at com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:251) at com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:148) at com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83) at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1271) at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:169) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:775) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:771) at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193) at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:771) at com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121) at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:318) at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:609) at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:210) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:373) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:710) at com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:114) at com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:98) at com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:172) at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713) at org.mortbay.jetty.servlet.Context.startContext(Context.java:140) at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282) at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518) at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152) at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130) at org.mortbay.jetty.Server.doStart(Server.java:224) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:936) ... 50 moreCaused by: java.lang.NoClassDefFoundError: javax/activation/DataSource at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.&lt;clinit&gt;(RuntimeBuiltinLeafInfoImpl.java:457) at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.&lt;init&gt;(RuntimeTypeInfoSetImpl.java:65) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85) at com.sun.xml.bind.v2.model.impl.ModelBuilder.&lt;init&gt;(ModelBuilder.java:156) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.&lt;init&gt;(RuntimeModelBuilder.java:93) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:319) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at com.sun.jersey.api.json.JSONJAXBContext.&lt;init&gt;(JSONJAXBContext.java:246) at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.&lt;init&gt;(JAXBContextResolver.java:65) at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver$$FastClassByGuice$$6a7be7f6.newInstance(&lt;generated&gt;) at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40) at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254) at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40) at com.google.inject.Scopes$1$1.get(Scopes.java:65) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:40) at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024) at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974) ... 89 moreCaused by: java.lang.ClassNotFoundException: javax.activation.DataSource at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521) ... 122 morehttps://api.travis-ci.org/v3/job/619217945/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15009" opendate="2019-12-2 00:00:00" fixdate="2019-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a utility for creating type inference logic via reflection</summary>
      <description>This is the last missing piece for completing FLIP-65 from an API point of view. The utility will take a UserDefinedFunction and return TypeInference that can be used for validation and planning.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.DataTypeTemplate.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.DataTypeExtractor.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.DataTypeExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15031" opendate="2019-12-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically calculate required network memory for fine-grained jobs</summary>
      <description>In cases where resources are specified, we expect each operator to declare required resources before using them. In this way, no resource related error should happen if resources are not used beyond what was declared. This ensures a deployed task would not fail due to insufficient resources in TM, which may result in unnecessary failures and may even cause a job hanging forever, failing repeatedly on deploying tasks to a TM with insufficient resources.Shuffle memory is the last missing piece for this goal at the moment. Minimum network buffers are required by tasks to work. Currently a task is possible to be deployed to a TM with insufficient network buffers, and fails on launching.To avoid that, we should calculate required network memory for a task/SlotSharingGroup before allocating a slot for it.The required shuffle memory can be derived from the number of required network buffers. The number of buffers required by a task (ExecutionVertex) isexclusive buffers for input channels(i.e. numInputChannel * buffersPerChannel) + required buffers for result partition buffer pool(currently is numberOfSubpartitions + 1)Note that this is for the NettyShuffleService case. For custom shuffle services, currently there is no way to get the required shuffle memory of a task.To make it simple under dynamic slot sharing, the required shuffle memory for a task should be the max required shuffle memory of all ExecutionVertex of the same ExecutionJobVertex. And the required shuffle memory for a slot sharing group should be the sum of shuffle memory for each ExecutionJobVertex instance within.</description>
      <version>1.10.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15033" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused RemoteEnvirnment.executeRemotely() (FLINK-11048)</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15034" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump FRocksDB version for memory control</summary>
      <description>Since FLINK-14483 has already been resolved and a new version of FRocksDB has been released, we should bump the FRocksDB version in Flink for next steps in memory control.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="15035" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce unknown memory setting to table in blink planner</summary>
      <description>After https://jira.apache.org/jira/browse/FLINK-14566We can just set unknown resources with setting whether managed memory is used.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.ResettableExternalBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.InnerJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.ExecutorUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.NodeResourceUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
      <file type="M">docs..includes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15042" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix python compatibility by excluding the Env.executeAsync() (FLINK-14854)</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.completeness.py</file>
    </fixedFiles>
  </bug>
  <bug id="15047" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnDistributedCacheITCase is unstable</summary>
      <description>See also https://api.travis-ci.com/v3/job/262854881/log.txtcc Zhenqiu Huang</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15053" opendate="2019-12-4 00:00:00" fixdate="2019-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurations with values contains space may cause TM failures on Yarn</summary>
      <description>Currently on Yarn setups, we are passing task executor specific configurations through dynamic properties in the starting command (see FLINK-13184).If the value of configuration contains space, the dynamic properties may not be correctly parsed, which could cause task executor failures. On occurrence can be found in FLINK-15047.It would be good to allow spaces when passing dynamic properties. E.g., surrounding the values with double quotation marks, or escaping special characters.cc &amp;#91;~fly_in_gis&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="15057" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set taskmanager.memory.total-process.size in jepsen tests</summary>
      <description>Set taskmanager.memory.total-process.size in flink-conf.yaml used by tests. Currently the taskmanager process fails due toorg.apache.flink.configuration.IllegalConfigurationException: Either Task Heap Memory size and Managed Memory size, or Total Flink Memory size, or Total Process Memory size need to be configured explicitly. at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:110) at org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.fromConfiguration(TaskManagerServicesConfiguration.java:219) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:357) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.&lt;init&gt;(TaskManagerRunner.java:153) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:327) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:298) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:295) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:295)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="15058" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Log required config keys if TaskManager memory configuration is invalid</summary>
      <description>Currently the error message isEither Task Heap Memory size and Managed Memory size, or Total Flink Memory size, or Total Process Memory size need to be configured explicitlyHowever, it would be good to immediately see which config keys are expected to be configured.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15059" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataTypeExtractorTest fails on travis</summary>
      <description>https://api.travis-ci.org/v3/job/620607358/log.txt16:28:41.138 [ERROR] testExtraction[27](org.apache.flink.table.types.extraction.DataTypeExtractorTest) Time elapsed: 0.059 s &lt;&lt;&lt; ERROR!org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$SimplePojoWithAssigningConstructor'. Please pass the required data type manually or allow RAW types. at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$SimplePojoWithAssigningConstructor'. Interpreting it as a structured type was also not successful. at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)Caused by: java.lang.UnsupportedOperationException: This feature requires ASM7 at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)16:28:41.141 [ERROR] testExtraction[28](org.apache.flink.table.types.extraction.DataTypeExtractorTest) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$PojoWithCustomFieldOrder'. Please pass the required data type manually or allow RAW types. at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$PojoWithCustomFieldOrder'. Interpreting it as a structured type was also not successful. at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)Caused by: java.lang.UnsupportedOperationException: This feature requires ASM7 at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367) at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.ExtractionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15073" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql client fails to run same query multiple times</summary>
      <description>Flink SQL&gt; select abs(-1);&amp;#91;INFO&amp;#93; Result retrieval cancelled.Flink SQL&gt; select abs(-1);&amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Table 'default: select abs(-1)' already exists. Please choose a different name.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15091" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinITCase.testFullJoinWithNonEquiJoinPred failed in travis</summary>
      <description>04:45:22.404 &amp;#91;ERROR&amp;#93; Tests run: 21, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.909 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.runtime.batch.table.JoinITCase 04:45:22.406 &amp;#91;ERROR&amp;#93; testFullJoinWithNonEquiJoinPred(org.apache.flink.table.planner.runtime.batch.table.JoinITCase) Time elapsed: 0.168 s &lt;&lt;&lt; ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.table.planner.runtime.batch.table.JoinITCase.testFullJoinWithNonEquiJoinPred(JoinITCase.scala:344) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 32 pages. Only 0 pages are remaining. details: https://api.travis-ci.org/v3/job/621407747/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15092" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using sql-client excute sql(select sum(cast(null as int)) from t123;) has a TableException</summary>
      <description>CREATE TABLE `t123` ( x INT) WITH ( 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'connector.path'='hdfs://zthdev/defender_test_data/daily/test_aggregates/sources/t123.csv', 'format.type'='csv');select sum(cast(null as int)) from t123; Excute the statement above , then you will see such exception: &amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason:org.apache.flink.table.api.TableException: Failed to push project into table source! table source with pushdown capability must override and change explainSource() API to explain the pushdown applied!  </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.RowCsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.RowCsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="15095" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bridge table schema&amp;#39;s primary key to metadata handler in blink planner</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="15096" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not use GlobalJobParameters to pass system configuration</summary>
      <description>GlobalJobParameters is a user only configuration that should not be used to ship system specific settings.Right now python uses it to ship information about custom archives, files, executables etc.A solution would be to pass required configuration when instantiating the operators.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.PythonDependencyInfoTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.PythonDependencyInfo.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.dependency.manager.py</file>
    </fixedFiles>
  </bug>
  <bug id="15100" opendate="2019-12-6 00:00:00" fixdate="2019-3-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add the interface and base implementation for SourceReader.</summary>
      <description>Add the interface and base implementation for SourceReader. Including threading model, SplitReader / RecordEmitter. This ticket should also integrate the SourceReader into the SourceReaderStreamTask.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexCoordinatorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReaderContext.java</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15102" opendate="2019-12-6 00:00:00" fixdate="2019-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users to construct a DataStream from a new Source API</summary>
      <description>This ticket should finish the implementation of allowing user to construct a DataStream from the new Source API, including the necessary information to generate a properly working JobGraph.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.BatchExecutorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15105" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test stalls on travis</summary>
      <description>Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test fails on release-1.9 nightly build stalls with "The job exceeded the maximum log length, and has been terminated".https://api.travis-ci.org/v3/job/621090394/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15106" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 3.x tests leave metastore_db folder under build directory</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveServerContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15107" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL-CLI can not execute insert into statement with lowercase "INSERT INTO" keyword</summary>
      <description>This was introduced by FLINK-15026 which has a always uppercase case "INSERT INTO" pattern matcher check.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15118" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make flink-scala-shell use Executors</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.java.org.apache.flink.api.java.FlinkILoopTest.java</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15120" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make FlinkYarnCli#isActive() &amp; #getApplicationId() respect config APPLICATION_ID</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="15124" opendate="2019-12-8 00:00:00" fixdate="2019-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>types with precision can&amp;#39;t be executed in sql client with blink planner</summary>
      <description>I created a table in sql client with blink planner:  create table t ( a int, b varchar, c decimal(10, 5))with ( 'connector.type' = 'filesystem', 'format.type' = 'csv', 'format.derive-schema' = 'true', 'connector.path' = 'xxxxxxx');The table description looks good:Flink SQL&gt; describe t; root |-- a: INT |-- b: STRING |-- c: DECIMAL(10, 5)But the select query failed:Flink SQL&gt; select * from t;[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(field$3,isNull$3,,DECIMAL(38, 18),None)] type is [DECIMAL(38, 18)], result type is [DECIMAL(10, 5)] </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.types.LogicalTypeAssignableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="15128" opendate="2019-12-8 00:00:00" fixdate="2019-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document support for Hive timestamp type</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15129" opendate="2019-12-8 00:00:00" fixdate="2019-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return JobClient instead of JobClient Future from executeAsync()</summary>
      <description>Currently, users have to write this when they want to use the JobClient:CompletableFuture&lt;JobClient&gt; jobClientFuture = env.executeAsync();JobClient jobClient = jobClientFuture.get();// or use thenApply/thenCompose etc.instead we could always return a JobClient right away and therefore remove one step for the user.I don't know if it's always the right choice, but currently we always return an already completed future that contains the JobClient. In the future we might want to return a future that actually completes at some later point, we would not be able to do this if we directly return a JobClient and would have to block in executeAsync().</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15138" opendate="2019-12-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add e2e Test for PyFlink</summary>
      <description>Currently, both the Table API and Python native UDF are supported in 1.10, I would like to add the e2e test for PyFlink.What do you think?</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15139" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>misc end to end test failed cause loss jars in converting to jobgraph</summary>
      <description>The test Running 'SQL Client end-to-end test (Old planner)' in misc e2e test failedlog:(a94d1da25baf2a5586a296d9e933743c) switched from RUNNING to FAILED.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkClassLoader info: URL ClassLoader:Class not resolvable through given classloader. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:266) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:430) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:419) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:144) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:432) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:254) ... 10 morelink: https://travis-ci.org/apache/flink/jobs/622261358 </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ExecutionConfigAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15146" opendate="2019-12-9 00:00:00" fixdate="2019-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix check that incremental cleanup size must be greater than zero</summary>
      <description> Hi , the value of cleanupSize is grater than or equal 0. Whether that the value is grater than 0 is more practical. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.state.StateTtlConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="1515" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>[Gelly] Enable access to aggregators and broadcast sets in vertex-centric iteration</summary>
      <description>Currently, aggregators and broadcast sets cannot be accessed through Gelly's runVertexCentricIteration method. The functionality is already present in the VertexCentricIteration and we just need to expose it.This could be done like this: We create a method createVertexCentricIteration, which will return a VertexCentricIteration object and we change runVertexCentricIteration to accept this as a parameter (and return the graph after running this iteration).The user can configure the VertexCentricIteration by directly calling the public methods registerAggregator, setName, etc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.PageRankExample.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConnectedComponentsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
    </fixedFiles>
  </bug>
  <bug id="15163" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>japicmp should use 1.9 as the old version</summary>
      <description>We should configure the japicmp-maven-plugin to use the latest Flink 1.9 release as the reference version to compare against. Currently 1.8.0 is used.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15166" opendate="2019-12-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle data compression wrongly decrease the buffer reference count.</summary>
      <description>FLINK-15140 report two relevant problems which are both triggered by broadcast partitioner, to make it more clear, I create this Jira to addresses the problems separately. For blocking shuffle compression, we recycle the compressed intermediate buffer each time after we write data out, however when the data is not compressed, the return buffer is the original buffer and should not be recycled, but we wrongly recycled it.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.ShuffleCompressionITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="1517" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify inputiterator for streaming so the channel index of the last record is accessible</summary>
      <description>Currently the channel id for the last received record is inaccessible from the iterator. To implement sorting operators on streams we need this information.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamVertex.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamTaskContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.InputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.CoStreamVertex.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.StreamInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.ReaderIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15171" opendate="2019-12-10 00:00:00" fixdate="2019-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression in serialisation benchmarks</summary>
      <description>There is quite significant performance regression in serialisation benchmarks in the commit range 2ecf7ca..9320f34 (which includes FLINK-14346).http://codespeed.dak8s.net:8000/timeline/?ben=serializerTuple&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=serializerRow&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=serializerPojo&amp;env=2it coincides with the performance improvement for heavy stringshttp://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&amp;env=2it might be caused by some accidental change in the benchmarking code (changing parallelism in one benchmarks is carried on to the next one?) or in the code itself.CC Roman Grebennikov Arvid Heise</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.StringValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="15172" opendate="2019-12-10 00:00:00" fixdate="2019-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the operator algorithm to lazily allocate memory</summary>
      <description>Now after FLINK-14063 , operators will get all manage memory of TaskManager,  The cost of over allocate memory is very high, lead to performance regression of small batch sql jobs: Allocate memory will have the cost of memory management algorithm. Allocate memory will have the cost of memory initialization, will set all memory to zero. And this initialization will require the operating system to actually allocate physical memory. Over allocate memory will squash the file cache too.We can optimize the operator algorithm, apply lazy allocation, and avoid meaningless memory allocation.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.TestMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.MemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.ListMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMapSpillMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.ResettableExternalBufferTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.ResettableExternalBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashBucketArea.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryInMemorySortBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryIndexedSortable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalSorter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15175" opendate="2019-12-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>syntax not supported in SQLClient for TPCDS queries</summary>
      <description>Flink SQL&gt; WITH customer_total_return AS&gt; ( SELECT&gt; sr_customer_sk AS ctr_customer_sk,&gt; sr_store_sk AS ctr_store_sk,&gt; sum(sr_return_amt) AS ctr_total_return&gt; FROM store_returns, date_dim&gt; WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000&gt; GROUP BY sr_customer_sk, sr_store_sk)&gt; SELECT c_customer_id&gt; FROM customer_total_return ctr1, store, customer&gt; WHERE ctr1.ctr_total_return &gt;&gt; (SELECT avg(ctr_total_return) * 1.2&gt; FROM customer_total_return ctr2&gt; WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)&gt; AND s_store_sk = ctr1.ctr_store_sk&gt; AND s_state = 'TN'&gt; AND ctr1.ctr_customer_sk = c_customer_sk&gt; ORDER BY c_customer_id&gt; LIMIT 100;[ERROR] Unknown or invalid SQL statement.It seems that the newest branch already support all TPCDS queries, but currently the sql client parser has not supported yet. Anyone already working on this? If not I can try it.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="15194" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Directories in distributed caches are not extracted in Yarn Per Job Cluster Mode</summary>
      <description>If we insert such code into the word count batch examples:File testDirectory = new File("test_directory");testDirectory.mkdirs();env.registerCachedFile(testDirectory.getAbsolutePath(), "test_directory");text = text.map(new RichMapFunction&lt;String, String&gt;() { @Override public String map(String value) throws Exception { File testDirectory = getRuntimeContext().getDistributedCache().getFile("test_directory"); if (!testDirectory.isDirectory()) { throw new RuntimeException( String.format("the directory %s is not a directory!", testDirectory.getAbsolutePath())); } return value; }});It works well in standalone mode but fails in Yarn Per Job Cluster Mode, the exception is:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:146) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:671) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:216) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:933) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1006) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1006)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7) at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:93) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:804) at org.apache.flink.api.java.DataSet.collect(DataSet.java:413) at org.apache.flink.api.java.DataSet.print(DataSet.java:1652) at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:115) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ... 11 moreCaused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7) at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:532) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110) ... 19 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:188) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:452) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: the directory /tmp/hadoop-zhongwei/nm-local-dir/usercache/zhongwei/appcache/application_1576030059607_0008/flink-dist-cache-bb275987-90cf-406a-9890-caed34983a04/da572c60eb63b13b7a90892f1958a7b7/test_directory.zip is not a directory! at org.apache.flink.examples.java.wordcount.WordCount$1.map(WordCount.java:95) at org.apache.flink.examples.java.wordcount.WordCount$1.map(WordCount.java:89) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.collect(ChainedMapDriver.java:79) at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:196) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)It seems the zip file is not extracted in yarn per job mode.Here is the complete code of the example:public class WordCount { public static void main(String[] args) throws Exception { final MultipleParameterTool params = MultipleParameterTool.fromArgs(args); final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); DataSet&lt;String&gt; text = null; if (params.has("input")) { for (String input : params.getMultiParameterRequired("input")) { if (text == null) { text = env.readTextFile(input); } else { text = text.union(env.readTextFile(input)); } } Preconditions.checkNotNull(text, "Input DataSet should not be null."); } else { System.out.println("Executing WordCount example with default input data set."); System.out.println("Use --input to specify file input."); text = WordCountData.getDefaultTextLineDataSet(env); } File testDirectory = new File("test_directory"); testDirectory.mkdirs(); env.registerCachedFile(testDirectory.getAbsolutePath(), "test_directory"); text = text.map(new RichMapFunction&lt;String, String&gt;() { @Override public String map(String value) throws Exception { File testDirectory = getRuntimeContext().getDistributedCache().getFile("test_directory"); if (!testDirectory.isDirectory()) { throw new RuntimeException( String.format("the directory %s is not a directory!", testDirectory.getAbsolutePath())); } return value; } }); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new Tokenizer()) .groupBy(0) .sum(1); if (params.has("output")) { counts.writeAsCsv(params.get("output"), "\n", " "); env.execute("WordCount Example"); } else { System.out.println("Printing result to stdout. Use --output to specify output path."); counts.print(); } } public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { String[] tokens = value.toLowerCase().split("\\W+"); for (String token : tokens) { if (token.length() &gt; 0) { out.collect(new Tuple2&lt;&gt;(token, 1)); } } } }}</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.testjob.YarnTestCacheJob.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.filecache.FileCacheDirectoriesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="15197" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add resource related config options to dynamical properties for Kubernetes</summary>
      <description>Since FLIP-49 and FLIP-53 have been completely merged. The new introduced resource config options should be passed to taskmanager via dynamic properties for Kubernetes. The dynamic properties are generated in KubernetesResourceManager.TaskExecutorResourceUtils#generateDynamicConfigsStr could be used to generate the dynamic properties. cc &amp;#91;~xintongsong&amp;#93;</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15199" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Benchmarks are not compiling</summary>
      <description>Recent changes in FLINK-14926 caused:[ERROR] COMPILATION ERROR : [INFO] -------------------------------------------------------------[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/state/benchmark/BackendUtils.java:[61,57] cannot infer type arguments for org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&lt;&gt;[INFO] 1 error[INFO] -------------------------------------------------------------[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.635 s[INFO] Finished at: 2019-12-11T14:58:37+01:00[INFO] Final Memory: 31M/751M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-hackathon-benchmarks: Compilation failure[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/state/benchmark/BackendUtils.java:[61,57] cannot infer type arguments for org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&lt;&gt;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1520" opendate="2015-2-11 00:00:00" fixdate="2015-9-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Read edges and vertices from CSV files</summary>
      <description>Add methods to create Vertex and Edge Datasets directly from CSV file inputs.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationWithCsvITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.GatherSumApplyConfigurationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSATriangleCount.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.GraphCsvReader.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GraphMetrics.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.ConnectedComponents.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="15201" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove verifications in context env for detach execution</summary>
      <description>From Aljoscha Krettek: I think we actually don't need these "verifications" anymore, with the new architecture where the Executor is called inside the execute() method and where we don't actually "hijack" the method anymore, we can actually have multiple execute() calls. We can address that in a follow-up, though.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.DetachedJobExecutionResult.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironmentFactory.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15203" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rephrase Hive&amp;#39;s data types doc</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15205" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add doc and exmaple of INSERT OVERWRITE and insert into partitioned table for Hive connector</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
    </fixedFiles>
  </bug>
  <bug id="1522" opendate="2015-2-11 00:00:00" fixdate="2015-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for the library methods and examples</summary>
      <description>The current tests in gelly test one method at a time. We should have some tests for complete applications. As a start, we could add one test case per example and this way also make sure that our graph library methods actually give correct results.I'm assigning this to andralungu because she has already implemented the test for SSSP, but I will help as well.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPathsExample.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.LabelPropagationExample.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.LabelPropagationExampleITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15221" opendate="2019-12-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support exactly once sink delivery semantic for Kafka in Table API</summary>
      <description>The Table Api doesn't support End to End Exactly once sematic like datastream Api.  Does Flink have a plan for this?</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicSink.java</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="15222" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move state benchmark utils into core repository</summary>
      <description>Currently we're maintaining the state benchmark utils in the flink-benchmark repository instead of core repository, which not only make it hard to find out compatibility issues if state backend codes are refactored and will cause problems like FLINK-15199, but also disobeys the instructions of flink-benchmark project:Recommended code structure is to define all benchmarks in Apache Flink and only wrap them here, in this repository, into executor classes.Such code structured is due to using GPL2 licensed jmh library for the actual execution of the benchmarks. Ideally we would prefer to have all of the code moved to Apache FlinkWe will improve this and prevent future incompatible problem in this JIRA.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15228" opendate="2019-12-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop vendor specific deployment documentation</summary>
      <description>Based on a mailing list discussion we want to drop vendor specific deployment documentationml discussion: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Drop-vendor-specific-deployment-documentation-td35457.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.zh.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.md</file>
      <file type="M">docs.ops.deployment.gce.setup.zh.md</file>
      <file type="M">docs.ops.deployment.gce.setup.md</file>
      <file type="M">docs.ops.deployment.aws.zh.md</file>
      <file type="M">docs.ops.deployment.aws.md</file>
      <file type="M">docs.internals.components.zh.md</file>
      <file type="M">docs.internals.components.md</file>
    </fixedFiles>
  </bug>
  <bug id="15234" opendate="2019-12-13 00:00:00" fixdate="2019-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive table created from flink catalog table shouldn&amp;#39;t have null properties in parameters</summary>
      <description>we store comment of a catalog table in Hive table's parameters. When it's null, we put a &lt;"comment", null&gt; k-v in the parameters. Hive table doesn't take null in its params.</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogTable.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="15245" opendate="2019-12-13 00:00:00" fixdate="2019-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink running in one cluster cannot write data to Hive tables in another cluster</summary>
      <description>Launch Flink cluster and write some data to a Hive table in another cluster. The job finishes successfully but data is not really written.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOutputFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="1525" opendate="2015-2-11 00:00:00" fixdate="2015-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide utils to pass -D parameters to UDFs</summary>
      <description>Hadoop users are used to setting job configuration through "-D" on the command line.Right now, Flink users have to manually parse command line arguments and pass them to the methods.It would be nice to provide a standard args parser with is taking care of such stuff.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.analyzer.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.history.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.analyze.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JobManagerInfoServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="15256" opendate="2019-12-13 00:00:00" fixdate="2019-12-13 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>unable to drop table in HiveCatalogITCase</summary>
      <description>@Test public void testCsvTableViaSQL() throws Exception { EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build(); TableEnvironment tableEnv = TableEnvironment.create(settings); tableEnv.registerCatalog("myhive", hiveCatalog); tableEnv.useCatalog("myhive"); String path = this.getClass().getResource("/csv/test.csv").getPath(); tableEnv.sqlUpdate("create table test2 (name String, age Int) with (\n" + " 'connector.type' = 'filesystem',\n" + " 'connector.path' = 'file://" + path + "',\n" + " 'format.type' = 'csv'\n" + ")"); Table t = tableEnv.sqlQuery("SELECT * FROM myhive.`default`.test2"); List&lt;Row&gt; result = TableUtils.collectToList(t); // assert query result assertEquals( new HashSet&lt;&gt;(Arrays.asList( Row.of("1", 1), Row.of("2", 2), Row.of("3", 3))), new HashSet&lt;&gt;(result) ); tableEnv.sqlUpdate("drop table myhive.`default`.tests2"); }The last drop table statement reports error as:org.apache.flink.table.api.ValidationException: Could not execute DropTable in path `myhive`.`default`.`tests2` at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:568) at org.apache.flink.table.catalog.CatalogManager.dropTable(CatalogManager.java:543) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:519) at org.apache.flink.table.catalog.hive.HiveCatalogITCase.testCsvTableViaSQL(HiveCatalogITCase.java:123) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.runTestMethod(FlinkStandaloneHiveRunner.java:169) at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.runChild(FlinkStandaloneHiveRunner.java:154) at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.runChild(FlinkStandaloneHiveRunner.java:92) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: org.apache.flink.table.catalog.exceptions.TableNotExistException: Table (or view) default.tests2 does not exist in Catalog test-catalog. at org.apache.flink.table.catalog.hive.HiveCatalog.dropTable(HiveCatalog.java:449) at org.apache.flink.table.catalog.CatalogManager.lambda$dropTable$14(CatalogManager.java:544) at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:566) ... 33 moreOK</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-modules.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleDescriptorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModuleFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModuleDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="15259" opendate="2019-12-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveInspector.toInspectors() should convert Flink constant to Hive constant</summary>
      <description>repro test: public class HiveModuleITCase { @Test public void test() { TableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(); tEnv.unloadModule("core"); tEnv.loadModule("hive", new HiveModule("2.3.4")); tEnv.sqlQuery("select concat('an', 'bn')"); }}seems that currently HiveInspector.toInspectors() didn't convert Flink constant to Hive constant before calling hiveShim.getObjectInspectorForConstantI don't think it's a blocker</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.WritableHiveObjectConversion.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.FlinkHiveException.java</file>
    </fixedFiles>
  </bug>
  <bug id="15260" opendate="2019-12-14 00:00:00" fixdate="2019-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uniformize Kubernetes executor name with the rest</summary>
      <description>Currently the Kubernetes executor is named "kubernetes-session-cluster" while all the others have the word executor in their name. This issue will rename it to "kubernetes-session-executor".</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.executors.KubernetesSessionClusterExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15263" opendate="2019-12-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add dedicated page for HiveCatalog</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15268" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shaded Hadoop S3A end-to-end test fails on travis</summary>
      <description>As titled, the 'Shaded Hadoop S3A end-to-end test' case failed with below error:java.io.IOException: regular upload failed: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AUtils.extractException(S3AUtils.java:291) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.putObject(S3ABlockOutputStream.java:448) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:360) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101) at org.apache.flink.fs.s3.common.hadoop.HadoopDataOutputStream.close(HadoopDataOutputStream.java:52) at org.apache.flink.core.fs.ClosingFSDataOutputStream.close(ClosingFSDataOutputStream.java:64) at java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188) at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341) at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161) at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:258) at org.apache.flink.api.java.io.CsvOutputFormat.close(CsvOutputFormat.java:170) at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:227) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException at org.apache.flink.fs.s3base.shaded.com.amazonaws.util.Md5Utils.md5AsBase64(Md5Utils.java:104) at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1647) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:1531) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$putObject$5(WriteOperationHelper.java:426) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260) at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)https://api.travis-ci.org/v3/job/625037121/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15269" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hive dialect limitation to overwrite and partition syntax</summary>
      <description>As http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Overwrite-and-partition-inserting-support-in-1-10-td35829.html#a35885 discussed.We should: Remove hive dialect limitation for supported "INSERT OVERWRITE" and "INSERT ... PARTITION(...)". Limit "CREATE TABLE ... PARTITIONED BY" to hive dialect.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="15270" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about how to specify third-party dependencies via API for Python UDFs</summary>
      <description>Currently we have already provided APIs and command line options to allow users to specify third-part dependencies which may be used in Python UDFs. There are already documentation about how to specify third-part dependencies in the command line options. We should also add documentation about how to specify third-party dependencies via API.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
    </fixedFiles>
  </bug>
  <bug id="15271" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about the Python environment requirements</summary>
      <description>Python UDF has specific requirements about the Python environments, such as Python 3.5+, Beam 2.15.0, etc.  We should add clear documentation about these requirements.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="15272" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error message when insert partition with values</summary>
      <description>Now, we not support insert partition with values like:Insert into mytable partition (date='2019-08-08') values ('jason', 25)Will throw a exception:schema not match.We should improve error message to tell user we not support this pattern.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15274" opendate="2019-12-16 00:00:00" fixdate="2019-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Filesystem documentation to reflect changes in shading</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15275" opendate="2019-12-16 00:00:00" fixdate="2019-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI documentation to include only current valid options</summary>
      <description>Currently the documentation for the CLI contains outdated/invalid information, such as deprecated and removed options. This has to be fixed before the release.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="15278" opendate="2019-12-16 00:00:00" fixdate="2019-1-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update StreamingFileSink documentation</summary>
      <description>Many times in the ML we have seen questions about the StreamingFileSink that could have been answered with better documentation that includes:1) shortcomings (especially in the case of S3 and also bulk formats)2) file lifecycle</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="15279" opendate="2019-12-16 00:00:00" fixdate="2019-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new `executeAsync()` method and the newly introduced `JobClient`</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.api.concepts.zh.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug id="15281" opendate="2019-12-16 00:00:00" fixdate="2019-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map Flink&amp;#39;s TypeInference to Calcite&amp;#39;s interfaces</summary>
      <description>After a TypeInference is available (either through reflective extraction or manual definition), the information needs to be connected to Calcite's interfaces. In particular, SqlOperandTypeInference, SqlOperandTypeChecker, SqlReturnTypeInference.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
    </fixedFiles>
  </bug>
  <bug id="15286" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Managed Memory Option for RocksDB not picked up from config</summary>
      <description>This is a missing lookup in the config in the State Backend's configure() method. Or, more precisely, in the State Backend's MemoryConfiguration's configure() method.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="15287" opendate="2019-12-17 00:00:00" fixdate="2019-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ack flink-hadoop-compatibility and flink-orc into flink-hive</summary>
      <description>flink-connector-hive should contain flink-hadoop-compatibility to reduce users' efforts in figuring out dependency jars, and flink-orc for adding orc dependency which is missing in hive 2.2.x.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15288" opendate="2019-12-17 00:00:00" fixdate="2019-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Starting jobmanager pod should respect containerized.heap-cutoff</summary>
      <description>Starting jobmanager pod should respect containerized.heap-cutoff. The cutoff will be used to leave some memory for jvm off-heap, for example meta space, thread native memory and etc. public static final ConfigOption&lt;Float&gt; CONTAINERIZED_HEAP_CUTOFF_RATIO = ConfigOptions .key("containerized.heap-cutoff-ratio") .defaultValue(0.25f) .withDeprecatedKeys("yarn.heap-cutoff-ratio") .withDescription("Percentage of heap space to remove from containers (YARN / Mesos / Kubernetes), to compensate" + " for other JVM memory usage.");</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15311" opendate="2019-12-18 00:00:00" fixdate="2019-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lz4BlockCompressionFactory should use native compressor instead of java unsafe</summary>
      <description>According to:https://lz4.github.io/lz4-java/1.7.0/lz4-compression-benchmark/Java java unsafe compressor has lower performance than native lz4 compressor.After FLINK-14845 , we use lz4 compression for shuffler.In testing, I found shuffle using java unsafe compressor.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15313" opendate="2019-12-18 00:00:00" fixdate="2019-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not insert decimal with precision into sink using TypeInformation</summary>
      <description>Sink DDL: val sinkDDL = s""" |CREATE TABLE T2 ( | d DECIMAL(10, 2), | cnt INT |) with ( | 'connector.type' = 'filesystem', | 'connector.path' = '$sinkFilePath', | 'format.type' = 'csv', | 'format.field-delimiter' = ',' |) """.stripMarginUsing blink with batch mode. (ensure insert BinaryRow into sink table) In FLINK-15124 , but we still use wrong precision to construct DataFormatConverter. This will lead to exception when inserting BinaryRow. (BinaryRow need correct precision to get) </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15322" opendate="2019-12-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet test fails with Hive versions prior to 1.2.0</summary>
      <description>Some data types are not supported by parquet tables in older Hive versions. Related to: https://issues.apache.org/jira/browse/HIVE-6384</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15332" opendate="2019-12-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen tests are broken due to copying un-relocated flink-s3-fs-hadoop* into lib</summary>
      <description>The Jepsen tests are currently broken because we copy flink-s3-fs-hadoop* into lib. With FLINK-11956 we removed the relocations from these classes and hence they need to reside in plugins.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="15338" opendate="2019-12-20 00:00:00" fixdate="2019-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TM Metaspace memory leak when submitting PyFlink UDF jobs multiple times</summary>
      <description>Start a standalone cluster and after submit PyFlink UDF jobs multiple times, the TM will fail with the following exception: Caused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:788) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:448) at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60) at java.lang.ClassLoader.loadClass(ClassLoader.java:380) at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:628) at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:531) at org.apache.beam.sdk.options.PipelineOptionsFactory.&lt;clinit&gt;(PipelineOptionsFactory.java:469) at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:173) at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractPythonScalarFunctionOperator.java:193) at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:139) at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:143) at org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:86) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1018) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454) at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$125/800044563.run(Unknown Source) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:834)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15342" opendate="2019-12-20 00:00:00" fixdate="2019-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify querying Hive view</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15344" opendate="2019-12-20 00:00:00" fixdate="2019-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update limitations in hive udf document</summary>
      <description>The limitation is not valid now.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="15349" opendate="2019-12-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add "create catalog" DDL to blink planner</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP+69+-+Flink+SQL+DDL+Enhancementsome customers who have internal streaming platform requested this feature, as it's not possible on a platform to load catalogs dynamically at runtime now via sql client yaml. Catalog DDL will come into play</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="15355" opendate="2019-12-21 00:00:00" fixdate="2019-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly streaming file sink fails with unshaded hadoop</summary>
      <description>org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1751) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:94) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:63) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628) at StreamingFileSinkProgram.main(StreamingFileSinkProgram.java:77) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ... 11 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1746) ... 20 moreCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:326) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., &lt;Exception on server side:org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job. at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ... 6 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:152) at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84) at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379) at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34) ... 7 moreCaused by: java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getTimeDuration(Ljava/lang/String;Ljava/lang/String;Ljava/util/concurrent/TimeUnit;)J at org.apache.hadoop.fs.s3a.S3ARetryPolicy.&lt;init&gt;(S3ARetryPolicy.java:113) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:257) at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126) at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61) at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441) at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362) at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298) at org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorage.&lt;init&gt;(MemoryBackendCheckpointStorage.java:85) at org.apache.flink.runtime.state.memory.MemoryStateBackend.createCheckpointStorage(MemoryStateBackend.java:295) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:279) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:205) at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:486) at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:338) at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:245) at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:217) at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:205) at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:119) at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105) at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278) at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:266) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:146) ... 10 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.CoreOptionsTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginManagerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginLoaderTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.core.configuration.html</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15367" opendate="2019-12-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle backwards compatibility of "taskmanager.heap.size" differently for standalone / active setups</summary>
      <description>Previously, "taskmanager.heap.size" were used differently for calculating TM memory sizes on standalone / active setups. To fully align with the previous behaviors, we need to map this deprecated key to "taskmanager.memory.flink.size" for standalone setups and "taskmanager.memory.process.size" for active setups.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.BashJavaUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.AbstractContainerizedClusterClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15369" opendate="2019-12-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniCluster use fixed network / managed memory sizes by default</summary>
      <description>Currently, Mini Cluster may allocate off-heap memory (managed &amp; network) according to the JVM free heap size and configured off-heap fractions. This could lead to unnecessary large off-heap memory usage and unpredictable / hard-to-understand behaviors.We believe a fix value for managed / network memory would be enough for a such a setup that runs Flink as a library.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15373" opendate="2019-12-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for framework / task off-heap memory config options</summary>
      <description>Update descriptions for "taskmanager.memory.framework.off-heap.size" and "taskmanager.memory.task.off-heap.size" to explicitly state that: Both direct and native memory are accounted Will be fully counted into MaxDirectMemorySizeDetailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15374" opendate="2019-12-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for jvm overhead config options</summary>
      <description>Update descriptions for "taskmanager.memory.jvm-overhead.&amp;#91;min|max|fraction&amp;#93;" to remove "I/O direct memory" and explicitly state that it's not counted into MaxDirectMemorySize.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15377" opendate="2019-12-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos WordCount test fails on travis</summary>
      <description>The "Run Mesos WordCount test" fails nightly run on travis with below error:rm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.INFO': Permission deniedrm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-fetcher.INFO': Permission deniedrm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.4a4fda410c57.invalid-user.log.INFO.20191224-031307.1': Permission denied...[FAIL] 'Run Mesos WordCount test' failed after 5 minutes and 26 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out fileshttps://api.travis-ci.org/v3/job/628795106/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15380" opendate="2019-12-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to set number of TM and number of Slot for MiniCluster in Scala shell</summary>
      <description>It is possible to set that for MiniCluster in other places, but unable to do that in Scala shell.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15381" opendate="2019-12-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT INTO VALUES statement fails if a cast project is applied</summary>
      <description>The following query will fail: @Test def test(): Unit = { val sinkDDL = """ |create table t2( | a int, | b string |) with ( | 'connector' = 'COLLECTION' |) """.stripMargin val query = """ |insert into t2 select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b) """.stripMargin tableEnv.sqlUpdate(sinkDDL) tableEnv.sqlUpdate(query) execJob("testJob") }exception:org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalSink(name=[`default_catalog`.`default_database`.`t2`], fields=[a, b])+- LogicalProject(EXPR$0=[$0], EXPR$1=[CAST($1):VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL]) +- LogicalValues(type=[RecordType(INTEGER a, CHAR(1) b)], tuples=[[{ 3, _UTF-16LE'c' }]])This exception indicates that the query uses an unsupported SQL feature.Please check the documentation for the set of currently supported SQL features.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkDefaultRelMetadataProvider.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15382" opendate="2019-12-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink failed generating python config docs</summary>
      <description>When generating config option docs with the command suggested by flink-docs/README.md, the generated docs/_includes/generated/python_configuration.html does not contain any config options despite that there are 4 options in PythonOptions. I encountered this problem at the commit 545534e43ed37f518fe59b6ddd8ed56ae82a234b on master branch.Command used to generate doc:mvn package -Dgenerate-config-docs -pl flink-docs -am -nsu -DskipTests</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15396" opendate="2019-12-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to ignore parse errors for JSON format</summary>
      <description>We already support 'format.ignore-parse-errors' to skip dirty records in CSV format. We can also support it in JSON format.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.table.descriptors.JsonTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.JsonValidator.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.Json.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="15406" opendate="2019-12-26 00:00:00" fixdate="2019-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB savepoints with heap timers cannot be restored by non-process functions</summary>
      <description>The savepoint is writted by "State Processor API" can't be restore by map or flatmap. But it can be retored by KeyedProcessFunction.  Following is the error message:java.lang.Exception: Could not write timer service of Flat Map -&gt; Map -&gt; Sink: device_first_user_create (1/8) to checkpoint state stream.java.lang.Exception: Could not write timer service of Flat Map -&gt; Map -&gt; Sink: device_first_user_create (1/8) to checkpoint state stream. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:466) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:399) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1282) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1216) at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:872) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:777) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:708) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88) at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:102) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:47) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:135) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:279) at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:301) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:406) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58) at org.apache.flink.streaming.api.operators.InternalTimersSnapshot.&lt;init&gt;(InternalTimersSnapshot.java:52) at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.snapshotTimersForKeyGroup(InternalTimerServiceImpl.java:291) at org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.write(InternalTimerServiceSerializationProxy.java:98) at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.snapshotStateForKeyGroup(InternalTimeServiceManager.java:139) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:462) ... 19 more  </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.KeyedStateBootstrapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15408" opendate="2019-12-26 00:00:00" fixdate="2019-11-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Interval join support no equi-condition</summary>
      <description>For Now, Interval join must has at least one equi-condition. Should we need to allow no equi-condition like regular join?For that, if sql like as follow:INSERT INTO A SELECT * FROM B join C on B.rowtime BETWEEN C.rowtime - INTERVAL '20' SECOND AND C.rowtime + INTERVAL '30' SECONDIt will has no matched rule to convert.Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=STREAM_PHYSICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, UpdateAsRetractionTraitDef=false, AccModeTraitDef=UNKNOWN.Missing conversion is FlinkLogicalJoin[convention: LOGICAL -&gt; STREAM_PHYSICAL]</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIntervalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15420" opendate="2019-12-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast string to timestamp will loose precision</summary>
      <description>cast('2010-10-14 12:22:22.123456' as timestamp(9))Will produce "2010-10-14 12:22:22.123" in blink planner, this should not happen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15426" opendate="2019-12-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test (Blink planner) fails on travis</summary>
      <description>TPC-DS end-to-end test (Blink planner) fails on travis with below error:The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Field types of query result and registered TableSink default_catalog.default_database.query2_sinkTable do not match.... at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)https://api.travis-ci.org/v3/job/629699422/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="15427" opendate="2019-12-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State TTL RocksDb backend end-to-end test stalls on travis</summary>
      <description>The 'State TTL RocksDb backend end-to-end test' case stalls and finally timedout with error message:The job exceeded the maximum log length, and has been terminated.https://api.travis-ci.org/v3/job/629699416/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="1543" opendate="2015-2-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper exception handling in actors</summary>
      <description>With Akka's actors it is important to not throw exceptions in the actor thread, if one does not want that the actor restarts or stops. Many of the Java components which are called from the actor's receive method throw exceptions which are not properly handled by the actor thread. Therefore, we have to catch these exceptions and handle them properly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.ActorLogMessages.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.accumulators.AccumulatorManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.InstanceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="15430" opendate="2019-12-28 00:00:00" fixdate="2019-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Java 64K method compiling limitation for blink planner.</summary>
      <description>Our Flink SQL version is migrated from 1.5 to 1.9, and from legacy planner to blink planner. We find that some large SQL meets the problem of code gen which exceeds Java 64k method limitation.After searching in issues, we find https://issues.apache.org/jira/browse/FLINK-8274 which fix the bug to some extent. But for blink planner, it has not been fixed for now.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionInfo.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GeneratedExpression.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15435" opendate="2019-12-29 00:00:00" fixdate="2019-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExecutionConfigTests.test_equals_and_hash in pyFlink fails when cpu core numbers is 6</summary>
      <description>My laptop's cpu core number is 6, so the default parallelism of the ExecutionEnvironment is 12. And the test will fail.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.common.tests.test.execution.config.py</file>
    </fixedFiles>
  </bug>
  <bug id="15439" opendate="2019-12-31 00:00:00" fixdate="2019-12-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect description about unsupported DDL in "Queries" page</summary>
      <description>DDL is supported now, document should be updated.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="1544" opendate="2015-2-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Extend streaming aggregation tests to include POJOs</summary>
      <description>Currently the streaming aggregation tests don't test pojo aggregations which makes newly introduced bugs harder to detect.New tests should be added.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15442" opendate="2019-12-31 00:00:00" fixdate="2019-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the Avro Confluent Schema Registry nightly end-to-end test</summary>
      <description>We have already harden the Avro Confluent Schema Registry test in FLINK-13567. However, there are still some defects in current mechanism. The loop variable i is not safe, it could be modified by the command. The process of downloading kafka 0.10 is not included in the scope of retry_times . I think we need to include it to tolerent transient network issue.We need to fix those issue to harden the Avro Confluent Schema Registry nightly end-to-end test.cc: Till Rohrmann Chesnay Schepler</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15446" opendate="2019-12-31 00:00:00" fixdate="2019-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve "Connect to External Systems" documentation page</summary>
      <description>1. Remove documentation for format schema, which is not necessary any more and is deprecated.2. Add DDL documentation for "Table Schema" and "Rowtime Attribute" sections.3. Update the comments in DDL for better rendering (do not wrap line).</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="15468" opendate="2020-1-3 00:00:00" fixdate="2020-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT OVERWRITE not supported from SQL CLI</summary>
      <description>Running INSERT OVERWRITE from SQL CLI will get:[ERROR] Unknown or invalid SQL statement.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15482" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to call Hive functions returning decimal type</summary>
      <description>Calling a Hive function with decimal return type will get:java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.flink.table.dataformat.Decimal at ExpressionReducer$5.map(Unknown Source) at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:117)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveScalarFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
    </fixedFiles>
  </bug>
  <bug id="15483" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Starting jobmanager pod should respect environment config option</summary>
      <description>Currently, we could use `containerized.master.env.` to set the user-defined environment variables. For Yarn, it works correctly. However, it could not take effect on Kubernetes deployment. Some users have tried the flink native integration and find this problem. This is nice to have in 1.10 and not a blocker. Since we could set the environment when building the image instead.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8ClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkMasterDeploymentDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15489" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI log refresh not working</summary>
      <description>There is no way to query the latest state of logs of jobmanager/taskmanager.The Web UI show only the first version that was ever displayed.How to reproduce: (not sure if necessary) configure logback as described here: https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster start a cluster show jobmanager logs in the Web UI run example job check again the jobmanager logs, there is no trace of the job. Clicking the refresh button does not help</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  <bug id="15490" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaITCase.testOneSourceMultiplePartitions fails on Travis</summary>
      <description>The test KafkaITCase.testOneSourceMultiplePartitions failed on Travis because it received a duplicate value:13:10:49,276 INFO org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper - ============================&gt; Failing mapper 1: count=2802, totalCount=3167org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:648) at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628) at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:35) at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOneSourceMultiplePartitionsExactlyOnceTest(KafkaConsumerTestBase.java:912) at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOneSourceMultiplePartitions(KafkaITCase.java:102) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:186) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:181) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:175) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:476) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380) at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: Received a duplicate: 4095 at org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink.invoke(ValidatingExactlyOnceSink.java:57) at org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink.invoke(ValidatingExactlyOnceSink.java:36) at org.apache.flink.streaming.api.functions.sink.SinkFunction.invoke(SinkFunction.java:52) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)https://api.travis-ci.org/v3/job/633245929/log.txt</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15495" opendate="2020-1-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default planner for SQL Client to Blink planner</summary>
      <description>As discussed in the mailing list &amp;#91;1&amp;#93;, we will change the default planner to Blink planner for SQL CLI. &amp;#91;1&amp;#93;: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="15499" opendate="2020-1-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No debug log describes the host of a TM before any task is deployed to it in YARN mode</summary>
      <description>When troubleshooting FLINK-15456, I noticed a TM hang in starting and not able to register to RM. However, there is no debug log on which host the TM located on and thus I can hardly find the logs of the problematic TM.I think we should print the host name when starting a TM, i.e. in this logs"TaskExecutor container_XXXX will be started ...".This would make it possible for us to troubleshoot similar problems. (not only for cases that TM hang in starting, but also for cases that TM exits in starting)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="15512" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Refactor the mechanism of how to constructure the cache and write buffer manager shared across RocksDB instances</summary>
      <description>FLINK-14484 introduce a LRUCache to share among RocksDB instances, so that the memory usage by RocksDB could be controlled well. However, due to the implementation and some bugs in RocksDB (issue-6247), we cannot limit the memory strictly.The way to walk around this issue is to consider the buffer which memtable would overuse (1/2 write buffer manager size). By introducing this, the actual cache size for user to share is not the same as the managed off-heap memory or user configured memory.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15515" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that Hive connector should be used with blink planner</summary>
      <description>HiveCatalog works with both old and blink planner. But read/write Hive tables only works with blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.scala.shell.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.scala.shell.hive.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t hide web frontend side pane automatically</summary>
      <description>As mentioned in FLINK-13386 the side pane hides automatically in some cases but not all cases. When I was debugging or trying the web frontend I found this behaviour a bit disconcerting. Could we disable the hiding by default? The user can still manually hide if they want to.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug id="15520" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prometheus E2E test should use DownloadCache</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="1553" opendate="2015-2-16 00:00:00" fixdate="2015-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connectors rework</summary>
      <description>The Kafka connectors should be reworked and tested because it is not fully functioning (e.g. the partitioner does not work correctly).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.SerializationSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTopology.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15542" opendate="2020-1-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>lz4-java licensing is incorrect</summary>
      <description>In FLINK-14845 an lz4-java dependency was moved from flink-table to flink-runtime. With this commit lz4 was included in the flink-runtime jar, and the NOTICE file for flink-runtime was adjusted appropriately.The follow-up that removed the shading (FLINK-15311) did not update the NOTICE files however; it is still listed in flink-runtime when it should be in flink-dist.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="15554" opendate="2020-1-10 00:00:00" fixdate="2020-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump jetty-util-ajax to 9.3.24</summary>
      <description>flink-fs-hadoop-azure has transitive dependency on jetty-util-ajax:9.3.19, which has a security vulnerability: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-7657This was fixed in 9.3.24.v20180605 (source). Starting from version 3.2.1 hadoop-azure is using this version as well, but for a quick resolution I propose bumping this single dependency for the time being.</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15558" opendate="2020-1-11 00:00:00" fixdate="2020-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Elasticsearch version from 7.3.2 to 7.5.1 for es7 connector</summary>
      <description>It would be better to track the newest ES 7.x client version just like we have done for Kafka universal connector.Currently, the ES7 connector track version 7.3.2 and the latest ES 7.x version is 7.5.1. We can upgrade it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15562" opendate="2020-1-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document example settings.xml for configuring snapshot repository</summary>
      <description>For SNAPSHOT versions, if using 3.0 or higher version maven, the "archetypeCatalog" is not valid. In FLINK-7839, we add note to inform that issue. In this ticket, we'd like to documents an example settings.xml and pass it to maven.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.md</file>
      <file type="M">docs.getting-started.project-setup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.getting-started.project-setup.scala.api.quickstart.md</file>
      <file type="M">docs.getting-started.project-setup.java.api.quickstart.zh.md</file>
      <file type="M">docs.getting-started.project-setup.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="15565" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible types of expression and result type thrown in codegen</summary>
      <description>The sql is: CREATE TABLE `over10k` ( t tinyint, si smallint, i int, b bigint, f float, d double, bo boolean, s varchar, ts timestamp, deci decimal(4,2), bin varchar ) WITH ( 'connector.path'='/daily_regression_batch_hive_1.10/test_window_with_specific_behavior/sources/over10k.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv' ); select s, rank() over (partition by s order by si), sum(b) over (partition by s order by si) from over10k limit 100;The data is : 109|277|65620|4294967305|97.25|7.80|true|nick quirinius|2013-03-01 09:11:58.703226|27.72|undecided 93|263|65725|4294967341|6.06|4.12|false|calvin king|2013-03-01 09:11:58.703299|32.44|values clariffication 108|383|65629|4294967510|39.55|47.67|false|jessica zipper|2013-03-01 09:11:58.703133|74.23|nap time 89|463|65537|4294967493|64.82|13.79|true|ethan white|2013-03-01 09:11:58.703243|89.52|nap time 88|372|65645|4294967358|34.48|11.18|true|quinn thompson|2013-03-01 09:11:58.703168|84.86|forestry 123|432|65626|4294967435|2.39|16.49|true|david white|2013-03-01 09:11:58.703136|61.24|joggying 57|486|65551|4294967397|36.11|9.88|true|katie xylophone|2013-03-01 09:11:58.703142|57.10|zync studies 59|343|65787|4294967312|66.89|6.54|true|mike laertes|2013-03-01 09:11:58.703209|27.56|xylophone band 74|267|65671|4294967409|21.14|14.64|true|priscilla miller|2013-03-01 09:11:58.703197|89.06|undecided 25|336|65587|4294967336|71.01|14.90|true|tom ichabod|2013-03-01 09:11:58.703127|74.32|zync studies 48|346|65712|4294967315|45.01|16.08|true|zach brown|2013-03-01 09:11:58.703108|21.68|zync studies 84|385|65776|4294967452|35.80|32.13|false|xavier zipper|2013-03-01 09:11:58.703311|99.46|education 58|389|65766|4294967416|95.55|20.62|false|sarah miller|2013-03-01 09:11:58.703215|70.92|history 22|403|65565|4294967381|99.65|35.42|false|yuri johnson|2013-03-01 09:11:58.703154|94.47|geology 55|428|65733|4294967535|99.54|5.35|false|jessica king|2013-03-01 09:11:58.703233|30.30|forestry 117|410|65706|4294967391|50.15|0.21|false|quinn johnson|2013-03-01 09:11:58.703248|65.99|yard duty 95|423|65573|4294967378|47.59|17.37|true|alice robinson|2013-03-01 09:11:58.703133|54.57|linguistics 87|332|65748|4294967320|19.83|41.67|false|fred ellison|2013-03-01 09:11:58.703289|79.02|mathematics 114|263|65674|4294967405|84.44|33.18|true|victor van buren|2013-03-01 09:11:58.703092|63.74|linguistics 5|369|65780|4294967488|92.02|38.59|true|zach polk|2013-03-01 09:11:58.703271|67.29|yard duty -3|430|65667|4294967469|65.50|40.46|true|yuri xylophone|2013-03-01 09:11:58.703258|30.94|american history 120|264|65769|4294967486|89.97|41.18|false|xavier hernandez|2013-03-01 09:11:58.703140|66.89|philosophy 107|317|65634|4294967488|5.68|18.89|false|priscilla ichabod|2013-03-01 09:11:58.703196|39.42|joggying 29|386|65723|4294967328|71.48|6.13|false|ulysses ichabod|2013-03-01 09:11:58.703215|86.65|xylophone band 22|434|65768|4294967543|44.25|27.56|false|tom polk|2013-03-01 09:11:58.703306|12.30|kindergarten -1|274|65755|4294967300|22.01|35.52|false|oscar king|2013-03-01 09:11:58.703141|33.35|chemistry 6|365|65603|4294967522|18.51|5.60|false|gabriella king|2013-03-01 09:11:58.703104|34.20|geology 97|414|65757|4294967325|31.82|22.37|false|rachel nixon|2013-03-01 09:11:58.703127|61.00|nap time 72|448|65538|4294967524|80.09|7.73|true|luke brown|2013-03-01 09:11:58.703090|95.81|american history 51|280|65589|4294967486|57.46|23.35|false|zach xylophone|2013-03-01 09:11:58.703299|11.54|education 12|447|65583|4294967389|0.98|29.79|true|yuri polk|2013-03-01 09:11:58.703305|1.89|wind surfing -1|360|65539|4294967464|4.08|39.51|false|oscar davidson|2013-03-01 09:11:58.703144|59.47|nap time 0|380|65569|4294967425|0.94|28.93|false|sarah robinson|2013-03-01 09:11:58.703176|88.81|xylophone band 66|478|65669|4294967339|23.66|38.34|true|yuri carson|2013-03-01 09:11:58.703228|64.68|opthamology 12|322|65771|4294967545|84.87|10.76|false|sarah allen|2013-03-01 09:11:58.703271|0.79|joggying 79|308|65563|4294967347|4.06|44.84|false|nick underhill|2013-03-01 09:11:58.703097|76.53|industrial engineering 4|382|65719|4294967329|7.26|39.92|true|fred polk|2013-03-01 09:11:58.703073|73.64|mathematics 10|448|65675|4294967392|26.20|16.30|true|rachel laertes|2013-03-01 09:11:58.703200|18.01|xylophone band 45|281|65685|4294967513|81.33|32.22|true|oscar allen|2013-03-01 09:11:58.703285|71.38|religion 57|288|65599|4294967422|90.33|44.25|false|bob young|2013-03-01 09:11:58.703185|11.16|biology 77|452|65706|4294967512|22.90|5.35|true|bob van buren|2013-03-01 09:11:58.703290|14.58|debate 103|492|65773|4294967404|58.29|48.28|false|yuri thompson|2013-03-01 09:11:58.703249|84.38|undecided 84|411|65737|4294967486|63.13|1.10|true|katie ichabod|2013-03-01 09:11:58.703086|29.57|american history 28|378|65589|4294967511|26.41|39.79|true|yuri polk|2013-03-01 09:11:58.703267|28.62|values clariffication 88|478|65752|4294967364|80.59|45.13|true|victor garcia|2013-03-01 09:11:58.703081|34.90|chemistry 37|388|65608|4294967350|32.94|39.06|false|mike polk|2013-03-01 09:11:58.703273|42.48|quiet hour 25|264|65648|4294967402|90.83|30.96|false|tom ichabod|2013-03-01 09:11:58.703268|65.58|history 17|455|65738|4294967508|15.73|27.01|false|david young|2013-03-01 09:11:58.703254|26.24|american history 62|438|65655|4294967511|91.77|1.90|false|sarah steinbeck|2013-03-01 09:11:58.703150|16.41|chemistry 65|298|65669|4294967328|68.89|2.75|true|david miller|2013-03-01 09:11:58.703077|51.86|values clariffication 25|491|65641|4294967387|94.82|10.04|false|ulysses thompson|2013-03-01 09:11:58.703124|63.75|linguistics 25|497|65708|4294967497|2.45|49.99|false|ethan laertes|2013-03-01 09:11:58.703320|49.72|yard duty 117|288|65591|4294967530|75.18|2.71|false|fred quirinius|2013-03-01 09:11:58.703221|99.58|geology 62|404|65706|4294967549|86.06|40.01|true|irene zipper|2013-03-01 09:11:58.703139|13.38|kindergarten 99|362|65709|4294967399|50.48|26.34|false|jessica white|2013-03-01 09:11:58.703294|83.53|kindergarten 62|395|65685|4294967446|56.73|14.87|false|victor johnson|2013-03-01 09:11:58.703194|31.42|history 62|386|65615|4294967359|44.03|43.78|true|luke underhill|2013-03-01 09:11:58.703099|86.73|nap time 15|302|65698|4294967526|91.38|3.59|true|wendy carson|2013-03-01 09:11:58.703111|9.46|religion 92|507|65699|4294967512|8.44|34.72|false|calvin xylophone|2013-03-01 09:11:58.703198|66.89|study skills 3|279|65756|4294967439|87.65|24.72|false|david white|2013-03-01 09:11:58.703233|47.19|study skills 114|330|65754|4294967500|76.20|39.35|true|rachel quirinius|2013-03-01 09:11:58.703145|76.16|undecided 24|500|65717|4294967535|60.96|21.51|false|victor falkner|2013-03-01 09:11:58.703318|82.83|nap time -2|331|65707|4294967335|67.12|13.51|false|bob ovid|2013-03-01 09:11:58.703285|62.32|joggying 101|463|65740|4294967425|52.27|11.58|true|priscilla robinson|2013-03-01 09:11:58.703078|13.09|yard duty 106|269|65577|4294967524|17.11|38.45|true|rachel falkner|2013-03-01 09:11:58.703197|79.89|xylophone band 121|500|65690|4294967517|49.31|9.85|false|luke robinson|2013-03-01 09:11:58.703074|37.91|topology 37|351|65587|4294967410|99.66|20.51|false|quinn falkner|2013-03-01 09:11:58.703221|80.69|history 6|340|65612|4294967345|54.08|3.53|true|oscar white|2013-03-01 09:11:58.703279|68.67|debate 115|366|65785|4294967330|90.00|25.79|true|jessica carson|2013-03-01 09:11:58.703143|2.72|xylophone band 124|307|65649|4294967368|81.66|19.35|true|wendy ichabod|2013-03-01 09:11:58.703254|73.76|opthamology 11|286|65752|4294967355|72.33|20.94|false|xavier carson|2013-03-01 09:11:58.703109|23.28|history 15|320|65716|4294967505|49.25|27.53|false|fred carson|2013-03-01 09:11:58.703263|18.08|industrial engineering 76|316|65706|4294967460|12.99|35.53|true|rachel davidson|2013-03-01 09:11:58.703300|85.43|quiet hour -2|485|65788|4294967510|9.99|22.75|false|luke carson|2013-03-01 09:11:58.703217|82.56|mathematics 87|482|65612|4294967327|16.51|22.21|true|katie nixon|2013-03-01 09:11:58.703083|47.09|xylophone band 21|400|65777|4294967354|4.05|11.10|false|david quirinius|2013-03-01 09:11:58.703205|25.69|geology 97|343|65764|4294967427|47.79|18.94|true|ethan miller|2013-03-01 09:11:58.703308|39.81|topology 2|292|65783|4294967420|38.86|12.14|true|wendy robinson|2013-03-01 09:11:58.703239|72.70|wind surfing 48|440|65570|4294967438|41.44|13.11|true|bob thompson|2013-03-01 09:11:58.703122|57.67|american history 87|333|65592|4294967296|71.77|8.28|false|yuri nixon|2013-03-01 09:11:58.703302|87.58|quiet hour -1|344|65616|4294967444|29.44|19.94|false|oscar falkner|2013-03-01 09:11:58.703203|28.22|geology 1|425|65625|4294967531|51.83|38.18|false|holly xylophone|2013-03-01 09:11:58.703198|0.31|geology 108|363|65715|4294967467|99.69|17.10|true|yuri xylophone|2013-03-01 09:11:58.703177|44.91|geology 93|500|65778|4294967442|82.52|38.24|true|xavier falkner|2013-03-01 09:11:58.703277|25.41|history 112|260|65612|4294967500|51.90|24.53|false|rachel falkner|2013-03-01 09:11:58.703211|65.45|american history 89|294|65754|4294967450|94.21|35.55|true|gabriella falkner|2013-03-01 09:11:58.703156|18.36|topology 32|389|65700|4294967525|42.65|32.59|true|yuri king|2013-03-01 09:11:58.703253|1.70|undecided 13|395|65715|4294967317|64.24|36.77|false|fred ovid|2013-03-01 09:11:58.703168|74.25|yard duty 5|262|65726|4294967543|8.85|12.89|true|rachel garcia|2013-03-01 09:11:58.703222|45.65|yard duty 65|324|65569|4294967315|93.15|41.46|false|alice brown|2013-03-01 09:11:58.703110|77.23|topology 73|477|65764|4294967542|27.96|44.68|false|bob steinbeck|2013-03-01 09:11:58.703173|90.95|undecided 6|337|65616|4294967456|38.34|34.04|true|rachel hernandez|2013-03-01 09:11:58.703223|60.63|debate 51|384|65649|4294967423|14.62|5.33|true|oscar king|2013-03-01 09:11:58.703232|21.96|history 87|369|65626|4294967403|20.94|26.46|true|ulysses hernandez|2013-03-01 09:11:58.703076|35.79|values clariffication 48|365|65558|4294967361|66.17|6.28|true|alice xylophone|2013-03-01 09:11:58.703081|51.13|study skills 12|388|65642|4294967298|58.26|34.09|false|jessica brown|2013-03-01 09:11:58.703081|92.61|linguistics 12|353|65703|4294967414|54.55|5.92|true|jessica johnson|2013-03-01 09:11:58.703289|91.71|chemistry 117|499|65566|4294967328|32.18|19.59|true|priscilla king|2013-03-01 09:11:58.703214|66.88|philosophy 116|363|65719|4294967513|18.59|48.19|false|priscilla johnson|2013-03-01 09:11:58.703237|55.47|history 21|433|65551|4294967366|84.35|34.09|false|oscar thompson|2013-03-01 09:11:58.703291|7.99|values clariffication -2|409|65717|4294967343|39.62|9.79|true|irene ichabod|2013-03-01 09:11:58.703315|64.80|joggying 23|495|65785|4294967473|30.91|21.95|true|fred robinson|2013-03-01 09:11:58.703240|66.34|nap time 30|507|65673|4294967453|83.51|40.92|true|oscar thompson|2013-03-01 09:11:58.703281|65.25|values clariffication 13|365|65594|4294967446|13.41|34.03|true|irene white|2013-03-01 09:11:58.703084|52.53|topology 92|419|65771|4294967310|64.82|3.01|false|yuri brown|2013-03-01 09:11:58.703271|18.05|undecided 81|351|65781|4294967473|48.46|15.80|false|bob nixon|2013-03-01 09:11:58.703254|99.35|debate 105|490|65543|4294967334|32.91|42.91|false|yuri steinbeck|2013-03-01 09:11:58.703233|42.19|xylophone band 25|402|65619|4294967340|6.28|49.92|true|victor xylophone|2013-03-01 09:11:58.703210|84.32|philosophy 88|485|65557|4294967391|95.95|46.22|true|irene xylophone|2013-03-01 09:11:58.703141|63.31|mathematics 81|285|65758|4294967338|37.83|38.23|true|irene ichabod|2013-03-01 09:11:58.703322|43.31|quiet hour 96|316|65764|4294967442|86.76|32.89|false|wendy miller|2013-03-01 09:11:58.703190|10.35|geology 43|321|65538|4294967422|81.78|6.07|false|zach van buren|2013-03-01 09:11:58.703273|26.02|topology 60|496|65614|4294967376|34.40|45.59|true|jessica steinbeck|2013-03-01 09:11:58.703076|81.95|xylophone band 44|395|65611|4294967443|15.58|1.53|false|gabriella thompson|2013-03-01 09:11:58.703295|11.00|values clariffication 73|409|65767|4294967371|36.93|36.16|true|quinn ellison|2013-03-01 09:11:58.703105|82.70|religion 121|330|65772|4294967508|70.46|44.50|true|quinn zipper|2013-03-01 09:11:58.703272|11.31|philosophy 61|421|65541|4294967410|34.59|27.52|false|calvin johnson|2013-03-01 09:11:58.703299|3.52|history 65|370|65674|4294967474|6.94|4.38|false|tom falkner|2013-03-01 09:11:58.703142|63.24|wind surfing 41|462|65699|4294967391|58.03|17.26|false|calvin xylophone|2013-03-01 09:11:58.703322|92.60|study skills 97|460|65591|4294967515|46.39|2.16|false|mike carson|2013-03-01 09:11:58.703265|97.16|values clariffication -1|435|65624|4294967377|73.60|45.63|true|irene hernandez|2013-03-01 09:11:58.703208|31.35|study skills 22|282|65782|4294967318|75.19|40.78|false|quinn ichabod|2013-03-01 09:11:58.703122|44.85|topology 46|487|65748|4294967318|67.01|24.13|false|victor zipper|2013-03-01 09:11:58.703273|95.40|linguistics 18|275|65757|4294967307|80.45|18.92|false|bob hernandez|2013-03-01 09:11:58.703307|38.25|education 103|264|65587|4294967306|97.65|11.36|false|david ovid|2013-03-01 09:11:58.703265|42.76|wind surfing 86|466|65642|4294967333|40.96|26.06|true|david young|2013-03-01 09:11:58.703155|2.99|kindergarten 119|437|65637|4294967494|18.93|31.04|true|calvin brown|2013-03-01 09:11:58.703241|30.45|debate 62|285|65593|4294967518|83.43|2.05|false|rachel xylophone|2013-03-01 09:11:58.703084|45.21|quiet hour 1|283|65752|4294967528|95.01|1.76|false|ethan ichabod|2013-03-01 09:11:58.703072|16.68|history 8|333|65732|4294967503|22.43|21.80|false|mike polk|2013-03-01 09:11:58.703160|71.80|industrial engineering 90|425|65648|4294967323|50.68|40.41|false|victor allen|2013-03-01 09:11:58.703146|58.75|kindergarten 110|319|65620|4294967332|32.36|35.17|true|ethan davidson|2013-03-01 09:11:58.703269|73.03|history 111|313|65711|4294967418|70.04|10.88|true|priscilla nixon|2013-03-01 09:11:58.703206|66.32|mathematics 96|399|65719|4294967401|52.35|4.01|true|rachel hernandez|2013-03-01 09:11:58.703076|32.45|values clariffication 83|353|65714|4294967384|10.12|15.81|false|rachel miller|2013-03-01 09:11:58.703110|16.39|philosophy 11|475|65747|4294967303|98.29|32.30|false|yuri king|2013-03-01 09:11:58.703285|11.06|forestry 84|295|65682|4294967463|17.75|23.28|true|alice zipper|2013-03-01 09:11:58.703306|79.77|industrial engineering 8|348|65626|4294967373|52.54|31.29|false|bob underhill|2013-03-01 09:11:58.703189|82.40|undecided 0|339|65603|4294967356|32.42|31.31|false|katie young|2013-03-01 09:11:58.703238|49.14|forestry 82|280|65688|4294967427|19.11|0.10|false|holly young|2013-03-01 09:11:58.703256|71.39|chemistry 119|465|65781|4294967467|23.83|0.95|false|yuri zipper|2013-03-01 09:11:58.703094|96.06|history 10|356|65586|4294967339|71.96|32.54|true|oscar zipper|2013-03-01 09:11:58.703091|73.01|quiet hour 25|364|65682|4294967449|50.96|34.46|true|sarah steinbeck|2013-03-01 09:11:58.703139|18.28|philosophy 47|270|65652|4294967393|85.46|33.87|true|luke zipper|2013-03-01 09:11:58.703173|96.68|philosophy 89|470|65676|4294967314|39.34|37.35|false|ulysses miller|2013-03-01 09:11:58.703303|69.67|values clariffication 105|393|65703|4294967359|19.00|45.80|false|oscar johnson|2013-03-01 09:11:58.703086|99.42|linguistics 120|415|65785|4294967498|54.68|32.92|true|calvin hernandez|2013-03-01 09:11:58.703086|93.09|linguistics 94|486|65649|4294967549|33.47|35.42|false|jessica carson|2013-03-01 09:11:58.703089|34.30|mathematics 38|288|65634|4294967304|5.10|44.83|false|ethan white|2013-03-01 09:11:58.703083|0.94|xylophone band 91|268|65578|4294967501|43.98|2.77|false|jessica white|2013-03-01 09:11:58.703195|51.68|joggying 123|409|65629|4294967431|29.23|27.30|false|ulysses garcia|2013-03-01 09:11:58.703141|70.01|philosophy 7|454|65697|4294967394|62.25|3.38|false|tom underhill|2013-03-01 09:11:58.703121|47.97|values clariffication 13|488|65662|4294967457|25.08|4.01|false|quinn van buren|2013-03-01 09:11:58.703272|35.40|history 118|388|65642|4294967438|52.78|15.67|true|rachel falkner|2013-03-01 09:11:58.703158|61.13|opthamology 1|315|65713|4294967509|43.80|24.95|false|nick brown|2013-03-01 09:11:58.703287|83.95|mathematics 11|416|65658|4294967433|19.94|8.97|false|jessica nixon|2013-03-01 09:11:58.703117|63.58|joggying 42|457|65669|4294967534|13.45|16.47|true|calvin polk|2013-03-01 09:11:58.703257|59.51|yard duty 119|467|65639|4294967304|57.17|35.89|false|nick nixon|2013-03-01 09:11:58.703088|0.98|history 5|383|65629|4294967302|70.92|32.41|false|rachel young|2013-03-01 09:11:58.703314|1.72|opthamology 108|304|65557|4294967498|26.30|33.01|true|tom nixon|2013-03-01 09:11:58.703189|70.64|opthamology 60|447|65778|4294967546|65.11|14.36|true|yuri robinson|2013-03-01 09:11:58.703284|45.69|joggying 65|406|65613|4294967522|93.10|16.27|false|xavier laertes|2013-03-01 09:11:58.703178|25.19|philosophy 113|482|65739|4294967311|51.17|36.29|true|priscilla steinbeck|2013-03-01 09:11:58.703084|13.07|kindergarten 58|453|65780|4294967484|25.45|1.99|false|alice ichabod|2013-03-01 09:11:58.703307|25.71|nap time 24|320|65759|4294967315|23.99|43.22|false|irene robinson|2013-03-01 09:11:58.703095|24.36|chemistry 112|438|65622|4294967483|62.47|21.21|false|tom laertes|2013-03-01 09:11:58.703257|54.45|nap time 89|382|65708|4294967459|40.10|45.17|false|luke ovid|2013-03-01 09:11:58.703325|59.38|yard duty 63|410|65561|4294967330|86.99|24.01|false|fred underhill|2013-03-01 09:11:58.703288|29.48|religion 103|462|65658|4294967533|48.98|46.63|true|wendy laertes|2013-03-01 09:11:58.703272|85.64|philosophy 97|279|65563|4294967322|79.42|41.65|false|yuri thompson|2013-03-01 09:11:58.703308|43.37|mathematics 122|375|65717|4294967513|99.32|27.37|true|rachel falkner|2013-03-01 09:11:58.703095|65.37|philosophy 25|481|65672|4294967454|98.90|37.58|false|oscar ovid|2013-03-01 09:11:58.703293|73.85|biology 71|409|65667|4294967420|1.98|44.05|true|alice brown|2013-03-01 09:11:58.703117|38.55|religion 86|399|65568|4294967404|26.97|34.10|true|priscilla ichabod|2013-03-01 09:11:58.703283|87.92|yard duty 114|348|65752|4294967368|18.90|42.15|false|irene zipper|2013-03-01 09:11:58.703154|63.92|debate 31|464|65683|4294967364|20.61|48.84|false|irene garcia|2013-03-01 09:11:58.703219|80.62|american history 30|302|65688|4294967477|7.75|5.34|false|quinn polk|2013-03-01 09:11:58.703085|80.36|geology 72|423|65665|4294967353|54.78|15.57|false|fred quirinius|2013-03-01 09:11:58.703219|56.86|philosophy 78|408|65609|4294967534|83.25|24.25|false|quinn falkner|2013-03-01 09:11:58.703074|29.42|quiet hour 35|308|65659|4294967371|89.52|45.35|true|luke carson|2013-03-01 09:11:58.703276|78.07|wind surfing 13|310|65558|4294967399|60.05|38.39|false|priscilla polk|2013-03-01 09:11:58.703194|53.92|mathematics 80|450|65537|4294967548|74.10|8.87|true|ulysses falkner|2013-03-01 09:11:58.703139|56.48|nap time 30|295|65743|4294967359|17.51|44.20|true|bob hernandez|2013-03-01 09:11:58.703242|59.71|quiet hour 25|372|65606|4294967412|99.40|36.98|false|yuri quirinius|2013-03-01 09:11:58.703242|87.18|zync studies -3|454|65733|4294967544|73.83|18.42|false|bob ichabod|2013-03-01 09:11:58.703240|95.56|debate 9|440|65773|4294967362|30.46|44.91|true|xavier falkner|2013-03-01 09:11:58.703098|62.35|religion 105|289|65576|4294967342|76.65|29.47|false|ulysses garcia|2013-03-01 09:11:58.703282|71.95|chemistry 116|263|65757|4294967525|94.04|37.06|false|priscilla hernandez|2013-03-01 09:11:58.703072|13.75|linguistics 124|458|65726|4294967483|7.96|0.29|false|zach laertes|2013-03-01 09:11:58.703281|1.46|study skills -3|507|65671|4294967305|60.28|41.50|false|quinn polk|2013-03-01 09:11:58.703244|77.17|industrial engineering -3|458|65679|4294967331|64.29|43.80|true|irene young|2013-03-01 09:11:58.703084|2.61|american history 17|435|65739|4294967438|44.39|9.29|false|alice thompson|2013-03-01 09:11:58.703241|68.01|undecided 33|390|65564|4294967305|8.20|17.36|false|calvin laertes|2013-03-01 09:11:58.703176|65.07|zync studies 73|474|65789|4294967421|62.00|40.44|true|alice quirinius|2013-03-01 09:11:58.703101|98.80|geology 46|313|65692|4294967310|93.40|34.70|true|fred hernandez|2013-03-01 09:11:58.703196|26.80|geology 50|302|65581|4294967387|2.73|18.54|false|jessica carson|2013-03-01 09:11:58.703282|58.24|study skills 115|311|65651|4294967423|44.94|33.29|true|ethan laertes|2013-03-01 09:11:58.703116|63.49|biology 88|368|65556|4294967428|37.79|47.21|true|tom laertes|2013-03-01 09:11:58.703149|7.26|topology 59|476|65560|4294967341|26.00|21.70|true|irene ovid|2013-03-01 09:11:58.703224|37.32|wind surfing 33|489|65723|4294967491|52.08|36.13|false|quinn robinson|2013-03-01 09:11:58.703174|29.70|chemistry 69|329|65580|4294967527|45.37|25.36|true|irene ichabod|2013-03-01 09:11:58.703267|95.34|joggying 8|342|65542|4294967486|86.51|30.05|true|ulysses johnson|2013-03-01 09:11:58.703164|4.89|kindergarten 47|327|65660|4294967329|53.96|10.07|false|fred white|2013-03-01 09:11:58.703313|48.34|zync studies 77|296|65771|4294967420|94.25|12.67|true|ulysses underhill|2013-03-01 09:11:58.703080|45.67|biology 63|451|65581|4294967493|44.66|40.63|true|alice miller|2013-03-01 09:11:58.703071|97.98|geology 103|303|65605|4294967540|54.00|47.97|true|fred davidson|2013-03-01 09:11:58.703087|68.42|zync studies 68|300|65577|4294967395|8.00|27.76|false|quinn quirinius|2013-03-01 09:11:58.703124|14.35|values clariffication 41|424|65684|4294967396|44.97|44.01|false|calvin polk|2013-03-01 09:11:58.703161|31.72|linguistics 84|448|65649|4294967425|5.81|28.49|true|ulysses ichabod|2013-03-01 09:11:58.703317|96.87|history 30|398|65577|4294967306|71.32|39.24|false|katie zipper|2013-03-01 09:11:58.703310|97.22|wind surfing 70|361|65695|4294967371|6.97|45.29|false|oscar falkner|2013-03-01 09:11:58.703268|79.32|opthamology 92|371|65702|4294967518|29.30|18.48|false|david ellison|2013-03-01 09:11:58.703192|30.01|topology 10|298|65666|4294967460|82.71|16.06|true|irene white|2013-03-01 09:11:58.703198|64.62|quiet hour 109|496|65699|4294967536|36.99|14.91|true|holly hernandez|2013-03-01 09:11:58.703123|66.43|geology 68|383|65597|4294967334|84.64|1.14|true|holly falkner|2013-03-01 09:11:58.703210|96.35|kindergarten 95|433|65738|4294967363|95.88|45.88|false|rachel steinbeck|2013-03-01 09:11:58.703308|34.85|history 37|262|65773|4294967482|26.04|4.86|true|oscar hernandez|2013-03-01 09:11:58.703285|92.63|linguistics 24|421|65676|4294967355|23.99|14.11|true|ulysses ovid|2013-03-01 09:11:58.703281|19.16|forestry 91|485|65607|4294967315|55.90|17.62|false|zach nixon|2013-03-01 09:11:58.703305|83.23|joggying 67|387|65790|4294967318|93.14|31.43|false|irene king|2013-03-01 09:11:58.703188|6.25|industrial engineering 82|262|65571|4294967465|56.70|30.18|true|irene van buren|2013-03-01 09:11:58.703167|3.00|study skills 98|505|65582|4294967365|17.40|40.51|false|sarah polk|2013-03-01 09:11:58.703121|56.65|history 22|268|65612|4294967462|9.69|4.64|false|xavier ichabod|2013-03-01 09:11:58.703304|3.86|linguistics 10|332|65685|4294967332|76.12|20.13|true|priscilla laertes|2013-03-01 09:11:58.703170|82.71|opthamology 36|317|65641|4294967471|56.22|36.78|true|tom johnson|2013-03-01 09:11:58.703296|53.38|biology 60|501|65555|4294967313|13.57|11.68|true|yuri davidson|2013-03-01 09:11:58.703183|10.42|religion 123|267|65560|4294967438|40.69|11.41|true|ethan allen|2013-03-01 09:11:58.703086|91.03|undecided -2|482|65558|4294967487|36.92|49.78|true|nick johnson|2013-03-01 09:11:58.703204|39.91|industrial engineering 59|270|65726|4294967372|48.94|37.15|false|oscar polk|2013-03-01 09:11:58.703221|12.67|quiet hour 119|385|65595|4294967373|36.66|15.82|true|jessica nixon|2013-03-01 09:11:58.703127|5.26|zync studies 122|306|65751|4294967471|56.79|48.37|true|bob hernandez|2013-03-01 09:11:58.703186|50.61|kindergarten 64|402|65777|4294967481|77.49|13.11|false|nick carson|2013-03-01 09:11:58.703264|66.64|study skills 48|465|65758|4294967485|75.39|30.96|false|ethan allen|2013-03-01 09:11:58.703076|10.00|joggying 117|458|65603|4294967342|53.32|32.59|true|ethan garcia|2013-03-01 09:11:58.703204|47.35|yard duty 23|283|65557|4294967415|24.61|14.57|false|fred white|2013-03-01 09:11:58.703082|12.44|chemistry 56|507|65538|4294967507|67.82|42.13|false|alice king|2013-03-01 09:11:58.703297|54.64|american history 96|436|65737|4294967528|81.66|27.09|false|tom zipper|2013-03-01 09:11:58.703199|85.16|debate 88|292|65578|4294967546|91.57|37.42|false|nick zipper|2013-03-01 09:11:58.703294|96.08|religion 73|481|65717|4294967391|40.07|27.66|true|yuri xylophone|2013-03-01 09:11:58.703120|18.21|history 80|280|65620|4294967482|58.09|40.39|false|fred polk|2013-03-01 09:11:58.703136|23.61|xylophone band 96|464|65659|4294967493|74.22|21.71|true|jessica ichabod|2013-03-01 09:11:58.703226|92.72|undecided 103|485|65707|4294967436|94.57|21.16|true|zach van buren|2013-03-01 09:11:58.703313|3.93|study skills 31|410|65566|4294967518|36.11|16.72|true|nick ellison|2013-03-01 09:11:58.703305|61.53|biology -3|270|65702|4294967512|38.05|1.07|true|david carson|2013-03-01 09:11:58.703136|28.07|philosophy 3|404|65709|4294967473|14.86|48.87|true|mike quirinius|2013-03-01 09:11:58.703099|37.99|xylophone band 124|473|65644|4294967314|65.16|19.33|false|oscar white|2013-03-01 09:11:58.703194|33.17|debate 103|321|65572|4294967353|64.79|0.22|false|david robinson|2013-03-01 09:11:58.703187|20.31|linguistics 41|395|65686|4294967428|61.99|11.61|false|sarah steinbeck|2013-03-01 09:11:58.703278|17.45|biology -3|469|65752|4294967350|55.41|32.11|true|oscar johnson|2013-03-01 09:11:58.703110|47.32|philosophy 98|336|65641|4294967519|82.11|7.91|true|tom davidson|2013-03-01 09:11:58.703320|83.43|debate 54|422|65655|4294967551|15.74|34.11|true|bob garcia|2013-03-01 09:11:58.703086|46.93|yard duty 70|462|65671|4294967385|82.68|7.94|false|fred white|2013-03-01 09:11:58.703167|45.89|joggying 62|325|65751|4294967342|36.71|28.42|true|priscilla garcia|2013-03-01 09:11:58.703239|0.56|mathematics 56|504|65635|4294967318|93.88|34.87|true|holly polk|2013-03-01 09:11:58.703227|89.14|american history 50|275|65697|4294967322|58.10|27.56|false|priscilla johnson|2013-03-01 09:11:58.703096|6.19|biology 114|428|65680|4294967498|62.68|3.90|true|yuri nixon|2013-03-01 09:11:58.703086|53.28|xylophone band 100|277|65739|4294967382|1.61|18.22|true|wendy garcia|2013-03-01 09:11:58.703137|78.35|industrial engineering 7|494|65601|4294967403|20.76|19.41|false|david underhill|2013-03-01 09:11:58.703164|70.81|topology 79|448|65744|4294967479|18.18|36.26|true|david xylophone|2013-03-01 09:11:58.703310|76.40|joggying 19|289|65562|4294967344|56.25|33.81|true|sarah van buren|2013-03-01 09:11:58.703301|64.05|forestry 10|508|65589|4294967473|96.49|7.56|false|priscilla brown|2013-03-01 09:11:58.703134|2.08|education 89|451|65686|4294967396|21.20|13.22|true|oscar king|2013-03-01 09:11:58.703127|49.12|undecided 45|323|65540|4294967436|29.79|5.69|false|tom falkner|2013-03-01 09:11:58.703102|53.85|nap time 34|319|65780|4294967523|80.40|9.05|true|sarah falkner|2013-03-01 09:11:58.703179|75.06|yard duty 30|510|65632|4294967373|60.94|21.31|true|gabriella steinbeck|2013-03-01 09:11:58.703146|69.16|undecided 72|350|65742|4294967491|3.33|30.48|false|katie johnson|2013-03-01 09:11:58.703315|55.83|topology 96|402|65620|4294967320|19.38|49.45|false|oscar steinbeck|2013-03-01 09:11:58.703303|25.84|yard duty 95|405|65536|4294967338|18.26|1.46|false|sarah thompson|2013-03-01 09:11:58.703073|29.27|education 80|396|65675|4294967379|30.21|28.41|false|rachel white|2013-03-01 09:11:58.703316|11.37|topology 5|507|65715|4294967297|87.39|16.09|true|sarah xylophone|2013-03-01 09:11:58.703321|0.46|nap time 52|322|65635|4294967296|13.25|10.02|false|wendy falkner|2013-03-01 09:11:58.703094|2.51|industrial engineering 64|345|65744|4294967316|23.26|29.25|true|sarah brown|2013-03-01 09:11:58.703245|96.45|kindergarten 97|502|65654|4294967405|0.09|3.10|false|victor robinson|2013-03-01 09:11:58.703141|29.03|religion 25|424|65599|4294967303|49.92|33.86|true|calvin miller|2013-03-01 09:11:58.703095|76.80|study skills 115|298|65599|4294967457|78.69|11.89|false|luke steinbeck|2013-03-01 09:11:58.703245|22.81|geology 49|496|65722|4294967407|17.46|33.62|false|ethan underhill|2013-03-01 09:11:58.703158|7.67|forestry 77|315|65592|4294967532|28.72|38.15|false|nick robinson|2013-03-01 09:11:58.703296|78.69|debate 33|258|65780|4294967448|5.78|19.07|true|calvin davidson|2013-03-01 09:11:58.703133|18.12|study skills 98|390|65592|4294967397|36.40|29.61|false|sarah young|2013-03-01 09:11:58.703314|74.60|wind surfing 41|415|65618|4294967426|2.23|46.43|true|nick van buren|2013-03-01 09:11:58.703225|14.78|yard duty 62|427|65671|4294967359|75.01|38.93|false|bob ovid|2013-03-01 09:11:58.703195|17.17|values clariffication -2|294|65588|4294967301|8.51|2.16|false|zach zipper|2013-03-01 09:11:58.703208|35.15|debate 94|309|65653|4294967447|6.14|5.65|false|yuri van buren|2013-03-01 09:11:58.703279|94.47|study skills 120|377|65615|4294967364|24.99|12.26|true|oscar nixon|2013-03-01 09:11:58.703250|71.62|industrial engineering 3|500|65756|4294967445|98.38|39.43|true|luke nixon|2013-03-01 09:11:58.703243|29.49|yard duty -1|505|65611|4294967338|75.26|22.98|false|mike allen|2013-03-01 09:11:58.703123|95.80|linguistics 124|466|65612|4294967456|72.76|15.57|false|calvin polk|2013-03-01 09:11:58.703235|37.15|biology 1|490|65591|4294967329|69.89|40.29|false|luke laertes|2013-03-01 09:11:58.703104|58.27|quiet hour 70|385|65553|4294967506|69.14|44.05|false|ethan xylophone|2013-03-01 09:11:58.703150|93.69|chemistry 68|330|65573|4294967506|66.87|17.31|true|jessica hernandez|2013-03-01 09:11:58.703124|30.57|zync studies 82|421|65699|4294967550|84.77|40.40|false|gabriella white|2013-03-01 09:11:58.703292|29.99|history 9|346|65646|4294967449|66.32|24.07|false|jessica xylophone|2013-03-01 09:11:58.703084|94.86|undecided 116|336|65638|4294967327|64.45|11.24|true|jessica falkner|2013-03-01 09:11:58.703087|60.05|study skills 19|376|65770|4294967536|79.12|20.11|false|victor carson|2013-03-01 09:11:58.703243|72.69|industrial engineering 27|433|65767|4294967395|22.53|18.81|false|bob polk|2013-03-01 09:11:58.703097|52.68|linguistics 31|468|65654|4294967361|33.08|29.95|false|bob young|2013-03-01 09:11:58.703210|16.48|philosophy 84|411|65564|4294967493|49.25|7.84|true|oscar nixon|2013-03-01 09:11:58.703274|47.54|american history 37|409|65769|4294967384|25.89|42.27|false|katie underhill|2013-03-01 09:11:58.703172|66.93|zync studies 10|356|65628|4294967475|98.07|13.86|false|david carson|2013-03-01 09:11:58.703222|7.37|nap time 105|437|65664|4294967535|2.05|17.01|true|holly laertes|2013-03-01 09:11:58.703144|5.69|industrial engineering 117|508|65788|4294967319|66.86|25.25|false|ulysses davidson|2013-03-01 09:11:58.703283|85.22|industrial engineering 108|322|65697|4294967529|20.24|40.23|true|mike carson|2013-03-01 09:11:58.703083|6.04|philosophy 80|426|65735|4294967533|73.85|41.99|false|quinn hernandez|2013-03-01 09:11:58.703098|69.55|mathematics 49|434|65692|4294967336|89.33|14.24|true|yuri underhill|2013-03-01 09:11:58.703127|3.91|quiet hour 74|501|65657|4294967451|88.85|11.09|true|bob king|2013-03-01 09:11:58.703175|51.36|quiet hour 8|380|65734|4294967369|84.11|10.24|false|victor underhill|2013-03-01 09:11:58.703291|78.90|opthamology 89|364|65735|4294967334|12.41|24.02|false|nick nixon|2013-03-01 09:11:58.703272|34.80|debate 53|479|65579|4294967303|7.50|43.05|false|rachel ellison|2013-03-01 09:11:58.703148|48.50|yard duty 67|493|65626|4294967489|98.74|32.74|false|katie thompson|2013-03-01 09:11:58.703263|87.95|geology 56|390|65676|4294967456|42.59|1.64|true|wendy king|2013-03-01 09:11:58.703307|39.31|joggying 13|431|65624|4294967330|94.05|30.76|false|quinn ichabod|2013-03-01 09:11:58.703180|1.72|biology 85|366|65627|4294967356|37.14|35.57|true|alice king|2013-03-01 09:11:58.703170|6.78|yard duty -2|286|65549|4294967493|9.20|1.23|true|ulysses king|2013-03-01 09:11:58.703218|93.35|study skills 51|344|65698|4294967309|83.66|6.12|false|zach ellison|2013-03-01 09:11:58.703158|29.28|yard duty 89|489|65610|4294967353|64.70|8.13|true|katie polk|2013-03-01 09:11:58.703120|56.34|education 95|327|65747|4294967522|1.16|12.00|true|bob van buren|2013-03-01 09:11:58.703284|3.45|opthamology 50|508|65541|4294967451|37.38|46.94|true|quinn steinbeck|2013-03-01 09:11:58.703081|20.90|forestry 6|301|65693|4294967454|89.07|41.96|true|alice ichabod|2013-03-01 09:11:58.703297|16.13|religion 7|322|65719|4294967434|1.02|29.24|false|quinn carson|2013-03-01 09:11:58.703293|47.99|forestry 99|469|65751|4294967356|10.10|42.47|false|wendy young|2013-03-01 09:11:58.703180|63.14|opthamology 18|269|65751|4294967544|87.84|0.60|true|mike steinbeck|2013-03-01 09:11:58.703167|36.04|religion 22|361|65729|4294967328|67.51|15.52|false|zach ovid|2013-03-01 09:11:58.703317|26.96|quiet hour 114|455|65723|4294967481|4.94|33.44|false|alice van buren|2013-03-01 09:11:58.703074|72.22|philosophy -3|384|65676|4294967453|71.97|31.52|false|alice davidson|2013-03-01 09:11:58.703226|14.28|xylophone band 37|334|65775|4294967518|17.88|45.96|false|zach ellison|2013-03-01 09:11:58.703260|9.92|nap time 28|427|65648|4294967309|45.65|3.90|true|bob robinson|2013-03-01 09:11:58.703308|89.89|chemistry 86|469|65780|4294967466|64.61|24.76|true|david steinbeck|2013-03-01 09:11:58.703241|0.68|linguistics 61|455|65567|4294967315|84.80|25.83|false|alice robinson|2013-03-01 09:11:58.703127|26.03|zync studies -3|387|65550|4294967355|84.75|22.75|true|holly thompson|2013-03-01 09:11:58.703073|52.01|biology 14|492|65690|4294967388|98.07|15.98|true|david miller|2013-03-01 09:11:58.703096|15.69|forestry 8|318|65687|4294967551|44.02|14.70|false|quinn thompson|2013-03-01 09:11:58.703205|23.43|joggying 117|502|65789|4294967441|55.39|8.22|false|tom allen|2013-03-01 09:11:58.703129|74.48|xylophone band 20|285|65783|4294967424|99.34|21.19|false|alice thompson|2013-03-01 09:11:58.703223|9.55|opthamology 4|478|65538|4294967312|21.90|0.85|false|sarah thompson|2013-03-01 09:11:58.703089|79.07|xylophone bandThe conf is:execution: planner: blink type: batchAfter excuse the sql above, there will be the exception : &amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression&amp;#91;GeneratedExpression(((int) 0),false,,INT NOT NULL,Some(0))&amp;#93; type is &amp;#91;INT NOT NULL&amp;#93;, result type is &amp;#91;SMALLINT&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15567" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for INSERT statements for Flink SQL</summary>
      <description>We missed to add documentation for INSERT statements which should be added under "SQL" page.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15575" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Filesystem Shades Wrong Package "httpcomponents"</summary>
      <description>Instead of shading "org.apache.httpcomponents" (this package does not exist) the azure filesystem should shade "org.apache.http". This e.g. causes problems when the azure filesystem and elasticsearch6 connector are both on the classpath.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15583" opendate="2020-1-14 00:00:00" fixdate="2020-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala walkthrough archetype does not compile on Java 11</summary>
      <description>While compiling a projected created with walkthrough archetype, the following error occurs02:55:58.048 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider02:55:58.048 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15584" opendate="2020-1-14 00:00:00" fixdate="2020-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15589" opendate="2020-1-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove beta tag from catalog and hive doc</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="15591" opendate="2020-1-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make up for feature parity by supporting CREATE TEMPORARY TABLE/VIEW in DDL</summary>
      <description>make up for feature parity by supporting CREATE TEMPORARY TABLE/VIEW in DDL as corresponding API to that in Table API. Table API already support such operations explicitly in 1.10</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogManagerTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.DropTableOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateTableOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="15592" opendate="2020-1-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming sql throw hive exception when it doesn&amp;#39;t use any hive table</summary>
      <description>I use the following streaming sql to query a kafka table whose metadata is store in hive metastore via HiveCatalog. But it will throw hive related exception which is very confusing.SQLSELECT *FROM ( SELECT *, ROW_NUMBER() OVER( ORDER BY event_ts) AS rownum FROM source_kafka)WHERE rownum &lt;= 10ExceptionCaused by: org.apache.flink.table.api.ValidationException: SQL validation failed. java.lang.reflect.InvocationTargetException at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:130) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:105) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:127) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:464) at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:103) ... 13 moreCaused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:77) at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.lambda$createReturnTypeInference$0(HiveAggSqlFunction.java:82) at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:470) at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:437) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:303) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:219) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlCallBinding.getOperandType(SqlCallBinding.java:237) at org.apache.calcite.sql.type.OrdinalReturnTypeInference.inferReturnType(OrdinalReturnTypeInference.java:40) at org.apache.calcite.sql.type.SqlTypeTransformCascade.inferReturnType(SqlTypeTransformCascade.java:54) at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:470) at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:437) at org.apache.calcite.sql.SqlOverOperator.deriveType(SqlOverOperator.java:86) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlAsOperator.deriveType(SqlAsOperator.java:133) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:479) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4105) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3389) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1008) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:968) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3122) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3104) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3376) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1008) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:968) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:943) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:650) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:126) ... 18 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeGetResultType(HiveFunctionUtils.java:73) ... 63 moreCaused by: org.apache.flink.table.functions.hive.FlinkHiveUDFException: Failed to get Hive result type from org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber at org.apache.flink.table.functions.hive.HiveGenericUDAF.getHiveResultType(HiveGenericUDAF.java:202) ... 68 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Only COMPLETE mode supported for row_number function at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber$GenericUDAFAbstractRowNumberEvaluator.init(GenericUDAFRowNumber.java:100) at org.apache.flink.table.functions.hive.HiveGenericUDAF.init(HiveGenericUDAF.java:93) at org.apache.flink.table.functions.hive.HiveGenericUDAF.getHiveResultType(HiveGenericUDAF.java:196) ... 68 moreERROR Took 2 sec. Last updated by anonymous at January 15 2020, 11:12:16 AM.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="15597" opendate="2020-1-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Relax sanity check of JVM memory overhead to be within its min/max</summary>
      <description>When the explicitly configured process and Flink memory sizes are verified with the JVM meta space and overhead, JVM overhead does not have to be the exact fraction.It can be just within its min/max range, similar to how it is now for network/shuffle memory check after FLINK-15300.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15606" opendate="2020-1-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate enable default background cleanup of state with TTL</summary>
      <description>Follow-up for FLINK-14898.Having an API method to enable TTL background cleanup does not make sense too much if it is already enabled by default so we can deprecate this method.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="15623" opendate="2020-1-16 00:00:00" fixdate="2020-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Buildling flink-python with maven profile docs-and-source fails</summary>
      <description>DescriptionBuilding flink-python with maven profile docs-and-source fails due to checkstyle violations. How to reproduceRunningmvn clean install -pl flink-python -Pdocs-and-source -DskipTests -DretryFailedDeploymentCount=10should fail with the following error[...][ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8343] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8344] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8345] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8346] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8347] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8348] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8349] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[ERROR] generated-sources/org/apache/flink/fnexecution/v1/FlinkFnApi.java:[8350] (regexp) RegexpSinglelineJava: Line has leading space characters; indentation should be performed with tabs only.[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 18.046 s[INFO] Finished at: 2020-01-16T16:44:01+00:00[INFO] Final Memory: 158M/2826M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project flink-python_2.11: You have 7603 Checkstyle violations. -&gt; [Help 1][ERROR]</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15631" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot use generic types as the result of an AggregateFunction in Blink planner</summary>
      <description>It is not possible to use a GenericTypeInfo for a result type of an AggregateFunction in a retract mode with state cleaning disabled. @Test def testGenericTypes(): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build() val tEnv = StreamTableEnvironment.create(env, setting) val t = env.fromElements(1, 2, 3).toTable(tEnv, 'a) val results = t .select(new GenericAggregateFunction()('a)) .toRetractStream[Row] val sink = new TestingRetractSink results.addSink(sink).setParallelism(1) env.execute() }class RandomClass(var i: Int)class GenericAggregateFunction extends AggregateFunction[java.lang.Integer, RandomClass] { override def getValue(accumulator: RandomClass): java.lang.Integer = accumulator.i override def createAccumulator(): RandomClass = new RandomClass(0) override def getResultType: TypeInformation[java.lang.Integer] = new GenericTypeInfo[Integer](classOf[Integer]) override def getAccumulatorType: TypeInformation[RandomClass] = new GenericTypeInfo[RandomClass]( classOf[RandomClass]) def accumulate(acc: RandomClass, value: Int): Unit = { acc.i = value } def retract(acc: RandomClass, value: Int): Unit = { acc.i = value } def resetAccumulator(acc: RandomClass): Unit = { acc.i = 0 }}The code above fails with:Caused by: java.lang.UnsupportedOperationException: BinaryGeneric cannot be compared at org.apache.flink.table.dataformat.BinaryGeneric.equals(BinaryGeneric.java:77) at GroupAggValueEqualiser$17.equalsWithoutHeader(Unknown Source) at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:177) at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:170) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)This is related to FLINK-13702</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.LazyBinaryFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15633" opendate="2020-1-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs cannot be built on Java 11</summary>
      <description>The javadoc-plugin fails when run Java 11. This isn't a big issue since we do our releases on JDK 8 anyway, but we should still fix it so users can reproduce releases on JDK 11.java.lang.ExceptionInInitializerError at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.&lt;clinit&gt;(AbstractJavadocMojo.java:190) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488) at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:86) at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105) at com.google.inject.internal.ConstructorInjector.access$000(ConstructorInjector.java:32) at com.google.inject.internal.ConstructorInjector$1.call(ConstructorInjector.java:89) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:115) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:133) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:87) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267) at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103) at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1051) at org.eclipse.sisu.space.AbstractDeferredClass.get(AbstractDeferredClass.java:48) at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:81) at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:53) at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:65) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:115) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:133) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68) at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:63) at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45) at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092) at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012) at org.eclipse.sisu.inject.Guice4$1.get(Guice4.java:162) at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:81) at org.eclipse.sisu.plexus.LazyPlexusBean.getValue(LazyPlexusBean.java:51) at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:263) at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:255) at org.apache.maven.plugin.internal.DefaultMavenPluginManager.getConfiguredMojo(DefaultMavenPluginManager.java:519) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:121) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345) at org.apache.maven.cli.MavenCli.main(MavenCli.java:191) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end 3, length 1 at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3116) at java.base/java.lang.String.substring(String.java:1885) at org.apache.commons.lang.SystemUtils.getJavaVersionAsFloat(SystemUtils.java:1133) at org.apache.commons.lang.SystemUtils.&lt;clinit&gt;(SystemUtils.java:818) ... 58 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15636" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF for flink planner under batch mode</summary>
      <description>Currently, Python UDF has been supported under flink planner(only stream) and blink planner(stream&amp;batch). This jira dedicates to add Python UDF support for flink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.BatchOptimizer.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="15639" opendate="2020-1-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to set toleration for jobmanager and taskmanger</summary>
      <description>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. Navigate to Kubernetes doc for more information.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15646" opendate="2020-1-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable K8s context support</summary>
      <description>One would be forced to first kubectl config use-context &lt;context&gt; to switch to the desired context if working with multiple K8S clusters or having multiple K8S "users" for interacting with the specified cluster,  so it is an important improvement to add an option(kubernetes.context) for configuring arbitrary contexts when deploying a Flink cluster. If that option is not specified, then the current context in the config file would be used.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubeClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15647" opendate="2020-1-19 00:00:00" fixdate="2020-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user-specified annotations for JM/TM pods</summary>
      <description>One can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. Kubernetes Annotation</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15648" opendate="2020-1-19 00:00:00" fixdate="2020-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to configure limit for CPU and memory requirement</summary>
      <description>Currently, the native Kubernetes mode could support same request and limit for the cpu and memory resources. It will be a good improvement to allow different resource requests and limits. And it is very useful especially for the CPU resource since it heavily depends on the upstream workloads.  For the implementation, we could have the four config options of limit-factor, which could make the JM/TM benefit from burst resources.kubernetes.jobmanager.cpu.limit-factor kubernetes.jobmanager.memory.limit-factorkubernetes.taskmanager.cpu.limit-factorkubernetes.taskmanager.memory.limit-factor </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesTaskManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.entrypoint.KubernetesWorkerResourceSpecFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="1565" opendate="2015-2-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document object reuse behavior</summary>
      <description>The documentation needs to be extended and describe the object reuse behavior of Flink and its implications for how to implement functions.The documentation must at least cover the default reuse mode: new objects through iterators and in reduce functions chaining behavior (objects are passed on to the next function which might modify it)Optionally, the documentation could describe the object reuse switch introduced by FLINK-1137.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="15657" opendate="2020-1-19 00:00:00" fixdate="2020-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the python table api doc link in Python API tutorial</summary>
      <description>Fix the python table api doc link</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="15658" opendate="2020-1-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The same sql run in a streaming environment producing a Exception, but a batch env can run normally.</summary>
      <description>summary:The same sql can run in a batch environment normally, but in a streaming environment there will be a exception like this:&amp;#91;ERROR&amp;#93; Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: &amp;#91;f1&amp;#93;The sql is:CREATE TABLE `tenk1` ( unique1 int, unique2 int, two int, four int, ten int, twenty int, hundred int, thousand int, twothousand int, fivethous int, tenthous int, odd int, even int, stringu1 varchar, stringu2 varchar, string4 varchar) WITH ( 'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/tenk1.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv');CREATE TABLE `int4_tbl` ( f1 INT) WITH ( 'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/int4_tbl.csv', 'format.empty-column-as-null'='true', 'format.field-delimiter'='|', 'connector.type'='filesystem', 'format.derive-schema'='true', 'format.type'='csv');select a.f1, b.f1, t.thousand, t.tenthous from tenk1 t, (select sum(f1)+1 as f1 from int4_tbl i4a) a, (select sum(f1) as f1 from int4_tbl i4b) bwhere b.f1 = t.thousand and a.f1 = b.f1 and (a.f1+b.f1+999) = t.tenthous;</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.KeySelectorUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15660" opendate="2020-1-19 00:00:00" fixdate="2020-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redundant AllocationID verification for allocateSlot in TaskSlotTable</summary>
      <description> In function TaskSlotTable::allocateSlot, first we will check whether allocationId is exist, when exist we will refused this allocation, this was introduced by FLINK-14589 . But in FLINK-14189, when allocationId exist, we think this is valid, which is contradictory with the first check. The code is following:https://github.com/apache/flink/blob/310452e800355f0dcc4bc9dd26e9cecba263f3d6/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/slot/TaskSlotTable.java#L261</description>
      <version>1.10.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15684" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "taskmanager.memory.flink.size" to the common options</summary>
      <description>Add "taskmanager.memory.flink.size" should be part of the common options.The options "taskmanager.memory.flink.size" and "taskmanager.memory.process.size" should mention each other in their description.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs..includes.generated.common.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="15686" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SELECT &amp;#39;ABC&amp;#39;; does not work in sql-client</summary>
      <description>A query like SELECT 'abc'; fails in sql-client with blink planner enabled with an error:org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.DynamicResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="15687" opendate="2020-1-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential test instabilities due to concurrent access to TaskSlotTable.</summary>
      <description>Working on FLINK-14742 revealed that the problem with that test instability was the modification of the taskSlotTable of the TaskManager under test from multiple threads, namely the test thread and the main thread of the rpcEnpoint. This data-structure is not thread-safe and this should not happen.This anti-pattern seems to be repeated in multiple tests like most of the tests in the TaskExecutorSubmissionTest (look for the call to the TaskSlotTable.allocateSlot()). There we seem to call taskSlotTable.allocateSlot() and then tmGateway.submitTask() which is essentially accessing the slot table from within the main rpc-endpoint thread.This JIRA is just to investigate if this is also a problem in those tests or not.cc Till Rohrmann, Chesnay Schepler , &amp;#91;~yangwang166&amp;#93;</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15688" opendate="2020-1-20 00:00:00" fixdate="2020-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add N-Ary Stream Operator in Flink</summary>
      <description>As described here: https://cwiki.apache.org/confluence/display/FLINK/FLIP-92%3A+Add+N-Ary+Stream+Operator+in+FlinkThe plan is to provide an N-Ary Stream Operator with more or less the following interface:abstract class StreamOperatorNG&lt;OUT&gt; { Collection&lt;Input&lt;?&gt;&gt; getInputs() // as well as all the other methods of existing StreamOperator // and AbstractStreamOperator: // setup()/open()/close()/snapshot()/restore() ...} abstract class Input&lt;T&gt; { // for determining whether two inputs are equal private final UUID uuid = UUID.randomUUID(); public abstract void processElement(StreamRecord&lt;T&gt; element) public abstract void processWatermark(Watermark watermark)}</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="15689" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass execution configuration from BatchTableEnvironment to ExecutionEnvironment</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15690" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In environments, call configure() in constructors with passed Configuration</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.pipeline.configuration.html</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentConfigurationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.completeness.py</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.cache.DistributedCache.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15692" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable limiting RocksDB memory consumption by default</summary>
      <description>Enable limiting RocksDB memory consumption by default. This means to change the default value of state.backend.rocksdb.memory.managed to true. This will change Flink's default behaviour for many users but I believe the benefits of a much more predictable memory behaviour of Flink will outweigh the downside of having fewer memory available.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.typeserializerupgrade.PojoSerializerUpgradeTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunctionTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.StateBackendTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendMigrationTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedOperatorsMetricTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestRuntimeContext.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15694" opendate="2020-1-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HDFS Section of Configuration Page</summary>
      <description>The section "HDFS" is outdated (and flagged as such).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="15701" opendate="2020-1-21 00:00:00" fixdate="2020-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allows transfer.sh to retry when fails to uploads logs</summary>
      <description>Occasionally we met with the error COMPRESSING build artifacts.build_infocontainer.logmvn-1.logmvn-2.logmvn.outUploading to transfer.shCould not save metadatawhen the travis uploading logs to transfer.sh. This makes many fail tests cannot be analyzed due to lack of detail logs. To amend this situation, we should allow uploading to retry when the uploading fails. </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15702" opendate="2020-1-21 00:00:00" fixdate="2020-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sqlClient classloader aligned with other components</summary>
      <description>Currently, Flink sqlClient still use hardcoded `parentFirst` classloader to load user specified jars and libraries, this is easily causing classes conflicts. In FLINK-13749 , we already make the classloader consistent in both client and remote components.So I think we should do the same for sqlClient.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15736" opendate="2020-1-23 00:00:00" fixdate="2020-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Java 17 (LTS)</summary>
      <description>Long-term issue for preparing Flink for Java 17.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory..index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15754" opendate="2020-1-24 00:00:00" fixdate="2020-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Config Options table.exec.resource.*memory from Documentation</summary>
      <description>Remove config options: table.exec.resource.external-buffer-memory table.exec.resource.hash-agg.memory table.exec.resource.hash-join.memory table.exec.resource.sort.memoryfrom documentation. Users are not expected to change these weights.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs..includes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15782" opendate="2020-1-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework JDBC sinks</summary>
      <description>refactor current code for re-use in a new exactly once JDBC sink expose existing sinks at the datastream level (for users, who don’t need exactly-once OR for whom upsert is a better option)existing API shouldn’t change (mostly table API)</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.executor.ParameterSetter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTypeUtilTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCAppendTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTypeUtil.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCSinkFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCAppendTableSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.AbstractJDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCAppendTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.AbstractJdbcOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcUpsertSinkFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.JDBCWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JdbcUpsertOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcBatchingOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.TableJdbcUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JdbcTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.executor.JdbcBatchStatementExecutor.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcBatchOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcDmlOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCLookupFunctionITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.executor.InsertOrUpdateJdbcExecutor.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.executor.KeyedBatchStatementExecutor.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.executor.SimpleBatchStatementExecutor.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUtils.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.TableJdbcOutputFormat.java</file>
      <file type="M">docs.dev.connectors.jdbc.md</file>
      <file type="M">docs.dev.connectors.jdbc.zh.md</file>
      <file type="M">docs.redirects.jdbc.md</file>
      <file type="M">docs.redirects.jdbc.zh.md</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.GenericJdbcSinkFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcConnectionOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcExecutionOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcStatementBuilder.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTableSource.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.SimpleJdbcConnectionProvider.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCAppenOnlyWriterTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JdbcE2eTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestCheckpoint.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcSinkFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcTypedQueryOptions.java</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.dev.connectors.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="15802" opendate="2020-1-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the new type inference for table functions</summary>
      <description>This will allow to use table functions with the new type inference. It requires different changes through the stack. Support for structured type is not included here. Which means that only a ROW type can be used in the first version.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.collector.WrappingCollector.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TypedFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.FlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DeferredTypeFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.ScalarFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15804" opendate="2020-1-29 00:00:00" fixdate="2020-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the new type inference in Scala Table API scalar functions</summary>
      <description>Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used "inline". We should support them as well.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15806" opendate="2020-1-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update recommended way to shut down detached Yarn session cluster</summary>
      <description>When starting a Yarn session cluster in detached mode, then it is printed that one should shut down the cluster via yarn application -kill. The problem of this approach is, however, that temporary files won't be cleaned up which remain on HDFS. Hence, I think a better way would be to reconnect via the yarn-session.sh tool and then to stop the cluster gracefully. This won't leave orphaned files behind.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="15807" opendate="2020-1-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Java 11 support</summary>
      <description>At several places we list Java 8 as the only supported Java version; we should Java 11 to the list.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="15813" opendate="2020-1-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default value of jobmanager.execution.failover-strategy to region</summary>
      <description>We should set the default value of jobmanager.execution.failover-strategy to region. This might require to adapt existing tests to make them pass.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailoverStrategyFactoryLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.FailoverStrategyLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs..includes.generated.expert.fault.tolerance.section.html</file>
      <file type="M">docs..includes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="15817" opendate="2020-1-31 00:00:00" fixdate="2020-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes Resource leak while deployment exception happens</summary>
      <description>When we deploy a new session cluster on Kubernetes cluster, usually there are four steps to create the Kubernetes components, and the creation order is as below: internal Service -&gt; rest Service -&gt; ConfigMap -&gt; JobManager Deployment.After the internal Service is created, any Exceptions that fail the cluster deployment progress would cause Kubernetes Resource leak, for example:  If failed to create rest Service due to service name constraint(FLINK-15816), the internal Service would not be cleaned up when the deploy progress terminates. If failed to create JobManager Deployment(a case is that jobmanager.heap.size is too small such as 512M, which is less than the default configuration value of 'containerized.heap-cutoff-min'), the internal Service, the rest Service, and the ConfigMap all leaks.This ticket proposes to do some clean-ups(cleans the residual Services and ConfigMap) if the cluster deployment progress terminates accidentally on the client-side.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15824" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Generalize CommonOption to support other option groups as well</summary>
      <description>The CommOption annotation that supports marking an option for the "common" group can be generalized to support multiple groups, identified by a name (String).The config docs generator would then write the option into those multiple generated section files that can be included in the configuration page.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.data.TestCommonOptions.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
    </fixedFiles>
  </bug>
  <bug id="15838" opendate="2020-1-31 00:00:00" fixdate="2020-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dangling CountDownLatch.await(timeout)</summary>
      <description>There are 16 occurrences in the codebase (all in test code) when the result of CountDownLatch.await(timeout, TimeUnit) is not checked. It's like not checking the result of File.delete(). The common fix is to wrap CDL.await() call into assertTrue().All 16 places could be found using the following structural search in IntelliJ:$x$.await($y$, $z$);With "CountDownLatch" type constraint on the $x$ variable.</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.AbstractByteBufTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.AbstractCloseableRegistryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15849" opendate="2020-2-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SQL-CLIENT document from type to data-type</summary>
      <description>There are documentation of type instead of data-type in sql-client.</description>
      <version>None</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="15852" opendate="2020-2-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job is submitted to the wrong session cluster</summary>
      <description>Steps to reproduce the problem: Deploy a YARN session cluster by command ./bin/yarn-session.sh -d Deploy a Kubernetes session cluster by command ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=test ... Try to submit a Job to the Kubernetes session cluster by command ./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=test examples/streaming/WordCount.jarIt's expected that the Job will be submitted to the Kubernetes session cluster whose cluster-id is test, however, the job was submitted to the YARN session cluster. </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="1586" opendate="2015-2-19 00:00:00" fixdate="2015-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for iteration visualization for Streaming programs</summary>
      <description>The plan visualizer currently does not support streaming programs containing iterations. There is no visualization at all due to an exception thrown in the visualizer script.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="15864" opendate="2020-2-3 00:00:00" fixdate="2020-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson-databind dependency to 2.10.1 for security reasons</summary>
      <description>The module flink-kubernetes defines an explicit dependency on jackson-databind:2.9.8. This is problematic since this jackson version contains security vulnerabilities. See FLINK-14104 for more information.If possible, I would suggest to remove the explicit version tag and to rely on the parent's dependency management.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15868" opendate="2020-2-3 00:00:00" fixdate="2020-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer fails due to jackson-dataformat-cbor conflict in 1.10 RC1</summary>
      <description>There appears to be an issue with incompatible dependencies being shaded in the connector. This only happens when running against the actual Kinesis service (i.e. when CBOR isn't disabled).</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="15913" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python Table Function Runner And Operator</summary>
      <description>This Jira will include two PRs: Add Python Table Function Runner and Operator in legacy planner Add Python Table Function Runner and Operator in blink planner</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonTableFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15914" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Miss the barrier alignment metric for the case of two inputs</summary>
      <description>When the StreamTwoInputSelectableProcessor was introduced before, it was missing to add the barrier alignment metric in the constructor. But it does not cause problems then, because only StreamTwoInputProcessor works at that time.After StreamTwoInputProcessor is replaced by StreamTwoInputSelectableProcessor as now, this bug is exposed and we will not see the barrier alignment metric for the case of two inputs.The solution is to add this metric while constructing the current StreamTwoInputProcessor.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.NoOpMetricRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups.java</file>
    </fixedFiles>
  </bug>
  <bug id="15916" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove outdated sections for Network Buffers and Async Checkpoints from the Large State Tuning Guide</summary>
      <description>The documentation page "Tuning Checkpoints and Large State" has a section on network buffers and async checkpoints.The network buffers section is not relevant any more and say so as well (before Flink 1.3 ...)The async snapshots section is also outdated (all state snapshots are async by default now) refers to a setup (rocks with heap timers) that is no longer the default and has its own warning already.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="15917" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Root Exception not shown in Web UI</summary>
      <description>Description On the job details page in the Exceptions → Root Exception tab, exceptions that cause the job to restart are not displayed. This is already a problem since 1.9.0 if jobmanager.execution.failover-strategy: region is configured, which we do in the default flink-conf.yaml.Workarounds Users that run into this problem can set jobmanager.scheduler: legacy and unset jobmanager.execution.failover-strategy in their flink-conf.yamlHow to reproduce In flink-conf.yaml set restart-strategy: fixed-delay so enable job restarts.$ bin/start-cluster.sh$ bin/flink run -d examples/streaming/TopSpeedWindowing.jar$ bin/taskmanager.sh stopAssert that no exception is displayed in the Web UI.Expected behavior The stacktrace of the exception should be displayed. Whether the exception should be also shown if only a partial region of the job failed is up for discussion.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15918" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uptime Metric not reset on Job Restart</summary>
      <description>DescriptionThe uptime metric is not reset when the job restarts, which is a change in behavior compared to Flink 1.8.This change of behavior exists since 1.9.0 if jobmanager.execution.failover-strategy: region is configured,which we do in the default flink-conf.yaml.WorkaroundsUsers that find this behavior problematic can set jobmanager.scheduler: legacy and unset jobmanager.execution.failover-strategy: region in their flink-conf.yamlHow to reproducetrivialExpected behavioruptime should be reset on any vertex restart.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="15920" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show thread name in logs on CI</summary>
      <description>Having thread names in log lines make it much easier to understand from which task they come.Enabling that by default on the CI setup helps with analyzing bugs and unstable tests.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.log4j-travis.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15921" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis-ci error: PYTHON exited with EXIT CODE: 143</summary>
      <description>Currently, some Travis CI failures occur, such as: &amp;#91;1&amp;#93;,&amp;#91;2&amp;#93;. The reason for the failure is that the python dependent `grpcio` released the latest version 1.27.0 &amp;#91;3&amp;#93; today, which resulted in the test cache not having the latest dependency, and the timeout of downloading in the repo. If the problem will be fixed after the first download when the network is in good condition. I am still watching the latest build &amp;#91;4&amp;#93;. If it fails for a long time, we will try to set a lower version of `grpcio` or optimize the current test case. I would like to watch for a while . What do you think?  &amp;#91;1&amp;#93;https://travis-ci.org/apache/flink/builds/646250268 &amp;#91;2&amp;#93;https://travis-ci.org/apache/flink/jobs/646281060&amp;#91;3&amp;#93; https://pypi.org/project/grpcio/#files&amp;#91;4&amp;#93;https://travis-ci.org/apache/flink/builds/646355253</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug id="15922" opendate="2020-2-5 00:00:00" fixdate="2020-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show "Warn - received late message for checkpoint" only when checkpoint actually expired</summary>
      <description>The message "Warn - received late message for checkpoint" is shown frequently in the logs, also when a checkpoint was purposefully canceled.In those case, this message is unhelpful and misleading.We should log this only when the checkpoint is actually expired.Meaning that when receiving the message, we check if we have an expired checkpoint for that ID. If yes, we log that message, if not, we simply drop the message.</description>
      <version>1.10.0</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15935" opendate="2020-2-6 00:00:00" fixdate="2020-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to use watermark when depends both on flink planner and blink planner</summary>
      <description>Run the following code in module `flink-examples-table` (must be under this module)/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.flink.table.examples.java;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;/** * javadoc. */public class TableApiExample { /** * * @param args * @throws Exception */ public static void main(String[] args) throws Exception { StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment(); bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings); bsTableEnv.sqlUpdate( "CREATE TABLE sink_kafka (\n" + " status STRING,\n" + " direction STRING,\n" + " event_ts TIMESTAMP(3),\n" + " WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n" + ") WITH (\n" + " 'connector.type' = 'kafka', \n" + " 'connector.version' = 'universal', \n" + " 'connector.topic' = 'generated.events2',\n" + " 'connector.properties.zookeeper.connect' = 'localhost:2181',\n" + " 'connector.properties.bootstrap.servers' = 'localhost:9092',\n" + " 'connector.properties.group.id' = 'testGroup',\n" + " 'format.type'='json',\n" + " 'update-mode' = 'append'\n" + ")\n"); }}  And hit the following error:Exception in thread "main" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts'Exception in thread "main" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4841) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5667) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlOperator.deriveType(SqlOperator.java:501) at org.apache.calcite.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:144) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:947) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateParameterizedExpression(SqlValidatorImpl.java:930) at org.apache.flink.table.planner.operations.SqlToOperationConverter.lambda$createTableSchema$8(SqlToOperationConverter.java:509) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.table.planner.operations.SqlToOperationConverter.createTableSchema(SqlToOperationConverter.java:505) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:177) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:130) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:484) at org.apache.flink.table.examples.java.TableApiExample.main(TableApiExample.java:43)Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 25 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15937" opendate="2020-2-6 00:00:00" fixdate="2020-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the Development Status for PyFlink</summary>
      <description>Correct the  `Development Status` value. From `'Development Status :: 1 - Planning'` to `'Development Status :: 5 - Production/Stable'`. The `Planning status` in PyPI means tell user that the package cannot be using in production &amp;#91;1&amp;#93;. So, correct the Development Status is very important for user. I would like to contains this fix in 1.10.0 release.  [1] https://pypi.org/search/?c=Development+Status+%3A%3A+1+-+Planning</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="15941" opendate="2020-2-6 00:00:00" fixdate="2020-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConfluentSchemaRegistryCoder should not perform HTTP requests for all request</summary>
      <description>The true reason is described by user Stephen Whelan: https://issues.apache.org/jira/browse/FLINK-15941?focusedCommentId=17033121&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17033121</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RegistryAvroSerializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="15948" opendate="2020-2-7 00:00:00" fixdate="2020-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resource will be wasted when the task manager memory is not a multiple of Yarn minimum allocation</summary>
      <description>If the taskmanager.memory.process.size is set to 2000m and the Yarn minimum allocation is 128m, we will get a container with 2048m. Currently, TaskExecutorProcessSpec is built with 2000m, so we will have 48m wasted and they could not be used by Flink.I think Flink has accounted all the jvm heap, off-heap, overhead resources. So we should not leave these free memory there. And i suggest to update the TaskExecutorProcessSpec according to the Yarn allocated container.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15949" opendate="2020-2-7 00:00:00" fixdate="2020-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden jackson dependency constraints</summary>
      <description>Replace the individual jackson dependency management entries with the jackson bom, and introduce an enforcer check to ban older jackson dependencies.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1595" opendate="2015-2-20 00:00:00" fixdate="2015-5-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a complex integration test for Streaming API</summary>
      <description>The streaming tests currently lack a sophisticated integration test that would test many api features at once. This should include different merging, partitioning, grouping, aggregation types, as well as windowing and connected operators.The results should be tested for correctness.A test like this would help identifying bugs that are hard to detect by unit-tests.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimePreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimeGroupedPreReducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15953" opendate="2020-2-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job Status is hard to read for some Statuses</summary>
      <description>The job status RESTARTING is rendered in a white font on white background which makes it hard to read (see attachments).</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
    </fixedFiles>
  </bug>
  <bug id="15966" opendate="2020-2-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Capture the call stack of RPC ask() calls.</summary>
      <description>Currently, when an RPC ask() call fails, we get a rather unhelpful exception with a stack trace from akka's internal scheduler.Instead, we should capture the call stack during the invocation and use it to give a helpful error message when the RPC call failed. This is especially helpful in cases where the future (and future handlers) are passed for later asynchronous result handling (which is the common case in most components (JM / TM / RM).The options should have a flag to turn it off, because when having a lot of concurrent ask calls (hundreds of thousands, during large deploy phases), it may be possible that the captured call.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15977" opendate="2020-2-10 00:00:00" fixdate="2020-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update pull request template to include Kubernetes as deployment candidates</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug id="15995" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>TO_BASE64 sql operator change operandTypeChecker from OperandTypes.ANY to OperandTypes.family(SqlTypeFamily.STRING)</summary>
      <description>For the TO_BASE64 sql operator, the operandTypeChecker is OperandTypes.ANY which is too large. I think we should change it to OperandTypes.family(SqlTypeFamily.STRING) .For if users use testSqlApi("to_base64(11)", "AQIDBA==")it will throw Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 30 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 206, Column 150: No applicable constructor/method found for actual parameters "int"; candidates are: "public static java.lang.String org.apache.flink.table.runtime.functions.SqlFunctionUtils.toBase64(org.apache.flink.table.dataformat.BinaryString)", "public static java.lang.String org.apache.flink.table.runtime.functions.SqlFunctionUtils.toBase64(byte[])" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)it is confusing to users. </description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15999" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract “Concepts” material from API/Library sections and start proper concepts section</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.times.clocks.svg</file>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.fig.event.ingestion.processing.time.svg</file>
      <file type="M">docs.fig.processes.svg</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.zh.md</file>
      <file type="M">docs.concepts.glossary.zh.md</file>
      <file type="M">docs.concepts.glossary.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.dev.stream.operators.process.function.zh.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.concepts.runtime.zh.md</file>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug id="16004" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude flink-rocksdb-state-memory-control-test jars from the dist</summary>
      <description>Currently flink-rocksdb-state-memory-control-test will be included in the dist as shown here. We should remove it.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-rocksdb-state-memory-control-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16007" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to push down the Java Calls contained in Python Correlate node</summary>
      <description>The Java Calls contained in Python Correlate node should be extracted to make sure the TableFunction works well.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16008" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add rules to transpose the join condition of Python Correlate node</summary>
      <description>Because the conditions can’t be executed in Python correlate node, this rule will transpose the conditions after the Python correlate node if the join type is inner join.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16014" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 plugin ClassNotFoundException SAXParser</summary>
      <description>While stress-testing s3 plugin on EMR. org.apache.flink.util.FlinkRuntimeException: Could not perform checkpoint 2 for operator Map (114/160). at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:839) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:104) at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:149) at org.apache.flink.streaming.runtime.io.InputProcessorUtil$1.lambda$notifyBarrierReceived$0(InputProcessorUtil.java:80) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:508) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:492) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.checkErroneousUnsafe(BufferPersisterImpl.java:262) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.add(BufferPersisterImpl.java:137) at org.apache.flink.runtime.io.network.BufferPersisterImpl.addBuffers(BufferPersisterImpl.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInflightDataSnapshot(StreamTask.java:935) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:898) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:870) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:826) ... 12 moreCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:145) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2251) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970) at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:152) at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:143) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.exists(SafetyNetWrapperFileSystem.java:102) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.get(BufferPersisterImpl.java:213) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.run(BufferPersisterImpl.java:167)Caused by: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:118) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:87) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1554) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4266) at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:876) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1262) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280) at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223) ... 9 moreCaused by: org.xml.sax.SAXException: SAX2 driver class org.apache.xerces.parsers.SAXParser not foundjava.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:230) at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:115) ... 32 moreCaused by: java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:149) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ... 34 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16018" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error reporting when submitting batch job (instead of AskTimeoutException)</summary>
      <description>While debugging the Shaded Hadoop S3A end-to-end test (minio) pre-commit test, I noticed that the JobSubmission is not producing very helpful error messages.Environment: A simple batch wordcount job a unavailable minio s3 filesystem serviceWhat happens from a user's perspective: The job submission fails after 10 seconds with a AskTimeoutException:2020-02-07T11:38:27.1189393Z akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-939201095]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.2020-02-07T11:38:27.1189538Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-02-07T11:38:27.1189616Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-02-07T11:38:27.1189713Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)2020-02-07T11:38:27.1189789Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2020-02-07T11:38:27.1189883Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2020-02-07T11:38:27.1189973Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2020-02-07T11:38:27.1190067Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2020-02-07T11:38:27.1190159Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2020-02-07T11:38:27.1190267Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2020-02-07T11:38:27.1190358Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2020-02-07T11:38:27.1190465Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-02-07T11:38:27.1190540Z at java.lang.Thread.run(Thread.java:748)What a user would expect: An error message indicating why the job submission failed.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">docs..includes.generated.web.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="16024" opendate="2020-2-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support filter pushdown in JDBC connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.resources.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16029" opendate="2020-2-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove register source and sink in test cases of blink planner</summary>
      <description>Many test cases of planner use TableEnvironement.registerTableSource() and registerTableSink() which should be avoid。We want to refactor these cases via TableEnvironment.connect().</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.descriptors.python.CustomConnectorDescriptor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TestLimitableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.InMemoryLookupableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimestampITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TimestampITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsvValidator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsv.java</file>
    </fixedFiles>
  </bug>
  <bug id="16040" opendate="2020-2-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Change local import to global import</summary>
      <description>There two reasons support doing this: Execute import will cost time. PEP8  claims that "Imports are always put at the top of the file" https://www.python.org/dev/peps/pep-0008/#imports</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16041" opendate="2020-2-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand "popular" documentation sections by default</summary>
      <description>Currently, when the documentation page is loaded all sections are collapsed, this means that some prominent subsections are not easily discoverable. I think we should expand the "Getting Started", "Concepts", and "API" sections by default. (See also https://cwiki.apache.org/confluence/display/FLINK/FLIP-42%3A+Rework+Flink+Documentation, because "API" doesn't exist yet in the current documentation.I attached screenshots to show what I mean.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
      <file type="M">docs.dev.index.zh.md</file>
      <file type="M">docs.dev.index.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="16046" opendate="2020-2-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop Elasticsearch 2.x connector</summary>
      <description>We should drop the ES 2.x connector as discussed here: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Drop-connectors-for-Elasticsearch-2-x-and-5-x-td37471.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch2-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch2SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch2-test.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.webbit</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.jzlib</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.jsr166y</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.jsr166e</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.hdrhistogram</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.licenses.LICENSE.base64</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewRequestIndexerBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewElasticsearchSinkFunctionBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">docs.dev.connectors.elasticsearch.zh.md</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="16053" opendate="2020-2-14 00:00:00" fixdate="2020-2-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Remove redundant metrics in PyFlink</summary>
      <description>We have recorded the metrics about how many elements it has processed in Python UDF. This kind of information is not necessary as there is also this kind of information in the Java operator. I have performed a simple test and find that removing it could improve the performance about 5% - 10%. Besides, as these metrics are still not exposed and it will be safe to remove it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="16059" opendate="2020-2-14 00:00:00" fixdate="2020-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Order properties in REST API docs alphabetically</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="16060" opendate="2020-2-14 00:00:00" fixdate="2020-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create MVP of multiple input processor</summary>
      <description>Implement MVP of StreamMultipleInputProcessor, without keyed context support and without input selection support.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.metrics.MinWatermarkGauge.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.LinkedBufferStorageTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.LinkedBufferStorage.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.TwoInputSelectionHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.SteppingMailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.InputSelectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InputSelection.java</file>
    </fixedFiles>
  </bug>
  <bug id="16070" opendate="2020-2-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner can not extract correct unique key for UpsertStreamTableSink</summary>
      <description>I reproduce an Elasticsearch6UpsertTableSink issue which user reported in mail list&amp;#91;1&amp;#93; that Blink planner can not extract correct unique key for following query, but legacy planner works well. // user codeINSERT INTO ES6_ZHANGLE_OUTPUT  SELECT aggId, pageId, ts_min as ts,  count(case when eventId = 'exposure' then 1 else null end) as expoCnt,  count(case when eventId = 'click' then 1 else null end) as clkCnt  FROM  (    SELECT        'ZL_001' as aggId,        pageId,        eventId,        recvTime,        ts2Date(recvTime) as ts_min    from kafka_zl_etrack_event_stream    where eventId in ('exposure', 'click')  ) as t1  group by aggId, pageId, ts_minI  found that blink planner can not extract correct unique key in `FlinkRelMetadataQuery.getUniqueKeys(relNode)`, legacy planner works well in  `org.apache.flink.table.plan.util.UpdatingPlanChecker.getUniqueKeyFields(...) `. A simple ETL job to reproduce this issue can refers&amp;#91;2&amp;#93; &amp;#91;1&amp;#93;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-1-10-es-sink-exception-td32773.html&amp;#91;2&amp;#93;https://github.com/leonardBang/flink-sql-etl/blob/master/etl-job/src/main/java/kafka2es/Kafka2UpsertEs.java  </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16071" opendate="2020-2-16 00:00:00" fixdate="2020-2-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce FlattenRowCoder to solve the performance issue of __get_item__ in Row</summary>
      <description>Optimizing the cost of the get item of the Row will gain tremendous improvements in python udf performance</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16072" opendate="2020-2-16 00:00:00" fixdate="2020-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the performance of the write/read null mask in FlattenRowCoder</summary>
      <description>Optimizing the write/read null mask in RowCoder will gain some performance improvements in python udf.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="16111" opendate="2020-2-17 00:00:00" fixdate="2020-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes deployment does not respect "taskmanager.cpu.cores".</summary>
      <description>The Kubernetes deployment uses `kubernetes.taskmanager.cpu` for configuring TM cpu cores, and will fallback to number-of-slots if not specified.FLINK-14188 introduced a common option `taskmanager.cpu.cores` (ATM not exposed to users and for internal usage only). A common logic is to decide the TM cpu cores following the fallback order of "common option -&gt; K8s/Yarn/Mesos specific option -&gt; numberOfSlot".The above fallback rules are not respected by the Kubernetes deployment.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="16115" opendate="2020-2-17 00:00:00" fixdate="2020-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aliyun oss filesystem could not work with plugin mechanism</summary>
      <description>From release-1.9, Flink suggest users to load all filesystem with plugin, including oss. However, it could not work for oss filesystem. The root cause is it does not shade the org.apache.flink.runtime.fs.hdfs and org.apache.flink.runtime.util. So they will always be loaded by system classloader and throw the following exceptions. 2020-02-17 17:28:47,247 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint. at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:64)Caused by: java.lang.NoSuchMethodError: org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.&lt;init&gt;(Lorg/apache/flink/fs/shaded/hadoop3/org/apache/hadoop/fs/FileSystem;)V at org.apache.flink.fs.osshadoop.OSSFileSystemFactory.create(OSSFileSystemFactory.java:85) at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61) at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441) at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362) at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298) at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:100) at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:89) at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:125) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ... 2 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16139" opendate="2020-2-18 00:00:00" fixdate="2020-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Co-location constraints are not reset on task recovery in DefaultScheduler</summary>
      <description>The colocation constraints are not reset on task recovery, which may lead to task recovery failures when allocating slots.We should reset the colocation constraints before resetting vertices, just like what we do in the legacy scheduler.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16144" opendate="2020-2-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add client.timeout setting and use that for CLI operations</summary>
      <description>Currently, the Cli uses the akka.client.timeout setting. This has historical reasons but can be very confusing for users. We should introduce a new setting client.timeout that is used for the client, with a fallback to the previous akka.client.timeout.</description>
      <version>None</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.executors.EmbeddedExecutorFactory.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.executors.EmbeddedExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">docs..includes.generated.execution.configuration.html</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="16152" opendate="2020-2-18 00:00:00" fixdate="2020-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Operator/index" into Chinese</summary>
      <description>The page is located at docs/dev/stream/operators/index.zh.md</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="16162" opendate="2020-2-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded to 10.0</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16163" opendate="2020-2-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to flink-shaded-zookeeper</summary>
      <description>Migrate to Zookeeper 3.4 provided by flink-shaded.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-curator.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphsStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounterTest.java</file>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperVersionedValue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperUtilityFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedValue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedCount.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRunningJobsRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.LastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultLastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16164" opendate="2020-2-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable maven-site-plugin</summary>
      <description>The maven-site-plugin can be used to generate a site for the project, like the sites used by various maven modules.We inherit an execution for this plugin, but don't actually use it.Hence, we can disable it to remove some noise from the build.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16173" opendate="2020-2-19 00:00:00" fixdate="2020-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce noise for used undeclared dependencies</summary>
      <description>When analyzing dependency usages there are a bunch of noisy entries, particularly around transitive Flink dependencies, hamcrest and powermock, that would create more work than benefit.These should be allowed to be used without having an explicit dependency.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16191" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message on Windows when RocksDB Paths are too long</summary>
      <description>Paths on Windows have a length limit by default (247 chars).If RocksDB tries to open a longer path, it throws a misleading exception (directory not found).We can check for this situation and improve the error message.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16217" opendate="2020-2-21 00:00:00" fixdate="2020-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client crashed when any uncatched exception is thrown</summary>
      <description>Currently, SQL CLI doesn't catch all the exceptions, for example, Calcite exceptions. We should catch any possible exceptions thrown by the planner, and print the root cause.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TerminalUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16222" opendate="2020-2-21 00:00:00" fixdate="2020-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use plugins mechanism for initializing MetricReporters</summary>
      <description>https://issues.apache.org/jira/browse/FLINK-11952 introduced Plugins mechanism into Flink. Metrics reporters initialization mechanism can profit from using this new functionality. Instead of placing MetricsReporters JARs into /libs, it should be additionally possible (and encouraged) to convert them into plugins and use the /plugins folder for initialization via independent plugin classloaders.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.ReporterSetup.java</file>
    </fixedFiles>
  </bug>
  <bug id="16231" opendate="2020-2-22 00:00:00" fixdate="2020-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector is missing jdk.tools exclusion against Hive 2.x.x</summary>
      <description>Click to add description</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16251" opendate="2020-2-24 00:00:00" fixdate="2020-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the cost of function call in ScalarFunctionOpertation</summary>
      <description>Currently, there are too many extra function calls cost in  ScalarFunctionOpertation.We need to optimize it to improve performance in Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="16252" opendate="2020-2-24 00:00:00" fixdate="2020-2-24 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the output emit logic to remove unnecessary overhead</summary>
      <description>We need to optimize the function call chains in process_outputs to improve the performance in Python UDF</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="16269" opendate="2020-2-25 00:00:00" fixdate="2020-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generic type can not be matched when convert table to stream.</summary>
      <description>The query result schema printed by table.printSchema(): |-- deviceId: BIGINT |-- channel: STRING |-- schemaId: BIGINT |-- productId: BIGINT |-- schema: LEGACY('RAW', 'ANY&lt;com.yunmo.iot.schema.Schema&gt;')then excuting table.toRetractStream&amp;#91;DeviceSchema&amp;#93;.print(), exception throwed:Exception in thread "main" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink do not match. Query schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: RAW('com.yunmo.iot.schema.Schema', ?)] Sink schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: LEGACY('RAW', 'ANY&lt;com.yunmo.iot.schema.Schema&gt;')]The com.yunmo.iot.schema.Schema is a generic type.The schema field of Query schema change from LEGACY('RAW' to RAW, but the Sink schema still a LEGACY('RAW'</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaPojos.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformations.java</file>
    </fixedFiles>
  </bug>
  <bug id="1627" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty channel connect deadlock</summary>
      <description>StephanEwen reports the following deadlock (https://travis-ci.org/StephanEwen/incubator-flink/jobs/52755844, logs: https://s3.amazonaws.com/flink.a.o.uce.east/travis-artifacts/StephanEwen/incubator-flink/477/477.2.tar.gz)."CHAIN Partition -&gt; Map (Map at testRestartMultipleTimes(SimpleRecoveryITCase.java:200)) (2/4)" daemon prio=10 tid=0x00007f5fdc008800 nid=0xe230 in Object.wait() [0x00007f5fca8f2000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000f2a13530&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory$ConnectingChannel.waitForChannel(PartitionRequestClientFactory.java:179) - locked &lt;0x00000000f2a13530&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory$ConnectingChannel.access$000(PartitionRequestClientFactory.java:125) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:64) at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:53) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestIntermediateResultPartition(RemoteInputChannel.java:92) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:287) - locked &lt;0x00000000f29dbcd8&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:306) at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:75) at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34) at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) at org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:91) at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:205) at java.lang.Thread.run(Thread.java:745)"CHAIN Partition -&gt; Map (Map at testRestartMultipleTimes(SimpleRecoveryITCase.java:200)) (3/4)" daemon prio=10 tid=0x00007f5fdc005000 nid=0xe22f in Object.wait() [0x00007f5fca9f3000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000f2a13530&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory$ConnectingChannel.waitForChannel(PartitionRequestClientFactory.java:179) - locked &lt;0x00000000f2a13530&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory$ConnectingChannel.access$000(PartitionRequestClientFactory.java:125) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:79) at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:53) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestIntermediateResultPartition(RemoteInputChannel.java:92) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:287) - locked &lt;0x00000000f2896f88&gt; (a java.lang.Object) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:306) at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:75) at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34) at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) at org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:91) at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:205) at java.lang.Thread.run(Thread.java:745)Two tasks try to connect to a task manager during a data shuffle. One of the two tries to establish the connection and then both wait for the connect to return (waitForChannel).The problem seems to be related to the channel listener never handing in the channel.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16271" opendate="2020-2-25 00:00:00" fixdate="2020-2-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce ArrowScalarFunctionOperator for vectorized Python UDF execution</summary>
      <description>The aim of this Jira is to introduce ArrowScalarFunctionOperator for vectorized Python UDF execution.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractBaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="16281" opendate="2020-2-25 00:00:00" fixdate="2020-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parameter &amp;#39;maxRetryTimes&amp;#39; can not work in JDBCUpsertTableSink</summary>
      <description>When I insert data to a mysql table that do no exists in my test database will get exception,Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test.gmv_table' doesn't existCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test.gmv_table' doesn't exist at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.Util.getInstance(Util.java:408) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3933) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3869) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2524) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2675) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2465) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1912) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2133) at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1810) ... 44 morebut after I increased the 'connector.write.max-retries' value from 1 to 3, the exception disappeared. So, I look up the flush  implement code :public synchronized void flush() throws Exception { checkFlushException(); for (int i = 1; i &lt;= maxRetryTimes; i++) { try { jdbcWriter.executeBatch(); batchCount = 0; break; } catch (SQLException e) { LOG.error("JDBC executeBatch error, retry times = {}", i, e); if (i &gt;= maxRetryTimes) { throw e; } Thread.sleep(1000 * i); } }}I found the `jdbcWriter` will clear its `batchedArgs` member after first call `jdbcWriter.executeBatch()` as follows://com.mysql.jdbc.PreparedStatementfinally { this.statementExecuting.set(false); clearBatch();}// clearBatch() function implementpublic void clearBatch() throws SQLException { synchronized (checkClosed().getConnectionMutex()) { if (this.batchedArgs != null) { this.batchedArgs.clear(); } }}and the next time（ where i&gt; 1） to call `jdbcWriter.executeBatch()` ,  the function will return empty array rather than execute the flush data//com.mysql.jdbc.PreparedStatementif (this.batchedArgs == null || this.batchedArgs.size() == 0) { return new long[0];}... // flush data code  </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.JDBCWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16288" opendate="2020-2-26 00:00:00" fixdate="2020-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting the TTL for discarding task pods on Kubernetes.</summary>
      <description>I'm experimenting with running Flink 1.10.0 on native Kubernetes (version 1.17).After a job ends the task pods that were used to run it are discarded quite quickly.I found that if my job goes wrong I have too little time to look at all of the logs.I propose having a new config setting that allows me to run Flink on k8s where I can set the minimum time before an idle task pod is discarded.That way I can start Flink with a pod ttl of an hour (or something like that) so I have enough time to go through the logs and figure out what I did wrong.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1629" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to start Flink on YARN in a detached mode</summary>
      <description>Right now, we expect the YARN command line interface to be connected with the Application Master all the time to control the "yarn session" or the job.For very long running sessions or jobs users want to just "fire and forget" a job/session to YARN.Stopping the session will still be possible using YARN's tools.Also, prior to "detaching" itself, the CLI frontend could print the required command to kill the session as a convenience.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.Messages.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.appMaster.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn-tests.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.WebInfoServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">docs.yarn.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="16292" opendate="2020-2-26 00:00:00" fixdate="2020-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute all end to end tests on AZP</summary>
      <description>Ensure that we execute all end to end tests on AZP: Make sure that all the e2e tests referenced in the splits are also referenced in the "run nightly tests" script make sure the java e2e tests are executed</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.setup.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16293" opendate="2020-2-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document using plugins in Kubernetes</summary>
      <description>It took me some time to figure out how to enable plugins when running Flink on Kubernetes.So I'm writing some documentation to save other people trying the same a lot of time.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="16302" opendate="2020-2-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Rest Handler to list TM Logfiles and enable reading Logs by Filename</summary>
      <description> list taskmanager all log file /taskmanagers/taskmanagerid/logs { "logs": [ { "name": "taskmanager.log", "size": 12529 } ]} read taskmanager log file  /taskmanagers/logs/&amp;#91;filename&amp;#93; response: same as taskmanager’s log</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="16303" opendate="2020-2-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Rest Handler to list JM Logfiles and enable reading Logs by Filename</summary>
      <description>list jobmanager all log file /jobmanager/logs { "logs": [ { "name": "jobmanager.log", "size": 12529 } ]} read jobmanager log file  /jobmanager/log/&amp;#91;filename&amp;#93; response: same as jobmanager's log</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.LogListInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerLogsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerFileMessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerCustomLogHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.LogListInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.LogInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.LogFileNamePathParameter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StdoutFileHandlerSpecification.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.LogFileHandlerSpecification.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ConstantTextHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="16331" opendate="2020-2-28 00:00:00" fixdate="2020-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove source licenses for old WebUI</summary>
      <description>When we removed the old WebUI we only removed the licenses from flink-runtime-web, but missed the ones for the source release.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">licenses.LICENSE.split</file>
      <file type="M">licenses.LICENSE.qtip2</file>
      <file type="M">licenses.LICENSE.moment-duration-format</file>
      <file type="M">licenses.LICENSE.moment</file>
      <file type="M">licenses.LICENSE.lodash</file>
      <file type="M">licenses.LICENSE.jquery</file>
      <file type="M">licenses.LICENSE.imagesloaded</file>
      <file type="M">licenses.LICENSE.graphlib</file>
      <file type="M">licenses.LICENSE.ev-emitter</file>
      <file type="M">licenses.LICENSE.dagre-d3</file>
      <file type="M">licenses.LICENSE.dagre</file>
      <file type="M">licenses.LICENSE.d3</file>
      <file type="M">licenses.LICENSE.bootstrap</file>
      <file type="M">licenses.LICENSE.angular-ui-router</file>
      <file type="M">licenses.LICENSE.angular-moment</file>
      <file type="M">licenses.LICENSE.angular-drag-and-drop-list</file>
      <file type="M">licenses.LICENSE.angular</file>
    </fixedFiles>
  </bug>
  <bug id="16343" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message when reading an unbounded source in batch mode</summary>
      <description>We can just ignore watermark in batch mode. cc jark</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16345" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Computed column can not refer time attribute column</summary>
      <description>If a computed column refer a time attribute column, computed column will lose  time attribute and cause validation fail.CREATE TABLE orders ( order_id STRING, order_time TIMESTAMP(3), amount DOUBLE, amount_kg as amount * 1000, // can not select computed column standard_ts which from column order_time that used as WATERMARK standard_ts as order_time + INTERVAL '8' HOUR, WATERMARK FOR order_time AS order_time) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'flink_orders', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'testGroup', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true');The query `select amount_kg from orders` runs normally,  the` he query `select standard_ts from orders` throws a validation exception message as following:[ERROR] Could not execute SQL statement. Reason: java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes: validated type: RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIMESTAMP(3) ts) NOT NULL converted type: RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" order_id, TIME ATTRIBUTE(ROWTIME) order_time, DOUBLE amount, DOUBLE amount_kg, TIME ATTRIBUTE(ROWTIME) ts) NOT NULL rel: LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[$3], ts=[$4]) LogicalWatermarkAssigner(rowtime=[order_time], watermark=[$1]) LogicalProject(order_id=[$0], order_time=[$1], amount=[$2], amount_kg=[*($2, 1000)], ts=[+($1, 28800000:INTERVAL HOUR)]) LogicalTableScan(table=[[default_catalog, default_database, orders, source: [Kafka010TableSource(order_id, order_time, amount)]]])   </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16359" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce WritableVectors for abstract writing</summary>
      <description>In FLINK-11899 , we need write vectors from parquet input streams.We need abstract vector writing, in future, we can provide OffHeapVectors.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapShortVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapLongVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapIntVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapFloatVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapDoubleVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapByteVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapBytesVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapBooleanVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.AbstractHeapVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.Dictionary.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.ColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.BytesColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.AbstractColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowTinyIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowSmallIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowBigIntColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.AbstractOrcColumnVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.AbstractOrcNoHiveVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="16363" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the execution behavior of TableEnvironment and StreamTableEnvironment</summary>
      <description>Both TableEnvironment.execute() and StreamExecutionEnvironment.execute can trigger a Flink table program execution. However if you use TableEnvironment to build a Flink table program, you must use TableEnvironment.execute() to trigger execution, because you can’t get the StreamExecutionEnvironment instance. If you use StreamTableEnvironment to build a Flink table program, you can use both to trigger execution. If you convert a table program to a DataStream program (using StreamExecutionEnvironment.toAppendStream/toRetractStream), you also can use both to trigger execution. So it’s hard to explain which `execute` method should be used.To correct current messy trigger point, we propose that: for TableEnvironment and StreamTableEnvironment, you must use TableEnvironment.execute() to trigger table program execution, once you convert the table program to a DataStream program (through toAppendStream or toRetractStream method), you must use StreamExecutionEnvironment.execute to trigger the DataStream program.please refer to FLIP-84 for more detail.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.TableFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.executor.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.BatchExecutorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.DummyStreamExecutionEnvironment.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.ExecutorBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.BatchExecutor.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamWindowSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="16377" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support inline user defined functions in expression dsl</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.aggregations.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ScalarFunctionConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CallExpressionConvertRule.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.resolver.ExpressionResolverTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.QueryOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CalculatedQueryOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="16378" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable JDK 11 Docker tests on AZP</summary>
      <description>Build log: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&amp;view=logs&amp;j=eec879f1-c5a2-5810-2b49-ba5c6bfecb27&amp;t=484f04d6-55db-5161-9f93-391b1677737dError: A JNI error has occurred, please check your installation and try againException in thread "main" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)Running the job failed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16393" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer unnecessarily creates record emitter thread w/o source sync</summary>
      <description>The asynchronous record emitter depends on the periodic watermark calculation. If no periodic watermark is configured then records will be directly emitted by the shard consumer threads and the record emitter thread never used. We should skip the thread creation in that case.</description>
      <version>1.8.3,1.9.2,1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="16408" opendate="2020-3-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bind user code class loader to lifetime of a slot</summary>
      <description>In order to avoid class leaks due to creating multiple user code class loaders and loading class multiple times in a recovery case, I would suggest to bind the lifetime of a user code class loader to the lifetime of a slot. More precisely, the user code class loader should live at most as long as the slot which is using it.</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.TestingHeartbeatServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingJobManagerTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobManagerTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.DefaultJobManagerTable.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.TestingLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.ContextClassLoaderLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.LibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TestingTaskSlotTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestTaskBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.DefaultJobTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.DefaultJobTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="16410" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporterEndToEndITCase fails with ClassNotFoundException</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5883&amp;view=logs&amp;j=b1623ac9-0979-5b0d-2e5e-1377d695c991&amp;t=e7804547-1789-5225-2bcf-269eeaa37447[INFO] Running org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase[ERROR] testReporter(org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase) Time elapsed: 0.005 s &lt;&lt;&lt; ERROR!java.lang.NoClassDefFoundError: org/apache/flink/runtime/rest/messages/RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.rest.messages.RequestBody at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.&lt;init&gt;(PrometheusReporterEndToEndITCase.java:119)[INFO]</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16413" opendate="2020-3-4 00:00:00" fixdate="2020-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce hive source parallelism when limit push down</summary>
      <description>User started hive source parallelism automatic inference. For example, Set the maximum parallelism of inference to 10.User have a similar SQL SELECT * from mytable limit 1;There are more than 10 files in the hive table mytable. Is it a bit wasteful to start 10 parallelism.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16455" opendate="2020-3-6 00:00:00" fixdate="2020-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink-sql-connector-hive modules to provide hive uber jars</summary>
      <description>Discussed in: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-flink-connector-hive-xx-modules-td38440.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="16485" opendate="2020-3-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support vectorized Python UDF in the batch mode of old planner</summary>
      <description>Currently, vectorized Python UDF is only supported in the batch/stream mode for the blink planner and stream mode for the old planner. The aim of this Jira is to add support in the batch mode for the old planner.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="16486" opendate="2020-3-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to add documentation for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="16496" opendate="2020-3-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve default flush strategy for HBase sink to make it work out-of-box</summary>
      <description>Currently, HBase sink provides 3 flush options:'connector.write.buffer-flush.max-size' = '2mb' -- default 2mb'connector.write.buffer-flush.max-rows' = '1000' -- no default value'connector.write.buffer-flush.interval' = '2s' -- no default valueThat means if flush interval is not set, the buffered output rows may not be flushed to database for a long time. That is a surprising behavior because no results are outputed by default. So we propose to have a default flush '1s' interval and '1000' rows and '2mb' size for HBase sink flush. This only applies to new HBase sink options:'sink.buffer-flush.max-rows' = '1000'. -- the same to ES sink'sink.buffer-flush.max-size' = '2mb' -- default value of HBase client'sink.buffer-flush.interval' = '1s' -- the same to JDBC sink</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase.src.test.java.org.apache.flink.connector.hbase.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.sink.HBaseSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.options.HBaseWriteOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.HBaseDynamicTableFactory.java</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="16497" opendate="2020-3-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve default flush strategy for JDBC sink to make it work out-of-box</summary>
      <description>Currently, old JDBC sink provides 2 flush options:'connector.write.flush.max-rows' = '5000', -- default is 5000'connector.write.flush.interval' = '2s', -- no default valueThat means if flush interval is not set, the buffered output rows may not be flushed to database for a long time. That is a surprising behavior because no results are outputed by default. So we propose to have a default flush '1s' interval and '100' rows for JDBC sink flush. This only applies to new JDBC sink options:'sink.buffer-flush.max-rows' = '100''sink.buffer-flush.interval' = '1s'</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcExecutionOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="16508" opendate="2020-3-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Name the ports exposed by the main Container in Pod</summary>
      <description>Currently, we expose some ports via the main Container of the JobManager and the TaskManager, but we forget to name those ports so that people could be confused because there is no description of the port usage. This ticket proposes to explicitly name the ports in the Container to help people understand the usage of those ports.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.AbstractServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1651" opendate="2015-3-4 00:00:00" fixdate="2015-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running mvn test got stuck</summary>
      <description>I keep getting my test stuck at this state:...Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in org.apache.flink.runtime.types.TypeTestRunning org.apache.flink.runtime.util.AtomicDisposableReferenceCounterTestTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.561 sec - in org.apache.flink.runtime.util.AtomicDisposableReferenceCounterTestRunning org.apache.flink.runtime.util.DataInputOutputSerializerTestTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.848 sec - in org.apache.flink.runtime.operators.DataSourceTaskTestRunning org.apache.flink.runtime.util.DelegatingConfigurationTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.runtime.util.DelegatingConfigurationTestRunning org.apache.flink.runtime.util.EnvironmentInformationTestTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.563 sec - in org.apache.flink.runtime.io.network.serialization.LargeRecordsTestRunning org.apache.flink.runtime.util.event.TaskEventHandlerTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.flink.runtime.util.event.TaskEventHandlerTestRunning org.apache.flink.runtime.util.LRUCacheMapTestTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec - in org.apache.flink.runtime.util.LRUCacheMapTestRunning org.apache.flink.runtime.util.MathUtilTestTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in org.apache.flink.runtime.util.MathUtilTestRunning org.apache.flink.runtime.util.NonReusingKeyGroupedIteratorTestTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.064 sec - in org.apache.flink.runtime.util.NonReusingKeyGroupedIteratorTestRunning org.apache.flink.runtime.util.ReusingKeyGroupedIteratorTestTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.runtime.util.ReusingKeyGroupedIteratorTestTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.238 sec - in org.apache.flink.runtime.taskmanager.TaskManagerTestTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.616 sec - in org.apache.flink.runtime.profiling.impl.InstanceProfilerTestTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.303 sec - in org.apache.flink.runtime.util.DataInputOutputSerializerTestTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.488 sec - in org.apache.flink.runtime.util.EnvironmentInformationTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.81 sec - in org.apache.flink.runtime.taskmanager.TaskManagerProcessReapingTestTests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.653 sec - in org.apache.flink.runtime.operators.MatchTaskTestTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.071 sec - in org.apache.flink.runtime.operators.sort.LargeRecordHandlerTestTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.534 sec - in org.apache.flink.runtime.operators.DataSinkTaskTestTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.98 sec - in org.apache.flink.runtime.operators.sort.NormalizedKeySorterTestTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.017 sec - in org.apache.flink.runtime.io.disk.ChannelViewsTestAfter this seemed like nothing happen. And the program just hang.I am using MacOSX with Java version 1.7.0_71</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerStartupTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16524" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16526" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception when computed column expression references a keyword column name</summary>
      <description>json_row ROW&lt;`timestamp` BIGINT&gt;,`timestamp` AS `json_row`.`timestamp`It translate to "SELECT json_row.timestamp FROM _temp_table_"Throws exception "Encountered ". timestamp" at line 1, column 157. Was expecting one of:..."</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16538" opendate="2020-3-11 00:00:00" fixdate="2020-3-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Restructure Python Table API documentation</summary>
      <description>Python Table API documentation is currently spread across a number of pages and it's difficult for a user to find out all the documentations available. Besides, there are also a few documentations which deserves specific page to describe the functionality, such as the environment setup, the Python dependency management, the vectorized Python UDF, etc. We want to improve the documentation by adding an item under the Table API as "Python Table API" and the structure of "Python Table API" is as following: Python Table API Installation User-defined Functions Vectorized Python UDF Dependency Management Metrics &amp; Logging Configuration  </description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="16539" opendate="2020-3-11 00:00:00" fixdate="2020-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql client set param error</summary>
      <description>When setting int type parameters in sql client, such as:set execution.parallelism = 10;The system threw an exception: //Caused by: org.apache.flink.table.api.ValidationException: Property 'parallelism' must be a integer value but was: 10 at org.apache.flink.table.descriptors.DescriptorProperties.validateComparable(DescriptorProperties.java:1572) at org.apache.flink.table.descriptors.DescriptorProperties.validateInt(DescriptorProperties.java:944) at org.apache.flink.table.descriptors.DescriptorProperties.validateInt(DescriptorProperties.java:937) at org.apache.flink.table.client.config.entries.ExecutionEntry.validate(ExecutionEntry.java:140) at org.apache.flink.table.client.config.entries.ConfigEntry.&lt;init&gt;(ConfigEntry.java:39) ... 11 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16559" opendate="2020-3-12 00:00:00" fixdate="2020-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot create Hive avro table in test</summary>
      <description>Trying to create a Hive avro table will hit the following exception:Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.&lt;init&gt;(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Lorg/codehaus/jackson/JsonNode;)V at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:76) at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:61) at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:170) at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:114) at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:83) at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449) at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436) at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641) at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:831)......</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16577" opendate="2020-3-13 00:00:00" fixdate="2020-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception will be thrown when computing columnInterval relmetadata in some case</summary>
      <description>Consider the following SQL // a: INT, c: LONGSELECT c, SUM(a) FROM T WHERE a &gt; 0.1 AND a &lt; 1 GROUP BY c Here the sql type of 0.1 is Decimal and 1 is Integer, and they are both in NUMERIC type family, and do not trigger type coercion, so the plan is:FlinkLogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])+- FlinkLogicalCalc(select=[c, a], where=[AND(&gt;(a, 0.1:DECIMAL(2, 1)), &lt;(a, 1))]) +- FlinkLogicalTableSourceScan(table=[[...]], fields=[a, b, c])When we calculate the filtered column interval of calc, it'll lead to validation exception of `FiniteValueInterval`:</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.ColumnIntervalUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16579" opendate="2020-3-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade Calcite version to 1.26 for Flink SQL</summary>
      <description>A taks to upgrade Calcite version to 1.23.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ProjectSemiAntiJoinTransposeRule.scala</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.collections.ByteHashSet.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.NormalizationRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SortTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.BasicOperatorTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.QueryOperationCatalogViewTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateJoinTransposeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ProjectPruneAggregateCallRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkPruneEmptyRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinToMultiJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateJoinTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.CalcPruneAggregateCallRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgramTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdPopulationSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.PythonOverWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.FlinkRewriteSubQueryRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ProjectSemiAntiJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinFilterTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinPushExpressionsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelShuttles.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.TableApiIdentifierParsingTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CustomizedConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.utils.HiveTableSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.metadata.FlinkRelMdCollation.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinFilterTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinJoinTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkSemiAntiJoinProjectTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.SubQueryDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16581" opendate="2020-3-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support state ttl for Mini-Batch deduplication using StateTtlConfig</summary>
      <description>This lead to OOM with long running streaming job.We should check all unbounded operations, should not lack state TTL</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingSemiAntiJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateViews.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateViews.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16589" opendate="2020-3-13 00:00:00" fixdate="2020-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Table SQL fails/crashes with big queries with lots of fields</summary>
      <description>Hi,My use case is a streaming application with a few streaming tables.I was trying to build a SELECT query (and registering it as a temporary view) with about 200 fields/expressions out of another streaming table. The application is successfully submitted to Flink cluster. However the worker processes keep crashing, with the exception as quoted below. It clearly mentioned in the log that this is a bug, so I fire this ticket. By the way, if I lower the number of fields down to 100 then it works nicely.Please advice.Thanks a lot for all the efforts bring Flink up. It is really amazing!java.lang.RuntimeException: Could not instantiate generated class 'GroupAggsHandler$9687'    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57)    at org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.open(MiniBatchGroupAggFunction.java:136)    at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:84)    at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007)    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52)    ... 10 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)    ... 12 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)    ... 15 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling "GroupAggsHandler$9687": Code of method "retract(Lorg/apache/flink/table/dataformat/BaseRow;)V" of class "GroupAggsHandler$9687" grows beyond 64 KB    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)    ... 21 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method "retract(Lorg/apache/flink/table/dataformat/BaseRow;)V" of class "GroupAggsHandler$9687" grows beyond 64 KB    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1009)    at org.codehaus.janino.CodeContext.write(CodeContext.java:901)    at org.codehaus.janino.CodeContext.writeShort(CodeContext.java:1026)    at org.codehaus.janino.UnitCompiler.writeConstantLongInfo(UnitCompiler.java:12274)    at org.codehaus.janino.UnitCompiler.pushConstant(UnitCompiler.java:10679)    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4936)    at org.codehaus.janino.UnitCompiler.access$8400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4414)    at org.codehaus.janino.UnitCompiler$16.visitUnaryOperation(UnitCompiler.java:4394)    at org.codehaus.janino.Java$UnaryOperation.accept(Java.java:4719)    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:3719)    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5569)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)    at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1503)    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1487)    at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3511)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)    ... 28 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16600" opendate="2020-3-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Respect the rest.bind-port for the Kubernetes setup</summary>
      <description>Our current logic only takes care of RestOptions.PORT but not RestOptions.BIND_PORT, which is a bug.For example, when one sets the RestOptions.BIND_PORT to a value other than RestOptions.PORT, jobs could not be submitted to the existing session cluster deployed via the kubernetes-session.sh.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16602" opendate="2020-3-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework the Service design for native Kubernetes deployment</summary>
      <description>At the moment we usually create two Services for a Flink application, one is the internal Service and the other is the so-called rest Service, the previous aims for forwarding request from the TMs to the JM, and the rest Service mainly serves as an external service for the Flink application. Here is a summary of the issues: The functionality boundary of the two Services is not clear enough since the internal Service could also become the rest Service when its exposed type is ClusterIP. For the high availability scenario, we create a useless internal Service which does not help forward the internal requests since the TMs directly communicate with the JM via the IP or hostname of the JM Pod. Headless service is enough to help forward the internal requests from the TMs to the JM. Service of ClusterIP type would add corresponding rules into the iptables, too many rules in the iptables would lower the kube-proxy's efficiency in refreshing iptables while notified of change events, which could possibly cause severe stability problems in a Kubernetes cluster. Therefore, we propose some improvements to the current design: Clarify the functionality boundary for the two Services, the internal Service only serves the internal communication from TMs to JM, while the rest Service makes the Flink cluster accessible from outside. The internal Service only exposes the RPC and BLOB ports while the external one exposes the REST port. Do not create the internal Service in the high availability case. Use HEADLESS type for the internal Service.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.AbstractServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.cli.KubernetesSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="16604" opendate="2020-3-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column key in JM configuration is too narrow</summary>
      <description>See the screenshot </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="16608" opendate="2020-3-16 00:00:00" fixdate="2020-3-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support primitive data types for vectorized Python UDF</summary>
      <description>As the title described, the aim of this JIRA is to support the primitive types for vectorized Python UDF.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.BaseRowArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.DateSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16613" opendate="2020-3-16 00:00:00" fixdate="2020-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Builds on Azure pipeline are not triggered</summary>
      <description>This is actually an issue at azure, but we can mitigate it, by making our build config more verbose: https://status.dev.azure.com/_event/179641421</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16615" opendate="2020-3-16 00:00:00" fixdate="2020-3-16 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce data structures and utilities to calculate Job Manager memory components</summary>
      <description>See FLIP-116: Unified Memory Configuration for Job Managers for details.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.BashJavaUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessSpec.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecorator.java</file>
      <file type="M">flink-dist.src.test.java.org.apache.flink.dist.BashJavaUtilsITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16623" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add the shorthand &amp;#39;desc&amp;#39; for describe on sql client</summary>
      <description>When get the table schema in the sql client, we can only use the describe command, not the shorthand desc, but the desc command is supported in many sql clients, such as spark, hive, mysql, etc. We should add the desc command in the flink sql client</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16624" opendate="2020-3-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user-specified annotations for the rest Service</summary>
      <description>There are some scenarios that people would like to customize annotations for the rest Service, for example: Specify the LB type to decide whether it should have an external IP. Specify the Security Policy for the LB.It's a common need, especially in the Cloud Environment.  This ticket proposes to add support for it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="16626" opendate="2020-3-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent REST handler from being closed more than once</summary>
      <description>In Flink 1.10.0 release, job cancellation can be problematic, as users frequently experience java.util.concurrent.TimeoutException at the client side, because the REST endpoint closes pre-maturely before sending out the response, this is because the jobCancellationHandler is incorrectly reused and closed twice. When executing the following command to stop a flink job with yarn per-job mode, the client keeps retrying untill timeout (1 minutes）and exit with failure. But the job stops successfully. Command :flink cancel $jobId yid appId The exception on the client side is : 2020-03-17 12:32:13,709 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel ... 2020-03-17 12:33:11,065 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel 2020-03-17 12:33:14,070 DEBUG org.apache.flink.runtime.rest.RestClient - Shutting down rest endpoint. 2020-03-17 12:33:14,070 DEBUG org.apache.flink.runtime.rest.RestClient - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to ocdt31.aicloud.local:51231/v1/jobs/cc61033484d4c0e7a27a8a2a36f4de7a?mode=cancel 2020-03-17 12:33:14,077 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1 2020-03-17 12:33:14,080 DEBUG org.apache.flink.runtime.rest.RestClient - Rest endpoint shutdown complete. 2020-03-17 12:33:14,083 ERROR org.apache.flink.client.cli.CliFrontend - Error while running the command. org.apache.flink.util.FlinkException: Could not cancel job cc61033484d4c0e7a27a8a2a36f4de7a. at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:545) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843) at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:538) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:904) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968) Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:543) ... 9 more------------------------------------------------------------ The program finished with the following exception:org.apache.flink.util.FlinkException: Could not cancel job cc61033484d4c0e7a27a8a2a36f4de7a. at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:545) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843) at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:538) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:904) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968) Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.cli.CliFrontend.lambda$cancel$7(CliFrontend.java:543) ... 9 moreActually, the job was cancelled. But the server also prints some exception: 2020-03-17 12:25:13,754 ERROR &amp;#91;flink-akka.actor.default-dispatcher-17&amp;#93; org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:766) - Failed to submit a listener notification task. Event loop shut down? java.util.concurrent.RejectedExecutionException: event executor terminated at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:855) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:340) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:333) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:766) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:764) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:421) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:149) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:224) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:176) at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:91) at org.apache.flink.runtime.rest.handler.AbstractRestHandler.lambda$respondToRequest$0(AbstractRestHandler.java:78) at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656) at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:874) at akka.dispatch.OnComplete.internal(Future.scala:264) at akka.dispatch.OnComplete.internal(Future.scala:261) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16635" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible okio dependency in flink-metrics-influxdb module</summary>
      <description>With FLINK-12147 we bumped influxdb-java from version 2.14 to 2.16. At the same time we fix the okio dependency to version 1.14.0. Since influxdb-java transitive dependency converter-moshi:jar:2.6.1 requires moshi:jar:1.8.0 which requires okio:jar:1.16.0, the influxdb metric reporter fails as described here. We should fix this incompatibility by removing the dependency management entry for okio.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16646" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink read orc file throw a NullPointerException</summary>
      <description>When I use OrcRowInputFormat to read multiple orc files, the system throws one NullPointerException .the code like this StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment();environment.setParallelism(1);String path = "file://tmp/dir";String schema = ..... ;OrcRowInputFormat orcRowInputFormat = new OrcRowInputFormat( path, schema, new org.apache.hadoop.conf.Configuration());DataStream dataStream =environment.createInput(orcRowInputFormat);dataStream.writeAsText("file:///tmp/aaa", FileSystem.WriteMode.OVERWRITE);environment.execute(); the exception is  Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at org.apache.flink.orc.shim.OrcShimV200.computeProjectionMask(OrcShimV200.java:188) at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:120) at org.apache.flink.orc.OrcSplitReader.&lt;init&gt;(OrcSplitReader.java:73) at org.apache.flink.orc.OrcRowSplitReader.&lt;init&gt;(OrcRowSplitReader.java:50) at org.apache.flink.orc.OrcRowInputFormat.open(OrcRowInputFormat.java:102) at org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator$SplitReader.run(ContinuousFileReaderOperator.java:315)  </description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16647" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Miss file extension when inserting to hive table with compression</summary>
      <description>When hive.exec.compress.output is on, we write into Hive tables with a compression codec. But we don't append a proper extension to the resulting files, which means these files can't be consumed later on.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.main.java.org.apache.flink.api.java.hadoop.mapred.utils.HadoopUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionTempFileManager.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOutputFormat.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputFileConfig.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveOutputFormatFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV110.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOutputFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16652" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BytesColumnVector should init buffer in Hive 3.x</summary>
      <description>The failed test is TableEnvHiveConnectorTest#testDifferentFormats when hive 3.x.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.AbstractOrcColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="16653" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ResultPartitionWriterTestBase for simplifying tests</summary>
      <description>At the moment there are at-least four implementations of `ResultPartitionWriter` interface used in unit tests. And there are about ten methods to be implemented for `ResultPartitionWriter` and most of them are dummy in tests.When we want to extend the methods for `ResultPartitionWriter`, the above four dummy implementations in tests have to be adjusted as well, to waste a bit efforts.Therefore abstract ResultPartitionWriterTestBase is proposed to implement the basic dummy methods for `ResultPartitionWriter`, and the previous four instances can all extend it to only implement one or two methods based on specific requirements in tests. And we will probably only need to adjust the ResultPartitionWriterTestBase when extending the `ResultPartitionWriter` interface.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AvailabilityTestResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AbstractCollectingResultPartitionWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16655" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the EmbeddedExecutor for executing the main() on the Dispatcher</summary>
      <description>Running the user's main method in "Application Mode" implies: 1) launching a dedicated cluster for the application's jobs2) running the user's main on the cluster, alongside the DispatcherThe EmbeddedExecutor will conceptually be like the existing Executors for session clusters, with the difference that this time there is no need to go through REST as this will be running already on the same machine as the Dispatcher.This executor, apart from the application mode can also be used in the case of the web submission and the StandaloneJobClusterEntryPoint which so far do not use executors.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.ParameterProgram.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.utils.TestProgram.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.ExecutorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16657" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Wire the EmbeddedExecutor to the Web Submission logic.</summary>
      <description>This will replace the current logic of web submission in the JarRunHandler with one that uses the newly introduced EmbeddedExecutor .</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StreamContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.DetachedApplicationRunner.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16661" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the YarnApplicationClusterEntrypoint</summary>
      <description>This will be the final step to support "Application Mode" for Yarn.So far, the idea is to ship the jar to the cluster as a LocalResource and the entrypoint will take care of running the user's main and providing the desired guarantees.The design will be further refined as the implementation of the dependencies of the subtask is done.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClientFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnSessionClusterExecutor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnJobClusterExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ExecutorCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTestUtils.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.TestJob.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.testjar.TestUserClassLoaderJobLib.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.testjar.TestUserClassLoaderJob.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.testjar.TestJobInfo.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.JarManifestParserTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.ClassPathPackagedProgramRetrieverTest.java</file>
      <file type="M">flink-container.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-container.src.test.assembly.test-assembly-test-user-classloader-job-lib-jar.xml</file>
      <file type="M">flink-container.src.test.assembly.test-assembly-test-user-classloader-job-jar.xml</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.JarManifestParser.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.ClassPathPackagedProgramRetriever.java</file>
      <file type="M">flink-container.pom.xml</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendTestUtils.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnApplicationClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.util.DummyClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.executors.EmbeddedExecutor.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.java</file>
    </fixedFiles>
  </bug>
  <bug id="16669" opendate="2020-3-19 00:00:00" fixdate="2020-4-19 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF in SQL function DDL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateTempSystemFunctionOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.FunctionDefinitionUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonGatewayServer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
    </fixedFiles>
  </bug>
  <bug id="16687" opendate="2020-3-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF in Java Correlate</summary>
      <description> I try to run PyFlink UDF with SQL UNNEST, execution of job failed, I defined a source from element, and use UDF split the string to list.raise org.codehaus.commons.compiler.CompileException: Cannot determine simple type name "PythonScalarFunction$0"import osfrom pyflink.table.udf import udffrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import StreamTableEnvironment, DataTypes, CsvTableSink@udf(input_types=[DataTypes.STRING()], result_type=DataTypes.ARRAY(DataTypes.STRING()))def format_string_to_array(item): return item.replace('[', '').replace(']', '').replace(', ', ',').split(',')if __name__ == '__main__': env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(1) st_env = StreamTableEnvironment.create(env) result_file = "result.csv" if os.path.exists(result_file): os.remove(result_file) st_env.register_table_sink("result_tab", CsvTableSink(["id", "url"], [DataTypes.STRING(), DataTypes.STRING()], result_file)) st_env.register_function("format_string_to_array", format_string_to_array) tab = st_env.from_elements([("1", "['www.bing.com', 'www.google.com']"), ("2", "['www.taobao.com']")], ['id', 'urls']) st_env.register_table("temp_table", tab) st_env.sql_query("Select id, A.url from temp_table, UNNEST(format_string_to_array(temp_table.urls)) AS A(url)").insert_into("result_tab") st_env.execute("udf") When I execute the program, I get the following exception: py4j.protocol.Py4JJavaError: An error occurred while calling o2.execute.: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 5d63838ad2043bf4a5d0bca83623959d) at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1640) at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620) at org.apache.flink.table.executor.StreamExecutor.execute(StreamExecutor.java:50) at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:643) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 5d63838ad2043bf4a5d0bca83623959d) at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:874) at akka.dispatch.OnComplete.internal(Future.scala:264) at akka.dispatch.OnComplete.internal(Future.scala:261) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110) ... 31 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ... 4 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:58) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1007) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)Caused by: org.codehaus.commons.compiler.CompileException: Line 6, Column 31: Cannot determine simple type name "PythonScalarFunction$0" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6746) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6507) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6486) at org.codehaus.janino.UnitCompiler.access$13800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6394) at org.codehaus.janino.UnitCompiler$21$1.visitReferenceType(UnitCompiler.java:6389) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3917) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6389) at org.codehaus.janino.UnitCompiler$21.visitType(UnitCompiler.java:6382) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3916) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$24.getType(UnitCompiler.java:8184) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6786) at org.codehaus.janino.UnitCompiler.access$14300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$2$1.visitFieldAccess(UnitCompiler.java:6412) at org.codehaus.janino.UnitCompiler$21$2$1.visitFieldAccess(UnitCompiler.java:6407) at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4299) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6407) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6403) at org.codehaus.janino.Java$Lvalue.accept(Java.java:4137) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6403) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6382) at org.codehaus.janino.Java$Rvalue.accept(Java.java:4105) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6768) at org.codehaus.janino.UnitCompiler.access$14100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$21$2$1.visitAmbiguousName(UnitCompiler.java:6410) at org.codehaus.janino.UnitCompiler$21$2$1.visitAmbiguousName(UnitCompiler.java:6407) at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4213) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6407) at org.codehaus.janino.UnitCompiler$21$2.visitLvalue(UnitCompiler.java:6403) at org.codehaus.janino.Java$Lvalue.accept(Java.java:4137) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6403) at org.codehaus.janino.UnitCompiler$21.visitRvalue(UnitCompiler.java:6382) at org.codehaus.janino.Java$Rvalue.accept(Java.java:4105) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6382) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8939) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) ... 13 more</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="16692" opendate="2020-3-20 00:00:00" fixdate="2020-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink joblistener can register from config</summary>
      <description>we should do as spark does ,which can register listener from conf such as "spark.extraListeners"。 And it will be convinient for users when users just want to set hook</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DeploymentOptions.java</file>
      <file type="M">docs..includes.generated.deployment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="16697" opendate="2020-3-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CVE-2020-1960] Disable JMX rebinding</summary>
      <description>Disable JMX rebinding.</description>
      <version>1.10.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16699" opendate="2020-3-20 00:00:00" fixdate="2020-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support accessing secured services via K8s secrets</summary>
      <description>Kubernetes Secrets can be used to provide credentials for a Flink application to access secured services.  This ticket proposes to Support to mount user-specified K8s Secrets into the JobManager/TaskManager Container Support to use a user-specified K8s Secret through an environment variable.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="16711" opendate="2020-3-23 00:00:00" fixdate="2020-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet columnar row reader read footer from wrong end</summary>
      <description>readFooter(conf, path, range(splitStart, splitLength))Should be:readFooter(conf, path, range(splitStart, splitStart + splitLength)) </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16727" opendate="2020-3-23 00:00:00" fixdate="2020-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix cast exception when having time point literal as parameters</summary>
      <description>I defined as ScalarFunction as follow: public class DateFunc extends ScalarFunction { public String eval(Date date) { return date.toString(); } @Override public TypeInformation&lt;?&gt; getResultType(Class&lt;?&gt;[] signature) { return Types.STRING; } @Override public TypeInformation&lt;?&gt;[] getParameterTypes(Class&lt;?&gt;[] signature) { return new TypeInformation[]{Types.INT}; }}I ues it in sql: `select func(DATE '2020-11-12') as a from source` , Flink throws 'cannot cast 2020-11-12 as class java.time.LocalDate ' The full code is in the Flinktest.zip Main class is com.lorinda.template.TestDateFunction</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16766" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support create StreamTableEnvironment without passing StreamExecutionEnvironment</summary>
      <description>Currently, when we create a BatchTableEnvironment, the ExecutionEnvironment is an optional parameter, while for the StreamTableEnvironment, the ExecutionEnvironment is not optional. We should make them consistent</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="16767" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to read Hive table with RegexSerDe</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveMapredSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTablePartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="16768" opendate="2020-3-25 00:00:00" fixdate="2020-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16771" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when filtering by decimal column</summary>
      <description>The following SQL can trigger the issue:create table foo (d decimal(15,8));insert into foo values (cast('123.123' as decimal(15,8)));select * from foo where d&gt;cast('123456789.123' as decimal(15,8));</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16772" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump derby to 10.12.1.1+ or exclude it</summary>
      <description>hive-metastore depends on derby 10.10/10.4, which are vulnerable to CVE-2015-1832.We should bump the version to at least 10.12.1.1 .Assuming that derby is only required for the server and not the client we could potentially even exclude it.phoenixjiangnan Can you help with this?</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16782" opendate="2020-3-25 00:00:00" fixdate="2020-5-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Avoid unnecessary check on expired time state when we set visibility as ReturnExpiredIfNotCleanedUp</summary>
      <description>Current implementation of getting unexpired value would always check whether this ttl value is expired first:// code placeholder&lt;V&gt; V getUnexpired(TtlValue&lt;V&gt; ttlValue) { return ttlValue == null || (expired(ttlValue) &amp;&amp; !returnExpired) ? null : ttlValue.getUserValue();}However, this check could be avoided if we set return expired time, we could improve the performance by check whether to returnExpired first.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.AbstractTtlDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16786" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix pyarrow version incompatible problem</summary>
      <description>As reported in FLINK-16483, we should make the version of pyarrow consistent between pyflink and beam. Other dependencies should also be checked.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="1680" opendate="2015-3-10 00:00:00" fixdate="2015-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Flink dependencies for Tachyon to 0.6.0</summary>
      <description>Looks like Tachyon has released new long awaited 0.6.0 release &amp;#91;1&amp;#93;.Need to update the dependencies to the new version.&amp;#91;1&amp;#93; https://github.com/amplab/tachyon/releases/tag/v0.6.0</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-tachyon.src.test.resources.tachyonHadoopConf.xml</file>
      <file type="M">flink-staging.flink-tachyon.src.test.resources.log4j.properties</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.HDFSTest.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.FileStateHandleTest.java</file>
      <file type="M">flink-staging.flink-tachyon.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16802" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set schema info in JobConf for Hive readers</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16803" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to make sure partition inherit table spec when writing to Hive tables</summary>
      <description>When inserting to a partition, Hive will update the partition's SD according to table SD. For example, consider the following use case:create table foo (x int,y int) partitioned by (p string);insert overwrite table foo partition (p='1') select * from ...;alter table foo set fileformat rcfile;insert overwrite table foo partition (p='1') select * from ...;The second INSERT will write RC files to the partition and therefore the partition SD needs to be updated to use RC file input format. Otherwise we hit exception when reading from this partition.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.FileSystemCommitterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.TableMetaStoreFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionLoader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableMetaStoreFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16806" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Input selection for MultipleInputStreamOperator</summary>
      <description>After FLINK-16060 support for MultipleInputStreamOperator is incomplete. After defining new base class for the StreamOperator (FLINK-16316) that would be suitable to use with MultipleInputStreamOperator, we can provide support for input selection in the MultipleInputStreamOperator.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.MultipleInputSelectionHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16807" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve reporting of errors during resource initialization</summary>
      <description>The factories for resources currently returns Optionals for handling failed instantiations, which are an expected occurrence. The factories themselves are only logging the exception.This has the downside that no error is encoded in the optional, so if no resource can be instantiated we cannot enrich the final exception in any way.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.java.org.pache.flink.metrics.tests.MetricsAvailabilityITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.util.FactoryUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.TravisDownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.PersistingDownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.LolCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.cache.DownloadCacheFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.KafkaResourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16813" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBCInputFormat doesn&amp;#39;t correctly map Short</summary>
      <description>currently when JDBCInputFormat converts a JDBC result set row to Flink Row, it doesn't check the type returned from jdbc result set.Short from jdbc result set actually returns an Integer </description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.source.row.converter.JDBCRowConverter.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.source.row.converter.AbstractJDBCRowConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16815" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add e2e tests for reading primitive data types from postgres with JDBCTableSource and PostgresCatalog</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16822" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The config set by SET command does not work</summary>
      <description>Users can add or change the properties for execution behavior through SET command in SQL client CLI, e.g. SET execution.parallelism=10, SET table.optimizer.join-reorder-enabled=true. But the table.xx config can't change the TableEnvironment behavior, because the property set from CLI does not be set into TableEnvironment's table config.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="16823" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The functioin TIMESTAMPDIFF doesn&amp;#39;t perform expected result</summary>
      <description>For example,In mysql bellow sql get result 6, but in flink the output is 5SELECT timestampdiff (MONTH, TIMESTAMP '2019-09-01 00:00:00',TIMESTAMP '2020-03-01 00:00:00' )   </description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TimestampDiffCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1687" opendate="2015-3-11 00:00:00" fixdate="2015-5-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Streaming file source/sink API is not in sync with the batch API</summary>
      <description>Streaming environment is missing file inputs like readFile, readCsvFile and also the more general createInput function, and outputs like writeAsCsv and write. Streaming and batch API should be consistent.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileSourceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamtask.StreamVertexTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.ProjectTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.complex.ComplexIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.DirectedOutputTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.GenSequenceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FromIteratorFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="16871" opendate="2020-3-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make more build information available during runtime.</summary>
      <description>This is a split from FLINK-15794 where (as discussed here) a file is generated during the build with the properties that were valid at build time.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EnvironmentInformationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16914" opendate="2020-4-1 00:00:00" fixdate="2020-4-1 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support ArrayType in vectorized Python UDF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseArraySerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ColumnarRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.VarCharWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.VarBinaryWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TinyIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TimeWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TimestampWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.SmallIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.IntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.FloatWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DoubleWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DecimalWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DateWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BooleanWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BigIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowVarCharWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowVarBinaryWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowTinyIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowTimeWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowTimestampWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowSmallIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowFloatWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowDoubleWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowDecimalWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowDateWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowBooleanWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BaseRowBigIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16945" opendate="2020-4-2 00:00:00" fixdate="2020-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="16946" opendate="2020-4-2 00:00:00" fixdate="2020-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="16947" opendate="2020-4-2 00:00:00" fixdate="2020-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1695" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16970" opendate="2020-4-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle JMXReporter separately from dist jar</summary>
      <description>The JMXReporter is currently bundled in the flink-dist jar.There isn't a real benefit to this; it could just as well be bundled in /lib, or with FLINK-16222 even in /plugins .This would allow users to exclude the reporter from the distribution, for cases where they do not want anyone to use this reporter.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16971" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDF in SQL Client (FLIP-114)</summary>
      <description>Support defining Python UDF in the SQL client environment file and specifying Python Dependencies via command line options.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionService.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.FunctionDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.FunctionDescriptor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.FunctionDefinitionUtil.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.bin.sql-client.sh</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="16980" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF doesn&amp;#39;t work with protobuf 3.6.1</summary>
      <description>PyFlink UDF execution module is not compatible with protobuf 3.6.1 because it uses a newer interface to access the enum value defined in proto model. We need to fix this.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16981" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-runtime tests are crashing the JVM on Java11 because of PowerMock</summary>
      <description>Nightly travis run: https://travis-ci.org/github/apache/flink/jobs/670686286?utm_medium=notification&amp;utm_source=slack22:11:49.063 [INFO] Reactor Summary:22:11:49.063 [INFO] 22:11:49.068 [INFO] flink-annotations .................................. SUCCESS [ 4.733 s]22:11:49.069 [INFO] flink-metrics ...................................... SUCCESS [ 0.250 s]22:11:49.069 [INFO] flink-metrics-core ................................. SUCCESS [ 3.012 s]22:11:49.069 [INFO] flink-core ......................................... SUCCESS [01:34 min]22:11:49.069 [INFO] flink-java ......................................... SUCCESS [ 30.494 s]22:11:49.074 [INFO] flink-runtime ...................................... FAILURE [25:01 min]22:11:49.074 [INFO] flink-scala ........................................ SKIPPED22:11:49.074 [INFO] flink-optimizer .................................... SKIPPED22:11:49.074 [INFO] flink-clients ...................................... SKIPPED22:11:49.074 [INFO] flink-streaming-java ............................... SKIPPED22:11:49.074 [INFO] flink-test-utils ................................... SKIPPED22:11:49.074 [INFO] flink-runtime-web .................................. SKIPPED22:11:49.074 [INFO] flink-statebackend-rocksdb ......................... SKIPPED22:11:49.074 [INFO] flink-streaming-scala .............................. SKIPPED22:11:49.074 [INFO] flink-scala-shell .................................. SKIPPED22:11:49.074 [INFO] ------------------------------------------------------------------------22:11:49.074 [INFO] BUILD FAILURE22:11:49.074 [INFO] ------------------------------------------------------------------------22:11:49.074 [INFO] Total time: 27:20 min22:11:49.077 [INFO] Finished at: 2020-04-03T22:11:49+00:0022:11:49.355 [INFO] Final Memory: 97M/330M22:11:49.355 [INFO] ------------------------------------------------------------------------22:11:49.361 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-runtime_2.11: There are test failures.22:11:49.362 [ERROR] 22:11:49.362 [ERROR] Please refer to /home/travis/build/apache/flink/flink-runtime/target/surefire-reports for the individual test results.22:11:49.362 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.22:11:49.362 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime &amp;&amp; /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp22:11:49.362 [ERROR] Error occurred in starting fork, check output in log22:11:49.362 [ERROR] Process Exit Code: 23922:11:49.362 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime &amp;&amp; /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp22:11:49.362 [ERROR] Error occurred in starting fork, check output in log22:11:49.362 [ERROR] Process Exit Code: 23922:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)22:11:49.362 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)22:11:49.363 [ERROR] at java.base/java.lang.reflect.Method.invoke(Method.java:566)22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)22:11:49.363 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?22:11:49.363 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime &amp;&amp; /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp22:11:49.363 [ERROR] Error occurred in starting fork, check output in log22:11:49.363 [ERROR] Process Exit Code: 23922:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)22:11:49.363 [ERROR] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)22:11:49.363 [ERROR] at java.base/java.lang.Thread.run(Thread.java:834)22:11:49.363 [ERROR] -&gt; [Help 1]22:11:49.363 [ERROR] 22:11:49.363 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.22:11:49.363 [ERROR] Re-run Maven using the -X switch to enable full debug logging.22:11:49.363 [ERROR] 22:11:49.363 [ERROR] For more information about the errors and possible solutions, please read the following articles:22:11:49.363 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException22:11:49.363 [ERROR] 22:11:49.363 [ERROR] After correcting the problems, you can resume the build with the command22:11:49.364 [ERROR] mvn &lt;goals&gt; -rf :flink-runtime_2.11</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17012" opendate="2020-4-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose stage of task initialization</summary>
      <description>Currently a task switches to running before fully initialized, does not take state initialization and operator initialization(#open ) in to account, which may take long time to finish. As a result, there would be a weird phenomenon that all tasks are running but throughput is 0. I think it could be good if we can expose the initialization stage of tasks. What to you think?</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerBatchSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.webmonitor.JobDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.UpdatePartitionConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteChannelStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.ExecutionState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-overview.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17013" opendate="2020-4-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDTF in old planner under batch mode</summary>
      <description>Currently, Python UDTF has been supported under flink planner(only stream) and blink planner. This jira dedicates to add Python UDTF support for flink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonTableFunction.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug id="17073" opendate="2020-4-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slow checkpoint cleanup causing OOMs</summary>
      <description>A user reported that he sees a decline in checkpoint cleanup speed when upgrading from Flink 1.7.2 to 1.10.0. The result is that a lot of cleanup tasks are waiting in the execution queue occupying memory. Ultimately, the JM process dies with an OOM.Compared to Flink 1.7.2, we introduced a dedicated ioExecutor which is used by the HighAvailabilityServices (FLINK-11851). Before, we use the AkkaRpcService thread pool which was a ForkJoinPool with a max parallelism of 64. Now it is a FixedThreadPool with as many threads as CPU cores. This change might have caused the decline in completed checkpoint discard throughput. This suspicion needs to be validated before trying to fix it!&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r390e5d775878918edca0b6c9f18de96f828c266a888e34ed30ce8494%40%3Cuser.flink.apache.org%3E</description>
      <version>1.7.3,1.8.0,1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RegionFailoverITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.CheckpointsUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SerializableRunnable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaningRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="17093" opendate="2020-4-12 00:00:00" fixdate="2020-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF doesn&amp;#39;t work when the input column is from composite field</summary>
      <description>For the following job:from pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import BatchTableEnvironment, StreamTableEnvironment, EnvironmentSettings, CsvTableSinkfrom pyflink.table.descriptors import Schema, Kafka, Jsonfrom pyflink.table import DataTypesfrom pyflink.table.udf import ScalarFunction, udfimport os@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], result_type=DataTypes.STRING())def get_host_ip(source, qr, sip, dip):    if source == "NGAF" and qr == '1':        return dip    return sip@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], result_type=DataTypes.STRING())def get_dns_server_ip(source, qr, sip, dip):    if source == "NGAF" and qr == '1':        return sip    return dipdef test_case():    env = StreamExecutionEnvironment.get_execution_environment()    env.set_parallelism(1)    t_env = StreamTableEnvironment.create(env)     from pyflink.table import Row   table = t_env.from_elements(      [("DNS", Row(source="source", devid="devid", sip="sip", dip="dip", qr="qr", queries="queries", answers="answers", qtypes="qtypes", atypes="atypes", rcode="rcode", ts="ts",))],    DataTypes.ROW([DataTypes.FIELD("stype", DataTypes.STRING()), DataTypes.FIELD("data", DataTypes.ROW([DataTypes.FIELD('source', DataTypes.STRING()), DataTypes.FIELD("devid", DataTypes.STRING()), DataTypes.FIELD('sip', DataTypes.STRING()), DataTypes.FIELD('dip', DataTypes.STRING()), DataTypes.FIELD("qr", DataTypes.STRING()), DataTypes.FIELD("queries", DataTypes.STRING()), DataTypes.FIELD("answers", DataTypes.STRING()), DataTypes.FIELD("qtypes", DataTypes.STRING()), DataTypes.FIELD("atypes", DataTypes.STRING()), DataTypes.FIELD("rcode", DataTypes.STRING()), DataTypes.FIELD("ts", DataTypes.STRING())])) ] )) result_file = "/tmp/test.csv" if os.path.exists(result_file): os.remove(result_file) t_env.register_table_sink("Results", CsvTableSink(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'], [DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()], "/tmp/test.csv")) t_env.register_function("get_host_ip", get_host_ip) t_env.register_function("get_dns_server_ip", get_dns_server_ip) t_env.register_table("source", table) standard_table = t_env.sql_query("select data.*, stype as dns_type from source")\ .where("dns_type.in('DNSFULL', 'DNS', 'DNSFULL_FROM_LOG', 'DNS_FROM_LOG')") t_env.register_table("standard_table", standard_table) final_table = t_env.sql_query("SELECT *, get_host_ip(source, qr, sip, dip) as host_ip," "get_dns_server_ip(source, qr, sip, dip) as dns_server_ip FROM standard_table") final_table.insert_into("Results") t_env.execute("test")if __name__ == '__main__': test_case()The plan is as following which is not correct: org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: KafkaTableSource(type, data) -&gt; Map -&gt; where: (IN(type, _UTF-16LE'DNSFULL', _UTF-16LE'DNS', _UTF-16LE'DNSFULL_FROM_LOG', _UTF-16LE'DNS_FROM_LOG')), select: (data, type) -&gt; select: (type, get_host_ip(type.source, type.qr, type.sip, type.dip) AS f0, get_dns_server_ip(type.source, type.qr, type.sip, type.dip) AS f1) -&gt; select: (f0.source AS source, f0.devid AS devid, f0.sip AS sip, f0.dip AS dip, f0.qr AS qr, f0.queries AS queries, f0.answers AS answers, f0.qtypes AS qtypes, f0.atypes AS atypes, f0.rcode AS rcode, f0.ts AS ts, type AS dns_type, f0 AS host_ip, f1 AS dns_server_ip) -&gt; to: Row -&gt; Sink: KafkaTableSink(source, devid, sip, dip, qr, queries, answers, qtypes, atypes, rcode, ts, dns_type, host_ip, dns_server_ip) (1/4) (8d064ab137866a2a9040392a87bcc59d) switched from RUNNING to FAILED.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcPythonCorrelateTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="17096" opendate="2020-4-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mini-batch group aggregation doesn&amp;#39;t expire state even if state ttl is enabled</summary>
      <description>At the moment, MiniBatch Group Agg include Local/Global doesn`t support State TTL, for streaming job, it will lead to OOM in long time running, so we need to make state data expire after ttl, the solution is that use incremental cleanup feature refer to FLINK-16581</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingSemiAntiJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchIncrementalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.StateListView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.PerKeyStateDataViewStore.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17106" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support create/drop view in Flink SQL</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogManagerTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropView.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateView.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="17107" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17111" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SHOW VIEWS in Flink SQL</summary>
      <description>SHOW TABLES and SHOW VIEWS are not SQL standard-compliant commands. MySQL supports SHOW TABLES which lists the non-TEMPORARY tables(and views) in a given database, and doesn't support SHOW VIEWS.Oracle/SQL Server/PostgreSQL don't support SHOW TABLES and SHOW VIEWS. A workaround is to query a system table which stores metadata of tables and views.Hive supports both SHOW TABLES and SHOW VIEWS. We follows the Hive style which lists all tables and views with SHOW TABLES and lists only views with SHOW VIEWS. </description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="17112" opendate="2020-4-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DESCRIBE view_name in Flink SQL</summary>
      <description>FLINK-14688 introduces DESCRIBE statement in sql parser, but doesn't implement it in planner side because the TableEnvironment.sqlUpdate returns nothing. Since FLINK-16366 introduces TableEnvironment.executeSql and returns TableResult, we can implement DESCRIBE statement in planner now.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17113" opendate="2020-4-13 00:00:00" fixdate="2020-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use executeSql to execute view statements and fix nullability loss problem</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.ViewExpansionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17114" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When the pyflink job runs in local mode and the command "python" points to Python 2.7, the startup of the Python UDF worker will fail.</summary>
      <description>When the PyFlink job runs in local mode and the command "python" points to Python 2.7, the startup of the Python UDF worker will fail because "python" is the default interpreter of the Python UDF worker. For this case we need to set the default value of "python.executable" to `sys.executable` i.e. the python interpreter which launches the job.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.utils.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="17119" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Cython support for composite types</summary>
      <description>Support Composite DataTypes in Cython</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="17125" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Usage Notes Page to Answer Common Questions Encountered by PyFlink Users</summary>
      <description>There are several common problems that PyFlink new users often encounter. We need to support usage notes to help them solve these problems quickly.</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17156" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkpoint cancellation to Unaligner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17170" opendate="2020-4-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot stop streaming job with savepoint which uses kinesis consumer</summary>
      <description>I am encountering a very strange situation where I can't stop with savepoint a streaming job.The job reads from kinesis and sinks to S3, very simple job, no mapping function, no watermarks, just source-&gt;sink. Source is using flink-kinesis-consumer, sink is using StreamingFileSink. Everything works fine, except stopping the job with savepoints.The behaviour happens only when multiple task managers are involved, having sub-tasks off the job spread across multiple task manager instances. When a single task manager has all the sub-tasks this issue never occurred.Using latest Flink 1.10.0 version, deployment done in HA mode (2 job managers), in EC2, savepoints and checkpoints written on S3.When trying to stop, the savepoint is created correctly and appears on S3, but not all sub-tasks are stopped. Some of them finished, but some just remain hanged. Sometimes, on the same task manager part of the sub-tasks are finished, part aren't.The logs don't show any errors. For the ones that succeed, the standard messages appear, with "Source: &lt;....&gt; switched from RUNNING to FINISHED".For the sub-tasks hanged the last message is "org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher - Shutting down the shard consumer threads of subtask 0 ..." and that's it. I tried using the cli (flink stop &lt;job_id&gt;)Timeout Message:root@ec2-XX-XX-XX-XX:/opt/flink/current/bin# ./flink stop cf43cecd9339e8f02a12333e52966a25root@ec2-XX-XX-XX-XX:/opt/flink/current/bin# ./flink stop cf43cecd9339e8f02a12333e52966a25Suspending job "cf43cecd9339e8f02a12333e52966a25" with a savepoint. ------------------------------------------------------------ The program finished with the following exception: org.apache.flink.util.FlinkException: Could not stop with a savepoint job "cf43cecd9339e8f02a12333e52966a25". at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:462) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:843) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:454) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:907) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:460) ... 9 more Using the monitoring api, I keep getting infinite message when querying based on the savepoint id, that the status id is still "IN_PROGRESS". When performing a cancel instead of stop, it works. But cancel is deprecated, so I am a bit concerned that this might fail also, maybe I was just lucky. I attached a screenshot with what the UI is showing when this happens </description>
      <version>1.10.0,1.11.3,1.12.2</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.containers.KinesaliteContainer.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.main.resources.org.apache.flink.streaming.kinesis.test.filter-large-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1718" opendate="2015-3-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sparse vector and sparse matrix types to machine learning library</summary>
      <description>Currently, the machine learning library only supports dense matrix and dense vectors. For future algorithms it would be beneficial to also support sparse vectors and matrices.I'd propose to use the compressed sparse column (CSC) representation, because it allows rather efficient operations compared to a map backed sparse matrix/vector implementation. Furthermore, this is also the format the Breeze library expects for sparse matrices/vectors. Thus, it is easy to convert to a sparse breeze data structure which provides us with many linear algebra operations.BIDMat &amp;#91;1&amp;#93; uses the same data representation.Resources:&amp;#91;1&amp;#93; https://github.com/BIDData/BIDMat</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseMatrixTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.BreezeMathTest.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1720" opendate="2015-3-18 00:00:00" fixdate="2015-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate ScalaDoc in Scala sources into overall JavaDoc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-hcatalog.pom.xml</file>
      <file type="M">flink-staging.flink-expressions.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1722" opendate="2015-3-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming not respecting FinalizeOnMaster for output formats</summary>
      <description>The Hadoop output formats execute a process in the end to move the produced files from a temp directory to the final location.The batch API is annotating output formats that execute something in the end with the FinalizeOnMaster interface.The streaming component is not respecting this interface. Hence, HadoopOutputFormats aren't writing their final data into the desired destination.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SimpleOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobTaskVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.OutputFormatVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.InputFormatVertex.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17232" opendate="2020-4-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rethink the implicit behavior to use the Service externalIP as the address of the Endpoint</summary>
      <description>Currently, for the LB/NodePort type Service, if we found that the LoadBalancer in the Service is null, we would use the externalIPs configured in the external Service as the address of the Endpoint. Again, this is another implicit toleration and may confuse the users.This ticket proposes to rethink the implicit toleration behaviour.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.LoadBalancerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="17273" opendate="2020-4-20 00:00:00" fixdate="2020-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix not calling ResourceManager#closeTaskManagerConnection in KubernetesResourceManager in case of registered TaskExecutor failure</summary>
      <description>At the moment, the KubernetesResourceManager does not call the method of ResourceManager#closeTaskManagerConnection once it detects that a currently registered task executor has failed. This ticket propoeses to fix this problem.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.TestingResourceEventHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ResourceEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.TestingKubernetesPod.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  <bug id="17311" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the logic of compressed in tgz before uploading artifacts</summary>
      <description> Script files in packages downloaded from Azure will lose executable permissions https://github.com/microsoft/azure-pipelines-tasks/issues/6364.So We need to add the logic of compressing the built result to tgz before uploading artifacts</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">tools.azure-pipelines.build-python-wheels.yml</file>
    </fixedFiles>
  </bug>
  <bug id="17327" opendate="2020-4-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka unavailability could cause Flink TM shutdown</summary>
      <description>Steps to reproduce: Start a Flink 1.10 standalone cluster Run a Flink job which reads from one Kafka topic and writes to another topic, with exactly-once checkpointing enabled Stop all Kafka Brokers after a few successful checkpointsWhen Kafka brokers are down: org.apache.kafka.clients.NetworkClient reported connection to broker could not be established Then, Flink could not complete snapshot due to Timeout expired while initializing transactional state in 60000ms After several snapshot failures, Flink reported Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints. Eventually, Flink tried to cancel the task which did not succeed within 3 min. According to logs, consumer was cancelled, but producer is still running Then Fatal error occurred while executing the TaskManager. Shutting it down...I will attach the logs to show the details.  Worth to note that if there would be no consumer but producer only in the task, the behavior is different: org.apache.kafka.clients.NetworkClient reported connection to broker could not be established after delivery.timeout.ms (2min by default), producer reports: FlinkKafkaException: Failed to send data to Kafka: Expiring 4 record(s) for output-topic-0:120001 ms has passed since batch creation Flink tried to cancel the upstream tasks and created a new producer The new producer obviously reported connectivity issue to brokers This continues till Kafka brokers are back.  Flink reported Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints. Flink cancelled the tasks and restarted them The job continues, and new checkpoint succeeded.  TM runs all the time in this scenarioI set Kafka transaction time out to 1 hour just to avoid transaction timeout during the test.To get a producer only task, I called env.disableOperatorChaining(); in the second scenario.    </description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
    </fixedFiles>
  </bug>
  <bug id="17330" opendate="2020-4-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid scheduling deadlocks caused by cyclic input dependencies between regions</summary>
      <description>Imagine a job like this:A &amp;#8211; (pipelined FORWARD) --&gt; B &amp;#8211; (blocking ALL-to-ALL) --&gt; DA &amp;#8211; (pipelined FORWARD) --&gt; C &amp;#8211; (pipelined FORWARD) --&gt; Dparallelism=2 for all vertices.We will have 2 execution pipelined regions:R1 = {A1, B1, C1, D1}R2 = {A2, B2, C2, D2}R1 has a cross-region input edge (B2-&gt;D1).R2 has a cross-region input edge (B1-&gt;D2).Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17332" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix restart policy not equals to Never for native task manager pods</summary>
      <description>Currently, we do not explicitly set the RestartPolicy for the task manager pods in the native K8s setups so that it is Always by default.  The task manager pod itself should not restart the failed Container, the decision should always be made by the job manager.Therefore, this ticket proposes to set the RestartPolicy to Never for the task manager pods.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17339" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default planner to blink and update test cases in both planners</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.EnvironmentSettings.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogViewITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogConstraintTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.TableUtilsStreamingITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.EnvironmentTest.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamWindowSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.StreamTableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.AggFunctionHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.MatchHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.RetractionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.MLEnvironment.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.MLEnvironmentTest.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.settings.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
    </fixedFiles>
  </bug>
  <bug id="17357" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>add "DROP catalog" DDL to blink planner</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP+69+-+Flink+SQL+DDL+Enhancementsome customers who have internal streaming platform requested this feature, as it's not possible on a platform to load catalogs dynamically at runtime now via sql client yaml. Catalog DDL will come into play(Similarly to https://issues.apache.org/jira/browse/FLINK-15349)</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateCatalog.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="17385" opendate="2020-4-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix precision problem when converting JDBC numberic into Flink decimal type</summary>
      <description>This is reported in the mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/JDBC-error-on-numeric-conversion-because-of-DecimalType-MIN-PRECISION-td34668.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17442" opendate="2020-4-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Cannot convert String or boxed-primitive arrays to DataStream using TypeInformation</summary>
      <description>It seems to be impossible to convert String or boxed primitive array types from table back to DataStream by specifying type info (inside a Tuple for instance).We get the following error:Query schema: &amp;#91;f0: ARRAY&lt;STRING&gt;&amp;#93;Sink schema: [f0: LEGACY('ARRAY', 'ANY&lt;[Ljava.lang.String;,....</description>
      <version>1.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.external.resources.md</file>
    </fixedFiles>
  </bug>
  <bug id="17448" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement ALTER TABLE for Hive dialect</summary>
      <description>Will cover ALTER table in this ticket</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableProperties.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTable.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17449" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement ADD/DROP partitions</summary>
      <description>Introduce ADD/DROP partitions operations. Will only implement syntax for the Hive parser in this ticket.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17450" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement function &amp; catalog DDLs for Hive dialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17451" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement view DDLs for Hive dialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateView.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17452" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support creating Hive tables with constraints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="17460" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create sql-jars for parquet and orc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="17461" opendate="2020-4-29 00:00:00" fixdate="2020-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JSON serialization and deseriazation schema for RowData type</summary>
      <description>Add support JsonRowDataDeserializationSchema and JsonRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17462" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CSV serialization and deseriazation schema for RowData type</summary>
      <description>Add support CsvRowDataDeserializationSchema and CsvRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSchemaConverter.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17466" opendate="2020-4-29 00:00:00" fixdate="2020-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>toRetractStream doesn&amp;#39;t work correctly with Pojo conversion class</summary>
      <description>The toRetractStream(table, Pojo.class) does not map the query columns properly to the pojo fields.This either leads to exceptions due to type incompatibility or simply incorrect results.It can be simple reproduced by the following test code:@Testpublic void testRetract() throws Exception { EnvironmentSettings settings = EnvironmentSettings .newInstance() .useBlinkPlanner() .inStreamingMode() .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment .create(StreamExecutionEnvironment.getExecutionEnvironment(), settings); tableEnv.createTemporaryView("person", env.fromElements(new Person())); tableEnv.toRetractStream(tableEnv.sqlQuery("select name, age from person"), Person.class).print(); tableEnv.execute("Test");}public static class Person { public String name = "bob"; public int age = 1;}Runtime Error:java.lang.ClassCastException: org.apache.flink.table.dataformat.BinaryString cannot be cast to java.lang.IntegerChanging the query to "select age,name from person" in this case would resolve the problem but it also highlights the possible underlying issue.</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamTableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaPojos.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17471" opendate="2020-4-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LICENSE and NOTICE files to root directory of python distribution</summary>
      <description>This is observed and proposed by Robert during 1.10.1 RC1 check:Another question that I had while checking the release was the"apache-flink-1.10.1.tar.gz" binary, which I suppose is the pythondistribution.It does not contain a LICENSE and NOTICE file at the root level (which isokay [1] for binary releases), but in the "pyflink/" directory. There isalso a "deps/" directory, which contains a full distribution of Flink,without any license files.I believe it would be a little bit nicer to have the LICENSE and NOTICEfile in the root directory (if the python wheels format permits) to makesure it is obvious that all binary release contents are covered by thesefiles.http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40910.html</description>
      <version>1.9.3,1.10.0,1.11.0</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.MANIFEST.in</file>
    </fixedFiles>
  </bug>
  <bug id="17474" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Test and correct case insensitive for parquet and orc in hive</summary>
      <description>Orc and parquet should be field names case insensitive to compatible with hive.Both hive mapred reader and vectorization reader.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveVectorizedParquetSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17476" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests to check recovery from snapshot created with different UC mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17480" opendate="2020-4-30 00:00:00" fixdate="2020-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support running PyFlink on Kubernetes</summary>
      <description>This is the umbrella issue for running PyFlink on Kubernetes in native mode.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.cli.PythonProgramOptionsTest.java</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.application.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ClassPathPackagedProgramRetriever.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationClusterEntryPoint.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptionsUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="17483" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update flink-sql-connector-elasticsearch7 NOTICE file to correctly reflect bundled dependencies</summary>
      <description>This issue is found during 1.10.1 RC1 check by Robert, that `com.carrotsearch:hppc` and `com.github:mustachejava` were included into the shaded binary to fix FLINK-16170 but not added into the NOTICE file of flink-sql-connector-elasticsearch7 module. More details please refer to the ML discussion thread.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="17495" opendate="2020-5-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for custom variables</summary>
      <description>Allow users to configure additional labels, presumably via a new config option metrics.scope.variables.add.E.g.,metrics.scope.variables.add: key1:value1;key2:value2 Configured variables should be added to the variables map of the root metric group; this may allow us in the future (once we have generalized scope formats a bit) to include them there as well. Original description:We need to add some custom labels on Prometheus, so we can query by them.?? ??Now we can add jobName\groupingKey to PrometheusPushGatewayReporter in version 1.10, but not in PrometheusReporter.Can we add AbstractPrometheusReporter#addDimension method to support this, so they will be no differences except for the metrics exposing mechanism pulling/pushing.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.ReporterSetup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistryImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.ReporterScopedSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug id="17496" opendate="2020-5-2 00:00:00" fixdate="2020-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression with amazon-kinesis-producer 0.13.1 in Flink 1.10.x</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17544" opendate="2020-5-6 00:00:00" fixdate="2020-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE JDBCUpsertOutputFormat</summary>
      <description>Encountered a situation where I get an NPE from JDBCUpsertOutputFormat. This occurs when close is called before open.This happened because I had a sink where it had a final field of type JDBCUpsertOutputFormat.The open operation of my sink was slow (blocked on something else) and open on the JDBCUpsertOutputFormat had not yet been called. In the mean time the job was cancelled, which caused close on my sink to be called, which then called close on the JDBCUpsertOutputFormat . This throws an NPE due to a lack of a guard on an internal field that is only initialised in the JDBCUpsertOutputFormat open operation.The close method already guards one potentially null value ..if (this.scheduledFuture != null) {But needs the additional guard below ...if (jdbcWriter != null) // &lt;&lt; THIS LINE NEEDED TO GUARD UNINITIALISE VAR try { jdbcWriter.close(); } catch (SQLException e) { LOG.warn("Close JDBC writer failed.", e); }See also FLINK-17545</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcFullTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.TableJdbcUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17578" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Union of 2 SideOutputs behaviour incorrect</summary>
      <description>Strange behaviour when using union() to merge outputs of 2 DataStreams, where both are sourced from SideOutputs.See example code with comments demonstrating the issue: def main(args: Array[String]): Unit = { val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val input = env.fromElements(1, 2, 3, 4) val oddTag = OutputTag[Int]("odds") val evenTag = OutputTag[Int]("even") val all = input.process { (value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]) =&gt; { if (value % 2 != 0) ctx.output(oddTag, value) else ctx.output(evenTag, value) } } val odds = all.getSideOutput(oddTag) val evens = all.getSideOutput(evenTag) // These print correctly // odds.print // -&gt; 1, 3 evens.print // -&gt; 2, 4 // This prints incorrectly - BUG? // odds.union(evens).print // -&gt; 2, 2, 4, 4 evens.union(odds).print // -&gt; 1, 1, 3, 3 // Another test to understand normal behaviour of .union, using normal inputs // val odds1 = env.fromElements(1, 3) val evens1 = env.fromElements(2, 4) // Union of 2 normal inputs // odds1.union(evens1).print // -&gt; 1, 2, 3, 4 // Union of a normal input plus an input from a sideoutput // odds.union(evens1).print // -&gt; 1, 2, 3, 4 evens1.union(odds).print // -&gt; 1, 2, 3, 4 // // So it seems that when both inputs are from sideoutputs that it behaves incorrectly... BUG? env.execute("Test job") }</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SideOutputITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.SideOutputITCase.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamEdge.java</file>
    </fixedFiles>
  </bug>
  <bug id="17579" opendate="2020-5-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the resource id of taskexecutor according to environment variable if exist in standalone mode</summary>
      <description>Allow user to specify the resource id of TaskExecutor through the environment variable in standalone mode. The name of that variable could be FLINK_STANDALONE_TASK_EXECUTOR_ID</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.taskmanager.KubernetesTaskExecutorRunner.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManager.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.configuration.html</file>
      <file type="M">docs..includes.generated.all.taskmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="17657" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix reading BIGINT UNSIGNED type field not work in JDBC</summary>
      <description>I use sql client read mysql table, but I found I can't read a table contain `BIGINT UNSIGNED` field. It will  Caused by: java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.lang.Long MySQL table: create table tb ( id BIGINT UNSIGNED auto_increment  primary key, cooper BIGINT(19) null ,user_sex VARCHAR(2) null ); my env yaml is env.yaml .</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17661" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add APIs for using new WatermarkStrategy/WatermarkGenerator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.IngestionTimeExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="17701" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk:tools dependency from all Hadoop dependencies for Java 9+ compatibility</summary>
      <description>Hadoop transitively pulls the system dependency jdk:tools which is not longer available on Java 9+. This causes errors when importing the code into an IDE with runs Java 11.This dependency is anyways not needed when running the code, because the classes are always present. It can be safely excluded form the transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17702" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorCoordinators must be notified of tasks cancelled as part of failover</summary>
      <description>The OperatorCoordinators are currently only notified of tasks that directly fail.However, tasks that are cancelled (as part of the regional failover) must be handled the same was and also send notifications to the OperatorCoordinator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17715" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports function DDLs in SQL-CLI</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17748" opendate="2020-5-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove registration of TableSource/TableSink in Table Env</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.src.main.scala.SpendReport.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.JavaTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.source.HBaseTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.test.java.org.apache.flink.connector.hbase.HBaseConnectorITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-batch-sql-test.src.main.java.org.apache.flink.sql.tests.BatchSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcTableSource.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcTableSourceITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetTableSource.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetTableSourceITCase.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.descriptors.ConnectTableDescriptor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.LegacyTableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17763" opendate="2020-5-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No log files when starting scala-shell</summary>
      <description>I see the following error when starting scala shell. Starting Flink Shell:ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2</description>
      <version>1.9.2,1.10.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17800" opendate="2020-5-18 00:00:00" fixdate="2020-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB optimizeForPointLookup results in missing time windows</summary>
      <description>My Setup:We have been using the RocksDb option of optimizeForPointLookup and running version 1.7 for years. Upon upgrading to Flink 1.10 we started receiving a strange behavior of missing time windows on a streaming Flink job. For the purpose of testing I experimented with previous Flink version and (1.8, 1.9, 1.9.3) and non of them showed the problem A sample of the code demonstrating the problem is here: val datastream = env .addSource(KafkaSource.keyedElements(config.kafkaElements, List(config.kafkaBootstrapServer))) val result = datastream .keyBy( _ =&gt; 1) .timeWindow(Time.milliseconds(1)) .print()  The source consists of 3 streams (being either 3 Kafka partitions or 3 Kafka topics), the elements in each of the streams are separately increasing. The elements generate increasing timestamps using an event time and start from 1, increasing by 1. The first partitions would consist of timestamps 1, 2, 10, 15..., the second of 4, 5, 6, 11..., the third of 3, 7, 8, 9... What I observe:The time windows would open as I expect for the first 127 timestamps. Then there would be a huge gap with no opened windows, if the source has many elements, then next open window would be having a timestamp in the thousands. A gap of hundred of elements would be created with what appear to be 'lost' elements. Those elements are not reported as late (if tested with the .sideOutputLateData operator). The way we have been using the option is by setting in inside the config like so:etherbi.rocksDB.columnOptions.optimizeForPointLookup=268435456We have been using it for performance reasons as we have huge RocksDB state backend.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateMisuseOptionTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBPriorityQueueSetFactory.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBCachingPriorityQueueSet.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptionsFactory.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="17808" opendate="2020-5-19 00:00:00" fixdate="2020-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename checkpoint meta file to "_metadata" until it has completed writing</summary>
      <description>In practice, some developers or customers would use some strategy to find the recent _metadata as the checkpoint to recover (e.g as many proposals in FLINK-9043 suggest). However, there existed a "_meatadata" file does not mean the checkpoint have been completed as the writing to create the "_meatadata" file could break as some force quit (e.g. yarn application -kill).We could create the checkpoint meta stream to write data to file named as "_metadata.inprogress" and renamed it to "_metadata" once completed writing. By doing so, we could ensure the "_metadata" is not broken.</description>
      <version>1.10.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.CheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.local.LocalRecoverableWriter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.local.LocalRecoverableFsDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="17809" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtil script logic does not work for paths with spaces</summary>
      <description>Multiple paths aren't quoted (class path, conf_dir) resulting in errors if they contain spaces.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17847" opendate="2020-5-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBoundsException happens when codegen StreamExec operator</summary>
      <description>user case://source table create table json_table( w_es BIGINT, w_type STRING, w_isDdl BOOLEAN, w_data ARRAY&lt;ROW&lt;pay_info STRING, online_fee DOUBLE, sign STRING, account_pay_fee DOUBLE&gt;&gt;, w_ts TIMESTAMP(3), w_table STRING) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'json-test2', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'test-jdbc', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true')// real data:{"w_es":1589870637000,"w_type":"INSERT","w_isDdl":false,"w_data":[{"pay_info":"channelId=82&amp;onlineFee=89.0&amp;outTradeNo=0&amp;payId=0&amp;payType=02&amp;rechargeId=4&amp;totalFee=89.0&amp;tradeStatus=success&amp;userId=32590183789575&amp;sign=00","online_fee":"89.0","sign":"00","account_pay_fee":"0.0"}],"w_ts":"2020-05-20T13:58:37.131Z","w_table":"cccc111"}//queryselect w_ts, 'test' as city1_id, w_data[0].pay_info AS cate3_id, w_data as pay_order_id from json_tableexception://Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848 at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598) at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590) at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534) at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117) at StreamExecCalc$10.processElement(Unknown Source) Looks like in the codegen StreamExecCalc$10 operator some operation visit a '-1' index which should be wrong, this bug exits both in 1.10 and 1.11 public class StreamExecCalc$10 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private final org.apache.flink.table.dataformat.BinaryString str$3 = org.apache.flink.table.dataformat.BinaryString.fromString("test"); private transient org.apache.flink.table.runtime.typeutils.BaseArraySerializer typeSerializer$5; final org.apache.flink.table.dataformat.BoxedWrapperRow out = new org.apache.flink.table.dataformat.BoxedWrapperRow(4); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$10( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output) throws Exception { this.references = references; typeSerializer$5 = (((org.apache.flink.table.runtime.typeutils.BaseArraySerializer) references[0])); this.setup(task, config, output); } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) element.getValue(); org.apache.flink.table.dataformat.SqlTimestamp field$2; boolean isNull$2; org.apache.flink.table.dataformat.BaseArray field$4; boolean isNull$4; org.apache.flink.table.dataformat.BaseArray field$6; org.apache.flink.table.dataformat.BinaryString field$8; boolean isNull$8; org.apache.flink.table.dataformat.BinaryString result$9; boolean isNull$9; isNull$2 = in1.isNullAt(4); field$2 = null; if (!isNull$2) { field$2 = in1.getTimestamp(4, 3); } isNull$4 = in1.isNullAt(3); field$4 = null; if (!isNull$4) { field$4 = in1.getArray(3); } field$6 = field$4; if (!isNull$4) { field$6 = (org.apache.flink.table.dataformat.BaseArray) (typeSerializer$5.copy(field$6)); } out.setHeader(in1.getHeader()); if (isNull$2) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, field$2); } if (false) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, ((org.apache.flink.table.dataformat.BinaryString) str$3)); } boolean isNull$7 = isNull$4 || false || field$6.isNullAt(((int) 0) - 1); org.apache.flink.table.dataformat.BaseRow result$7 = isNull$7 ? null : field$6.getRow(((int) 0) - 1, 4); if (isNull$7) { result$9 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; isNull$9 = true; } else { isNull$8 = result$7.isNullAt(0); field$8 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; if (!isNull$8) { field$8 = result$7.getString(0); } result$9 = field$8; isNull$9 = isNull$8; } if (isNull$9) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$9); } if (isNull$4) { out.setNullAt(3); } else { out.setNonPrimitiveValue(3, field$6); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}     </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17902" opendate="2020-5-24 00:00:00" fixdate="2020-6-24 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support the new interfaces about temporary functions in PyFlink</summary>
      <description>The interfaces such as createTemporarySystemFunction, dropTemporarySystemFunction, createFunction, dropFunction, createTemporaryFunction, dropTemporaryFunction in the Java TableEnvironment are currently not available in the PyFlink. The aim of this JIRA is to add support of them in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="17923" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It will throw MemoryAllocationException if rocksdb statebackend and Python UDF are used in the same slot</summary>
      <description>For the following job:import loggingimport osimport shutilimport sysimport tempfilefrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import TableConfig, StreamTableEnvironment, DataTypesfrom pyflink.table.udf import udfdef word_count(): content = "line Licensed to the Apache Software Foundation ASF under one " \ "line or more contributor license agreements See the NOTICE file " \ "line distributed with this work for additional information " \ "line regarding copyright ownership The ASF licenses this file " \ "to you under the Apache License Version the " \ "License you may not use this file except in compliance " \ "with the License" t_config = TableConfig() env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env, t_config) # register Results table in table environment tmp_dir = tempfile.gettempdir() result_path = tmp_dir + '/result' if os.path.exists(result_path): try: if os.path.isfile(result_path): os.remove(result_path) else: shutil.rmtree(result_path) except OSError as e: logging.error("Error removing directory: %s - %s.", e.filename, e.strerror) logging.info("Results directory: %s", result_path) sink_ddl = """ create table Results( word VARCHAR, `count` BIGINT ) with ( 'connector' = 'blackhole' ) """ t_env.sql_update(sink_ddl) @udf(input_types=[DataTypes.BIGINT()], result_type=DataTypes.BIGINT()) def inc(count): return count + 1 t_env.register_function("inc", inc) elements = [(word, 1) for word in content.split(" ")] t_env.from_elements(elements, ["word", "count"]) \ .group_by("word") \ .select("word, count(1) as count") \ .select("word, inc(count) as count") \ .insert_into("Results") t_env.execute("word_count")if __name__ == '__main__': logging.basicConfig(stream=sys.stdout, level=logging.INFO, format="%(message)s") word_count()It will throw the following exception if rocksdb state backend is used:Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681_(1/1) from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:317) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:144) ... 9 moreCaused by: java.io.IOException: Failed to acquire shared cache resource for RocksDB at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:212) at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:516) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:301) at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ... 11 moreCaused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not created the shared memory resource of size 536870920. Not enough memory left to reserve from the slot's managed memory. at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:603) at org.apache.flink.runtime.memory.SharedResources.createResource(SharedResources.java:130) at org.apache.flink.runtime.memory.SharedResources.getOrAllocateSharedResource(SharedResources.java:72) at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:617) at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:566) at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:208) ... 15 moreCaused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 536870920 bytes. Only 454033416 bytes are remaining. at org.apache.flink.runtime.memory.MemoryManager.reserveMemory(MemoryManager.java:461) at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:601) ... 20 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonFunctionFactoryTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="17931" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="1797" opendate="2015-3-28 00:00:00" fixdate="2015-4-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add jumping pre-reducer for Count and Time windows</summary>
      <description>There is currently only support for sliding and tumbling windows. This should be an easy extension of the tumbling pre-reducer</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.WindowUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimePreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimeGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="18042" opendate="2020-5-31 00:00:00" fixdate="2020-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 11.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.TestUtils.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="18070" opendate="2020-6-2 00:00:00" fixdate="2020-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time attribute been materialized after sub graph optimize</summary>
      <description>Hi, I want to use window aggregate after create temporary, and has multiple sinks. But throw exception:java.lang.AssertionError: type mismatch:ref:TIME ATTRIBUTE(PROCTIME) NOT NULLinput:TIMESTAMP(3) NOT NULLI look into the optimizer logic, there is comment at CommonSubGraphBasedOptimizer:"1. In general, for multi-sinks users tend to use VIEW which is a natural common sub-graph."After sub graph optimize, time attribute from source have been convert to basic TIMESTAMP type according to FlinkRelTimeIndicatorProgram. But my create view sql is simple query, I think didn't need to materialized time attribute in theory.Here is my code:// connector.type COLLECTION is for debug usetableEnv.sqlUpdate("CREATE TABLE source (\n" + " `ts` AS PROCTIME(),\n" + " `order_type` INT\n" + ") WITH (\n" + " 'connector.type' = 'COLLECTION',\n" + " 'format.type' = 'json'\n" + ")\n");tableEnv.createTemporaryView("source_view", tableEnv.sqlQuery("SELECT * FROM source"));tableEnv.sqlUpdate("CREATE TABLE sink (\n" + " `result` BIGINT\n" + ") WITH (\n" + " 'connector.type' = 'COLLECTION',\n" + " 'format.type' = 'json'\n" + ")\n");tableEnv.sqlUpdate("INSERT INTO sink \n" + "SELECT\n" + " COUNT(1)\n" + "FROM\n" + " `source_view`\n" + "WHERE\n" + " `order_type` = 33\n" + "GROUP BY\n" + " TUMBLE(`ts`, INTERVAL '5' SECOND)\n");tableEnv.sqlUpdate("INSERT INTO sink \n" + "SELECT\n" + " COUNT(1)\n" + "FROM\n" + " `source_view`\n" + "WHERE\n" + " `order_type` = 34\n" + "GROUP BY\n" + " TUMBLE(`ts`, INTERVAL '5' SECOND)\n");</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18168" opendate="2020-6-7 00:00:00" fixdate="2020-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error results when use UDAF with Object Array return type</summary>
      <description>I get error results when I use an UDAF with Object Array return type (e.g. Row[]). I find that the problem is we reuse 'reuseArray' as the return value of ObjectArrayConverter.toBinaryArray(). It leads to 'prevAggValue' and 'newAggValue' in GroupAggFunction.processElement() contains exactly the same BinaryArray, so 'equaliser.equalsWithoutHeader(prevAggValue, newAggValue)' is always true.</description>
      <version>None</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataStructureConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.util.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18197" opendate="2020-6-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should add more logs for hive streaming integration</summary>
      <description>When found bugs for hive streaming integration, it is very hard to debugging because we don't have useful logs.Should add more logs for Hive table streaming source/sink and lookup join.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.SuccessFileCommitPolicy.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileCommitter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionCommitPolicy.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.MetastoreCommitPolicy.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousMonitoringFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="18219" opendate="2020-6-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception if jar submission hits OOM</summary>
      <description>When using the jar submission from the WebUI, if an OOM occurs in the user-code then the exception is not enriched with additional pointers to the relevant config options.2020-06-09 15:12:33,075 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - Unhandled exception.java.lang.OutOfMemoryError: Java heap space at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:65) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:99) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler$$Lambda$306/0x0000000840598c40.get(Unknown Source) ~[?:?] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) [?:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?] at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:834) [?:?]</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestRestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestMessageHeaders.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerITCase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.TestRestServerEndpoint.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="18237" opendate="2020-6-10 00:00:00" fixdate="2020-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalArgumentException when reading filesystem partitioned table with stream mode</summary>
      <description>IllegalArgumentException when reading filesystem partitioned table with stream mode.Caused by: java.lang.IllegalArgumentException: FileInputFormats with multiple paths are not supported yet. at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.&lt;init&gt;(ContinuousFileMonitoringFunction.java:126) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.&lt;init&gt;(ContinuousFileMonitoringFunction.java:110) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createFileInput(StreamExecutionEnvironment.java:1513) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1480) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.createInput(StreamExecLegacyTableSourceScan.scala:200) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.PhysicalLegacyTableSourceScan.getSourceTransformation(PhysicalLegacyTableSourceScan.scala:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:98) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]When reading filesystem partitioned table, there will maybe many directories to read, but ContinuousFileMonitoringFunction not support multiple paths, we should not use it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemStreamITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18310" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure while parsing reporter interval does not reliably revert back to default</summary>
      <description>The reporter interval is parsed in 2 steps, one for the time amount, one for the time unit.The result of the first step is used regardless of whether the second step succeeds.This means that if you configure "3000 ms", we fall back to the default time unit (seconds), but stick to the configured amount (3000).</description>
      <version>1.10.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18315" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into partitioned table can fail with values</summary>
      <description></description>
      <version>1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18329" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dist NOTICE issues</summary>
      <description>akka-actor version incorrect 2.5.1 -&gt; 2.5.21</description>
      <version>1.10.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18493" opendate="2020-7-6 00:00:00" fixdate="2020-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make target home directory used to store yarn files configurable</summary>
      <description>When submitting applications to yarn, the file system's default home directory, like /user/user_name on hdfs, is used as the base directory to store flink files. But in different file system access control strategies, the default home directory may be inaccessible, and it's more preferable for users to specify thier own paths if they wish to.</description>
      <version>1.10.0,1.10.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18507" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="18514" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building fails with JDK 14 installed</summary>
      <description>On a system with JDK 14 installed the build fails with [INFO] --- gmavenplus-plugin:1.8.1:execute (merge-categories) @ flink-end-to-end-tests ---[INFO] Using plugin classloader, includes GMavenPlus classpath.java.lang.NoClassDefFoundError: Could not initialize class org.codehaus.groovy.vmplugin.v7.Java7 at org.codehaus.groovy.vmplugin.VMPluginFactory.&lt;clinit&gt;(VMPluginFactory.java:43) at org.codehaus.groovy.reflection.GroovyClassValueFactory.&lt;clinit&gt;(GroovyClassValueFactory.java:35) at org.codehaus.groovy.reflection.ClassInfo.&lt;clinit&gt;(ClassInfo.java:107) at org.codehaus.groovy.reflection.ReflectionCache.getCachedClass(ReflectionCache.java:95) at org.codehaus.groovy.reflection.ReflectionCache.&lt;clinit&gt;(ReflectionCache.java:39)Which is the same exception as seen herehttps://github.com/gradle/gradle/issues/10248This is a known problem in groovy that has been fixed in version 2.5.10: https://issues.apache.org/jira/browse/GROOVY-9211</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18595" opendate="2020-7-14 00:00:00" fixdate="2020-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock during job shutdown</summary>
      <description>https://travis-ci.org/github/apache/flink/jobs/707843779Found one Java-level deadlock:============================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": waiting to lock monitor 0x00007f51f655e228 (object 0x00000000812b9180, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Flat Map -&gt; Sink: Unnamed (9/12)""Flat Map -&gt; Sink: Unnamed (9/12)": waiting to lock monitor 0x000055fb00bb4b88 (object 0x00000000812b9210, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641)."Java stack information for the threads listed above:==================================================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.b4511: No such processuffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.releaseAll(RemoteInputChannel.java:665) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:254) - locked &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:431) - locked &lt;0x0000000080ba2488&gt; (a java.lang.Object) at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:85) at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:901) at org.apache.flink.runtime.taskmanager.Task$$Lambda$434/985222953.run(Unknown Source) at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1370) at java.lang.Thread.run(Thread.java:748)"Flat Map -&gt; Sink: Unnamed (9/12)": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.addExclusiveBuffer(RemoteInputChannel.java:629) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.recycle(RemoteInputChannel.java:314) - locked &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.streaming.runtime.io.CachedBufferStorage.close(CachedBufferStorage.java:113) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.cleanup(CheckpointedInputGate.java:216) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.close(StreamTaskNetworkInput.java:208) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.close(StreamOneInputProcessor.java:82) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanup(StreamTask.java:298) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:555) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:480) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock.</description>
      <version>1.10.0,1.10.1,1.11.0,1.11.1</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.BufferManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18703" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new data structure converters when legacy types are not present</summary>
      <description>FLINK-16999 introduce the new data structure converters that are already in place for the new sources/sinks and new scalar/table functions. We can enable it for all usages (or almost all usages) if legacy types are not present.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecksTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
    </fixedFiles>
  </bug>
  <bug id="18730" opendate="2020-7-27 00:00:00" fixdate="2020-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="18731" opendate="2020-7-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18844" opendate="2020-8-7 00:00:00" fixdate="2020-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support maxwell-json format to read Maxwell changelogs</summary>
      <description>Hi,i have finish these code .So, can assign this issule  to me ?</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug id="18861" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18978" opendate="2020-8-17 00:00:00" fixdate="2020-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support full table scan of key and namespace from statebackend</summary>
      <description>Support full table scan of keys and namespaces from the state backend. All operations assume the calling code already knows what namespace they are interested in interacting with.This is a prerequisite to support reading window operators with the state processor api because window panes are stored as additional namespace components.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.iterator.RocksStateKeysIterator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="1907" opendate="2015-4-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala Interactive Shell</summary>
      <description>Build an interactive Shell for the Scala api.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.FlinkTestBase.scala</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19070" opendate="2020-8-28 00:00:00" fixdate="2020-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19077" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import process time temporal join operator</summary>
      <description>import TemporalProcessTimeJoinOperator for Processing-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.LegacyTemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacyTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="19078" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import rowtime join temporal operator</summary>
      <description>import TemporalRowTimeJoinOperator for EventTime-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19079" opendate="2020-8-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support row time deduplicate operator</summary>
      <description>To convert a insert-only table to versioned table, the recommended way is use deduplicate query as following, the converted versioned_view owns primary key and event time and thus can be a versioned table.CREATE VIEW versioned_rates ASSELECT currency, rate, currency_timeFROM (      SELECT *,      ROW_NUMBER() OVER (PARTITION BY currency -- inferred primary key ORDER BY currency_time  -- the event time  DESC) AS rowNum FROM rates)WHERE rowNum = 1;But currently deduplicate operator only support on process time, this issue aims to support deduplicate on Event time. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecChangelogNormalize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19097" opendate="2020-8-31 00:00:00" fixdate="2020-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support add_jars() for Python StreamExecutionEnvironment</summary>
      <description>Add add_jars() interface in Python StreamExecutionEnvironment to enable users to specify jar dependencies in their Python DataStream Job.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="19109" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="19110" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="19252" opendate="2020-9-16 00:00:00" fixdate="2020-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jaas file created under io.tmp.dirs - folder not created if not exists</summary>
      <description>https://issues.apache.org/jira/browse/FLINK-14433 The security module installation happens before the tmp directory check: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java#L390</description>
      <version>1.10.0,1.10.1,1.10.2</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.modules.JaasModuleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.JaasModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="19655" opendate="2020-10-15 00:00:00" fixdate="2020-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when using blink planner and TemporalTableFunction after setting IdleStateRetentionTime</summary>
      <description>My Code here:EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);tableEnv.getConfig().setIdleStateRetentionTime(Time.seconds(60), Time.seconds(600));final Table table = tableEnv.from("tableName");final TableFunction&lt;?&gt; function = table.createTemporalTableFunction( temporalTableEntry.getTimeAttribute(), String.join(",", temporalTableEntry.getPrimaryKeyFields()));tableEnv.registerFunction(temporalTableEntry.getName(), function);And NPE throwed when I executed my program.java.lang.NullPointerException at org.apache.flink.table.runtime.operators.join.temporal.BaseTwoInputStreamOperatorWithStateRetention.registerProcessingCleanupTimer(BaseTwoInputStreamOperatorWithStateRetention.java:109) at org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperator.processElement2(TemporalProcessTimeJoinOperator.java:98) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processRecord2(StreamTwoInputProcessor.java:145) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.lambda$new$1(StreamTwoInputProcessor.java:107) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessor.java:362) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)And When I changed to useOldPlanner, it worked fine.And when I debuged the code ,I found BaseTwoInputStreamOperatorWithStateRetention#open did not be executed.Here is BaseTwoInputStreamOperatorWithStateRetention#open code.public void open() throws Exception { initializeTimerService(); if (stateCleaningEnabled) { ValueStateDescriptor&lt;Long&gt; cleanupStateDescriptor = new ValueStateDescriptor&lt;&gt;(CLEANUP_TIMESTAMP, Types.LONG); latestRegisteredCleanupTimer = getRuntimeContext().getState(cleanupStateDescriptor); } }Here is TemporalProcessTimeJoinOperator#open code.public void open() throws Exception { this.joinCondition = generatedJoinCondition.newInstance(getRuntimeContext().getUserCodeClassLoader()); FunctionUtils.setFunctionRuntimeContext(joinCondition, getRuntimeContext()); FunctionUtils.openFunction(joinCondition, new Configuration()); ValueStateDescriptor&lt;BaseRow&gt; rightStateDesc = new ValueStateDescriptor&lt;&gt;("right", rightType); this.rightState = getRuntimeContext().getState(rightStateDesc); this.collector = new TimestampedCollector&lt;&gt;(output); this.outRow = new JoinedRow(); // consider watermark from left stream only. super.processWatermark2(Watermark.MAX_WATERMARK); }I compared the code with oldplaner(TemporalProcessTimeJoin#open).May be TemporalProcessTimeJoinOperator#open should add super.open()?Here is TemporalProcessTimeJoin#open code.override def open(): Unit = { LOG.debug(s"Compiling FlatJoinFunction: $genJoinFuncName \n\n Code:\n$genJoinFuncCode") val clazz = compile( getRuntimeContext.getUserCodeClassLoader, genJoinFuncName, genJoinFuncCode) LOG.debug("Instantiating FlatJoinFunction.") joinFunction = clazz.newInstance() FunctionUtils.setFunctionRuntimeContext(joinFunction, getRuntimeContext) FunctionUtils.openFunction(joinFunction, new Configuration()) val rightStateDescriptor = new ValueStateDescriptor[Row]("right", rightType) rightState = getRuntimeContext.getState(rightStateDescriptor) collector = new TimestampedCollector[CRow](output) cRowWrapper = new CRowWrappingCollector() cRowWrapper.out = collector super.open() }</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19656" opendate="2020-10-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically replace delimiter in metric name components</summary>
      <description>The metric name consists of various components (like job ID, task ID), that are then joined by a delimiter(commonly .).The delimiter isn't just for convention, but also carries semantics for many metric backends, as they organize metrics based on the delimiter.This can behave in unfortunate ways if the delimiter is contained with a given component, as it will now be split up by the backend.We should automatically filter such occurrences to prevent this from happening.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.ScopeFormat.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.CharacterFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19809" opendate="2020-10-26 00:00:00" fixdate="2020-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ServiceConnectionManager</summary>
      <description>The Slotpool has to interact with the ResourceManager to declare the resource requirements.We do not want to provide full access to the ResourceManagerGateway (and as such should wrap it in some form), but we also have to handle the case where no ResourceManager is connected.Introduce a component for handling this.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="19810" opendate="2020-10-26 00:00:00" fixdate="2020-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically run a basic NOTICE file check on CI</summary>
      <description>For every release, we are manually validating the NOTICE files, according to this wiki page: https://cwiki.apache.org/confluence/display/FLINK/LicensingThe most time-consuming task is ensuring that all modules that deploy a shaded dependency to maven central are properly documenting this dependency in their NOTICE file.I would like to add a tool to Flink that checks if all shaded dependencies are at least mentioned in the NOTICE file.We will still need to perform a manual checks, but the tool should catch the most common, severe and difficult to find problems.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19811" opendate="2020-10-26 00:00:00" fixdate="2020-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkRexUtil#simplify should simplify search in conjunctions</summary>
      <description>The new version of Calcite introduces the SEARCH rex call to express range conditions. However SEARCH}}s in conjunctions are currently not simplified. For example, {{AND(=($2, 2020), SEARCH($2, Sarg&amp;#91;2020, 2021&amp;#93;)) is not simplified while it should be simplified to =($2, 2020).This issue is caused by CALCITE-4364. We could currently extend the RexSimplify class to temporarily cover this issue.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20183" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the default PYTHONPATH is overwritten in client side</summary>
      <description></description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20184" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="20302" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Done">
    <buginformation>
      <summary>Recommend DataStream API with BATCH execution mode in DataSet docs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20519" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend HBase notice with transitively bundled dependencies</summary>
      <description>The HBase 2.2 sql connector bundles hbase which themselves bundles various dependencies. We should investigate what is being bundled and list it in our notice file.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="2052" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear up Serializable warnings in streaming operators</summary>
      <description>The recent stream operator rework removed the "serialVersionUID" fields from operators which causes compiler warnings.These should be re-added to all serializable classes</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.ProjectTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowPartitioner.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMerger.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFlattener.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedStreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedActiveDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamProject.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFilter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamCounter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamWindow.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.StreamProjection.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.KeySelector.java</file>
    </fixedFiles>
  </bug>
  <bug id="20520" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that metric names can contain characters that need to be escaped</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="21925" opendate="2021-3-23 00:00:00" fixdate="2021-7-23 01:00:00" resolution="Done">
    <buginformation>
      <summary>FLIP-169 Support configuring fine grained resource requirements via DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="21926" opendate="2021-3-23 00:00:00" fixdate="2021-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Fine Grained Resource Management</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="22184" opendate="2021-4-9 00:00:00" fixdate="2021-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest client shutdown on failure runs in netty thread</summary>
      <description>Then using the CLI to run any command, if the request fails then the RestClient is shut down from the netty thread, which causes problems because when shutting down we try to shut down netty, but the netty thread is still busy shutting things down.</description>
      <version>1.10.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23539" opendate="2021-7-29 00:00:00" fixdate="2021-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>InfluxDBReporter should filter characters in tags</summary>
      <description>I find that the job cannot report any metrics to influxdb when a SQL statement contains '\n', because '\n' has the special meaning to influxdb .From time to time, many strange tables that use partial SQL statement as name are created in influxdb. </description>
      <version>1.10.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.java.org.apache.flink.metrics.influxdb.MeasurementInfoProviderTest.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.MeasurementInfoProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="23936" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug id="23995" opendate="2021-8-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect: `insert overwrite table partition if not exists` will throw exception when tablename is like &amp;#39;database.table&amp;#39;</summary>
      <description>when run such hive sql insert overwrite table default.dest2 partition (p1=1,p2='static') if not exists select x from src it will throw exceptionCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found default</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="24560" opendate="2021-10-15 00:00:00" fixdate="2021-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-yarn-tests should copy examples after package phase</summary>
      <description>flink-yarn-tests currently copies some jars in the process-resources phase. As a result mvn test would fail in a clean environment because said jars don't exist yet.</description>
      <version>1.10.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24561" opendate="2021-10-15 00:00:00" fixdate="2021-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build archetype jars in compile phase</summary>
      <description>The quickstart jars are currently build in the package phase, which conceptually makes sense.However, when running mvn test in a clean environment the build fails because the quickstart e2e test cannot resolve the quickstart dependencies.Usually maven figures out that it can use the compiled classes as a stand-in for (test-)jar dependencies, but this doesn't work for the quickstarts because those use a special packaging process.I don't see any harm in building the archetype jars in the compile phase to solve this issue.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25905" opendate="2022-2-1 00:00:00" fixdate="2022-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Promoting Experimental methods is marked as a violation</summary>
      <description>Since @Experimental is not in the japicmp exclude list an experimental method in a public class is also considered as public.</description>
      <version>1.10.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
