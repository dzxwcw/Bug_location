<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="4639" opendate="2016-9-20 00:00:00" fixdate="2016-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Calcite features more pluggable</summary>
      <description>Some users might want to extend the feature set of the Table API by adding or replacing Calcite optimizer rules, modifying the parser etc. It would be good to have means to hook into the Table API and change Calcite behavior. We should implement something like a CalciteConfigBuilder.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4689" opendate="2016-9-27 00:00:00" fixdate="2016-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a simple slot provider for the new job manager</summary>
      <description>In flip-6 branch, we need to adjust existing scheduling model. In the first step, we should introduce a simple / naive slot provider which just ignore all the sharing or location constraint, to make whole thing work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="4693" opendate="2016-9-27 00:00:00" fixdate="2016-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add session group-windows for batch tables</summary>
      <description>Add Session group-windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4799" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-add build-target symlink to project root</summary>
      <description>We have previously removed the plugin which created the 'build-target' link to the build target directory. See FLINK-4732. At least one user has requested to re-add the link.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4801" opendate="2016-10-11 00:00:00" fixdate="2016-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input type inference is faulty with custom Tuples and RichFunctions</summary>
      <description>This issue has been discussed on the ML:http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Type-problem-in-RichFlatMapFunction-when-using-GenericArray-type-td13929.htmlThis returns the wrong type: public static class Foo&lt;K&gt; extends Tuple2&lt;K[], K&gt; { public Foo() { } public Foo(K[] value0, K value1) { super(value0, value1); } } DataSource&lt;Foo&lt;T&gt;&gt; fooDataSource = env.fromElements(foo); DataSet&lt;Foo&lt;T&gt;&gt; ds = fooDataSource.join(fooDataSource) .where(field).equalTo(field) .with(new RichFlatJoinFunction&lt;Foo&lt;T&gt;, Foo&lt;T&gt;, Foo&lt;T&gt;&gt;() { @Override public void join(Foo&lt;T&gt; first, Foo&lt;T&gt; second, Collector&lt;Foo&lt;T&gt;&gt; out) throws Exception { out.collect(first); } });</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.TypeExtractorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
    </fixedFiles>
  </bug>
  <bug id="4809" opendate="2016-10-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators should tolerate checkpoint failures</summary>
      <description>Operators should try/catch exceptions in the synchronous and asynchronous part of the checkpoint and send a DeclineCheckpoint message as a result.The decline message should have the failure cause attached to it.The checkpoint barrier should be sent anyways as a first step before attempting to make a state checkpoint, to make sure that downstream operators do not block in alignment.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.BlockingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="4876" opendate="2016-10-21 00:00:00" fixdate="2016-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="4877" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactorings around FLINK-3674 (User Function Timers)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecord.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestTimeProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileMonitoringTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.CollectingOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.NoOpTimerService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.WindowingTestHarness.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.ExtractTimestampsOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StreamTaskTimerITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.Triggerable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="4881" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docker: Remove dependency on shared volumes</summary>
      <description>Our Dockerfile assumes a shared volume configuration to access the config. Instead, we can configure the Docker container to directly write the hostname into /etc/hosts and use "jobmanager" as the default hostname.This has been proposed here: https://github.com/apache/flink/pull/2667</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.README.md</file>
      <file type="M">flink-contrib.docker-flink.docker-entrypoint.sh</file>
      <file type="M">flink-contrib.docker-flink.docker-compose.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4891" opendate="2016-10-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Remove flink-contrib/flink-operator-stats</summary>
      <description>As per discussion on the mailing list, remove module flink-contrib/flink-operator-stats.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-operator-stats.src.test.java.org.apache.flink.contrib.operatorstatistics.OperatorStatsAccumulatorTest.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.test.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.LossyCountingTest.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.test.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.CountMinHeavyHitterTest.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.OperatorStatisticsConfig.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.OperatorStatisticsAccumulator.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.OperatorStatistics.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.LossyCounting.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.HeavyHitterMergeException.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.HeavyHitter.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.CountMinHeavyHitter.java</file>
      <file type="M">flink-contrib.flink-operator-stats.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4906" opendate="2016-10-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use constants for the name of system-defined metrics</summary>
      <description></description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorIOMetricGroup.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4925" opendate="2016-10-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate meters into IOMetricGroups</summary>
      <description></description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorIOMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="4927" opendate="2016-10-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 YARN Resource Manager</summary>
      <description>The Flink YARN Resource Manager communicates with YARN's Resource Manager to acquire and release containers.It is also responsible to notify the JobManager eagerly about container failures.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4929" opendate="2016-10-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 YARN TaskManager Runner</summary>
      <description>The YARN TaskManager Runner has the following responsibilities: Read the configuration and all environment variables and compute the effective configuration Start all services (Rpc, High Availability, Security, etc) Instantiate and start the Task Manager Runner</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4930" opendate="2016-10-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 YARN client</summary>
      <description>The FLIP-6 YARN client can follow parts of the existing YARN client.The main difference is that it does not wait for the cluster to be fully started and for all TaskManagers to register. It simply submits Set up all configurations and environment variables Set up the resources: Flink jar, utility jars (logging), user jar Set up attached tokens / certificates Submit the Yarn application Listen for leader / attempt to connect to the JobManager to subscribe to updates Integration with the Flink CLI (command line interface)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClientV2.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4975" opendate="2016-10-31 00:00:00" fixdate="2016-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a limit for how much data may be buffered during checkpoint alignment</summary>
      <description>During checkpoint alignment, data may be buffered/spilled.We should introduce an upper limit for the spilled data volume. After exceeding that limit, the checkpoint alignment should abort and the checkpoint be canceled.</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="4983" opendate="2016-10-31 00:00:00" fixdate="2016-12-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Web UI: Add favicon</summary>
      <description>Makes the tab easier to find when having multiple tabs open</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
    </fixedFiles>
  </bug>
  <bug id="4985" opendate="2016-10-31 00:00:00" fixdate="2016-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Report Declined/Canceled Checkpoints to Checkpoint Coordinator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeAllWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.DeclineCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4998" opendate="2016-11-2 00:00:00" fixdate="2016-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager fails when num task slots &gt; Yarn vcores</summary>
      <description>The ResourceManager fails to acquire containers when the users configures the number of task slots to be greater than the maximum number of virtual cores of the Yarn cluster.We should check during deployment that the task slots are not configured to be larger than the virtual cores.2016-11-02 14:39:01,948 ERROR org.apache.flink.yarn.YarnFlinkResourceManager - FATAL ERROR IN YARN APPLICATION MASTER: Connection to YARN Resource Manager failedorg.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested virtual cores &lt; 0, or requested virtual cores &gt; max configured, requestedVirtualCores=3, maxVirtualCores=1</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5013" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Kinesis connector doesn&amp;#39;t work on old EMR versions</summary>
      <description>A user reported on the mailing list that our Kinesis connector doesn't work with EMR 4.4.0: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kinesis-Connector-Dependency-Problems-td9790.htmlThe problem seems to be that Flink is loading older libraries from the "YARN container classpath", which on EMR contains the default Amazon libraries.We should try to shade kinesis and its amazon dependencies into a different namespace.</description>
      <version>None</version>
      <fixedVersion>1.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5019" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper isRestored result for tasks that did not write state</summary>
      <description>When a subtask is restored from a checkpoint that does not contain any state (e.g. because the subtask did not write state in the previous run), the result of StateInitializationContext::isRestored will incorrectly return false.We should ensure that empty state is somehow reflected in a checkpoint and return true on restore, independent from the presence of state.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateHandles.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="5028" opendate="2016-11-7 00:00:00" fixdate="2016-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stream Tasks must not go through clean shutdown logic on cancellation</summary>
      <description></description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5033" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP operators don&amp;#39;t properly advance time</summary>
      <description>The CEP operator don't properly advance time in case of watermarks. Arriving watermarks should trigger the triggering of timeouts and pruning of event sequences. However, this works only if an element has arrived in the meantime. This is a bug and the time should also be advanced in case that no element arrived before the watermark.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPBasePatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="5038" opendate="2016-11-9 00:00:00" fixdate="2016-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Errors in the "cancelTask" method prevent closeables from being closed early</summary>
      <description>The title says it all</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5039" opendate="2016-11-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro GenericRecord support is broken</summary>
      <description>Avro GenericRecord support was introduced in FLINK-3691, but it seems like the GenericRecords are not properly (de)serialized.This can be easily seen with a program like this: env.createInput(new AvroInputFormat&lt;&gt;(new Path("somefile.avro"), GenericRecord.class)) .first(10) .print();which will print records in which all fields have the same value:{"foo": 1478628723066, "bar": 1478628723066, "baz": 1478628723066, ...}{"foo": 1478628723179, "bar": 1478628723179, "baz": 1478628723179, ...}If I'm not mistaken, the AvroInputFormat does essentially TypeExtractor.getForClass(GenericRecord.class), but GenericRecords are not POJOs.Furthermore, each GenericRecord contains a pointer to the record schema. I guess the current naive approach will serialize this schema with each record, which is quite inefficient (the schema is typically more complex and much larger than the data). We probably need a TypeInformation and TypeSerializer specific to Avro GenericRecords, which could just use avro serialization.</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5040" opendate="2016-11-9 00:00:00" fixdate="2016-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set correct input channel types with eager scheduling</summary>
      <description>When we do eager deployment all intermediate stream/partition locations are already known when scheduling an intermediate stream/partition consumer. Nonetheless we saw tasks with "unknown input channels" that were updated lazily during runtime. This was caused by a wrong producer execution state check requiring the producers to be in RUNNING or DEPLOYING state when creating consumer input channels.(We had a bogus fix for this in FLINK-3232. With that "fix" we actually did not fix anything correctly and instead doubled the number of schedule or update consumer messages we sent.)</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.NetworkEnvironmentConfiguration.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.ScheduleMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.PartitionState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5047" opendate="2016-11-10 00:00:00" fixdate="2016-1-10 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add sliding group-windows for batch tables</summary>
      <description>Add Slide group-windows for batch tables as described in FLIP-11.There are two ways to implement sliding windows for batch:1. replicate the output in order to assign keys for overlapping windows. This is probably the more straight-forward implementation and supports any aggregation function but blows up the data volume.2. if the aggregation functions are combinable / pre-aggregatable, we can also find the largest tumbling window size from which the sliding windows can be assembled. This is basically the technique used to express sliding windows with plain SQL (GROUP BY + OVER clauses). For a sliding window Slide(10 minutes, 2 minutes) this would mean to first compute aggregates of non-overlapping (tumbling) 2 minute windows and assembling consecutively 5 of these into a sliding window (could be done in a MapPartition with sorted input). The implementation could be done as an optimizer rule to split the sliding aggregate into a tumbling aggregate and a SQL WINDOW operator. Maybe it makes sense to implement the WINDOW clause first and reuse this for sliding windows.3. There is also a third, hybrid solution: Doing the pre-aggregation on the largest non-overlapping windows (as in 2) and replicating these results and processing those as in the 1) approach. The benefits of this is that it a) is based on the implementation that supports non-combinable aggregates (which is required in any case) and b) that it does not require the implementation of the SQL WINDOW operator. Internally, this can be implemented again as an optimizer rule that translates the SlidingWindow into a pre-aggregating TublingWindow and a final SlidingWindow (with replication).see FLINK-4692 for more discussion</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5048" opendate="2016-11-10 00:00:00" fixdate="2016-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka Consumer (0.9/0.10) threading model leads problematic cancellation behavior</summary>
      <description>The FLinkKafkaConsumer (0.9 / 0.10) spawns a separate thread that operates the KafkaConsumer. That thread is shielded from interrupts, because the Kafka Consumer has not been handling thread interrupts well.Since that thread is also the thread that emits records, it may block in the network stack (backpressure) or in chained operators. The later case leads to situations where cancellations get very slow unless that thread would be interrupted (which it cannot be).I propose to change the thread model as follows: A spawned consumer thread pull from the KafkaConsumer and pushes its pulled batch of records into a blocking queue (size one) The main thread of the task will pull the record batches from the blocking queue and emit the records.This allows actually for some additional I/O overlay while limiting the additional memory consumption - only two batches are ever held, one being fetched and one being emitted.</description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetention09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetention010ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ExceptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5049" opendate="2016-11-10 00:00:00" fixdate="2016-1-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Instability in QueryableStateITCase.testQueryableStateWithTaskManagerFailure</summary>
      <description>https://api.travis-ci.org/jobs/174843846/log.txt?deansi=trueorg.apache.flink.test.query.QueryableStateITCasetestQueryableStateWithTaskManagerFailure(org.apache.flink.test.query.QueryableStateITCase) Time elapsed: 8.096 sec &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Count moved backwards at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.assertTrue(Assert.java:41) at org.apache.flink.test.query.QueryableStateITCase.testQueryableStateWithTaskManagerFailure(QueryableStateITCase.java:470)</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.AbstractQueryableStateITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5050" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON.org license is CatX</summary>
      <description>We should exclude org.json:json dependency from hive-exec dependency.[INFO] +- org.apache.flink:flink-java:jar:1.2-SNAPSHOT:provided...[INFO] +- org.apache.hive.hcatalog:hcatalog-core:jar:0.12.0:compile...[INFO] | +- org.apache.hive:hive-exec:jar:0.12.0:compile[INFO] | | +- com.google.protobuf:protobuf-java:jar:2.4.1:compile[INFO] | | +- org.iq80.snappy:snappy:jar:0.2:compile[INFO] | | +- org.json:json:jar:20090211:compile</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-batch-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5051" opendate="2016-11-11 00:00:00" fixdate="2016-5-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Backwards compatibility for serializers in backend state</summary>
      <description>When a new state is register, e.g. in a keyed backend via `getPartitionedState`, the caller has to provide all type serializers required for the persistence of state components. Explicitly passing the serializers on state creation already allows for potentiall version upgrades of serializers.However, those serializers are currently not part of any snapshot and are only provided at runtime, when the state is registered newly or restored. For backwards compatibility, this has strong implications: checkpoints are not self contained in that state is currently a blackbox without knowledge about it's corresponding serializers. Most cases where we would need to restructure the state are basically lost. We could only convert them lazily at runtime and only once the user is registering the concrete state, which might happen at unpredictable points.I suggest to adapt our solution as follows: As now, all states are registered with their set of serializers. Unlike now, all serializers are written to the snapshot. This makes savepoints self-contained and also allows to create inspection tools for savepoints at some point in the future. Introduce an interface Versioned with long getVersion() and boolean isCompatible(Versioned v) which is then implemented by serializers. Compatible serializers must ensure that they can deserialize older versions, and can then serialize them in their new format. This is how we upgrade.We need to find the right tradeoff in how many places we need to store the serializers. I suggest to write them once per parallel operator instance for each state, i.e. we have a map with state_name -&gt; tuple3&lt;serializer&lt;KEY&gt;, serializer&lt;NAMESPACE&gt;, serializer&lt;STATE&gt;&gt;. This could go before all key-groups are written, right at the head of the file. Then, for each file we see on restore, we can first read the serializer map from the head of the stream, then go through the key groups by offset.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.QueryableStateClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.JavaSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayOutputStreamWithPos.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPos.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.ValueStateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.ReducingStateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.ListStateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.FoldingStateDescriptor.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5075" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer incorrectly determines shards as newly discovered when tested against Kinesalite</summary>
      <description>A user reported that when our Kinesis connector is used against Kinesalite (https://github.com/mhart/kinesalite), we're incorrectly determining already found shards as newly discovered:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Subtask-keeps-on-discovering-new-Kinesis-shard-when-using-Kinesalite-td10133.htmlI suspect the problem to be the mock Kinesis API implementations of Kinesalite doesn't completely match with the official AWS Kinesis behaviour.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="5076" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutting down TM when shutting down new mini cluster</summary>
      <description>Currently we don't shut down task manager when shutting down mini cluster. It will cause mini cluster can not exit normally.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="5084" opendate="2016-11-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Java Table API integration tests by unit tests</summary>
      <description>The Java Table API is a wrapper on top of the Scala Table API. Instead of operating directly with Expressions like the Scala API, the Java API accepts a String parameter which is parsed into Expressions.We could therefore replace the Java Table API ITCases by tests that check that the parsing step produces a valid logical plan.This could be done by creating two Table objects for an identical query once with the Scala Expression API and one with the Java String API and comparing the logical plans of both Table objects. Basically something like the following:val ds1 = CollectionDataSets.getSmall3TupleDataSet(env).toTable(tEnv, 'a, 'b, 'c)val ds2 = CollectionDataSets.get5TupleDataSet(env).toTable(tEnv, 'd, 'e, 'f, 'g, 'h)val joinT1 = ds1.join(ds2).where('b === 'e).select('c, 'g)val joinT2 = ds1.join(ds2).where("b = e").select("c, g")val lPlan1 = joinT1.logicalPlanval lPlan2 = joinT2.logicalPlanAssert.assertEquals("Logical Plans do not match", lPlan1, lPlan2)</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.ExplainTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5091" opendate="2016-11-17 00:00:00" fixdate="2016-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formalize the AppMaster environment for docker compability</summary>
      <description>For scenarios where the AppMaster is launched from a docker image, it would be ideal to use the installed Flink rather than rely on a special file layout in the sandbox directory.This is related to DCOS integration, which (in 1.2) will launch the AppMaster via Marathon (as a top-level DCOS service). The existing code assumed that only the dispatcher (coming in 1.3) would launch the AppMaster.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CommonTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="5107" opendate="2016-11-21 00:00:00" fixdate="2016-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job Manager goes out of memory from long history of prior execution attempts</summary>
      <description>We have observed that the job manager can run out of memory during long running jobs with many vertexes. Analysis of the heap dump shows, that the ever-growing history of prior execution attempts is the culprit for this problem.We should limit this history to a number of n most recent attempts.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AllVerticesIteratorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractSubtaskAttemptRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5109" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Content-Encoding Header in REST API responses</summary>
      <description>On REST API calls the Flink runtime responds with the header Content-Encoding, containing the value "utf-8". According to the HTTP/1.1 standard this header is invalid. ( https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5 ) Possible acceptable values are: gzip, compress, deflate. Or it should be omitted.The invalid header may cause malfunction in projects building against Flink.The invalid header may be present in earlier versions aswell.Proposed solution: Remove lines from the project, where CONTENT_ENCODING header is set to "utf-8". (I could do this in a PR.)Possible solution but may need further knowledge and skills than mine: Introduce content-encoding. Doing so may need some configuration beacuse then Flink would have to encode the responses properly (even paying attention to the request's Accept-Encoding headers).</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5119" opendate="2016-11-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5123" opendate="2016-11-22 00:00:00" fixdate="2016-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add description how to do proper shading to Flink docs.</summary>
      <description></description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.building.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug id="514" opendate="2014-6-9 00:00:00" fixdate="2014-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix invalid Javadoc references</summary>
      <description>When building the project, there are a lot of warnings, because of invalid Javadoc references. One major source of this is imo that Eclipse does not complain with the default settings. Let's make sure to keep it in mind for future doc comments and fix the bad ones (although low priority for the system itself, I think it affects the perception of the code quality).---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/514Created by: uceLabels: build system, enhancement, simple-issue, Created at: Wed Feb 26 15:35:50 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.SingleElementIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableHashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SocketInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelReaderInputView.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.GUIServletStub.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BasicTypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.ExternalProcessInputSplit.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.IntListComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.IntPairComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.StringPairComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.io.avro.example.User.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.LevelDBState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.MemcachedState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.RedisState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CoReduceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.StreamInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.RemoteCollectorImpl.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple1.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple10.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple11.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple12.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple13.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple14.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple15.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple16.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple17.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple18.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple19.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple2.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple20.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple21.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple22.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple23.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple24.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple25.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple3.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple4.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple5.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple6.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple7.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple8.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple9.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5143" opendate="2016-11-23 00:00:00" fixdate="2016-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EXISTS to list of supported operators</summary>
      <description>EXISTS is supported in certain cases. We should add it so that e.g. TPC-H query 4 runs properly.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5144" opendate="2016-11-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while applying rule AggregateJoinTransposeRule</summary>
      <description>AggregateJoinTransposeRule seems to cause errors. We have to investigate if this is a Flink or Calcite error. Here a simplified example:select sum(l_extendedprice)from lineitem, partwhere p_partkey = l_partkey and l_quantity &lt; ( select avg(l_quantity) from lineitem where l_partkey = p_partkey )Exception:Exception in thread "main" java.lang.AssertionError: Internal error: Error occurred while applying rule AggregateJoinTransposeRule at org.apache.calcite.util.Util.newInternal(Util.java:792) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:342) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:213) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:819) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:334) at org.apache.flink.api.table.BatchTableEnvironment.optimize(BatchTableEnvironment.scala:251) at org.apache.flink.api.table.BatchTableEnvironment.translate(BatchTableEnvironment.scala:286) at org.apache.flink.api.scala.table.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:139) at org.apache.flink.api.scala.table.package$.table2RowDataSet(package.scala:77) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.runQ17(TPCHQueries.scala:826) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.main(TPCHQueries.scala:57) at org.apache.flink.api.scala.sql.tpch.TPCHQueries.main(TPCHQueries.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(BIGINT l_partkey, BIGINT p_partkey) NOT NULLrowtype of set:RecordType(BIGINT p_partkey) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:1838) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:273) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1820) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1766) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:1032) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1052) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1942) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:136) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5159" opendate="2016-11-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve perfomance of inner joins with a single row input</summary>
      <description>All inner joins (including a cross join) can be implemented as a MapFunction if one of their inputs is a single row. This row can be passed to a MapFunction as a BroadcastSet.This approach is going to be more lightweight than the other current strategies.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5160" opendate="2016-11-25 00:00:00" fixdate="2016-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SecurityContextTest#testCreateInsecureHadoopCtx fails on windows</summary>
      <description>The test uses the wrong method to retrieve the current user name when run on Windows.</description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.OperatingSystem.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.SecurityUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5164" opendate="2016-11-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop-compat IOFormat tests fail on Windows</summary>
      <description>The HaddopMapredITCase fails on windows with the following exception:java.lang.NullPointerException at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) at org.apache.hadoop.util.Shell.runCommand(Shell.java:445) at org.apache.hadoop.util.Shell.run(Shell.java:418) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650) at org.apache.hadoop.util.Shell.execCommand(Shell.java:739) at org.apache.hadoop.util.Shell.execCommand(Shell.java:722) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:631) at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:468) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:424) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:907) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:800) at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123) at org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormatBase.open(HadoopOutputFormatBase.java:145) at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:178) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:654) at java.lang.Thread.run(Thread.java:745)I suggest to disable the test on windows.</description>
      <version>1.1.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.hadoop.mapred.WordCountMapredITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.hadoop.mapreduce.WordCountMapreduceITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapred.WordCountMapredITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapred.HadoopIOFormatsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapreduce.WordCountMapreduceITCase.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopMapredITCase.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputOutputITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5169" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make consumption of input channels fair</summary>
      <description>The input channels on the receiver side of the network stack queue incoming data and notify the input gate about available data. These notifications currently determine the order in which input channels are consumed, which can lead to unfair consumption patterns where faster channels are favored over slower ones.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.SequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.BufferFileWriterReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.SpillingBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.IteratorWrappingTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIOTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIOTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestSubpartitionConsumer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
    </fixedFiles>
  </bug>
  <bug id="5170" opendate="2016-11-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getAkkaConfig will use localhost if hostname is specified</summary>
      <description>in AkkaUtil.scala, def getAkkaConfig(configuration: Configuration, hostname: String, port: Int): Config = { getAkkaConfig(configuration, if (hostname == null) Some((hostname, port)) else None) }when hostname is specified, it use None.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcServiceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5173" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade RocksDB dependency</summary>
      <description>The current RocksDB version has some bugs which have been observed to cause data corruption in at least one case.Newer RocksDB versions also support Microsoft Windows.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5183" opendate="2016-11-28 00:00:00" fixdate="2016-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Support multiple jobs per Python plan file</summary>
      <description>Support running multiple jobs per Python plan file.</description>
      <version>1.1.3</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonMapPartition.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCoGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="5196" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log InputChannelDescriptor</summary>
      <description>Logging the InputChannelDescriptors is very noisy and usually infeasible to parse for larger setups with all-to-all connections.In a log of a larger scale Flink job this lead to a 11 fold reduction in file size (175 to 15 MBs).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5199" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging of submitted job graph actions in HA case</summary>
      <description>Include the involved paths (ZK and FS) when logging and make sure they happen for each operation (put, get, delete).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.SubmittedJobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="5211" opendate="2016-11-30 00:00:00" fixdate="2016-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include an example configuration for all reporters</summary>
      <description>We should extend the reporter documentation to include an example configuration for every reporter.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="5218" opendate="2016-12-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eagerly close checkpoint streams on cancellation</summary>
      <description>Some output streams perform blocking operations that cannot be properly interrupted. This causes cancellations to take very long when happening concurrently to large synchronous state snapshot operations.Closing the streams concurrently helps to abort these blocking operations.This might already be fixed in 1.2 by the CloseableRegistry.</description>
      <version>1.1.3</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.FileStateBackendTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5219" opendate="2016-12-1 00:00:00" fixdate="2016-3-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add non-grouped session windows for batch tables</summary>
      <description>Add non-grouped session windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5223" opendate="2016-12-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation of UDTF in Table API &amp; SQL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5232" opendate="2016-12-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Thread default uncaught exception handler on the JobManager</summary>
      <description>When some JobManager threads die because of uncaught exceptions, we should bring down the JobManager. If a thread dies from an uncaught exception, there is a high chance that the JobManager becomes dysfunctional.The only sfae thing is to rely on the JobManager being restarted by YARN / Mesos / Kubernetes / etc.I suggest to add this code to the JobManager launch:Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler() { @Override public void uncaughtException(Thread t, Throwable e) { try { LOG.error("Thread {} died due to an uncaught exception. Killing process.", t.getName()); } finally { Runtime.getRuntime().halt(-1); } }});</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.JobManagerLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.InstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.akka.FlinkUntypedActorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5239" opendate="2016-12-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly unpack thrown exceptions in RPC methods</summary>
      <description>When an RPC invocation fails, the exception is an InvocationTargetException that contains the actual exception.We should report the actual exception to the sender, rather than the InvocationTargetException.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5240" opendate="2016-12-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly Close StateBackend in StreamTask when closing/canceling</summary>
      <description>Right now, the StreamTask never calls close() on the state backend.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5247" opendate="2016-12-3 00:00:00" fixdate="2016-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect check in allowedLateness() method. Make it a no-op for non-event time windows.</summary>
      <description>Related to FLINK-3714 and FLINK-4239</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="5254" opendate="2016-12-5 00:00:00" fixdate="2016-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Yarn HA Services</summary>
      <description>The Yarn HighAvailability Services should be *Default* This option takes the YARN Application's working directory as HA storage It automatically uses that working directory for the BlobStore It creates a HDFS based "RunningJobsRegistry" (see below) ResourceManager leader election has a pre-configured leader, via the configuration, pointing to the AppMaster address.*ZooKeeper Based* The ZooKeeper based services use ZooKeeper for the ResourceManager and JobManager leader election. That way, they are safe against network partition scenarios that otherwise lead to "split brain" situationsA prototype for the simple "single job" RunningJobsRegistry based on HDFS is here: https://github.com/StephanEwen/incubator-flink/commit/aaa2d7758797b2d6c9b6da42be6a5c4989468e3b</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.StandaloneLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.AllocatedSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.ZookeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.EmbeddedLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.AbstractNonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.NonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.EmbeddedNonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
    </fixedFiles>
  </bug>
  <bug id="5279" opendate="2016-12-7 00:00:00" fixdate="2016-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message when trying to access keyed state in non-keyed operator</summary>
      <description>When trying to access keyed state in a non-keyed operator, the error message is not very helpful. You get a trace like this:java.lang.RuntimeException: Error while getting state...Caused by: java.lang.RuntimeException: State key serializer has not been configured in the config. This operation cannot use partitioned state.It will be helpful to users if this is more explicit to users, stating that the API can only be used on keyed streams, etc.If this applies to the current master as well, we should fix it there, too.</description>
      <version>1.1.3</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.CoBroadcastWithNonKeyedOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5303" opendate="2016-12-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CUBE/ROLLUP/GROUPING SETS operator in SQL</summary>
      <description>Add support for such operators as CUBE, ROLLUP and GROUPING SETS in SQL.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5304" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Change method name from crossApply to join in Table API</summary>
      <description>Currently, the UDTF in Table API is used with crossApply, but is used with JOIN in SQL. UDTF is something similar to Table, so it make sense to use .join("udtf(c) as (s)") in Table API too, and join is more familiar to users than crossApply.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.functions.TableFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5349" opendate="2016-12-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix code sample for Twitter connector</summary>
      <description>There is a typo in code sample for Twitter connector.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.twitter.md</file>
    </fixedFiles>
  </bug>
  <bug id="5353" opendate="2016-12-16 00:00:00" fixdate="2016-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Sink loses well-formed documents when there are malformed documents</summary>
      <description></description>
      <version>1.1.3</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="5357" opendate="2016-12-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordCountTable fails</summary>
      <description>The execution of org.apache.flink.table.examples.java.WordCountTable fails:Exception in thread "main" org.apache.flink.table.api.TableException: POJO does not define field name: TMP_0 at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:85) at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:81) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.flink.table.typeutils.TypeConverter$.determineReturnType(TypeConverter.scala:81) at org.apache.flink.table.plan.nodes.dataset.DataSetCalc.translateToPlan(DataSetCalc.scala:110) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:305) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:289) at org.apache.flink.table.api.java.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:146) at org.apache.flink.table.examples.java.WordCountTable.main(WordCountTable.java:56) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5377" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve savepoint docs</summary>
      <description>The savepoint docs are very detailed and focus on the internals. They should better convey what users have to take care of.The following questions should be answered:What happens if I add a new operator that requires state to my flow?What happens if I delete an operator that has state to my flow?What happens if I reorder stateful operators in my flow?What happens if I add or delete or reorder operators that have no state in my flow?Should I apply .uid to all operators in my flow?Should I apply .uid to only the operators that have state?</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
      <file type="M">docs.fig.savepoints-program.ids.png</file>
      <file type="M">docs.fig.savepoints-overview.png</file>
    </fixedFiles>
  </bug>
  <bug id="5378" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
