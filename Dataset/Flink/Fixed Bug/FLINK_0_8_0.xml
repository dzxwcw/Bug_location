<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="1185" opendate="2014-10-27 00:00:00" fixdate="2014-10-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>State interfaces for streaming checkpointing</summary>
      <description>The implementation of the interfaces provided for the user to use stateful streaming computations.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.GraphState.java</file>
    </fixedFiles>
  </bug>
  <bug id="1187" opendate="2014-10-27 00:00:00" fixdate="2014-10-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>API support for registering persistent states</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamVertex.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.StreamInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11871" opendate="2019-3-11 00:00:00" fixdate="2019-3-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce LongHashTable to improve performance when join key fits in long</summary>
      <description>We can do further optimizations if we know the join key fits in long, a single long field or two integer fields are both ok. For example, we can combine the hash code and actual key into one field, there will be no hash collision, can save a lot of unnecessary logic.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.util.SegmentsUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryStringSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryMapSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryGenericSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11872" opendate="2019-3-11 00:00:00" fixdate="2019-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update lz4 license file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11877" opendate="2019-3-12 00:00:00" fixdate="2019-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement the runtime handling of the InputSelectable interface</summary>
      <description>Introduces a new `Input` interface to represent the logical input of operators. Introduces a new `NetworkInput` class to represent the network input of operators. Introduces a new `StreamTwoInputSelectableProcessor` class to implement selectively reading. Introduces a new `TwoInputSelectableStreamTask` class to execute the operators with the selectively reading. Adds benchmarks for `StreamTwoInputProcessor` and `StreamTwoInputSelectableProcessor` to ensure that StreamTwoInputSelectableProcessor's throughput is the same or the regression is acceptable in the case of constant `InputSelection.ALL`.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.CodeGenOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedClass.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SimpleOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="11878" opendate="2019-3-12 00:00:00" fixdate="2019-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement the runtime handling of BoundedOneInput and BoundedMultiInput</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.InputSelectionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputSelectableStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputSelectableProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InputSelection.java</file>
    </fixedFiles>
  </bug>
  <bug id="11879" opendate="2019-3-12 00:00:00" fixdate="2019-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JobGraph validators for the uses of InputSelectable, BoundedOneInput and BoundedMultiInput</summary>
      <description>Rejects the jobs containing operators which were implemented `InputSelectable` in case of enabled checkpointing. Rejects the jobs containing operators which were implemented `BoundedInput` or `BoundedMultiInput` in case of enabled checkpointing. Rejects the jobs containing operators which were implemented `InputSelectable` in case that credit-based flow control is disabled.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.generated.GeneratedResultFutureWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.generated.GeneratedFunctionWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.generated.GeneratedCollectorWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.GeneratedClass.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SimpleOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="11882" opendate="2019-3-12 00:00:00" fixdate="2019-3-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce BytesHashMap to batch hash agg</summary>
      <description>Introduce bytes based hash table.It can be used for performing aggregations where the aggregated values are fixed-width.Because the data is stored in continuous memory, AggBuffer of variable length cannot be applied to this HashMap. The KeyValue form in hash map is designed to reduce the cost of key fetching in lookup.Add a test to do a complete hash agg. When HashMap has enough memory, pure hash AGG is performed; when memory is insufficient, it degenerates into sort agg.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1189" opendate="2014-10-27 00:00:00" fixdate="2014-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporal cross operator for streaming</summary>
      <description>Provide an operator that crosses (produces the Cartesian product of) windows of data streams.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="1202" opendate="2014-11-3 00:00:00" fixdate="2014-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed file outputs should remove partially complete files</summary>
      <description>Without this, fault tolerance retries may fail because files already exist.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1203" opendate="2014-11-3 00:00:00" fixdate="2014-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCast exceptions in parallel tests (Surefire Bug?)</summary>
      <description>I am frequently seeing weird (non-deterministic) class cast exception in the tests, where apparently casts fail in serial parts of the program. The casting attempts have nothing to do with the program context.java.lang.ClassCastException: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:39)I am wondering whether that might be a strange bug in the forked execution, a possible bug in maven surefire.I propose to deactivate the "reuseFork" option in surefire, to create a clean JVM (and class loaders) for each test.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1208" opendate="2014-11-4 00:00:00" fixdate="2014-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip comment lines in CSV input format. Allow user to specify comment character.</summary>
      <description>The current skipFirstLine is limited. Skipping arbitrary lines that start with a certain character would be much more flexible while still easy to implement.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.io.CsvInputFormatTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvInputFormat.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12081" opendate="2019-4-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce aggregation operator code generator to blink batch</summary>
      <description>1.Introduce aggregation code generator without keys.2.Introduce sort aggregation code generator to deal with all aggregate functions with keys. (Require input in keys order.)3.Introduce hash aggregation code generator to deal with DeclarativeAggregateFunction and aggregateBuffers can be update(e.g.: setInt) in BinaryRow. (Hash Aggregate performs much better than Sort Aggregate)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.OneInputOperatorWrapper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.util.BaseRowTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.codegen.agg.TestLongAvgFunc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="1209" opendate="2014-11-4 00:00:00" fixdate="2014-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forgetting to close an iteration leads to a confusing error message</summary>
      <description>The rror message you get is "Unknown operator - SolutionSetPlaceholder / WorksetPlaceholder / PartialSolutionPlaceholder"</description>
      <version>0.7.0-incubating,0.8.0</version>
      <fixedVersion>0.8.0,0.7.1-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.OperatorTranslation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.IterativeDataSet.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="1210" opendate="2014-11-4 00:00:00" fixdate="2014-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Error Message in Delta Iteratione when Next Workset does not Depend on Workset.</summary>
      <description>Currently, the job fails with a NullPointerException in the NepheleJobGraphGenerator</description>
      <version>0.7.0-incubating,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="12100" opendate="2019-4-3 00:00:00" fixdate="2019-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka 0.10/0.11 tests fail on Java 9</summary>
      <description>java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter at kafka.utils.CoreUtils$.urlSafeBase64EncodeNoPadding(CoreUtils.scala:294) at kafka.utils.CoreUtils$.generateUuidAsBase64(CoreUtils.scala:282) at kafka.server.KafkaServer$$anonfun$getOrGenerateClusterId$1.apply(KafkaServer.scala:335) at kafka.server.KafkaServer$$anonfun$getOrGenerateClusterId$1.apply(KafkaServer.scala:335) at scala.Option.getOrElse(Option.scala:121) at kafka.server.KafkaServer.getOrGenerateClusterId(KafkaServer.scala:335) at kafka.server.KafkaServer.startup(KafkaServer.scala:190) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.getKafkaServer(KafkaTestEnvironmentImpl.java:430) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.prepare(KafkaTestEnvironmentImpl.java:256) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.startClusters(KafkaTestBase.java:137) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:100) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:92) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496) ... 33 more</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1214" opendate="2014-11-5 00:00:00" fixdate="2014-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent partitioning pushdown unless partitions fields match exactly</summary>
      <description>Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).Right now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.Since we cannot determine that robustly yet, I suggest to disable this optimization for now.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.compiler.iterations.ConnectedComponentsTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.SingleInputNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="12143" opendate="2019-4-9 00:00:00" fixdate="2019-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mechanism to ship plugin jars in the cluster</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.Plugin.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.ConnectionLimitingFactory.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlayTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlay.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.bat</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink.bat</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-container.docker.Dockerfile</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginManagerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginLoaderTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.docker.embedded.job.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.wordcount.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.azure.fs.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.dummy.fs.sh</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.GlobalConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12161" opendate="2019-4-11 00:00:00" fixdate="2019-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports partial-final optimization for stream group aggregate</summary>
      <description>To resolve data-skew for distinct aggregates on stream, we introduce a rule named SplitAggregateRule which rewrites an aggregate query with distinct aggregations into an expanded double aggregations. The first aggregation compute the results in sub-partition(with bucket) and the results are combined by the second aggregation.if two-stage aggregation is also enabled, we find that many plans have common pattern, looks like:... (output)StreamExecGlobalGroupAggregate (final global agg)+- StreamExecExchange +- StreamExecLocalGroupAggregate (final local agg) +- StreamExecGlobalGroupAggregate (partial global agg) +- .... (input)There is no exchange between the final local aggregate and the partial global aggregate, so they will be executed in a same JobVertex, and could share state. We introduce a rule named IncrementalAggregateRule to do that optimization.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.TwoStageAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.dataview.DataViewUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.PartialFinalType.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.AggSqlFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12170" opendate="2019-4-12 00:00:00" fixdate="2019-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for Over aggregate</summary>
      <description>This issue aims to generate optimized plan for Over aggregate queries on both Batch and Stream, e.g. SELECT a, b, c, RANK() OVER (PARTITION BY b ORDER BY c) FROM MyTablecurrently, Stream requires all over aggregate functions must be computed on the same window, e.g. SELECT c, COUNT(a) OVER (PARTITION BY c ORDER BY proctime RANGE UNBOUNDED PRECEDING), SUM(a) OVER (PARTITION BY b ORDER BY proctime RANGE UNBOUNDED PRECEDING)from MyTable the above sql is not supported because the partition keys are different.Batch does not have such limitation.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SortAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.HashAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.LeadLagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="1218" opendate="2014-11-5 00:00:00" fixdate="2014-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t use "new Integer". Use "Integer.valueOf" instead.</summary>
      <description>There are lots of "new Integer" or something line it in the code.We should use valueOf to avoid wasted instance creation.</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.WordCountITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.WebLogAnalysisITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQueryAsterixITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TeraSortITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.GlobalSortingMixedOrderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.GlobalSortingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.DanglingPageRankWithCombinerNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.DanglingPageRankNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.ConnectedComponentsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorIterativeITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.NothingSerializer.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JsonFactory.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDXC2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDX1Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDC3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.AggregateTranslationTest.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.graph.TransitiveClosureNaive.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ShortSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ShortComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.LongSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.LongComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.IntSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.IntComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.FloatSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.FloatComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.DoubleSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.DoubleComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ByteSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ByteComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BooleanSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldSetTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldListTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.serialization.TypeSerializationTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.BatchGroupReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.db.DBStateTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.EncoderDecoderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="1221" opendate="2014-11-6 00:00:00" fixdate="2014-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use program source line of invocation as the default function name</summary>
      <description>Right now, the default function name is getClass().getName().In many cases, this is not really very revealing. Using line in the source code where the function is invoked is more helpful.This can be easily obtained byThread.getCurrentThread().getStackTrace()[0]</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.ScalaAPICompletenessTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.crossDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FlatMapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FilterOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.AggregateOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Union.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Operator.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.NamesTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1222" opendate="2014-11-6 00:00:00" fixdate="2014-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let tasks finish as soon as their data is sent</summary>
      <description>Currently tasks finish only after their successor has left the run() method, because only then the close acknowledgements are sent. Sending the close acknowledgements earlier would free resources earlier and lead to more understandable execution timestamps.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.channels.InputChannel.java</file>
    </fixedFiles>
  </bug>
  <bug id="1223" opendate="2014-11-6 00:00:00" fixdate="2014-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow value escaping in CSV files</summary>
      <description>The CSV Parser currently does not interpret escaped valuesThe example from herehttp://en.wikipedia.org/wiki/Comma-separated_values#ExampleYear,Make,Model,Description,Price1997,Ford,E350,"ac, abs, moon",3000.001999,Chevy,"Venture ""Extended Edition""","",4900.00Does not work currently.Here escaping inside the string field generates an error.For reference An interesting post about the fallacies that could be encountered when parsing CSV files.http://tburette.github.io/blog/2014/05/25/so-you-want-to-write-your-own-CSV-code/</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.CsvInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.GenericCsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12232" opendate="2019-4-17 00:00:00" fixdate="2019-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support database related operations in HiveCatalog</summary>
      <description>Support database related operations in HiveCatalog, which implements ReadableWritableCatalog API</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogDatabase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogDatabase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="12238" opendate="2019-4-17 00:00:00" fixdate="2019-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support database related operations in GenericHiveMetastoreCatalog and setup flink-connector-hive module</summary>
      <description>Support database related operations in GenericHiveMetastoreCatalog, which implements ReadableWritableCatalog API</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogDatabase.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1224" opendate="2014-11-7 00:00:00" fixdate="2014-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix .getExecutionEnvironment of the StreamingExecutionEnvironment to return proper context</summary>
      <description>The .getExecutionEnvironment() method for the StreamingexecutionEnvironment does not work properly because it always returns LocalEnvironment for running on the minicluster. A fix needed to make it possible to return the proper context when executing from command line client.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.ClusterUtil.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="1235" opendate="2014-11-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compiler rejectes non-nested iterations in constant path of an Iteration.</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.IterationsCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.BulkIterationNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="1249" opendate="2014-11-18 00:00:00" fixdate="2014-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add custom partitioner for CoGroup</summary>
      <description>Currently, custom partitioners exist only for Join, Partition, and the different groupings.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.JoinCustomPartitioningTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CoGroupOperatorBase.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.operators.CoGroupCompatibilityTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.BinaryCustomPartitioningCompatibilityTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.CoGroupNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="1253" opendate="2014-11-19 00:00:00" fixdate="2014-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests occasionally die with GarbageCollectionOverhead exceeded</summary>
      <description>I have seen tests occasionally dying from GC Overhead Limit exceeded exception. I assume it happens because we reuse the JVMs across tests in the unit tests, possibly some test artifacts linger.I suggest to add the -XX:-UseGCOverheadLimit option to the tests, so that they do not break on the build server.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12611" opendate="2019-5-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make time indicator nullable in blink</summary>
      <description>SQL: select max(rowtime), count(a) from TThere will be a AssertionError: type mismatch:aggCall type:TIMESTAMP(3) NOT NULLinferred type:TIMESTAMP(3)Agg type checking is done before TimeIndicator materializes. So there is a exception.And before introducing nullable of LogicalType, we should modify this to avoid more potential TypeCheck problems.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeMaterializeSqlFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="1263" opendate="2014-11-20 00:00:00" fixdate="2014-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Manual Partition operations are considered to keep data types constant</summary>
      <description>This leads to a big loss in optimization potential and fully voids some cases.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.SingleInputSemanticProperties.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.SingleInputOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.UtilSinkJoinOpDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.OperatorDescriptorDual.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossStreamOuterSecondDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossStreamOuterFirstDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossBlockOuterSecondDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossBlockOuterFirstDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CartesianProductDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.BinaryUnionOpDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.TwoInputNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="1264" opendate="2014-11-20 00:00:00" fixdate="2014-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom partitioners are not properly forwarded to the runtime</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1273" opendate="2014-11-22 00:00:00" fixdate="2014-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the "Void" type to the basic types.</summary>
      <description>We should at the void type (as represented by void.class and {java.lang.Void.class}} to the basic types. The void type would have a one byte serializer (I think zero bytes would not work if someone uses void as an isolated data type). Void could be comparable - all instances would be equal.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.SerializedFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.SequentialFormatTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.GenericArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.EnumSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.EnumComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="12732" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add savepoint reader for consuming partitioned operator state</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.ExistingSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.concurrent.NeverCompleteFuture.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskManagerRuntimeInfo.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointRuntimeContext.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointLocalRecoveryProvider.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.splits.KeyGroupRangeInputSplit.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.MultiStateKeyIterator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.functions.KeyedStateReaderFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="1276" opendate="2014-11-23 00:00:00" fixdate="2014-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misspelled class name SlotAvalablbilityListener.java</summary>
      <description>1. Misspelled Class name - 'SlotAvailablblityListener'.2. All methods in MathUtils.java are declared as static final.3. Many other minor fixes</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.SlotAvailablilityListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Instance.java</file>
    </fixedFiles>
  </bug>
  <bug id="1278" opendate="2014-11-25 00:00:00" fixdate="2014-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the Record special code paths</summary>
      <description>There are some legacy Record code paths in the runtime, which are often forgotten to be kept in sync and cause errors if people actually use records.</description>
      <version>0.8.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.RecordReaderIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.RecordOutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.RecordOutputEmitter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.RecordOutputCollector.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputEmitter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="1290" opendate="2014-11-27 00:00:00" fixdate="2014-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer prunes all candidates when unable to reuse sort properties</summary>
      <description>Programs fail with an exception that no plan could be created.The bug can be reproduced by the following code:val data : DataSet[(Long, Long)] = ...data.distinct(0, 1).groupBy(0).reduceGroup(...)</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.JoinCustomPartitioningTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.CoGroupCustomPartitioningTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.AbstractJoinDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="1291" opendate="2014-11-27 00:00:00" fixdate="2014-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove GC options from JobManager and TaskManager scripts</summary>
      <description>Currently, the start scripts set a series of GC options The Tenured Gen GC is set to ConcurrentMarkSweep The Perm Gen is set to be collected during swipes (CMSClassUnloading) The ratio between Perm Gen and New Gen is fixedI propose to remove all of those options: Users can pass their own GC arguments through the JAVA_OPTS CMS is not considered a competitive GC any more (starting from Java 7, G1 seems much better) PermGen garbage collection happens in all GCs anyways The ratio between TenuredGen and New Gen has proven harmful in the past</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1292" opendate="2014-11-27 00:00:00" fixdate="2014-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for longer normalized keys</summary>
      <description>The normalized key sorter currently has a hardwired limit of a normalized key size of 8 bytes. This still comes from the "single key" era.For composite keys, this length is easily exceeded and sorts cannot happen as fast as the could. On the other hand, setting the value arbitrarily large impacts memory consumption.I propose to extend this to allow for 8 bytes per key up to a total length of 16 bytes. This fits at least two composite longs, or a sequence of Ints.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithRankComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithRankAndDanglingComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithAdjacencyListComparator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorter.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.WritableComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12920" opendate="2019-6-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop support for register_table_sink with field_names and field_types parameters</summary>
      <description>The following registerTableSink API in TableEnvironment is deprecated:@Deprecatedvoid registerTableSink(String name, String[] fieldNames, TypeInformation&lt;?&gt;[] fieldTypes, TableSink&lt;?&gt; tableSink);We can drop the support of it in Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="1296" opendate="2014-12-1 00:00:00" fixdate="2014-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for very large record for sorting</summary>
      <description>Currently, very large records (multiple hundreds of megabytes) can break the sorter if the overflow the sort buffer.Furthermore, if a merge is attempted of those records, pulling multiple of them concurrently into memory can break the machine memory.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memorymanager.MemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.FileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousBlockReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AbstractFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.RandomAccessInputView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortLargeRecordsITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.SeekableFileChannelInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IOManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelOutputView.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimeStatefulSerializerFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MassiveStringSortingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.IntArrayList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.SortBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.InMemorySorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.FixedLengthRecordSorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.CompactingHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="12962" opendate="2019-6-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allows pyflink to be pip installed</summary>
      <description>The aim of this JIRA is to support to build a pip installable package.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.version.py</file>
      <file type="M">flink-python.pyflink.shell.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-shell.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="12963" opendate="2019-6-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add savepoint writer for bootstrapping new savepoints</summary>
      <description>Implement a savepoint writer for bootstrapping new savepoints.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.NewSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.BoundedStreamConfig.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.OperatorSubtaskStateReducer.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.Savepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.OnDiskSavepointMetadata.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.ExistingSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.BootstrapTransformationTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadata.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.MergeOperatorStates.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.KeyedOperatorTransformation.java</file>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WritableSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.ModifiableSavepointMetadata.java</file>
      <file type="M">docs.dev.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="12964" opendate="2019-6-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add commented-out defaults to sql client yaml file to make it easier for users to adopt</summary>
      <description>Add defaults for catalogs, similar to existing defaults of tables and functions, e.g.tables: [] # empty list# A typical table source definition looks like:# - name: ...# type: source-table# connector: ...# format: ...# schema: ...# A typical view definition looks like:# - name: ...# type: view# query: "SELECT ..."# A typical temporal table definition looks like:# - name: ...# type: temporal-table# history-table: ...# time-attribute: ...# primary-key: ...#==============================================================================# User-defined functions#==============================================================================# Define scalar, aggregate, or table functions here.functions: [] # empty list# A typical function definition looks like:# - name: ...# from: class# class: ...# constructor: ...</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="1310" opendate="2014-12-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Closure Cleaner logs very aggressive</summary>
      <description>The Scala Closure Cleaner puts out a lot of messages on INFO level. I vote to reduce this to DEBUG level, as it is not very informative and floods the log.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1311" opendate="2014-12-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auxiliary nodes in iterations are not correctly identified as "dynamic" or "static"</summary>
      <description>The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.IterationCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="13110" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add shim of SimpleGenericUDAFParameterInfo for Hive 1.2.1 and 2.3.4</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
    </fixedFiles>
  </bug>
  <bug id="13115" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rule to support partition pruning for PartitionableTableSource</summary>
      <description>This issue aims to support partition pruning for PartitionableTableSource</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecScanTableSourceRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13116" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Catalog statistic is not bridged to blink planner</summary>
      <description>This issue aims to let blink planner could use catalog statistic</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.testGetStatsFromCatalog.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.plan.stats.TableStats.java</file>
    </fixedFiles>
  </bug>
  <bug id="13117" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Serialize Issue when restoring from savepoint</summary>
      <description>Hi, I have one question.My application should be maintained without loss of data.So I use savepoint to deploy a new application. Normally there is no problem in restarting.However, when the schema of some case classes is changed, a serialize error occurs at restart. How can I resolve this?Flink version is 1.8.0Here is the error log.Thank you.org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible. at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:527) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:475) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:613) at org.apache.flink.runtime.state.KeyedStateFactory.createInternalState(KeyedStateFactory.java:47) at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72) at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:286) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.getOrCreateKeyedState(AbstractStreamOperator.java:568) at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.open(WindowOperator.java:240) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:424) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:290) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.executor.BatchExecutorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13118" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce JDBC table factory</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTableSource.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialect.java</file>
    </fixedFiles>
  </bug>
  <bug id="13119" opendate="2019-7-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add blink table config to documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
    </fixedFiles>
  </bug>
  <bug id="1315" opendate="2014-12-10 00:00:00" fixdate="2014-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spurious failure in compiler due to corrupt branch tracking logic</summary>
      <description>The optimizer fails in that case with the following stack trace:Exception in thread "main" org.apache.flink.compiler.CompilerException: Bug: Tracing dams for deadlock detection is broken. at org.apache.flink.compiler.dag.TwoInputNode.placePipelineBreakersIfNecessary(TwoInputNode.java:618) at org.apache.flink.compiler.dag.TwoInputNode.instantiate(TwoInputNode.java:553) at org.apache.flink.compiler.dag.TwoInputNode.addLocalCandidates(TwoInputNode.java:504) at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:436) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.TwoInputNode.getAlternativePlans(TwoInputNode.java:305) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:268) at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:105) at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:104) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.BulkIterationNode.instantiateCandidate(BulkIterationNode.java:296) at org.apache.flink.compiler.dag.SingleInputNode.addLocalCandidates(SingleInputNode.java:367) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:315) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:105) at org.apache.flink.compiler.dag.BinaryUnionNode.getAlternativePlans(BinaryUnionNode.java:104) at org.apache.flink.compiler.dag.SingleInputNode.getAlternativePlans(SingleInputNode.java:258) at org.apache.flink.compiler.dag.DataSinkNode.getAlternativePlans(DataSinkNode.java:194) at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:561) at org.apache.flink.compiler.PactCompiler.compile(PactCompiler.java:466) at org.apache.flink.client.LocalExecutor.executePlan(LocalExecutor.java:233) at org.apache.flink.api.java.LocalEnvironment.execute(LocalEnvironment.java:51) at org.apache.flink.api.scala.ExecutionEnvironment.execute(ExecutionEnvironment.scala:391)The program that fails in the optimizer is from a user. I have not been able to reduce it significantly into a standalone test case that reproduces the bug.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.WorksetIterationPlanNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.SingleInputPlanNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.BulkIterationPlanNode.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.IterationsCompilerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="1316" opendate="2014-12-10 00:00:00" fixdate="2014-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webclient fails to display plans where nodes are references from multiple iteration closures</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.resources.web-docs.js.graphCreator.js</file>
    </fixedFiles>
  </bug>
  <bug id="13160" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTaleSource should implment PartitionableTableSource</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="1318" opendate="2014-12-10 00:00:00" fixdate="2014-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make quoted String parsing optional and configurable for CSVInputFormats</summary>
      <description>With the current implementation of the CSVInputFormat, quoted string parsing kicks in, if the first non-whitespace character of a field is a double quote. I see two issues with this implementation:1. Quoted String parsing cannot be disabled2. The quoting character is fixed to double quotes (")I propose to add parameters to disable quoted String parsing and set the quote character.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.programming.guide.md</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.io.CsvInputFormatTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.VarLengthStringParserTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.StringValueParserTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.StringParserTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringValueParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FieldParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.GenericCsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1323" opendate="2014-12-12 00:00:00" fixdate="2014-5-12 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Implement memory management profiles</summary>
      <description>Flink currently allocates a sizeable chunk of the JVM heap for the framework, where hashing and sorting is done amongst others. This enables graceful spilling to disk, when this datasize exceeds memory, however might be counterproductive for jobs with large user state and is completely unutilized by streaming at the moment.</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MassiveStringSortingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTablePerformanceComparison.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.SpillingBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerPerformanceBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.ChannelViewsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobKeyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableHashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.io.SerializedUpdateBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.RequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IORequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IOManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelWriterOutputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelReaderInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.Channel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BulkBlockChannelReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.ChannelReaderInputViewIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13231" opendate="2019-7-11 00:00:00" fixdate="2019-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a ratelimiter to pubsub source</summary>
      <description>Replace MaxMessagesToAcknowledge limit by introducing a rate limiter. See: https://github.com/apache/flink/pull/6594#discussion_r300215868</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-gcp-pubsub.src.main.java.org.apache.flink.streaming.examples.gcp.pubsub.PubSubExample.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="1324" opendate="2014-12-14 00:00:00" fixdate="2014-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Iterations may fail when a cached data set based on a sort is not fully consumed</summary>
      <description>This happens for example when a sort feeds into a merge join and the merge join exits early (zig zag merge determined that it can stop).You can reproduce this bug by running the transitive closure in Scala (or Java with an enforced merge join) using the following toy data set.Path(2, 1),Path(4, 1),Path(6, 3),Path(8, 3),Path(10, 1),Path(12, 1),Path(14, 3),Path(16, 3),Path(18, 1),Path(20, 1) );</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatingMap.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatableDotProductMatch.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatableDotProductCoGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13362" opendate="2019-7-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Kafka &amp; ES &amp; FileSystem DDL</summary>
      <description>Add documentation for Kafka &amp; ES &amp; FileSystem DDL “Connect to External Systems”: Add DDL for Kafka &amp; ES &amp; FileSystem.</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="13390" opendate="2019-7-23 00:00:00" fixdate="2019-4-23 01:00:00" resolution="Done">
    <buginformation>
      <summary>Clarify the exact meaning of state size when executing incremental checkpoint</summary>
      <description>This issue is inspired from a user mail which confused about the state size meaning.I think changing the description of state size and add some notices on documentation could help this. Moreover, change the log when complete checkpoint should be also taken into account.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">docs..includes.generated.common.state.backends.section.html</file>
      <file type="M">docs..includes.generated.checkpointing.configuration.html</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
    </fixedFiles>
  </bug>
  <bug id="13393" opendate="2019-7-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink-planner not support generic TableSource</summary>
      <description>Now there is a exception when user use table source like:class MyTableSource[T] extend StreamTableSource[T]The reason is that blink-planner use TypeExtractor to extract class from TableSource, and use this class to DataFormatConverter.Now, table source has DataType return type, so we don't need extract class from TableSource, we can just use conversionClass of DataType.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1385" opendate="2015-1-10 00:00:00" fixdate="2015-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to YARN client to disable resource availability checks</summary>
      <description>The YARN client checks which resources are available in the cluster at the time of the initial deployment.If somebody wants to deploy a Flink YARN session to a full YARN cluster, the Flink Client will reject the session.I'm going to add an option which disables the check. The YARN session will then allocate new containers as they become available in the cluster.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="1386" opendate="2015-1-11 00:00:00" fixdate="2015-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java Quickstart Fails to import properly in Eclipse</summary>
      <description></description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1391" opendate="2015-1-12 00:00:00" fixdate="2015-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kryo fails to properly serialize avro collection types</summary>
      <description>Before FLINK-610, Avro was the default generic serializer.Now, special types coming from Avro are handled by Kryo .. which seems to cause errors like:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: java.lang.NullPointerException at org.apache.avro.generic.GenericData$Array.add(GenericData.java:200) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:143) at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:148) at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:244) at org.apache.flink.runtime.plugable.DeserializationDelegate.read(DeserializationDelegate.java:56) at org.apache.flink.runtime.io.network.serialization.AdaptiveSpanningRecordDeserializer.getNextRecord(AdaptiveSpanningRecordDeserializer.java:71) at org.apache.flink.runtime.io.network.channels.InputChannel.readRecord(InputChannel.java:189) at org.apache.flink.runtime.io.network.gates.InputGate.readRecord(InputGate.java:176) at org.apache.flink.runtime.io.network.api.MutableRecordReader.next(MutableRecordReader.java:51) at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:53) at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:170) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:257) at java.lang.Thread.run(Thread.java:744)</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-staging.flink-avro.src.test.java.org.apache.flink.api.io.avro.generated.User.java</file>
      <file type="M">flink-staging.flink-avro.src.test.java.org.apache.flink.api.io.avro.generated.Colors.java</file>
      <file type="M">flink-staging.flink-avro.src.test.java.org.apache.flink.api.io.avro.AvroRecordInputFormatTest.java</file>
      <file type="M">flink-staging.flink-avro.src.test.java.org.apache.flink.api.avro.EncoderDecoderTest.java</file>
      <file type="M">flink-staging.flink-avro.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoComparator.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
      <file type="M">docs.example.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="1392" opendate="2015-1-12 00:00:00" fixdate="2015-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serializing Protobuf - issue 1</summary>
      <description>Hi, I started to experiment with Parquet using Protobuf.When I use the standard Protobuf class: com.twitter.data.proto.tutorial.AddressBookProtosThe code which I run, can be found here: https://github.com/FelixNeutatz/incubator-flink/blob/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce/example/ParquetProtobufOutput.javaI get the following exception: Exception in thread "main" java.lang.Exception: Deserializing the InputFormat (org.apache.flink.api.java.io.CollectionInputFormat) failed: Could not read the user code wrapper: Error while deserializing element from collection at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:60) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$5.apply(JobManager.scala:179) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$5.apply(JobManager.scala:172) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:172) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:34) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:27) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:27) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:52) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.operators.util.CorruptConfigurationException: Could not read the user code wrapper: Error while deserializing element from collection at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:285) at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:57) ... 25 moreCaused by: java.io.IOException: Error while deserializing element from collection at org.apache.flink.api.java.io.CollectionInputFormat.readObject(CollectionInputFormat.java:108) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:274) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:236) at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:281) ... 26 moreCaused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationExceptionSerialization trace:phone_ (com.twitter.data.proto.tutorial.AddressBookProtos$Person) at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125) at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:130) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:106) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) at org.apache.flink.api.java.io.CollectionInputFormat.readObject(CollectionInputFormat.java:103) ... 42 moreCaused by: java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableCollection.add(Collections.java:1075) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22) at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ... 48 more</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1419" opendate="2015-1-19 00:00:00" fixdate="2015-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DistributedCache doesn&amp;#39;t preserver files for subsequent operations</summary>
      <description>When subsequent operations want to access the same files in the DC it frequently happens that the files are not created for the following operation.This is fairly odd, since the DC is supposed to either a) preserve files when another operation kicks in within a certain time window, or b) just recreate the deleted files. Both things don't happen.Increasing the time window had no effect.I'd like to use this issue as a starting point for a more general discussion about the DistributedCache. Currently:1. all files reside in a common job-specific directory2. are deleted during the job.One thing that was brought up about Trait 1 is that it basically forbids modification of the files, concurrent access and all. Personally I'm not sure if this a problem. Changing it to a task-specific place solved the issue though.I'm more concerned about Trait #2. Besides the mentioned issue, the deletion is realized with the scheduler, which adds a lot of complexity to the current code. (It really is a pain to work on...) If we moved the deletion to the end of the job it could be done as a clean-up step in the TaskManager, With this we could reduce the DC to a cacheFile(String source) method, the delete method in the TM, and throw out everything else.Also, the current implementation implies that big files may be copied multiple times. This may be undesired, depending on how big the files are.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="1422" opendate="2015-1-20 00:00:00" fixdate="2015-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing usage example for "withParameters"</summary>
      <description>I am struggling to find a usage example of the "withParameters" method in the documentation. At the moment I only see this note:Note: As the content of broadcast variables is kept in-memory on each node, it should not become too large. For simpler things like scalar values you can simply make parameters part of the closure of a function, or use the withParameters(...) method to pass in a configuration.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="14221" opendate="2019-9-25 00:00:00" fixdate="2019-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support drop temp system functions and temp catalog functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="1485" opendate="2015-2-5 00:00:00" fixdate="2015-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in Documentation - Join with Join-Function</summary>
      <description>Small typo in documentationIn the java example for "Join with Join-Function"</description>
      <version>0.8.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="15620" opendate="2020-1-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Done">
    <buginformation>
      <summary>State TTL: Remove deprecated enable default background cleanup</summary>
      <description>Follow-up for FLINK-15606.</description>
      <version>None</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.ttl.RocksDBTtlStateTestBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.DataStreamStateTTLTestProgram.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.state.StateTtlConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="15640" opendate="2020-1-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to set user specified labels for JM/TM pod</summary>
      <description>Navigate to Kubernetes doc for more information.public static final ConfigOption&lt;String&gt; JOB_MANAGER_USER_LABELS = key("kubernetes.jobmanager.labels") .noDefaultValue() .withDescription("The labels to be set for JobManager replica controller, service and pods. " + "Specified as key:value pairs separated by commas. such as version:alphav1,deploy:test.");public static final ConfigOption&lt;String&gt; TASK_MANAGER_USER_LABELS = key("kubernetes.taskmanager.labels") .noDefaultValue() .withDescription("The labels to be set for TaskManager pods. " + "Specified as key:value pairs separated by commas. such as version:alphav1,deploy:test.");</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesTaskManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="1567" opendate="2015-2-17 00:00:00" fixdate="2015-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to switch between Avro and Kryo serialization for GenericTypes</summary>
      <description>Allow users to switch the underlying serializer for GenericTypes.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9,0.8.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-staging.flink-avro.src.test.java.org.apache.flink.api.io.avro.AvroPojoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.WebInfoServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.AvroSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="15670" opendate="2020-1-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka Shuffle: uses Kafka as a message bus to shuffle and persist data at the same time</summary>
      <description>This Source/Sink pair would serve two purposes:1. You can read topics that are already partitioned by key and process them without partitioning them again (avoid shuffles)2. You can use this to shuffle through Kafka, thereby decomposing the job into smaller jobs and independent pipelined regions that fail over independently.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.PropertiesUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.shuffle.StreamKafkaShuffleSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffleConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaShuffleFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.PropertiesUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15672" opendate="2020-1-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Log4j 2 by default</summary>
      <description>Log4j 1.x is outdated and has multiple problems: No dynamic adjustments of Log Level Problems with newer Java Versions Not actively developed any moreSwitching to Log4J 2 by default would solve this.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-scala.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-scala.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-scala-shell.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime-web.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-optimizer.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-mesos.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-kubernetes.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-fs-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-parquet.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-orc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-compress.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-dist.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-container.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-hbase.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-clients.src.test.resources.log4j-test.properties</file>
      <file type="M">tools.log4j-travis.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.resources.log4j.properties</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.resources.log4j.properties</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.resources.log4j.properties</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.local.recovery.and.scheduling.sh</file>
      <file type="M">flink-dist.src.main.resources.log4j-bash-utils.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-yarn-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-cli.properties</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.travis.nightly.sh</file>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">tools.releasing.LICENSE.slf4j</file>
      <file type="M">tools.releasing.collect.license.files.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.logging.zh.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.log4j-provider.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.bat</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster-job.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.Slf4jReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.TestUtils.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunctionTest.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="15675" opendate="2020-1-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add exception and documentation that Python UDF is not supported for Flink Planner under batch mode</summary>
      <description>We should add straightforward exceptions and document to info users that Python UDF is not supported for Flink Planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="1578" opendate="2015-2-18 00:00:00" fixdate="2015-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Overhaul BLOB manager</summary>
      <description>The BLOB manager need improvements: Decent failure tests Better error handling (letting the client know what happened) Better error logging Retries upon failed fetches A bit of control over the maximum number of concurrent connections and the backlog</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.AbstractIDTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobKey.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.AbstractID.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="1582" opendate="2015-2-18 00:00:00" fixdate="2015-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SocketStream gets stuck when socket closes</summary>
      <description>When the server side of the socket closes the socket stream reader does not terminate. When the socket is reinitiated it does not reconnect just gets stuck.It would be nice to add options for the user have the reader should behave when the socket is down: terminate immediately (good for testing and examples) or wait a specified time - possibly forever.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.SourceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15991" opendate="2020-2-11 00:00:00" fixdate="2020-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Chinese documentation for FLIP-49 TM memory model</summary>
      <description>Chinese translation of FLINK-15143</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.index.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="15993" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeout to 404 documentation redirect, add explanation</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.404.base.html</file>
    </fixedFiles>
  </bug>
  <bug id="1664" opendate="2015-3-9 00:00:00" fixdate="2015-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forbid sorting on POJOs</summary>
      <description>Flink's groupSort, partitionSort, and outputSort operators allow to sort partitions or groups of a DataSet.If the sort is defined on a POJO field, the sort order is not well defined. Internally, the POJO is recursively decomposed into atomic fields (primitives or generic types) and sorted by sorting these atomic fields. Thereby, the order of these atomic fields is not well defined (I believe it is lexicographic order of the POJO's member names).IMO, the best approach is to forbid sorting on POJO types for now. Instead, it is always possible to select the nested fields of the POJO that should be used for sorting. Later we can relax this restriction.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DataSinkTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.SortPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.CompositeType.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.TypeInformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="16650" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support LocalZonedTimestampType for Python UDF in blink planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.common.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.AbstractArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.BaseRowArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.AbstractPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18202" opendate="2020-6-9 00:00:00" fixdate="2020-7-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Protobuf format</summary>
      <description>PB&amp;#91;1&amp;#93; is a very famous and wildly used (de)serialization framework. The ML&amp;#91;2&amp;#93; also has some discussions about this. It's a useful feature.This issue maybe needs some designs, or a FLIP.&amp;#91;1&amp;#93; https://developers.google.com/protocol-buffers&amp;#91;2&amp;#93; http://apache-flink.147419.n8.nabble.com/Flink-SQL-UDF-td3725.html</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
