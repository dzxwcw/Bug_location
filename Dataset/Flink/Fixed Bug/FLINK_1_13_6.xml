<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="24492" opendate="2021-10-9 00:00:00" fixdate="2021-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect implicit type conversion between numeric and (var)char</summary>
      <description>The result of the sql "select 1 = '1'" is false. This is caused by the CodeGen. CodeGenÂ  incorrectly transform this "=" to "BinaryStringData.equals (int 1)". And "&lt;&gt;" has the same wrong result.In my opinion, "=" should have the same behavior with "&gt;" and "&lt;", which have the correct results. So before calcite solves this bug or flink supports this kind of implicit type conversion, we'd better temporarily forbidding this implicit type conversion in "=" and "&lt;&gt;".</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25472" opendate="2021-12-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Log4j 2.17.1</summary>
      <description>We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.</description>
      <version>1.12.8,1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="26223" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making ZK-related logs available in tests</summary>
      <description>Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.We have multiple options: Introduce an extension to change the ZK log level for specific tests Lower the ZK log level again and make the logs being written to the standard log files Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs</description>
      <version>1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2642" opendate="2015-9-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Table API crashes when executing word count example</summary>
      <description>I tried to run the examples provided in the documentation of Flink's Table API. Unfortunately, the Scala word count example provided in the documentation doesn't work and does not give a meaningful exception.(Other examples work fine)Here my code:package org.apache.flink.examples.scalaimport org.apache.flink.api.scala._import org.apache.flink.api.scala.table._object WordCount { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment case class WC(word: String, count: Int) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable val result = expr.groupBy('word).select('word, 'count.sum as 'count).toDataSet[WC] result.print() }}Here the thrown exception:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.ExpressionSelectFunction caused an exception: null at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1368) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.openTask(ChainedMapDriver.java:47) at org.apache.flink.runtime.operators.RegularPactTask.openChainedTasks(RegularPactTask.java:1408) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:142) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:30) at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:23) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) at org.apache.flink.api.table.codegen.IndentStringContext.j(Indenter.scala:23) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:55) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:32) at org.apache.flink.api.table.codegen.ExpressionCodeGenerator.generate(ExpressionCodeGenerator.scala:66) at org.apache.flink.api.table.runtime.ExpressionSelectFunction.open(ExpressionSelectFunction.scala:46) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33) at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1366) ... 5 more</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26467" opendate="2022-3-3 00:00:00" fixdate="2022-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile RowDataToStringConverter lazily</summary>
      <description>Currently, we prepare for `print()` whenever `sqlQuery` is called. However, we could postpone the compilation until it is really needed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="26961" opendate="2022-3-31 00:00:00" fixdate="2022-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1</summary>
      <description>There is a High CVE-2020-36518, https://github.com/advisories/GHSA-57j2-w4cx-62h2which was fixed with 2.13.2.1</description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27059" opendate="2022-4-5 00:00:00" fixdate="2022-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-compress</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressionFactoryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27155" opendate="2022-4-9 00:00:00" fixdate="2022-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce multiple reads to the same Changelog file in the same taskmanager during restore</summary>
      <description>BackgroundIn the current implementation, State changes of different operators in the same taskmanager may be written to the same changelog file, which effectively reduces the number of files and requests to DFS.But on the other hand, the current implementation also reads the same changelog file multiple times on recovery. More specifically, the number of times the same changelog file is accessed is related to the number of ChangeSets contained in it. And since each read needs to skip the preceding bytes, this network traffic is also wasted.The result is a lot of unnecessary request to DFS when there are multiple slots and keyed state in the same taskmanager.ProposalWe can reduce multiple reads to the same changelog file in the same taskmanager during restore.One possible approach is to read the changelog file all at once and cache it in memory or local file for a period of time when reading the changelog file.I think this could be a subtask of v2 FLIP-158: Generalized incremental checkpoints .Hi ym , romanÂ  how do you think about ?</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.DeactivatedChangelogStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorageFactory.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFormat.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageForRecovery.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageFactory.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorage.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.fs.state.changelog.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="27159" opendate="2022-4-11 00:00:00" fixdate="2022-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support first_value/last_value in the Table API</summary>
      <description>Currently, first_value/last_value are not supported in Table API.table.group_by(col("a")) .select( col("a"), call("FIRST_VALUE", col("b")))</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27196" opendate="2022-4-12 00:00:00" fixdate="2022-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old format interfaces</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TableFormatFactoryBaseTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFormatFactoryBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFactoryService.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.SerializationSchemaFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.DeserializationSchemaFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.SchemaValidator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27282" opendate="2022-4-18 00:00:00" fixdate="2022-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of wrong positions mapping in RowCoder</summary>
      <description></description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.examples.table.basic.operations.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
      <file type="M">docs.content.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="27776" opendate="2022-5-25 00:00:00" fixdate="2022-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when UDAF used in sliding window does not implement merge method in PyFlink</summary>
      <description>We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:class SumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return accumulator[0] def create_accumulator(self): return [0] def accumulate(self, accumulator, *args): accumulator[0] = accumulator[0] + args[0] def retract(self, accumulator, *args): accumulator[0] = accumulator[0] - args[0] def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] = accumulator[0] + other_acc[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.BIGINT()</description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="27778" opendate="2022-5-25 00:00:00" fixdate="2022-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>table-planner should not use CodeSplitUtils#newName</summary>
      <description>The table-planner has a direct dependency on the table-code-splitter, as several CastRules use CodeSplitUtil.newName.This dependency is a bit hidden. In the IDE it is pulled in transitively via table-runtime, and in maven it uses the table-code-splitter dependency bundled by table-runtime.It would be nice if we could add a provided dependency to the table-code-splitter to properly document this.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToBinaryCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="27837" opendate="2022-5-30 00:00:00" fixdate="2022-12-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support statement set in the SQL Gateway</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="28522" opendate="2022-7-12 00:00:00" fixdate="2022-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-sequence-file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SerializableHadoopConfigurationTest.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SequenceStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="28765" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create a doc for protobuf format</summary>
      <description>After this feature has been done https://issues.apache.org/jira/browse/FLINK-18202, we should write a doc to introduce how to use the protobuf format in SQL.Â </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.connectors.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28978" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector doesn&amp;#39;t work for new AWS regions</summary>
      <description>The current validation in the Kinesis connector checks that the AWS Region string specified is present in the Regions enum attached in the AWS SDK. This is not desirable because every time AWS launches a new region, we will have to update the AWS SDK shaded into the connector.Â We want to change it such that we validate the shape of the string, allowing for future AWS Regions.Â Â Current list of regions:ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, ap-southeast-3, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global</description>
      <version>1.13.6,1.14.5,1.15.1</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSGeneralUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.main.java.org.apache.flink.connector.aws.util.AWSGeneralUtil.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
