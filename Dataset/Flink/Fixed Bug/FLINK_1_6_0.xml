<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10001" opendate="2018-7-31 00:00:00" fixdate="2018-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Kubernetes documentation</summary>
      <description>We should update Flink's K8s documentation. This includes running it on MiniKube as well as on a K8s cluster.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.docker-compose.test.yml</file>
      <file type="M">flink-container.kubernetes.README.md</file>
      <file type="M">flink-container.docker.README.md</file>
      <file type="M">flink-container.docker.docker-compose.yml</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="10005" opendate="2018-7-31 00:00:00" fixdate="2018-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingFileSink ignores checkpoint/processing time rolling policies</summary>
      <description>The StreamingFileSink supports different policies to determine whether a new part file should be created; on each checkpoint, once a certain size is reached or on processing time.This feature only works correctly for size thresholds, other policies are ignored.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="10009" opendate="2018-7-31 00:00:00" fixdate="2018-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the casting problem for function TIMESTAMPADD in Table</summary>
      <description>There seems to be a bug in TIMESTAMPADD function. For example, TIMESTAMPADD(MINUTE, 1, DATE '2016-06-15') throws a ClassCastException ( java.lang.Integer cannot be cast to java.lang.Long). Actually, it tries to cast an integer date to a long timestamp in RexBuilder.java:1524 - return TimestampString.fromMillisSinceEpoch((Long) o).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10021" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>All-round E2E tests query metric with wrong operator name</summary>
      <description>In FLINK-8994 the naming scheme for operators of the DataStreamAllroundTestProgram was modified, but the change wasn't propagated to other users of the test.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10027" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging to the StreamingFileSink</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="10028" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce ByteArrayData[Input|Output]View</summary>
      <description>In many place, I found that we require to a have a combination of ByteArray&amp;#91;Input/Output&amp;#93;StreamWithPos and the corresponding Data&amp;#91;Input|Output&amp;#93;ViewStreamWrapper, because we essentially want an adapter from byte[] to Data&amp;#91;Input|Output&amp;#93;View.Instead of handling a combination of two related objects all over the place, I suggest to introduce ByteArrayData&amp;#91;Input|Output&amp;#93;View that combine the features.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBCachingPriorityQueueSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="10029" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Streaming File Sink for better separation of concerns.</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.TestUtils.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkWriterTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWisePartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.OnCheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileInfo.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.DefaultBucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.SimpleVersionedStringSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.DateTimeBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.BasePathBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="10066" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keep only archived version of previous executions</summary>
      <description>Currently, the execution vertex stores a limited amount of previous executions in a bounded list. This happens primarily for archiving purposes and to remember previous locations and allocation ids. We remember the whole execution to eventually convert it into an archived execution.This seems unnecessary and dangerous as we have observed that this strategy is prone to memory leaks in the job manager. With a very high vertex count or parallelism, remembering complete executions can become very memory intensive. Instead I suggest to eagerly transform the executions into the archived version before adding them to the list, i.e. only the archived version is ever still referenced after the execution becomes obsolete. This gives better control over which information about the execution should really be kept in memory.</description>
      <version>1.4.3,1.5.2,1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EvictingBoundedListTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedJobGenerationUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EvictingBoundedList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecution.java</file>
    </fixedFiles>
  </bug>
  <bug id="10068" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation for async/RocksDB-based timers</summary>
      <description>Documentation how to activate RocksDB based timers, and update that snapshotting now works async, expect for heap-timers + rocks-incremental-snapshot).</description>
      <version>None</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="10069" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for updates SSL model</summary>
      <description>Add docs about the "internal" versus "external" connectivity and new configuration options.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="10070" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink cannot be compiled with maven 3.0.x</summary>
      <description>In FLINK-9986 we bumped the version of the git-commit-id-plugin to 2.1.14 which is incompatible with various maven versions, like 3.0.X.We can either bump the version to 2.2.4 to support all versions, or downgrade to 2.1.9 and rework FLINK-9986 with property exclusions.Additionally we should setup a test that checks the compatibility of Flink with various maven versions.</description>
      <version>1.5.3,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10071" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document usage of INSERT INTO in SQL Client</summary>
      <description>Document the usage of INSERT INTO statements in SQL.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="10075" opendate="2018-8-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTTP connections to a secured REST endpoint flood the log</summary>
      <description>When connecting with a browser (or other client tool) to a secured REST endpoint, the decoder throws many exceptions indicating that the received data is not an SSL record.This massively floods the log, drowning out everything else (see below).Proposed SolutionIf a NotSslRecordException is caught, Netty should send a response HTTP 301 with a new location of https://host:port/The response would need to bypass the SSL handler because it must come in plain text.Fallback SolutionIf the proper solution cannot work, we should reduce the log level for that particular exception to TRACE.Sample Log OutputLog message that is written per each request (there are many per web UI page)2018-08-06 19:07:57,734 WARN org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Unhandled exceptionorg.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: org.apache.flink.shaded.netty4.io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 474554202f7061727469616c732f6f766572766965772e68746d6c20485454502f312e310d0a486f73743a206c6f63616c686f73743a383038310d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a4163636570743a20746578742f68746d6c0d0a557365722d4167656e743a204d6f7a696c6c612f352e3020285831313b204c696e7578207838365f363429204170706c655765624b69742f3533372e333620284b48544d4c2c206c696b65204765636b6f29204368726f6d652f34372e302e323532362e313131205361666172692f3533372e33360d0a526566657265723a20687474703a2f2f6c6f63616c686f73743a383038312f0d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174652c20736463680d0a4163636570742d4c616e67756167653a20656e2d55532c656e3b713d302e382c64653b713d302e360d0a49662d4d6f6469666965642d53696e63653a204d6f6e2c2030362041756720323031382031353a34343a313720474d540d0a0d0a at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.5.2,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10076" opendate="2018-8-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.18</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10085" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update AbstractOperatorRestoreTestBase</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10094" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always backup default config for end-to-end tests</summary>
      <description>end-to-end tests frequently modify the configuration before running a test; to set state backends, # of task slots, reporters, etc.Currently it is the responsibility of the test to create a backup of the config before doing any modifications, which is then restored by the runner after the test.Instead we can have the test runner always create a backup before running the test.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stateful.stream.job.upgrade.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.queryable.state.restart.tm.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.local.recovery.and.scheduling.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10095" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Change the serialisation order in TTL value wrapper</summary>
      <description>The first implementation of TTL value wrapper has the following serialisation order: first user value, then last modification timestamp.This was planned for potential optimisation where we could peek at the last bytes of last element of list to check whether the whole list expired or not. After careful consideration, it is not the frequent case and the list is handled per entry mostly.Having the fixed part (the timestamp) at the beginning looks more promising in future as we can skip looking into the rest of bytes if needed when working directly with serialised value.This issue suggests to change the serialisation order.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateSnapshotTransformer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10102" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EXECUTION_FAILOVER_STRATEGY docs are wrong</summary>
      <description>According to the documentation this option controls "The maximum number of prior execution attempts kept in history.", which obviously isn't correct.PR: https://github.com/apache/flink/pull/6303</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10107" opendate="2018-8-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails for releases</summary>
      <description>It seems that SQL JARs for Kafka 0.10 and Kafka 0.9 have conflicts that only occur for releases and not SNAPSHOT builds. This might be due to their file name. Depending on the file name either 0.9 is loaded before 0.10 and vice versa.One of the following errors occured:2018-08-08 18:28:51,636 ERROR org.apache.flink.kafka09.shaded.org.apache.kafka.clients.ClientUtils - Failed to close coordinatorjava.lang.NoClassDefFoundError: org/apache/flink/kafka09/shaded/org/apache/kafka/common/requests/OffsetCommitResponse at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:473) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:357) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.maybeAutoCommitOffsetsSync(ConsumerCoordinator.java:439) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.close(ConsumerCoordinator.java:319) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.ClientUtils.closeQuietly(ClientUtils.java:63) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1277) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1258) at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:286)Caused by: java.lang.ClassNotFoundException: org.apache.flink.kafka09.shaded.org.apache.kafka.common.requests.OffsetCommitResponse at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$ChildFirstClassLoader.loadClass(FlinkUserCodeClassLoaders.java:120) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 8 morejava.lang.NoSuchFieldError: producer at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.invoke(FlinkKafkaProducer010.java:369) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:689) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:667) at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:689) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:667) at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) at org.apache.flink.table.runtime.CRowWrappingCollector.collect(CRowWrappingCollector.scala:37) at org.apache.flink.table.runtime.CRowWrappingCollector.collect(CRowWrappingCollector.scala:28)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10115" opendate="2018-8-9 00:00:00" fixdate="2018-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Content-length limit is also applied to FileUploads</summary>
      <description>Uploading jar files via WEB UI not working. After initializing upload... it only shows saving... and file never shows up on UI to be able to submit it</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10124" opendate="2018-8-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use ByteArrayDataInput/OutputView instead of stream + wrapper</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBAppendingState.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayDataInputView.java</file>
    </fixedFiles>
  </bug>
  <bug id="10127" opendate="2018-8-12 00:00:00" fixdate="2018-8-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add TypeInformation and serializers for JDK8 Instant</summary>
      <description>Currently Flink's basic types include all Java primitives and their boxed form, plus void, String, Date, BigDecimal, and BigInteger. New JDK8 Instance type should be added as well</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.Types.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.BasicTypeInfoTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.Types.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="10135" opendate="2018-8-13 00:00:00" fixdate="2018-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Certain cluster-level metrics are no longer exposed</summary>
      <description>In the documentation for metrics in the Flink 1.5.0 release, it says that the following metrics are reported by the JobManager:numRegisteredTaskManagersnumRunningJobstaskSlotsAvailabletaskSlotsTotalIn the job manager REST endpoint (http://&lt;job-manager&gt;:8081/jobmanager/metrics), those metrics don't appear.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="10142" opendate="2018-8-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="10145" opendate="2018-8-14 00:00:00" fixdate="2018-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add replace supported in TableAPI and SQL</summary>
      <description>replace is an useful function for String. for example:select replace("Hello World", "World", "Flink") // return "Hello Flink"select replace("ababab", "abab", "z") // return "zab"It is supported as a UDF in Hive, more details please see&amp;#91;1&amp;#93;&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="10150" opendate="2018-8-15 00:00:00" fixdate="2018-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained batch operators interfere with each other other</summary>
      <description>The flink web ui displays an inconsistent number of "Records received" / "Records sent” in the job overview "Subtasks" view.When I run the example wordcount batch job with a small input file on flink 1.3.2 I get 3 records sent by the first subtask and 3 records received by the second subtaskThis is the result I would expect.If I run the same job on flink 1.4.0 / 1.5.2 / 1.6.0 I get 13 records sent by the first subtask and 3 records received by the second subtaskIn real life jobs the numbers are much more strange.</description>
      <version>1.4.0,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="10153" opendate="2018-8-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add tutorial section to documentation</summary>
      <description>The current documentation does not feature a dedicated tutorials section and has a few issues that should be fix in order to help our (future) users getting started with Flink.I propose to add a single "Tutorials" section to the documentation where users find step-by-step guides. The tutorials section help users with different goals: Get a quick idea of the overall system Implement a DataStream/DataSet/Table API/SQL job Set up Flink on a local machine (or run a Docker container)There are already a few guides to get started but they are located at different places and should be moved into the Tutorials section. Moreover, some sections such as "Project Setup" contain content that addresses users with very different intentions.I propose to add a new Tutorials section and move all existing tutorials there (and later add new ones). move the "Quickstart" section to "Tutorials". remove the "Project Setup" section and move the pages to other sections (some pages will be split up or adjusted).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.start.index.md</file>
      <file type="M">docs.start.flink.on.windows.md</file>
      <file type="M">docs.start.dependencies.md</file>
      <file type="M">docs.start.building.md</file>
      <file type="M">docs.redirects.scala.shell.md</file>
      <file type="M">docs.redirects.linking.with.optional.modules.md</file>
      <file type="M">docs.redirects.linking.with.flink.md</file>
      <file type="M">docs.redirects.java8.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.internals.index.md</file>
      <file type="M">docs.internals.ide.setup.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
      <file type="M">docs.dev.stream.python.md</file>
      <file type="M">docs.dev.scala.shell.md</file>
      <file type="M">docs.dev.index.md</file>
      <file type="M">docs.dev.best.practices.md</file>
      <file type="M">docs.dev.batch.examples.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug id="10154" opendate="2018-8-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure we always read at least one record in KinesisConnector</summary>
      <description>It's possible in some cases to request zero records from Kinesis in the Kinesis connector.  This can happen when the "adpative reads" feature is enabled. </description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10159" opendate="2018-8-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHarness#initializeState(xyz) calls after TestHarness#open() are being silently ignored</summary>
      <description>This is an old issue. Incorrect order of initializeState and open result to initializeState being ignored. For example in this code:testHarness = createTestHarness(topic);testHarness.setup();testHarness.open();testHarness.initializeState(snapshot1);Which is miss-leading both for Flink developers and for users (since we recommend using test harness for unit tests).</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.checkpoint.ListCheckpointedTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10164" opendate="2018-8-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for resuming from savepoints to StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="10166" opendate="2018-8-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency problems when executing SQL query in sql-client</summary>
      <description>When tried to run query:select count(distinct name) from (Values ('a'), ('b')) AS NameTable(name)in sql-client.sh I got:[ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 43, Column 10: Unknown variable or type "org.apache.commons.codec.binary.Base64"</description>
      <version>1.6.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.typeutils.TypeCheckUtilsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.TreeNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.HashCalcCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.descriptors.DescriptorProperties.java</file>
    </fixedFiles>
  </bug>
  <bug id="10174" opendate="2018-8-20 00:00:00" fixdate="2018-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getbytes with no charsets test error for hex and toBase64</summary>
      <description>Hex and toBase64 builtin method use str.getBytes() with no Charset. It maybe depend on local execution environment for special Unicode and maybe result in errors when test Hex for special Unicode</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1018" opendate="2014-7-10 00:00:00" fixdate="2014-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logistic Regression deadlocks</summary>
      <description>We are currently running our implementation of logistic regression with batch gradient descent on the cluster.Unfortunatelly for datasets &gt; 1GB it seems to deadlock inside of the iteration. This means the first iteration is never finished.The iteration does a map over all points, the map gets the iteration input as broadcast variable. The result of the map is reduced and the result of the reducer (1 tuple) is crossed with the iteration input.There should be no reason for the deadlock, since the data is still quite small compared to the cluster size (4 nodes a 32GB). Also the datasize stays constant throughout the algorithm.Here is the generated plan. I will also attach the full algorithm.{ "nodes": [ { "id": 2, "type": "source", "pact": "Data Source", "contents": "[([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.", "parallelism": "1", "subtasks_per_instance": "1", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "0.0 B" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "step_function": [ { "id": 8, "type": "source", "pact": "Data Source", "contents": "TextInputFormat (hdfs://cloud-7:45010/tmp/input/higgs.M.txt) - UTF-8", "parallelism": "64", "subtasks_per_instance": "16", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "8.0.31 GB" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "8.0.31 GB" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "8.0.31 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 7, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$6", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 8, "ship_strategy": "Forward"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "8.0.31 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 11, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$1", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 7, "ship_strategy": "Forward"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 10, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 11, "ship_strategy": "Forward"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 9, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2", "parallelism": "1", "subtasks_per_instance": "1", "predecessors": [ {"id": 10, "ship_strategy": "Redistribute"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 12, "type": "pact", "pact": "Bulk Partial Solution", "contents": "Partial Solution", "parallelism": "64", "subtasks_per_instance": "16", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "0.0 B" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 6, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$3", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 7, "side": "first", "ship_strategy": "Forward", "temp_mode": "CACHED"}, {"id": 9, "side": "second", "ship_strategy": "Broadcast"}, {"id": 12, "side": "second", "ship_strategy": "Broadcast"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 5, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 6, "ship_strategy": "Forward"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 4, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4", "parallelism": "1", "subtasks_per_instance": "1", "predecessors": [ {"id": 5, "ship_strategy": "Redistribute"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 3, "type": "pact", "pact": "Cross", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$5", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 4, "side": "first", "ship_strategy": "Broadcast"}, {"id": 12, "side": "second", "ship_strategy": "Forward", "temp_mode": "PIPELINE_BREAKER"} ], "driver_strategy": "Nested Loops (Blocked Outer: de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4)", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] } ], "partial_solution": 12, "next_partial_solution": 3, "id": 1, "type": "bulk_iteration", "pact": "Bulk Iteration", "contents": "Bulk Iteration", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 2, "ship_strategy": "Redistribute"} ], "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 0, "type": "sink", "pact": "Data Sink", "contents": "TextOutputFormat (hdfs://cloud-7:45010/tmp/output/logreg) - UTF-8", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 1, "ship_strategy": "Forward"} ], "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] } ]}</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.PipelineBreakerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10192" opendate="2018-8-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client table visualization mode does not update correctly</summary>
      <description>The table visualization modes does not seem to update correctly.When I run a query that groups and aggregates on a few (6) distinct keys, the client visualizes some keys multiple times. Also the aggregated values do not seem to be correct.Due to the small number of keys, these get frequently updated.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.TypedResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="10195" opendate="2018-8-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ Source With Checkpointing Doesn&amp;#39;t Backpressure Correctly</summary>
      <description>The connection between the RabbitMQ server and the client does not appropriately back pressure when auto acking is disabled. This becomes very problematic when a downstream process throttles the data processing to slower then RabbitMQ sends the data to the client.The difference in records ends up being stored in the flink's heap space, which grows indefinitely (or technically to "Integer Max" Deliveries). Looking at RabbitMQ's metrics the number of unacked messages looks like steadily rising saw tooth shape.Upon further invesitgation it looks like this is due to how the QueueingConsumer works, messages are added to the BlockingQueue faster then they are being removed and processed, resulting in the previously described behavior.This may be intended behavior, however this isn't explicitly obvious in the documentation or any of the examples I have seen.</description>
      <version>1.4.0,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig.java</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug id="10247" opendate="2018-8-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService in separate thread pool</summary>
      <description>In order to make the MetricQueryService run independently of the main Flink components, it should get its own dedicated thread pool assigned.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.SerializableOptional.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10253" opendate="2018-8-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService with lower priority</summary>
      <description>We should run the MetricQueryService with a lower priority than the main Flink components. An idea would be to start the underlying threads with a lower priority.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10254" opendate="2018-8-29 00:00:00" fixdate="2018-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix inappropriate checkNotNull in stateBackend</summary>
      <description>The checkNotNull is unnecessary of numberOfKeyGroups with a primitive type , we just have to make sure it is bigger than 1.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="10267" opendate="2018-8-30 00:00:00" fixdate="2018-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[State] Fix arbitrary iterator access on RocksDBMapIterator</summary>
      <description>Currently, RocksDBMapIterator would load 128 entries into local cacheEntries every time if needed. Both RocksDBMapIterator#next() and RocksDBMapIterator#hasNext() action might trigger to load RocksDBEntry into cacheEntries.However, if the iterator's size larger than 128 and we continue to access the iterator with following order: hasNext() -&gt; next() -&gt; hasNext() -&gt; remove(), we would meet weird exception when we try to remove the 128th element:java.lang.IllegalStateException: The remove operation must be called after a valid next operation.Since we could not control user's access on iterator, we should fix this bug to avoid unexpected exception.</description>
      <version>1.5.3,1.6.0</version>
      <fixedVersion>1.5.4,1.6.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10274" opendate="2018-8-31 00:00:00" fixdate="2018-1-31 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The stop-cluster.sh cannot stop cluster properly when there are multiple clusters running</summary>
      <description>When you are preparing to do a Flink framework version upgrading by using the strategy shadow copy , you have to run multiple clusters concurrently,  however when you are ready to stop the old version cluster after upgrading, you would find the stop-cluster.sh wouldn't work as you expected, the following is the steps to duplicate the issue: There is already a running Flink 1.5.x cluster instance; Installing another Flink 1.6.x cluster instance at the same cluster machines; Migrating the jobs from Flink 1.5.x  to Flink 1.6.x ; go to the bin dir of the Flink 1.5.x cluster instance and run stop-cluster.sh ;You would expect the old Flink 1.5.x cluster instance be stopped ,right? Unfortunately the stopped cluster is the new installed Flink 1.6.x cluster instance instead!</description>
      <version>1.5.1,1.5.2,1.5.3,1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.environment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10282" opendate="2018-9-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide separate thread-pool for REST endpoint</summary>
      <description>The REST endpoints currently share their thread-pools with the RPC system, which can cause the Dispatcher to become unresponsive if the REST parts are overloaded.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ExecutorThreadFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10289" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Classify Exceptions to different category for apply different failover strategy</summary>
      <description>We need to classify exceptions and treat them with different strategies. To do this, we propose to introduce the following Throwable Types, and the corresponding exceptions: NonRecoverable We shouldn’t retry if an exception was classified as NonRecoverable For example, NoResouceAvailiableException is a NonRecoverable Exception Introduce a new Exception UserCodeException to wrap all exceptions that throw from user code PartitionDataMissingError In certain scenarios producer data was transferred in blocking mode or data was saved in persistent store. If the partition was missing, we need to revoke/rerun the produce task to regenerate the data. Introduce a new exception PartitionDataMissingException to wrap all those kinds of issues. EnvironmentError It happened due to hardware, or software issues that were related to specific environments. The assumption is that a task will succeed if we run it in a different environment, and other task run in this bad environment will very likely fail. If multiple task failures in the same machine due to EnvironmentError, we need to consider adding the bad machine to blacklist, and avoiding schedule task on it. Introduce a new exception EnvironmentException to wrap all those kind of issues. Recoverable We assume other issues are recoverable.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.SuppressRestartsException.java</file>
    </fixedFiles>
  </bug>
  <bug id="10291" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint currently generates the JobGraph from the user code when being started. Due to the nature of how the JobGraph is generated, it will get a random JobID assigned. This is problematic in case of a failover because then, the JobMaster won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the JobID assignment or make it configurable.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.StreamingPlan.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetrieverTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10295" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="10309" opendate="2018-9-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancel with savepoint fails with java.net.ConnectException when using the per job-mode</summary>
      <description>The problem occurs when using the Yarn per-job detached mode. Trying to cancel with savepoint fails with the following exception before being able to retrieve the savepoint path:exception stack trace : org.apache.flink.util.FlinkException: Could not cancel job xxxx.        at org.apache.flink.client.cli.CliFrontend.lambda$cancel$4(CliFrontend.java:585)        at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:960)        at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:577)        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1034)        at java.lang.Thread.run(Thread.java:748)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)        at org.apache.flink.client.program.rest.RestClusterClient.cancelWithSavepoint(RestClusterClient.java:398)        at org.apache.flink.client.cli.CliFrontend.lambda$cancel$4(CliFrontend.java:583)        ... 6 moreCaused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$5(FutureUtils.java:213)        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:274)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)        ... 1 moreCaused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connect refuse: xxx/xxx.xxx.xxx.xxx:xxx        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)        ... 16 moreCaused by: java.net.ConnectException: Connect refuse: xxx/xxx.xxx.xxx.xxx:xxx        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281)        ... 7 moresome discussion in mailing list : http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Cancel-flink-job-occur-exception-td24056.html</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AsynchronousJobOperationKey.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="10331" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of flush requests to the network stack</summary>
      <description>With the re-design of the record writer interaction with the result(sub)partitions, flush requests can currently pile up in these scenarios: a previous flush request has not been completely handled yet and/or is still enqueued or the network stack is still polling from this subpartition and doesn't need a new notificationThese lead to increased notifications in low latency settings (low output flusher intervals) which can be avoided.</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="10342" opendate="2018-9-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka duplicate topic consumption when topic name is changed</summary>
      <description>In case of topic name is simply renamed for a KafkaConsumer Flink starts to consume from old and a new topic in the same time which can lead to unexpected behavior.Here is the PR with reproduce: https://github.com/apache/flink/pull/6691 </description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicsDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10354" opendate="2018-9-17 00:00:00" fixdate="2018-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoints should be counted as retained checkpoints</summary>
      <description>This task is about reverting FLINK-6328.The problem is that you can get incorrect results with exactly-once sinks if there is a failure after taking a savepoint but before taking the next checkpoint because the savepoint will also have manifested side effects to the sink.</description>
      <version>1.6.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10355" opendate="2018-9-17 00:00:00" fixdate="2018-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Counting of table columns should start with 1 instead of 0</summary>
      <description>When  I register an external Table using a CsvTableSource.It throws an exception :"Parsing error for column 1".But I finally found that the second column is the error column.I think that the order of the column should start from 1.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.RowCsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="10357" opendate="2018-9-17 00:00:00" fixdate="2018-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed with mismatch</summary>
      <description>The Streaming File Sink end-to-end test failed on an Amazon instance with the following result: FAIL File Streaming Sink: Output hash mismatch. Got f2000bbc18a889dc8ec4b6f2b47bf9f5, expected 6727342fdd3aae2129e61fc8f433fb6f.head hexdump of actual:0000000 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n0000010 8 \n 9 \n0000014</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10358" opendate="2018-9-17 00:00:00" fixdate="2018-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink kinesis connector could throw NPE during getRecords() call</summary>
      <description>When extending the flink kinesis connector to consume from a dynamodb stream, it was found NPE could be thrown at here . This is because the getRecords API in dynamodb streams does not return the millisBehindLatest field and has it set to null.  Null check is probably needed here.See FLINK-4582 for the context of building dynamodb streams connector on top of the Kinesis connector.  </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10362" opendate="2018-9-18 00:00:00" fixdate="2018-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundled S3 connectors load wrong Hadoop config</summary>
      <description>The bundles S3 connectors internally build on Hadoop's file system support, but are packaged to not expose that at all - they are Hadoop-independent, from the user's perspective.Hence, they should not try to take external Hadoop configurations into account, but only the Flink configuration.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopConfigLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="10363" opendate="2018-9-18 00:00:00" fixdate="2018-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 FileSystem factory prints secrets into logs</summary>
      <description>The file system factory logs all values it applies from the flink configuration.That frequently includes access keys, which should not leak into logs.The loader should only log the keys, not the values.</description>
      <version>None</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopConfigLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="10365" opendate="2018-9-18 00:00:00" fixdate="2018-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate shaded Hadoop classes for filesystems</summary>
      <description>We currently have have three bundled/shaded filesystem connectors that build on top of Hadoop's classes. More will probably come. Each of them re-builds the shaded Hadoop module, including creating the relocated config, adapting native code loading, etc.We should factor that out into a single base project to avoid duplicating work.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10366" opendate="2018-9-18 00:00:00" fixdate="2018-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a shared base for S3 file systems</summary>
      <description>We are adding extensions to the FileSystem logic on top of what Hadoop offers, like entropy injection, recoverable writers.To be able to share the code across the Presto and Hadoop s3a implementations, we should rebase them both onto a common shared project.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.resources.core-site.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.core-default-shaded.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.hadoop.util.NativeCodeLoader.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.hadoop.conf.Configuration.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopConfigLoader.java</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.AbstractFileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.core-default-shaded.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.hadoop.util.NativeCodeLoader.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.hadoop.conf.Configuration.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.flink.fs.s3presto.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10522" opendate="2018-10-10 00:00:00" fixdate="2018-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check if RecoverableWriter supportsResume and act accordingly.</summary>
      <description>So far we assumed that all `RecoverableWriters` support "resuming", i.e. after recovering from a failure or from a savepoint they could keep writing to the previously "in-progress" file. This assumption holds for all current writers, but in order to be able to accommodate also filesystems that may not support this operation, we should check upon initialization if the writer supports resuming and if yes, we go as before, if not, we recover for commit and commit the previously in-progress file.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.1,1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="1054" opendate="2014-8-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windows: JM web interface job analze view does not show task details on</summary>
      <description>The job analyze view does not show the start/end times of tasks (neither in stack nor in flow layout).</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JobmanagerInfoServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.JobManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="10543" opendate="2018-10-14 00:00:00" fixdate="2018-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leverage efficient timer deletion in relational operators</summary>
      <description>FLINK-9423 added support for efficient timer deletions. This feature is available since Flink 1.6 and should be used by the relational operator of SQL and Table API.Currently, we use a few workarounds to handle situations when deleting timers would be the better solution.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.triggers.StateCleaningCountTrigger.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.KeyedProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5750" opendate="2017-2-9 00:00:00" fixdate="2017-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7521" opendate="2017-8-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the 10MB limit from the current REST implementation.</summary>
      <description>In the current AbstractRestServer we impose an upper bound of 10MB in the states we can transfer. This is in the line .addLast(new HttpObjectAggregator(1024 * 1024 * 10)) of the server implementation. This limit is restrictive for some of the usecases planned to use this implementation (e.g. the job submission client which has to send full jars, or the queryable state client which may have to receive states bigger than that).This issue proposes the elimination of this limit.</description>
      <version>1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="8563" opendate="2018-2-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support consecutive DOT operators</summary>
      <description>We added support for accessing fields of arrays of composite types in FLINK-7923. However, accessing another nested subfield is not supported by Calcite. See CALCITE-2162. We should fix this once we upgrade to Calcite 1.16.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8795" opendate="2018-2-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala shell broken for Flip6</summary>
      <description>I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?I followed the instructions from the Flink master branch so I did the following.git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests cd build-target ./bin/start-scala-shell.sh localAnd Here is the code I ranval dataStream = senv.fromElements(1, 2, 3, 4)dataStream.countWindowAll(2).sum(0).print()senv.execute("My streaming program")And I finally get this exceptionCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) </description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8810" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move end-to-end test scripts to end-to-end module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.python.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.classloader.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.presto.s3.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.hadoop.s3a.sh</file>
      <file type="M">test-infra.end-to-end-test.test.hadoop.free.sh</file>
      <file type="M">test-infra.end-to-end-test.test.batch.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test-data.words</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8812" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possible resource leak in Flip6</summary>
      <description>In this build (https://travis-ci.org/zentol/flink/builds/347373839) I set the codebase to flip6 for half the profiles to find failing tests.The "libraries" job (https://travis-ci.org/zentol/flink/jobs/347373851) failed with an OutOfMemoryError.This could mean that there is a memory-leak somewhere.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="8842" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default REST port to 8081</summary>
      <description>In order to avoid confusion, we should set the default REST port to 8081.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="8858" opendate="2018-3-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for INSERT INTO in SQL Client</summary>
      <description>The current design of SQL Client embedded mode doesn't support long running queries. It would be useful for simple jobs that can be expressed in a single sql statement if we can submit sql statements stored in files as long running queries. </description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-factory-services-file</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.assembly.test-table-source-factory.xml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ProgramDeployer.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.MaterializedResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DynamicResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ChangelogResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ChangelogCollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8871" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Checkpoint cancellation is not propagated to stop checkpointing threads on the task manager</summary>
      <description>Flink currently lacks any form of feedback mechanism from the job manager / checkpoint coordinator to the tasks when it comes to failing a checkpoint. This means that running snapshots on the tasks are also not stopped even if their owning checkpoint is already cancelled. Two examples for cases where this applies are checkpoint timeouts and local checkpoint failures on a task together with a configuration that does not fail tasks on checkpoint failure. Notice that those running snapshots do no longer account for the maximum number of parallel checkpoints, because their owning checkpoint is considered as cancelled.Not stopping the task's snapshot thread can lead to a problematic situation where the next checkpoints already started, while the abandoned checkpoint thread from a previous checkpoint is still lingering around running. This scenario can potentially cascade: many parallel checkpoints will slow down checkpointing and make timeouts even more likely. A possible solution is introducing a cancelCheckpoint method  as counterpart to the triggerCheckpoint method in the task manager gateway, which is invoked by the checkpoint coordinator as part of cancelling the checkpoint.</description>
      <version>1.3.2,1.4.1,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointingCustomKvStateProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointedStreamingProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.FailingSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.CancellingIntegerSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.AccumulatingIntegerSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpointedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CoStreamCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ContinuousFileProcessingCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.ExceptionallyDoneFuture.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.common.AcknowledgeOnCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.FailureMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
    </fixedFiles>
  </bug>
  <bug id="8897" opendate="2018-3-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8901" opendate="2018-3-8 00:00:00" fixdate="2018-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN application name for Flink (per-job) submissions claims it is using only 1 TaskManager</summary>
      <description>If (with FLIP-6) a per-job YARN session is created without specifying the number of nodes, it will show up as "Flink session with 1 TaskManagers", e.g. this job:./bin/flink run -m yarn-cluster -yjm 768 -ytm 3072 -ys 2 -p 20 -c org.apache.flink.streaming.examples.wordcount.WordCount ./examples/streaming/WordCount.jar --input /usr/share/doc/rsync-3.0.6/COPYING</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Flip6YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="8906" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flip6DefaultCLI is not tested in org.apache.flink.client.cli tests</summary>
      <description>Various tests in org.apache.flink.client.cli only test with the DefaultCLI but should also test Flip6DefaultCLI.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.DefaultCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendModifyTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendListTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendCancelTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8916" opendate="2018-3-11 00:00:00" fixdate="2018-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing Mode is always shown to be "At Least Once" in Web UI</summary>
      <description>This only happens in flip6 mode. The CheckpointConfigHandler returns the checkpoint mode uppercased. For example:{"mode":"EXACTLY_ONCE","interval":5000,"timeout":600000,"min_pause":0,"max_concurrent":1,"externalization":{"enabled":false,"delete_on_cancellation":true}}However, the Web UI expects the value to be lower cased: &lt;tr&gt; &lt;td&gt;Checkpointing Mode&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] == 'exactly_once'"&gt;Exactly Once&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] != 'exactly_once'"&gt;At Least Once&lt;/td&gt; &lt;/tr&gt;</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="8934" opendate="2018-3-13 00:00:00" fixdate="2018-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancel slot requests for otherwisely fulfilled requests</summary>
      <description>If a slot request is fulfilled with a different allocation id, then we should cancel the other slot request at the ResourceManager. Otherwise we might have some stale slot requests which first need to time out before slots are available again.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="8944" opendate="2018-3-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ListShards for shard discovery in the flink kinesis connector</summary>
      <description>Currently the DescribeStream AWS API used to get list of shards is has a restricted rate limits on AWS. (5 requests per sec per account). This is problematic when running multiple flink jobs all on same account since each subtasks calls the Describe Stream. Changing this to ListShards will provide more flexibility on rate limits as ListShards has a 100 requests per second per data stream limits.More details on the mailing list. https://goo.gl/mRXjKh</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="9057" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in CreditBasedSequenceNumberingViewReader when cancelling before initilization was complete</summary>
      <description>RescalingITCase unveiled an exception which may occur when shutting down before completely initializing the network stack:https://travis-ci.org/apache/flink/jobs/35661210001:08:13,458 WARN org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline - An exception was thrown by a user handler's exceptionCaught() method while handling the following exception:java.lang.NullPointerException at org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.releaseAllResources(CreditBasedSequenceNumberingViewReader.java:192) at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.releaseAllResources(PartitionRequestQueue.java:322) at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.channelInactive(PartitionRequestQueue.java:298) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:294) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:829) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:610) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9059" opendate="2018-3-22 00:00:00" fixdate="2018-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unified table source and sink declaration in environment file</summary>
      <description>1) Add a common property called "type" with single value 'source'.2) in yaml file, replace "sources" with "tables".</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TestTableSourceFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceFactoryServiceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.TableSourceDescriptor.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="9076" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make credit-based floating buffers optional</summary>
      <description>Currently, floating buffers (per gate) are always required in case credit-based flow control is enabled. This, however, increases our minimum number of required network buffers.Instead, without changing too much, we could already work with a minimum of one or zero floating buffers and set the max to the configured value. This way, if there are not enough buffers, all {{LocalBufferPool}}s will at least share the available ones.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmarkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkPointToPointBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputChannelTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9091" opendate="2018-3-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure while enforcing releasability in building flink-json module</summary>
      <description>Got the following when building flink-json module:[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message....[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (dependency-convergence) on project flink-json: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9093" opendate="2018-3-27 00:00:00" fixdate="2018-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If Google can&amp;#39;t be accessed,the document can&amp;#39;t be use</summary>
      <description>these links can't be visited.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug id="9107" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="9108" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
  <bug id="9144" opendate="2018-4-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spilling batch job hangs</summary>
      <description>A user on the mailing list reported that his batch job stops to run with Flink 1.5 RC1: https://lists.apache.org/thread.html/43721934405019e7255fda627afb7c9c4ed0d04fb47f1c8f346d4194@%3Cdev.flink.apache.org%3E</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9163" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden e2e tests&amp;#39; signal traps and config restoration during abort</summary>
      <description>Signal traps on certain systems, e.g. Linux, may be called concurrently when the trap is caught during its own execution. In that case, our cleanup may just be wrong and may also overly eagerly delete flink-conf.yaml.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9274" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name to Kafka Partition Discovery</summary>
      <description>For debugging, threads should have names to filter on and get a quick overview. The Kafka partition discovery thread(s) currently don't have any name assigned.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9275" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set more distinctive output flusher thread names</summary>
      <description>All output flusher threads are named "OutputFlusher" while at the only place the StreamWriter is initialized, we already have the task name at hand.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="9289" opendate="2018-5-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallelism of generated operators should have max parallism of input</summary>
      <description>The DataSet API aims to chain generated operators such as key extraction mappers to their predecessor. This is done by assigning the same parallelism as the input operator.If a generated operator has more than two inputs, the operator cannot be chained anymore and the operator is generated with default parallelism. This can lead to a NoResourceAvailableException: Not enough free slots available to run the job. as reported by a user on the mailing list: https://lists.apache.org/thread.html/60a8bffcce54717b6273bf3de0f43f1940fbb711590f4b90cd666c9a@%3Cuser.flink.apache.org%3EI suggest to set the parallelism of a generated operator to the max parallelism of all of its inputs to fix this problem.Until the problem is fixed, a workaround is to set the default parallelism at the ExecutionEnvironment:ExecutionEnvironment env = ...env.setParallelism(2);</description>
      <version>1.4.2,1.5.2,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.linkanalysis.PageRank.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.KeyFunctions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9306" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute YARN IT tests for legacy and new mode</summary>
      <description>Currently, we are not executing the YARN IT cases for legacy mode.I opened a PR that changes that but it's currently failing on one of the tests in legacy mode: https://github.com/apache/flink/pull/5953</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9309" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recommend HA setup on Production Readiness Checklist</summary>
      <description>It would be good to recommend the HA setup on the Production Readiness Checklist to ensure that users are aware of this feature.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug id="9353" opendate="2018-5-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Kubernetes integration</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="9358" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Closing of unestablished RM connections can cause NPE</summary>
      <description>When closing an unestablished RM connection, a NPE is thrown. The reason is that we try to unmonitor a non-existing heartbeat target.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="9365" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add version information to remote rpc messages</summary>
      <description>In order to quickly detect components which use incompatible RPC versions, we should add a version information to all remote RPC messages. That way we could easily detect if an older incompatible component tries to communicate with a newer version.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9366" opendate="2018-5-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distribute Cache only works for client-accessible files</summary>
      <description>In FLINK-8620 the distributed cache was modified to the distribute files via the blob store, instead of downloading them from a distributed filesystem.Previously, taskmanagers would download requested files from the DFS. Now, they retrieve it form the blob store. This requires the client to preemptively upload all files used with distributed cache.As a result it is no longer possible to use the distributed cache for files that reside in a cluster-internal DFS, as the client cannot download it. This is a regression from the previous behavior and may break existing setups.aljoscha dawidwys</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-core.xml</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobGraphTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.cache.DistributedCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="9380" opendate="2018-5-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing end-to-end tests should not clean up logs</summary>
      <description>Some of the end-to-end tests clean up their logs also in the failure case. This makes debugging and understanding the problem extremely difficult. Ideally, the scripts says where it stored the respective logs.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9383" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend DistributedCache E2E test to cover directories</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.distributed.cache.via.blob.sh</file>
      <file type="M">flink-end-to-end-tests.flink-distributed-cache-via-blob-test.src.main.java.org.apache.flink.streaming.tests.DistributedCacheViaBlobTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="9384" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaAvroTableSource failed to work due to type mismatch</summary>
      <description>An exception was thrown when using KafkaAvroTableSource as follows:Exception in thread "main" org.apache.flink.table.api.TableException: TableSource of type org.apache.flink.streaming.connectors.kafka.Kafka011AvroTableSource returned a DataStream of type GenericType&lt;org.apache.flink.types.Row&gt; that does not match with the type Row(id: Integer, name: String, age: Integer, event: GenericType&lt;java.util.Map&gt;) declared by the TableSource.getReturnType() method. Please validate the implementation of the TableSource. at org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.translateToPlan(StreamTableSourceScan.scala:100) at org.apache.flink.table.api.StreamTableEnvironment.translateToCRow(StreamTableEnvironment.scala:885) at org.apache.flink.table.api.StreamTableEnvironment.translate(StreamTableEnvironment.scala:812) at org.apache.flink.table.api.StreamTableEnvironment.writeToSink(StreamTableEnvironment.scala:279) at org.apache.flink.table.api.Table.writeToSink(table.scala:862) at org.apache.flink.table.api.Table.writeToSink(table.scala:830) at org.apache.flink.quickstart.StreamingJobAvro.main(StreamingJobAvro.java:85) It is caused by a discrepancy between the type returned by the TableSource and the type returned by the DataStream. I've already fixed it, would someone please review the patch and see if it could be merged. </description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSourceTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9386" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove netty-router dependency</summary>
      <description>netty-router 1.10 blocks upgrade to 4.1, while netty-router 2.2.0 has broken compatibility in a way that it's unusable by us (it doesn't allow to sort router paths as in https://issues.apache.org/jira/browse/FLINK-8000 ). I propose to copy &amp; simplify &amp; modify netty-router code to suite our needs.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RedirectHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ConstantTextHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.RedirectHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9392" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add @FunctionalInterface annotations to all core functional interfaces</summary>
      <description>The @FunctionalInterface annotation should be added to all SAM interfaces in order to prevent accidentally breaking them (as non SAMs).We had a case of that before for the SinkFunction which was compatible through default methods, but incompatible for users that previously instantiated that interface through a lambda.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.ReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.Partitioner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapPartitionFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.JoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupCombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FoldFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatMapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatJoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FilterFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CrossFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CoGroupFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.BroadcastVariableInitializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9408" opendate="2018-5-22 00:00:00" fixdate="2018-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry JM-RM connection in case of explicit disconnect</summary>
      <description>The JM should try to reconnect to the RM not only in the case of a heartbeat timeout but also in case of an explicit disconnect.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.EstablishedResourceManagerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="9416" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make job submission retriable operation in case of a ongoing leader election</summary>
      <description>When starting a session cluster, it can happen that the job submission fails if the REST server endpoint has already gained leadership but if the leadership election for the Dispatcher is still ongoing. In such a case, we receive a error response saying that the leader election is still ongoing and fail the job submission. I think it would be nicer to also make the submission step a retriable operation in order to avoid this race condition.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="942" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config key "env.java.opts" does not effect JVM args</summary>
      <description>Setting custom args for the JVM as in env.java.opts: -Dio.netty.leakDetectionLevel=paranoidhas no effect for the started JVMs.A fix is coming up.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.taskmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.jobmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9420" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for SQL IN sub-query operator in streaming</summary>
      <description>In FLINK-6094 we implemented non-windowed inner joins. The Table API &amp; SQL should now support the IN operator for sub-queries in streaming. Batch support has been added in FLINK-4565. We need to add unit tests, an IT case, and update the docs about that.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="9428" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow operators to flush data on checkpoint pre-barrier</summary>
      <description>Some operators maintain some small transient state that may be inefficient to checkpoint, especially when it would need to be checkpointed also in a re-scalable way.An example are opportunistic pre-aggregation operators, which have small the pre-aggregation state that is frequently flushed downstream.Rather that persisting that state in a checkpoint, it can make sense to flush the data downstream upon a checkpoint, to let it be part of the downstream operator's state.This feature is sensitive, because flushing state has a clean implication on the downstream operator's checkpoint alignment. However, used with care, and with the new back-pressure-based checkpoint alignment, this feature can be very useful.Because it is sensitive, I suggest to make this only an internal feature (accessible to operators) and NOT expose it in the public API at this point.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9429" opendate="2018-5-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E not working locally</summary>
      <description>The quickstart e2e test is not working locally. It seems as if the job does not produce anything into Elasticsearch. Furthermore, the test does not terminate with control-C.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9438" opendate="2018-5-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for (Registry)AvroDeserializationSchema</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="9483" opendate="2018-5-31 00:00:00" fixdate="2018-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Building Flink" doc doesn&amp;#39;t highlight quick build command</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.start.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="9503" opendate="2018-6-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate integration tests for iterative aggregators</summary>
      <description>Migrate integration tests in org.apache.flink.test.iterative.aggregators to use collect() instead of temp files. Related to parent jira.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9508" opendate="2018-6-3 00:00:00" fixdate="2018-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>General Spell Check on Flink Docs</summary>
      <description>Fixing Flink docs misspelling </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.filesystems.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="9513" opendate="2018-6-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Wrap state binder with TTL logic</summary>
      <description>The main idea is to wrap user state value with a class holding the value and the expiration timestamp (maybe meta data in future) and use the new object as a value in the existing implementations:class TtlValue&lt;V&gt; {  V value;  long expirationTimestamp;}The original state binder factory is wrapped with TtlStateBinder if TTL is enabled:state = ttlConfig.updateType == DISABLED ? bind(binder) : bind(new TtlStateBinder(binder, timerService));TtlStateBinder decorates the states produced by the original binder with TTL logic wrappers and adds TtlValue serialisation logic:TtlStateBinder { StateBinder binder;    ProcessingTimeProvier timeProvider; // System.currentTimeMillis() &lt;V&gt; TtlValueState&lt;V&gt; createValueState(valueDesc) {         serializer = new TtlValueSerializer(valueDesc.getSerializer);         ttlValueDesc = new ValueDesc(serializer, ...);         // or implement custom TypeInfo         originalStateWithTtl = binder.createValueState(valueDesc);     return new TtlValueState(originalStateWithTtl, timeProvider); }      // List, Map, ...}TTL serializer should add expiration timestamp</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlValueStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlReducingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStatePerElementTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlListStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlFoldingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlAggregatingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockInternalMergingState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockInternalMapState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockInternalKvState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="9518" opendate="2018-6-4 00:00:00" fixdate="2018-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL setup Docs config example has wrong keys password</summary>
      <description>In creating keystores and turststore section password is set to password but in setup config section it is abc123</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="9526" opendate="2018-6-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BucketingSink end-to-end test failed on Travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/388130914</description>
      <version>1.6.0</version>
      <fixedVersion>1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9530" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task numRecords metrics broken for chains</summary>
      <description>The numRecordsIn/Out metrics for tasks is currently broken. We are wrongly adding up the numRecordsIn/Out metrics for all operators in the chain, instead of just the head/tail operators.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  <bug id="9539" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate flink-shaded 4.0</summary>
      <description>With the recent release of flink-shaded 4.0 we should bump the versions for all dependencies (except netty which is handled in FLINK-3952).We can now remove the exclusions from the jackson dependencies as they are now properly hidden.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9546" opendate="2018-6-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The heartbeatTimeoutIntervalMs of HeartbeatMonitor should be larger than 0</summary>
      <description>The heartbeatTimeoutIntervalMs of HeartbeatMonitor should be larger than 0, currently the arg check looks likePreconditions.checkArgument(heartbeatTimeoutIntervalMs &gt;= 0L, "The heartbeat timeout interval has to be larger than 0.");it should bePreconditions.checkArgument(heartbeatTimeoutIntervalMs &gt; 0L, "The heartbeat timeout interval has to be larger than 0.");</description>
      <version>None</version>
      <fixedVersion>1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="9549" opendate="2018-6-7 00:00:00" fixdate="2018-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix FlickCEP Docs broken link and minor style changes</summary>
      <description></description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="9563" opendate="2018-6-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate integration tests for CEP</summary>
      <description>Covers all integration tests underapache-flink/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9590" opendate="2018-6-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistogramDump should be immutable</summary>
      <description>The HistogramDump represents the contents of a histogram at one point in time, and should thus not be mutable.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.MetricDump.java</file>
    </fixedFiles>
  </bug>
  <bug id="9593" opendate="2018-6-15 00:00:00" fixdate="2018-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify AfterMatch semantics with SQL MATCH_RECOGNIZE</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.UntilConditionITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SameElementITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.AfterMatchSkipITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.GroupPattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.sharedbuffer.EventId.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFAStateSerializer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFAState.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.MigrationUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.DeweyNumber.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.ComputationState.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.AfterMatchSkipStrategy.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="9594" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for e2e test changes introduced with FLINK-9257</summary>
      <description>There were some changes introduced with FLINK-9257 in how end-to-end tests are structured and how we handle triggering failure from a test case. This should be documented for future developers writing e2e tests</description>
      <version>None</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="9595" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions to docs about ceased support of KPL version used in Kinesis connector</summary>
      <description>The KPL version used in the Kinesis connector for FlinkKinesisProducer is not longer supported by AWS Kinesis Streams: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Flink-1-4-and-below-STOPS-writing-to-Kinesis-after-June-12th-td22687.html#a22701We should add a notice about this to the Kinesis connectors, and how to bypass it by specifying the KPL version when building the Kinesis connector.</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="962" opendate="2014-6-21 00:00:00" fixdate="2014-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move documentation from old website into source code</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9622" opendate="2018-6-20 00:00:00" fixdate="2018-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DistributedCacheDfsTest failed on travis</summary>
      <description>DistributedCacheDfsTest#testDistributeFileViaDFS() failed flakey on travis.instance: https://api.travis-ci.org/v3/job/394399700/log.txt</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FileUtilsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="9629" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datadog metrics reporter does not have shaded dependencies</summary>
      <description>flink-metrics-datadog-1.5.0.jar does not contain shaded dependencies for okhttp3 and okio</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9637" opendate="2018-6-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add public user documentation for TTL feature</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="9655" opendate="2018-6-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoint E2E test fails on travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/396395491 ==============================================================================Running 'Resuming Externalized Checkpoint after terminal failure (file, sync) end-to-end test'==============================================================================Flink dist directory: /home/travis/build/zentol/flink-ci/flink/build-targetTEST_DATA_DIR: /home/travis/build/zentol/flink-ci/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-47420177246Starting cluster.Starting standalonesession daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Starting taskexecutor daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Running externalized checkpoints test, with ORIGINAL_DOP=file NEW_DOP=false and STATE_BACKEND_TYPE=false STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INCREMENTAL=false SIMULATE_FAILURE=false ...Job () is running.Waiting for job (1) to have at least completed checkpoints ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9672" opendate="2018-6-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail fatally if we cannot submit job on added JobGraph signal</summary>
      <description>The SubmittedJobGraphStore signals when new JobGraphs are added. If this happens, then the leader should recover this job and submit it. If the recovery/submission should fail for some reason, then we should fail fatally to restart the process which will then try to recover the jobs again.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="9679" opendate="2018-6-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ConfluentRegistryAvroSerializationSchema</summary>
      <description>Implement AvroSerializationSchema using Confluent Schema Registry</description>
      <version>1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.java.org.apache.flink.schema.registry.test.TestAvroConsumerConfluent.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.RegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.SchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.ConfluentSchemaRegistryCoder.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9681" opendate="2018-6-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make sure minRetentionTime not equal to maxRetentionTime</summary>
      <description>Currently, for a group by(or other operators), if minRetentionTime equals to maxRetentionTime, the group by operator will register a timer for each record coming at different time which cause performance problem. The reasoning for having two parameters is that we can avoid to register many timers if we have more freedom when to discard state. As min equals to max cause performance problem it is better to make sure these two parameters are not same.Any suggestions are welcome.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.queryConfig.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="9702" opendate="2018-7-2 00:00:00" fixdate="2018-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement in (de)serialization of keys and values for RocksDB state</summary>
      <description>When Flink interacts with state in RocksDB, object (de)serialization often contributes significantly to performance overhead. I think there are some aspects that we can improve here to reduce the costs in this area. In particular, currently every state has to serialize the backen's current key before each state access. We could reduce this effort by sharing serialized key bytes across all state interactions. Furthermore, we can reduce the amount of `byte[]` and stream/view that are involved.</description>
      <version>1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.DataOutputSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayOutputStreamWithPos.java</file>
    </fixedFiles>
  </bug>
  <bug id="9735" opendate="2018-7-4 00:00:00" fixdate="2018-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential resource leak in RocksDBStateBackend#getDbOptions</summary>
      <description>Here is related code: if (optionsFactory != null) { opt = optionsFactory.createDBOptions(opt); }opt, an DBOptions instance, should be closed before being rewritten.getColumnOptions has similar issue.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="9771" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Show Plan" option under Submit New Job in WebUI not working</summary>
      <description>Show Plan button under Submit new job in WebUI not working.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobPlanInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobPlanInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="9772" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of Hadoop API outdated</summary>
      <description>It looks like the documentation of the Hadoop Compatibility is somewhat outdated? At least the text and examples in section Using Hadoop InputFormats mention methodsenv.readHadoopFile and env.createHadoopInputwhich do not exist anymore since 1.4.0.   </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug id="9781" opendate="2018-7-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-maven-plugin fails on java 9</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/40171125811:10:02.157 [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---11:10:04.861 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/java:-1: info: compiling11:10:04.862 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/scala:-1: info: compiling11:10:04.862 [INFO] Compiling 1486 source files to /home/travis/build/zentol/flink/flink-runtime/target/classes at 153113460486211:10:06.135 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.135 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.135 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.135 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.136 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.136 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.136 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.136 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.136 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.136 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.136 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.136 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.136 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.136 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.136 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [INFO] java.lang.reflect.InvocationTargetException11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.137 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.137 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.137 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.138 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.138 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.138 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.149 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.149 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.151 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.153 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.153 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.153 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.172 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.172 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.172 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.172 [INFO] ... 6 more11:10:06.196 [INFO] ------------------</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9789" opendate="2018-7-10 00:00:00" fixdate="2018-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watermark metrics for an operator&amp;task shadow each other</summary>
      <description>In FLINK-4812 we reworked the watermark metrics to be exposed for each operator.In FLINK-9467 we made further modifications to also expose these metrics again for tasks.In both JIRAs we register a single metric multiple times, for example the input watermark metric is registered for both the first operator in the task, and the task itself.Unfortunately, the metric system assumes metric objects to be unique, as can be seen in virtually all reporter implementations as well as the MetricQueryService.As a result the watermark metrics override each other in the reporter, causing only one to be reported, whichever was registered last.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="9792" opendate="2018-7-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot add html tags in options description</summary>
      <description>Right now it is impossible to add any html tags in options description, because all "&lt;" and "&gt;" are escaped. Therefore some links there do not work.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.util.Utils.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigOption.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9795" opendate="2018-7-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Mesos documentation for flip6</summary>
      <description>Mesos documentation would benefit from an overhaul after flip6 became the default cluster management model.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs..includes.generated.high.availability.zookeeper.configuration.html</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug id="9801" opendate="2018-7-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist is missing dependency on flink-examples</summary>
      <description>For the assembly of flink-dist we copy various batch/streaming examples directly from the respective /target directory.Never mind that this is already a problem as is (see FLINK-9582), flink-dist defines no dependency on these modules.If you were to only compile flink-dist with the -am flag (to also build all dependencies) it thus may or may not happen that these modules are actually compiled, which could cause these examples to not be included in the final assembly.</description>
      <version>1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9815" opendate="2018-7-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase flaky</summary>
      <description>The test fails because of dangling yarn applications.Logs: https://api.travis-ci.org/v3/job/402657694/log.txtIt was also reported previously in FLINK-8161 : https://issues.apache.org/jira/browse/FLINK-8161?focusedCommentId=16480216&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16480216</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9839" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Streaming job with SSL</summary>
      <description>None of the existing e2e tests run with an SSL configuration but there should be such a test as well.</description>
      <version>1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="984" opendate="2014-6-25 00:00:00" fixdate="2014-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distinct operator for POJO DataSets without KeySelector and on UnsortedGrouping</summary>
      <description>Two issues regarding the distinct operator: The distinct operator does not support distinct() with POJO datasets. I think it should be possible to implement a reflection based KeySelector which selects all public fields as key? All other grouped operators work by groupBy(GROUPING).operator(...) whereas the distinct operator does distinct(GROUPING). I think it would be more consistent to allow groupBy(GROUPING).distinct().</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.DistinctITCase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DistrinctTranslationTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.DOPChangeTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="9841" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI only show partial taskmanager log</summary>
      <description> In the web UI, we select a task manager and click the "log" tab, but the UI only show the partial log (first part), can never update even if we click the "refresh" button.However, the job manager is always OK.The reason is the resource be closed twice.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9846" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Kafka table sink factory</summary>
      <description>FLINK-8866 implements a unified way of creating sinks and using the format discovery for searching for formats (FLINK-8858). It is now possible to add a Kafka table sink factory for streaming environment that uses the new interfaces.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.factories.utils.TestTableFormatFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.factories.utils.TestSerializationSchema.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="9853" opendate="2018-7-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hex support in table api and sql</summary>
      <description>like in mysql, HEX could take int or string arguments, For a integer argument N, it returns a hexadecimal string representation of the value of N. For a string argument str, it returns a hexadecimal string representation of str where each byte of each character in str is converted to two hexadecimal digits. Syntax:HEX(100) = 64HEX('This is a test String.') = '546869732069732061207465737420537472696e672e'See more: link MySQL</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="9857" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Processing-time timers fire too early</summary>
      <description>The firing of processing-time timers is off by one. This leads to problems in edge cases, as discovered here (mailing list) when elements arrive at the timestamp that is the end of the window.The problem is here (github). For event-time, we fire timers when the watermark is &gt;= the timestamp, this is correct because a watermark T says that we will not see elements with a timestamp smaller or equal to T. For processing time, a time of T does not say that we won't see an element with timestamp T, which makes processing-time timers fire one ms too early.I think we can fix it by turning that &lt;= into a &lt;.</description>
      <version>1.3.4,1.4.2,1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
    </fixedFiles>
  </bug>
  <bug id="9858" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>State TTL End-to-End Test</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9860" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty resource leak on receiver side</summary>
      <description>The Hadoop-free Wordcount end-to-end test fails with the following exception:ERROR org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector - LEAK: ByteBuf.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.Recent access records: Created at: org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:331) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:185) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:176) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:137) org.apache.flink.shaded.netty4.io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114) org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:147) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)We might have a resource leak on the receiving side of our network stack.https://api.travis-ci.org/v3/job/404225956/log.txt</description>
      <version>1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9861" opendate="2018-7-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for reworked BucketingSink</summary>
      <description>We should add a end-to-end test for the reworked BucketingSink to verify that the sink works with different FileSystems.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9862" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update end-to-end test to use RocksDB backed timers</summary>
      <description>We should add or modify an end-to-end test to use RocksDB backed timers.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9870" opendate="2018-7-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Support field mapping and time attributes for table sinks</summary>
      <description>FLINK-7548 reworked the table source design and implemented the interfaces DefinedFieldMapping, DefinedProctimeAttribute, and DefinedRowtimeAttributes.However, these interfaces need to be implemented by table sinks as well in order to map a table back into a sink similar how source do it for reading input data.The current unified sink design assumes that this is possible.</description>
      <version>1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.InMemoryTableFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.SchemaValidator.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9871" opendate="2018-7-17 00:00:00" fixdate="2018-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Description class for ConfigOptions with rich formatting</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9873" opendate="2018-7-17 00:00:00" fixdate="2018-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log actual state when aborting checkpoint due to task not running</summary>
      <description>Currently, if a checkpoint is triggered while a task s not in a RUNNING state the following message is logged:Checkpoint triggering task {} of job {} is not being executed at the moment.We can improve this message to include the actual task state to help diagnose problems.This message is also a bit ambiguous, as "being executed" could mean many things, from not "RUNNING", to not being "DEPLOYED", or to not existing at all.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9874" opendate="2018-7-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>set_conf_ssl in E2E tests fails on macOS</summary>
      <description>Setting up a cluster with SSL support in the end-to-end tests with `set_conf_ssl` will fail under macOS because in the commandhostname -Iis used, but '-I' is not a supported parameter for the hostname command under macOS</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9878" opendate="2018-7-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IO worker threads BLOCKED on SSL Session Cache while CMS full gc</summary>
      <description>According to https://github.com/netty/netty/issues/832, there is a JDK issue during garbage collection when the SSL session cache is not limited. We should allow the user to configure this and further (advanced) SSL parameters for fine-tuning to fix this and similar issues. In particular, the following parameters should be configurable: SSL session cache size SSL session timeout SSL handshake timeout SSL close notify flush timeout</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.4,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLEngineFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9885" opendate="2018-7-18 00:00:00" fixdate="2018-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Elasticsearch 6.x connector</summary>
      <description>We have decided to try and merge the pending Elasticsearch 6.x PRs. This should also come with an end-to-end test that covers this.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9886" opendate="2018-7-18 00:00:00" fixdate="2018-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build SQL jars with every build</summary>
      <description>Currently, the shaded fat jars for SQL are only built in the -Prelease profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing META-INF entry and proper shading. We should build them with every release. If a build should happen quicker one can use the -Pfast profile.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9887" opendate="2018-7-18 00:00:00" fixdate="2018-8-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Ensure serializer upgrades work with timer service remake</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedTwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.TriggerTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimerSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.InternalPriorityQueueTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendSnapshotMigrationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSnapshotRestoreWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSetFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.StateMigrationException.java</file>
    </fixedFiles>
  </bug>
  <bug id="9892" opendate="2018-7-19 00:00:00" fixdate="2018-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable local recovery in Jepsen tests</summary>
      <description>Until FLINK-9635 is fixed, local recovery should be disabled in the Jepsen tests.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="9902" opendate="2018-7-20 00:00:00" fixdate="2018-7-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve and refactor window checkpointing IT cases</summary>
      <description>Windowing IT cases currently have a lot of duplicated code that could be unified and deduplicated. Furthermore, the test will also not fail on problems with timer snapshots because either there are no timers in the snapshot or all timers will still be re-inserted before they trigger. We can cover timers as well if we change this.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.SuccessException.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.WindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeAllWindowCheckpointingITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9903" opendate="2018-7-20 00:00:00" fixdate="2018-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for bulk writers.</summary>
      <description></description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileInfo.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.DefaultBucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.DateTimeBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.Bucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.BasePathBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.SafetyNetWrapperFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="9909" opendate="2018-7-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove cancellation of input futures from ConjunctFutures</summary>
      <description>With FLINK-8749, we introduced that a ConjunctFutures cancels all of its input futures if it is cancelled. This has, however, some unpleasant side effects since a all of the cancelled future's completing callbacks won't be called. Since this can lead to subtle bugs like in FLINK-9908, I would propose to remove this feature and require the user to do the cancellation of input futures explicitly.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FutureUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="9921" opendate="2018-7-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the rolling policy interface.</summary>
      <description>Update the `shouldRollOnEvent` method in the rolling policy to also have as argument the element itself.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.TestUtils.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rolling.policies.OnCheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rolling.policies.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
    </fixedFiles>
  </bug>
  <bug id="9923" opendate="2018-7-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OneInputStreamTaskTest.testWatermarkMetrics fails on Travis</summary>
      <description>OneInputStreamTaskTest.testWatermarkMetrics fails on Travis withjava.lang.AssertionError: expected:&lt;1&gt; but was:&lt;-9223372036854775808&gt; at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.junit.Assert.assertEquals(Assert.java:631) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.testWatermarkMetrics(OneInputStreamTaskTest.java:731)https://api.travis-ci.org/v3/job/407196285/log.txt</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.metrics.WatermarkGauge.java</file>
    </fixedFiles>
  </bug>
  <bug id="993" opendate="2014-6-30 00:00:00" fixdate="2014-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent package naming of nightly builds and releases</summary>
      <description>stratosphere-bin-0.5.1.tgzstratosphere-bin-0.5.1-hadoop2.tgzstratosphere-bin-0.5.1-yarn.tar.gzvs.stratosphere-0.6-SNAPSHOT.tgzstratosphere-0.6-hadoop2-SNAPSHOT.tgzstratosphere-0.6-SNAPSHOT-yarn.tar.tgzNames and file extensions should be consistent.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">docs.example.connectors.md</file>
      <file type="M">.travis.yml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrimitiveInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.PrimitiveInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">docs.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="9933" opendate="2018-7-24 00:00:00" fixdate="2018-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify taskmanager memory default values</summary>
      <description>The default value for NETWORK_BUFFERS_MEMORY_MIN is currently defined is String.valueOf(64L &lt;&lt; 20), which in the documentation is represented as "67108864".Now that we have the MemorySize utility we can change the default to "64 mb".The same applies to NETWORK_BUFFERS_MEMORY_MAX.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.configuration.html</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="9934" opendate="2018-7-24 00:00:00" fixdate="2018-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka table source factory produces invalid field mapping</summary>
      <description>The Kafka table source factory produces an invalid field mapping when referencing a rowtime attribute from an input field. The check in TableSourceUtil#validateTableSource therefore can fail.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.SchemaValidatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.SchemaValidator.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9935" opendate="2018-7-24 00:00:00" fixdate="2018-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Table API: grouping by window and attribute causes java.lang.ClassCastException:</summary>
      <description> Grouping by window AND some other attribute(s) seems broken. Test case attached:class BatchStatisticsIntegrationTest extends FlatSpec with Matchers { trait BatchContext { implicit lazy val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment implicit val tableEnv: BatchTableEnvironment = TableEnvironment.getTableEnvironment(env) val data = Seq( (1532424567000L, "id1", "location1"), (1532424567000L, "id2", "location1"), (1532424567000L, "id3", "location1"), (1532424568000L, "id1", "location2"), (1532424568000L, "id2", "location3") ) val rawDataSet: DataSet[(Long, String, String)] = env.fromCollection(data) val table: Table = tableEnv.fromDataSet(rawDataSet, 'rowtime, 'id, 'location) } it should "be possible to run Table API queries with grouping by tumble window and column(s) on batch data" in new BatchContext { val results = table .window(Tumble over 1.second on 'rowtime as 'w) .groupBy('w, 'location) .select( 'w.start.cast(Types.LONG), 'w.end.cast(Types.LONG), 'location, 'id.count ) .toDataSet[(Long, Long, String, Long)] .collect() results should contain theSameElementsAs Seq( (1532424567000L, 1532424568000L, "location1", 3L), (1532424568000L, 1532424569000L, "location2", 1L), (1532424568000L, 1532424569000L, "location3", 1L) ) }}It seems like during execution time, the 'rowtime attribute replaces 'location and that causes ClassCastException.[info] Cause: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String[info] at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:28)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:160)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:46)[info] at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)[info] at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:88)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.sendToTarget(RecordWriter.java:129)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:105)[info] at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)[info] at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)[info] at org.apache.flink.api.java.operators.translation.RichCombineToGroupCombineWrapper.combine(RichCombineToGroupCombineWrapper.java:52)Here is some debug information that I was able to get. So, field serializers don't match the type of Row fields:this.instance = {Row@68451} "1532424567000,(3),1532424567000" fields = {Object[3]@68461} 0 = {Long@68462} 1532424567000 1 = {CountAccumulator@68463} "(3)" 2 = {Long@68462} 1532424567000this.serializer = {RowSerializer@68452} fieldSerializers = {TypeSerializer[3]@68455} 0 = {StringSerializer@68458} 1 = {TupleSerializer@68459} 2 = {LongSerializer@68460} arity = 3 nullMask = {boolean[3]@68457}  </description>
      <version>1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9936" opendate="2018-7-24 00:00:00" fixdate="2018-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos resource manager unable to connect to master after failover</summary>
      <description>When deployed in mesos session cluster mode, the connector monitor keeps reporting unable to connect to mesos after restart. In fact, scheduler driver already connected to mesos master, but when the connected message is lost. This is because leadership is not granted yet and fence id is not set, the rpc service ignores the connected message. So we should connect to mesos master after leadership is granted.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-jepsen.scripts.run-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9938" opendate="2018-7-24 00:00:00" fixdate="2018-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State TTL cleanup of full state snapshot upon checkpointing</summary>
      <description>We can try to piggyback full state scan during certain checkpoint processes in backends, check TTL expiration for every entry and evict expired entries from the snapshot.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlValueState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlReduceFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlMapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlFoldingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlFoldFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlAggregatingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlAggregateFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.AbstractTtlState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.AbstractTtlDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupPartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.NestedMapsStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableSnapshot.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.TtlStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.verify.AbstractTtlStateVerifier.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.DataStreamStateTTLTestProgram.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9942" opendate="2018-7-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guard handlers against null fields in requests</summary>
      <description>In FLINK-8233 the ObjectMapper used for the REST API was modified to not fail on missing creator properties. This means that any field for any request may be null.Since fields not being null was an assumption that handlers were previously built on, we now have to scan every implementation to ensure they can't fail with an NPE.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointDisposalRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9946" opendate="2018-7-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E test archetype version is hard-coded</summary>
      <description>mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-${TEST_TYPE} \ -DarchetypeVersion=1.6-SNAPSHOT \ -DgroupId=org.apache.flink.quickstart \ -DartifactId=${ARTIFACT_ID} \ -Dversion=${ARTIFACT_VERSION} \ -Dpackage=org.apache.flink.quickstart \ -DinteractiveMode=false</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9947" opendate="2018-7-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document unified table sources/sinks/formats</summary>
      <description>The recent unification of table sources/sinks/formats needs documentation. I propose a new page that explains the built-in sources, sinks, and formats as well as a page for customization of public interfaces.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.Json.java</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="9949" opendate="2018-7-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Kill Flink processes when tearing down cluster</summary>
      <description>Because Flink processes are not killed at the end of the tests, it can happen that surviving instances create znodes in the ZooKeeper ensemble of the next test run. This creates ambiguity when we retrieve the address of the leading REST server from ZooKeeper.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="9951" opendate="2018-7-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update scm developerConnection</summary>
      <description>The developer connection must be updated to point to the update remote.</description>
      <version>1.3.3,1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="996" opendate="2014-7-2 00:00:00" fixdate="2014-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException while translating union node</summary>
      <description>The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9969" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unreasonable memory requirements to complete examples/batch/WordCount</summary>
      <description>setup on AWS EMR: 5 worker nodes (m4.4xlarge nodes)  1 master node (m4.large)following command fails with out of memory errors:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 16000 examples/batch/WordCount.jarOnly increasing memory over 17.2GB example completes. At the same time after disabling flip6 following command succeeds:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 1000 examples/batch/WordCount.jar</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="9972" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug memory logging not working</summary>
      <description>It seems like with introduction of Flip6, debug memory logging is not being initialised anymore and following config properties are ignored:  `taskmanager.debug.memory.log` `taskmanager.debug.memory.log-interval`after disabling flip6 it works just fine.</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.MemoryLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9975" opendate="2018-7-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty dependency of Hadoop &gt;= 2.7 is not relocated</summary>
      <description>Previously, in flink-shaded-hadoop, we also relocate Netty (org.jboss.netty) to not conflict with user code. Since Hadoop 2.7 the Netty version they depend on has been upgraded and we missed relocating io.netty accordingly.</description>
      <version>1.4.2,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9977" opendate="2018-7-27 00:00:00" fixdate="2018-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refine the docs for Table/SQL built-in functions</summary>
      <description>There exist some syntax errors or inconsistencies in documents and Scala docs of the Table/SQL built-in functions. This issue aims to make some improvements to them.Also, according to FLINK-10103, we should use single quotes to express strings in SQL. For example, CONCAT("AA", "BB", "CC") should be replaced with CONCAT('AA', 'BB', 'CC'). </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="9981" opendate="2018-7-27 00:00:00" fixdate="2018-8-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Tune performance of RocksDB implementation</summary>
      <description>General performance tuning/polishing for the RocksDB implementation. We can figure out how caching/seeking can be improved.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.HeapInternalTimerServiceTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBOrderedSetStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.CachingInternalPriorityQueueSetWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOrderedSetStore.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.InternalPriorityQueueTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.TreeOrderedSetCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.TestOrderedStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.SimpleCachingInternalPriorityQueueSetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.OrderedSetCacheTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CachingInternalPriorityQueueSetTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TieBreakingPriorityComparator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.InternalPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.TreeOrderedSetCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSnapshotRestoreWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CachingInternalPriorityQueueSet.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPosTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPos.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9986" opendate="2018-7-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary information from .version.properties file</summary>
      <description>To log the revision flink-runtime creates a .version.properties file using the git-commit-id-plugin that is stored within the jar.Here's an example:git.commit.id.abbrev=1a9b648git.commit.user.email=chesnay@apache.orggit.commit.message.full=Commit for release 1.5.2\ngit.commit.id=1a9b6486a2d268d4fb8282c32d65fcc701d18e42git.commit.message.short=Commit for release 1.5.2git.commit.user.name=zentolgit.build.user.name=zentolgit.build.user.email=chesnay@apache.orggit.branch=1a9b6486a2d268d4fb8282c32d65fcc701d18e42git.commit.time=25.07.2018 @ 17\:10\:13 GMTgit.build.time=25.07.2018 @ 20\:47\:15 GMTgit.remote.origin.url=https\://github.com/zentol/flink.gitmost of this information isn't used, as flink-runtime only access git.commit.id.abbrev and git.commit.time.The build, remote and branch information should be removed as they are neither relevant, nor consistent, as releases can be created on any branch, under any git alias, against any remote.To exclude properties we have to bump the plugin version to 2.1.9.</description>
      <version>1.4.2,1.5.1,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9988" opendate="2018-7-27 00:00:00" fixdate="2018-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>job manager does not respect property jobmanager.web.address</summary>
      <description>As flink does not have any built in authentication mechanism, we used to setup nginx in front of it and start jobmanager on 127.0.0.1.but starting from version 1.5.0 - it does not work anymore.distespecting on jobmanager.web.address it always start on 0.0.0.0</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9995" opendate="2018-7-30 00:00:00" fixdate="2018-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Clean up Mesos Logs and Working Directory</summary>
      <description>When tearing down Mesos, all files in the log and working directory should be cleaned up, or we risk running out of disk space after running enough tests in succession.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.mesos.clj</file>
    </fixedFiles>
  </bug>
</bugrepository>
