<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="1696" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add multiple linear regression to ML library</summary>
      <description>Add multiple linear regression to ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20188" opendate="2020-11-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description></description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="20423" opendate="2020-11-30 00:00:00" fixdate="2020-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug id="21185" opendate="2021-1-28 00:00:00" fixdate="2021-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new interface for catalog to listen on temporary object operations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21885" opendate="2021-3-19 00:00:00" fixdate="2021-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor Typo Fix in JDBC Connector Documentation</summary>
      <description>There currently exists a minor typo within the JDBC Datastream documentation:A JDBC batch is executed as soon as one of the following condition is true: the configured batch interval time is elapsed the maximum batch size is reached a Flink checkpoint has startedSince it's plural, condition should be changed to conditions.Â </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="21887" opendate="2021-3-20 00:00:00" fixdate="2021-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add show views test in CliClientITCase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.catalog.database.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21888" opendate="2021-3-20 00:00:00" fixdate="2021-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain our own ASTNode class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.SelectClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.IdentifiersASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserStorageFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseDriver.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTLexer.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTHintParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.FromClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserAuthorizationParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserASTBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.HiveParserCreateViewDesc.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.CreateTableASDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="22002" opendate="2021-3-29 00:00:00" fixdate="2021-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>taskmanager.slot.timeout should fall back to akka.ask.timeout</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15634&amp;view=logs&amp;j=955770d3-1fed-5a0a-3db6-0c7554c910cb&amp;t=14447d61-56b4-5000-80c1-daa459247f6a&amp;l=6424org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase2021-03-29T00:27:25.3406344Z [ERROR] testSingleAggOnTable_HashAgg_WithLocalAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase) Time elapsed: 21.908 s &lt;&lt;&lt; ERROR!2021-03-29T00:27:25.3407190Z java.lang.RuntimeException: Failed to fetch next result2021-03-29T00:27:25.3407792Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)2021-03-29T00:27:25.3408502Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)2021-03-29T00:27:25.3409188Z at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:117)2021-03-29T00:27:25.3416724Z at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:350)2021-03-29T00:27:25.3417510Z at java.util.Iterator.forEachRemaining(Iterator.java:115)2021-03-29T00:27:25.3418416Z at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:108)2021-03-29T00:27:25.3419031Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:298)2021-03-29T00:27:25.3419657Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:138)2021-03-29T00:27:25.3420638Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:104)2021-03-29T00:27:25.3421384Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:182)2021-03-29T00:27:25.3422284Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_HashAgg_WithLocalAgg(AggregateReduceGroupingITCase.scala:135)2021-03-29T00:27:25.3422975Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-03-29T00:27:25.3423504Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-03-29T00:27:25.3424298Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-03-29T00:27:25.3425229Z at java.lang.reflect.Method.invoke(Method.java:498)2021-03-29T00:27:25.3426107Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2021-03-29T00:27:25.3426756Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2021-03-29T00:27:25.3427743Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2021-03-29T00:27:25.3428520Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2021-03-29T00:27:25.3429128Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2021-03-29T00:27:25.3429715Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2021-03-29T00:27:25.3433435Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2021-03-29T00:27:25.3433977Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-03-29T00:27:25.3434476Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2021-03-29T00:27:25.3435607Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2021-03-29T00:27:25.3436460Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2021-03-29T00:27:25.3437054Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2021-03-29T00:27:25.3437673Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2021-03-29T00:27:25.3438765Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2021-03-29T00:27:25.3439362Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2021-03-29T00:27:25.3440504Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2021-03-29T00:27:25.3441100Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2021-03-29T00:27:25.3441673Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2021-03-29T00:27:25.3442205Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-03-29T00:27:25.3442710Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2021-03-29T00:27:25.3443420Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2021-03-29T00:27:25.3444095Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2021-03-29T00:27:25.3444749Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2021-03-29T00:27:25.3445380Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2021-03-29T00:27:25.3446224Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-03-29T00:27:25.3447054Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-03-29T00:27:25.3447698Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-03-29T00:27:25.3448295Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-03-29T00:27:25.3448851Z Caused by: java.io.IOException: Failed to fetch job execution result2021-03-29T00:27:25.3449667Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)2021-03-29T00:27:25.3450406Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)2021-03-29T00:27:25.3451138Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)2021-03-29T00:27:25.3451674Z ... 42 more2021-03-29T00:27:25.3452201Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-03-29T00:27:25.3452872Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2021-03-29T00:27:25.3453864Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2021-03-29T00:27:25.3454600Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)2021-03-29T00:27:25.3455163Z ... 44 more2021-03-29T00:27:25.3455633Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-03-29T00:27:25.3456722Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)2021-03-29T00:27:25.3457420Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)2021-03-29T00:27:25.3458088Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2021-03-29T00:27:25.3458672Z at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)2021-03-29T00:27:25.3459380Z at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)2021-03-29T00:27:25.3460223Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:114)2021-03-29T00:27:25.3460987Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)2021-03-29T00:27:25.3461530Z ... 44 more2021-03-29T00:27:25.3462007Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy2021-03-29T00:27:25.3462740Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)2021-03-29T00:27:25.3463566Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)2021-03-29T00:27:25.3464342Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233)2021-03-29T00:27:25.3465041Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224)2021-03-29T00:27:25.3465873Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215)2021-03-29T00:27:25.3466611Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:669)2021-03-29T00:27:25.3467405Z at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:56)2021-03-29T00:27:25.3468253Z at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1869)2021-03-29T00:27:25.3469061Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1437)2021-03-29T00:27:25.3469687Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1377)2021-03-29T00:27:25.3470309Z at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1205)2021-03-29T00:27:25.3471109Z at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$11(Execution.java:856)2021-03-29T00:27:25.3471720Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-03-29T00:27:25.3472333Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-03-29T00:27:25.3472927Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2021-03-29T00:27:25.3473530Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)2021-03-29T00:27:25.3474147Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)2021-03-29T00:27:25.3474801Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)2021-03-29T00:27:25.3475437Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)2021-03-29T00:27:25.3476005Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2021-03-29T00:27:25.3476522Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2021-03-29T00:27:25.3477047Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2021-03-29T00:27:25.3477587Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2021-03-29T00:27:25.3478127Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2021-03-29T00:27:25.3478663Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-03-29T00:27:25.3479199Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-03-29T00:27:25.3479964Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2021-03-29T00:27:25.3481778Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2021-03-29T00:27:25.3482443Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2021-03-29T00:27:25.3483152Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2021-03-29T00:27:25.3483668Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2021-03-29T00:27:25.3484339Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2021-03-29T00:27:25.3484999Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2021-03-29T00:27:25.3485922Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2021-03-29T00:27:25.3486533Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2021-03-29T00:27:25.3487139Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2021-03-29T00:27:25.3487750Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2021-03-29T00:27:25.3488903Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.2021-03-29T00:27:25.3490086Z at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)2021-03-29T00:27:25.3490729Z at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)2021-03-29T00:27:25.3491369Z at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)2021-03-29T00:27:25.3492002Z at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)2021-03-29T00:27:25.3492646Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-03-29T00:27:25.3493299Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-03-29T00:27:25.3494129Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)2021-03-29T00:27:25.3494833Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-03-29T00:27:25.3495506Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-03-29T00:27:25.3496159Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-03-29T00:27:25.3496816Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-03-29T00:27:25.3497477Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1044)2021-03-29T00:27:25.3498061Z at akka.dispatch.OnComplete.internal(Future.scala:263)2021-03-29T00:27:25.3498569Z at akka.dispatch.OnComplete.internal(Future.scala:261)2021-03-29T00:27:25.3499094Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2021-03-29T00:27:25.3499642Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2021-03-29T00:27:25.3500344Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-03-29T00:27:25.3501155Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)2021-03-29T00:27:25.3502325Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2021-03-29T00:27:25.3503122Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2021-03-29T00:27:25.3503739Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)2021-03-29T00:27:25.3504306Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2021-03-29T00:27:25.3504894Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2021-03-29T00:27:25.3505528Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2021-03-29T00:27:25.3506142Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2021-03-29T00:27:25.3506803Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2021-03-29T00:27:25.3507875Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2021-03-29T00:27:25.3509196Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2021-03-29T00:27:25.3510052Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2021-03-29T00:27:25.3510597Z at java.lang.Thread.run(Thread.java:748)2021-03-29T00:27:25.3511727Z Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.2021-03-29T00:27:25.3512850Z at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:68)2021-03-29T00:27:25.3513557Z at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:832)2021-03-29T00:27:25.3514225Z at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)2021-03-29T00:27:25.3514854Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)2021-03-29T00:27:25.3515424Z at java.util.concurrent.FutureTask.run(FutureTask.java:266)2021-03-29T00:27:25.3516090Z at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)2021-03-29T00:27:25.3516881Z at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)2021-03-29T00:27:25.3517585Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2021-03-29T00:27:25.3518218Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2021-03-29T00:27:25.3518794Z ... 1 more2021-03-29T00:27:25.3547493Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_797#796750252]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.2021-03-29T00:27:25.3548645Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2021-03-29T00:27:25.3549245Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2021-03-29T00:27:25.3550046Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)2021-03-29T00:27:25.3550605Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2021-03-29T00:27:25.3551179Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2021-03-29T00:27:25.3551798Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2021-03-29T00:27:25.3552404Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2021-03-29T00:27:25.3553050Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2021-03-29T00:27:25.3554300Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2021-03-29T00:27:25.3555232Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2021-03-29T00:27:25.3555928Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2021-03-29T00:27:25.3556425Z ... 1 more</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22070" opendate="2021-3-31 00:00:00" fixdate="2021-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  <bug id="22198" opendate="2021-4-11 00:00:00" fixdate="2021-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaTableITCase hang.</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16287&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6625There is no any artifacts.</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22243" opendate="2021-4-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reactive Mode parallelism changes are not shown in the job graph visualization in the UI</summary>
      <description>As reported here FLINK-22134, the parallelism in the visual job graph on top of the detail page is not in sync with the parallelism listed in the task list below, when reactive mode causes a parallelism change.</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22345" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CoordinatorEventsExactlyOnceITCase hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16731&amp;view=logs&amp;j=02c4e775-43bf-5625-d1cc-542b5209e072&amp;t=e5961b24-88d9-5c77-efd3-955422674c25&amp;l=9896"main" #1 prio=5 os_prio=0 tid=0x00007fa8c800b800 nid=0x58b3 waiting on condition [0x00007fa8cfd1c000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x000000008147a7e8&gt; (a java.util.concurrent.CompletableFuture$Signaller) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:802) at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:187) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22407" opendate="2021-4-22 00:00:00" fixdate="2021-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump log4j to 2.14.1</summary>
      <description>Flink is currently relying on log4j 2.12.1 .Unfortunately, this vesrion has a bug related to json layout that prevents a user from adding additional log fields, as reported here https://issues.apache.org/jira/browse/LOG4J2-2652Â as well as here: https://stackoverflow.com/questions/57003440/why-is-log4j2-jsonlayout-keyvaluepair-printing-empty-logevent-messagesThe problem is fixed in Log4j 2.13.1.Is there a good reason to keep Log4j 2.12.1, or can we upgrade?As an illustration, the presence ofÂ  additional1 in the snippet below: rootLogger.level = INFOrootLogger.appenderRef.console.ref = LogConsole appender.console.name = LogConsoleappender.console.type = CONSOLEappender.console.layout.type = JsonLayoutappender.console.layout.complete = falseappender.console.layout.compact = trueappender.console.layout.eventEol = trueappender.console.layout.properties = trueappender.console.layout.includeStacktrace=trueappender.console.layout.stacktraceAsString=true appender.console.layout.additional1.type=KeyValuePairappender.console.layout.additional1.key=timestampappender.console.layout.additional1.value=$${date:yyyy-MM-dd'T'HH:mm:ss.SSSZ}Â  leads to missing fields in the resulting logs, e.g.:{"logEvent":"Recover all persisted job graphs.","timestamp":"2021-04-21T16:50:31.722+0000"}{"logEvent":"Successfully recovered 0 persisted job graphs.","timestamp":"2021-04-21T16:50:31.723+0000"}{"logEvent":"Starting the SlotManager.","timestamp":"2021-04-21T16:50:31.732+0000"}{"logEvent":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","timestamp":"2021-04-21T16:50:31.822+0000"}Â  Removing the additional1 resolves the issue and yield json logs containing all expected fields:{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess","message":"Successfully recovered 0 persisted job graphs.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":216868000},"threadId":48,"contextMap":{},"threadPriority":5}{"thread":"flink-akka.actor.default-dispatcher-3","level":"INFO","loggerName":"org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl","message":"Starting the SlotManager.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":313130000},"threadId":19,"contextMap":{},"threadPriority":5}{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.rpc.akka.AkkaRpcService","message":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":408714000},"threadId":48,"contextMap":{},"threadPriority":5}Â Â Â </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="22487" opendate="2021-4-27 00:00:00" fixdate="2021-5-27 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support `print` to print logs in PyFlink</summary>
      <description>Currently, if users want to print logs, they need to use logging module.@udf(result_type=DataTypes.BIGINT())def add(i, j): import logging logging.info("debug") return i + jIt will be more convenient to use `print` to print logs.Unable to find source-code formatter for language: python. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yaml@udf(result_type=DataTypes.BIGINT())def add(i, j): print("debug") return i + j</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">docs.content.docs.dev.python.debugging.md</file>
      <file type="M">docs.content.zh.docs.dev.python.debugging.md</file>
    </fixedFiles>
  </bug>
  <bug id="22563" opendate="2021-5-4 00:00:00" fixdate="2021-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add migration guide for new StateBackend interfaces</summary>
      <description></description>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="22573" opendate="2021-5-5 00:00:00" fixdate="2021-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncIO can timeout elements after completion</summary>
      <description>AsyncIO emits completed elements over the mailbox at which any timer is also canceled. However, if the mailbox cannot process (heavy backpressure), it may be that the timer still triggers on a completed element.</description>
      <version>1.11.3,1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22617" opendate="2021-5-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add log when create bulk format</summary>
      <description>Â Hive table sinkÂ  has some log that tells us whether to use native or mapred .Â Â LOG.info("Hive streaming sink: Use MapReduce RecordWriter writer.");LOG.info("Hive streaming sink: Use native parquet&amp;orc writer.");LOG.info( "Hive streaming sink: Use MapReduce RecordWriter writer because BulkWriter Factory not available.");Â I have some ideas we can add logÂ  to make it moreÂ  obvious when read hive for `createBulkFormatForSplit`.Â  Â  Â </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22636" opendate="2021-5-11 00:00:00" fixdate="2021-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group job specific ZooKeeper HA services under common jobs/&lt;JobID&gt; zNode</summary>
      <description>In order to better clean up Zookeeper HA services, I suggest grouping job-specific services under a common jobs/&lt;JobID&gt; zNode. That way, it becomes trivial to clean up the job-specific Zookeeper data (simply deleting the jobs/&lt;JobID&gt; node.Currently, our Zookeeper structure is not really structured well. The current layout looks like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; &lt;job-id&gt; -&gt; checkpoint-1 -&gt; checkpoint-counter -&gt; &lt;job-id&gt; -&gt; counter -&gt; leaderlatch -&gt; dispatcher_lock -&gt; resourc_emanager_lock -&gt; &lt;job-id&gt; -&gt; leader -&gt; dispatcher_lock -&gt; resource_manager_lock -&gt; &lt;job-id&gt;The new layout could look like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; jobs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; checkpoint-1 -&gt; checkpoint_id_counter -&gt; counter -&gt; leader -&gt; latch -&gt; connection_info -&gt; leader -&gt; dispatcher -&gt; latch -&gt; connection_info -&gt; resource_manager -&gt; latch -&gt; connection_info</description>
      <version>1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.zk.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22638" opendate="2021-5-11 00:00:00" fixdate="2021-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keep channels blocked on alignment timeout</summary>
      <description>In order to improve checkpointing time we could keep channels blocked in case an alignment timeout occurs. That way we prioritize the channels that we have not received the barrier yet. This solution is based on the assumption that all upstream operators are working with aligned checkpoints and we do not mind delaying the subsequent checkpoints on the blocked channels.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.WaitingForFirstBarrierUnaligned.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.WaitingForFirstBarrier.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CollectingBarriersUnaligned.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CollectingBarriers.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.ChannelState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingWaitingForFirstBarrier.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.java</file>
    </fixedFiles>
  </bug>
  <bug id="22643" opendate="2021-5-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many TCP connections among TaskManagers for large scale jobs</summary>
      <description>For the large scale jobs, there will be too many TCP connections among TaskManagers. Let's take an example.For a streaming job with 20 JobVertices, each JobVertex has 500 parallelism. We divide the vertices into 5 slot sharing groups. Each TaskManager has 5 slots. Thus there will be 400 taskmanagers in this job. Let's assume that job runs on a cluster with 20 machines.If all the job edges are all-to-all edges, there will be 19 * 20 * 399 * 2 = 303,240 TCP connections for each machine. If we run several jobs on this cluster, the TCP connections may exceed the maximum limit of linux, which is 1,048,576. This will stop the TaskManagers from creating new TCP connections and cause task failovers.As we run our production jobs on a K8S cluster, the job always failover due to exceptions related to network, such as Sending the partition request to 'null' failed, and etc.We think that we can decrease the number of connections by letting tasks reuse the same connection.Â We implemented a POC that makes all tasks on the same TaskManager reuse one TCP connection. For the example job we mentioned above, the number of connections will decrease from 303,240 to 15960. With the POC, the frequency of meeting exceptions related to network in our production jobs drops significantly.The POC is illustrated in: https://github.com/wsry/flink/commit/bf1c09e80450f40d018a1d1d4fe3dfd2de777fdcÂ </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22694" opendate="2021-5-18 00:00:00" fixdate="2021-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor TPCH end to end tests</summary>
      <description>Currently the TPCH test use yaml to init the environment. However, it's suggested to use sql file to init right now.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q9.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q8.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q7.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q6.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q5.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q4.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q3.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q22.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q21.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q20.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q2.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q19.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q18.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q17.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q16.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q15.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q14.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q13.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q12.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q11.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q10.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.sink.q1.yaml</file>
      <file type="M">flink-end-to-end-tests.flink-tpch-test.src.main.java.org.apache.flink.table.tpch.TpchResultComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22696" opendate="2021-5-18 00:00:00" fixdate="2021-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Confluent Schema Registry e2e Test on jdk 11</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22697" opendate="2021-5-18 00:00:00" fixdate="2021-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up examples to not use legacy planner anymore</summary>
      <description>Clean up the `flnk-examples-table` module to not reference the legacy planner anymore.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.StreamSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.TPCHQuery3Table.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.WordCountTable.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.WordCountSQL.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.basics.WordCountSQL.java</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22722" opendate="2021-5-20 00:00:00" fixdate="2021-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for Kafka New Source</summary>
      <description>Documentation describing the usage of Kafka FLIP-27 new source is required in Flink documentations.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="22745" opendate="2021-5-21 00:00:00" fixdate="2021-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MesosWorkerStore is started with an illegal namespace</summary>
      <description>The MesosWorkerStore is started with an illegal namespace because of FLINK-22636.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperUtilityFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22782" opendate="2021-5-26 00:00:00" fixdate="2021-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy planner from Chinese docs</summary>
      <description>FLINK-22740 should also be applied to Chinese docs.It should remove: Remove reference ofÂ useBlink/LegacyPlanner RemoveÂ DataSet Remove legacy planner mentioning</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.modules.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.python.table.api.connectors.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.dialect.md</file>
    </fixedFiles>
  </bug>
  <bug id="22784" opendate="2021-5-26 00:00:00" fixdate="2021-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen tests broken due to change in zNode layout</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.client.clj</file>
    </fixedFiles>
  </bug>
  <bug id="22794" opendate="2021-5-28 00:00:00" fixdate="2021-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade JUnit Vintage for flink-sql-parser and flink-sql-hive-parser</summary>
      <description>In FLINK-22778,set junit.version:4.13.2,it cause flink-sql-parser test work failed.and now junit-jupiter-engine:5.5.2;junit-vintage-engine:5.5.2.org.junit.vintage.engine.JUnit4VersionCheck#parseVersion cannot identify the version number correctly,e.g 4.13.2.so we can update sql-parser and sql-hive-parser to 5.7.0 at least.below is release note:JUnit VintageBug Fixes The Vintage engine no longer fails when resolving aÂ MethodSelectorÂ for methods of test classes that cannot be found via reflection. This allows selecting Spock feature methods by their source code name even though they have a generated method name in the bytecode.New Features and Improvements The internalÂ JUnit4VersionCheckÂ classâââwhich verifies that a supported version of JUnit 4 is on the classpathââânow implements a lenient version ID parsing algorithm in order to support custom version ID formats such asÂ 4.12.0,Â 4.12-patch_1, etc.Â Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">tools.ci.stage.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22821" opendate="2021-6-1 00:00:00" fixdate="2021-9-1 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>FlinkKafkaProducerMigrationTest fails with "Address already in use"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18469&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6832Jun 01 01:27:33 java.net.BindException: Address already in useJun 01 01:27:33 at sun.nio.ch.Net.bind0(Native Method)Jun 01 01:27:33 at sun.nio.ch.Net.bind(Net.java:461)Jun 01 01:27:33 at sun.nio.ch.Net.bind(Net.java:453)Jun 01 01:27:33 at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)Jun 01 01:27:33 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)Jun 01 01:27:33 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78)Jun 01 01:27:33 at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)Jun 01 01:27:33 at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:120)Jun 01 01:27:33 at org.apache.curator.test.TestingZooKeeperMain.runFromConfig(TestingZooKeeperMain.java:93)Jun 01 01:27:33 at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:148)Jun 01 01:27:33 at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.IPv6HostnamesITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.ClientTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22822" opendate="2021-6-1 00:00:00" fixdate="2021-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in JDBC module</summary>
      <description>Remove references to flink-table-planner in the JDBC module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22844" opendate="2021-6-2 00:00:00" fixdate="2021-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc to introduce ExplainDetails for EXPLAIN sytnax</summary>
      <description>Link toÂ FLINK-20562,add doc to introduct this new sytax.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
    </fixedFiles>
  </bug>
  <bug id="22847" opendate="2021-6-2 00:00:00" fixdate="2021-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SET should print options quoted</summary>
      <description>In FLINK-22770 we exposed SET/RESET in the parser and introduced quoting the options when using SET, but kept the unquoted support in SQL client for now as well.To facilitate the quoted syntax for a future deprecation of the unquoted one, SET; should print the current options using quotes.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.YamlConfigUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help-command.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
    </fixedFiles>
  </bug>
  <bug id="22849" opendate="2021-6-2 00:00:00" fixdate="2021-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop remaining usages of legacy planner in E2E tests and Python</summary>
      <description>This removes the remaining usages of legacy planner outside of the flink-table module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22856" opendate="2021-6-2 00:00:00" fixdate="2021-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move our Azure pipelines away from Ubuntu 16.04 by September</summary>
      <description>Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-python-wheels.yml</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="22862" opendate="2021-6-3 00:00:00" fixdate="2021-7-3 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support profiling for Python user-defined functions</summary>
      <description>We will support profiling to help users to analyze the performance bottleneck in their python udf</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.python.debugging.md</file>
      <file type="M">docs.content.zh.docs.dev.python.debugging.md</file>
    </fixedFiles>
  </bug>
  <bug id="22871" opendate="2021-6-4 00:00:00" fixdate="2021-8-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support to execute PyFlink jobs in YARN application mode</summary>
      <description>for now pyflink is not support hadoop yarn application mode, cause of yarn nodemanager may not have suitable python versionafter test for use venv(python virtual environment) that uploaded by 'python.files' properties, then change 'env.pythonExec' path, it also worksso,is there any possiable to support this in a suitable wayÂ Â </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonDependencyUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptionsUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="22876" opendate="2021-6-4 00:00:00" fixdate="2021-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding SharedObjects junit rule to ease test development</summary>
      <description>Most test rely on static variables to sync between test code and UDFs to avoid serialization issues. However, static variables are error-prone and the tests look quaint. SharedObjects allow test developers to forget about the serialization and just use the objects across thread boundaries.The main idea is that shared objects are bound to the scope of a test case instead of a class. That allows us to: get rid of all nasty reset methods for reused parts and most importantly avoid test bugs that result in us forgetting to reset (think of a latch in a shared WaitingSink) itâs easier to reason about the test setup it will allow us to share more code and provide more primitives (e.g. have some canonical way to wait for certain events) run tests in parallel that reuse classes depending on shared objects it will also be significantly easier to write tests for contributors.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22878" opendate="2021-6-4 00:00:00" fixdate="2021-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow prefix syntax for ConfigOption.mapType</summary>
      <description>The current factory design does not allow placeholder options in EncodingFormatFactory or DecodingFormatFactory.The past has shown that placeholder options are used at a couple of locations.See FLINK-22475 or KafkaOptions#PROPERTIES_PREFIX.We should think about adding an additional functionality to ReadableConfig or a special ConfigOption type to finally solve this problem. This could also be useful for FLIP-129. And would solve the current shortcomings for Confluent Avro registry.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
    </fixedFiles>
  </bug>
  <bug id="22880" opendate="2021-6-4 00:00:00" fixdate="2021-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove "blink" term in code base</summary>
      <description>Apart from FLINK-22879 and some API parts (such as EnvironmentSettings and old SQL Client YAML), we should not use the term "blink" in the code base and documentation anymore. For giving some background information, we should only document that the current planner was called "blink planner" in the past.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpch.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.parse.SetOperationParseStrategyTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.InternalConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.RichTableSourceQueryOperation.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.OverConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CustomizedConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.tsextractors.TimestampExtractor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.RowtimeAttributeDescriptor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.ProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.PartitionableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.NestedFieldsProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LookupableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LimitableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FilterableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FieldComputer.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedRowtimeAttributes.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedProctimeAttribute.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedFieldMapping.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.PartitionableTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.OverwritableTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Planner.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.StatementSetImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.StreamTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.InputFormatTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.UpsertStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.StreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.RetractStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.OutputFormatTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.AppendStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSinkFactory.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonFunctionFactoryTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.TimestampSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.TimeSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.StringSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.RowDataSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.MapDataSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.DateSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.ArrayDataSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.RowDataPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.set.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.schema.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.explain.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.distinct.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.catalog.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.dialect.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.table.environment.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.versioned.tables.md</file>
      <file type="M">docs.content.zh.docs.dev.table.config.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.set.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.raw.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.catalog.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.dialect.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.docs.dev.python.table.table.environment.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.docs.dev.table.sql.set.md</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.AbstractDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.CloseableIteratorTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.python.python.job.py</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.src.main.java.org.apache.flink.python.tests.BlinkBatchPythonUdfSqlJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.src.main.java.org.apache.flink.python.tests.BlinkStreamPythonUdfSqlJob.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.modified-query.q15.sql</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.modified-query.q20.sql</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-data.tpch.modified-query.q6.sql</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22890" opendate="2021-6-6 00:00:00" fixdate="2021-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Few tests fail in HiveTableSinkITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18692&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=420bd9ec-164e-562e-8947-0dacde3cec91&amp;l=23189Jun 05 01:22:13 [ERROR] Errors: Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testBatchAppend:138 Â» Validation Could not execute CREATE ...Jun 05 01:22:13 [ERROR] HiveTableSinkITCase.testDefaultSerPartStreamingWrite:156-&gt;testStreamingWrite:494 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testHiveTableSinkWithParallelismInStreaming:100-&gt;testHiveTableSinkWithParallelismBase:108 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testPartStreamingMrWrite:179-&gt;testStreamingWrite:423 Â» ValidationJun 05 01:22:13 [ERROR] HiveTableSinkITCase.testStreamingSinkWithTimestampLtzWatermark:360-&gt;fetchRows:384 Â» TestTimedOut</description>
      <version>1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22893" opendate="2021-6-6 00:00:00" fixdate="2021-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leader retrieval fails with NoNodeException</summary>
      <description>The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&amp;view=logs&amp;j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&amp;t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&amp;l=4382</description>
      <version>1.11.1,1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22920" opendate="2021-6-8 00:00:00" fixdate="2021-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guava version conflict in flink-format module</summary>
      <description>In the Flink-ORC and Flink-Parquet modules, The hadoop-common dependency contains the 11.0.2 version of guava, which conflicts with the 29.0-jre version required by the flink-table-planner-blink module. We should exclude guava from the hadoop-common dependency. Otherwise, running the unit test through the IDE throws a NoClassDefFoundError</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22963" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of taskmanager.memory.task.heap.size in the official document is incorrect</summary>
      <description>When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22964" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector-base exposes dependency to flink-core.</summary>
      <description>Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.Fix is to make `flink-core` provided in `connector-base`.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22970" opendate="2021-6-11 00:00:00" fixdate="2021-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The documentation for `TO_TIMESTAMP` UDF has an incorrect description</summary>
      <description>According to this ML discussion http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/confused-about-TO-TIMESTAMP-document-description-td44352.htmlThe description for `TO_TIMESTAMP` udf is not correct. It will use UTC+0 timezone instead of session timezone. We should fix this documentation.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="23004" opendate="2021-6-16 00:00:00" fixdate="2021-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misleading log</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23011" opendate="2021-6-16 00:00:00" fixdate="2021-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-27 sources are generating non-deterministic results when using event time</summary>
      <description>FLIP-27 sources currently start in the StreamStatus.IDLE state and they switch to ACTIVE only after emitting first Watermark. Until this happens, downstream operators are ignoring IDLE inputs from calculating the input (min) watermark. An extreme example to what problem this leads to, are completely bogus results if for example one FLIP-27 source subtask is slower than others for some reason:env.getConfig().setAutoWatermarkInterval(2000);env.setParallelism(2);env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 10));DataStream&lt;Long&gt; eventStream = env.fromSource( new NumberSequenceSource(0, Long.MAX_VALUE), WatermarkStrategy.&lt;Long&gt;forMonotonousTimestamps() .withTimestampAssigner(new LongTimestampAssigner()), "NumberSequenceSource") .map( new RichMapFunction&lt;Long, Long&gt;() { @Override public Long map(Long value) throws Exception { if (getRuntimeContext().getIndexOfThisSubtask() == 0) { Thread.sleep(1); } return 1L; } });eventStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(1))).sum(0).print();(...)private static class LongTimestampAssigner implements SerializableTimestampAssigner&lt;Long&gt; { private long counter = 0; @Override public long extractTimestamp(Long record, long recordTimeStamp) { return counter++; }}In such case, after 2 seconds (setAutoWatermarkInterval) the not throttled subtask (subTaskId == 1) generates very high watermarks. The other source subtask (subTaskId == 0) emits very low watermarks. If the non throttled watermark reaches the downstream WindowOperator first, while the other input channel is still idle, it will take those high watermarks as combined input watermark for the the whole WindowOperator. When the input channel from the throttled source subtask finally receives it's ACTIVE status and a much lower watermark, that's already too late.Actual output of the example program:15962000100010001000100010001000(...)while the expected output should be always "2000" (2000 records fitting in every 1 second global window)2000200020002000(...).</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.CombinedWatermarkStatus.java</file>
    </fixedFiles>
  </bug>
  <bug id="23014" opendate="2021-6-17 00:00:00" fixdate="2021-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support streaming window Deduplicate in planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.SimplifyWindowTableFunctionRules.java</file>
    </fixedFiles>
  </bug>
  <bug id="23015" opendate="2021-6-17 00:00:00" fixdate="2021-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement streaming window Deduplicate operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="23023" opendate="2021-6-17 00:00:00" fixdate="2021-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support offset in window TVF</summary>
      <description>Window offset is an optional parameter which could be used to change the alignment of windows.There are something we need clarify about window offset:(1) In SQL,Â window offset is an optionalÂ parameter, if it is specified, it is the last parameter of the window.for Tumble windowTUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE, INTERVAL '5' MINUTE)for Hop WindowHOP(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '1' MINUTE, INTERVAL '15' MINUTE,INTERVAL '5' MINUTE)for Cumulate WindowCUMULATE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '1' MINUTE, INTERVAL '15' MINUTE,Â INTERVAL '5' MINUTE)(2) Window offset could be positive duration and negative duration.(3) Window offset is used to change the alignment of Windows. The same record may be assigned to the different window after set window offset. But it always apply a rule, timestamp &gt;= window_start &amp;&amp; timestamp &lt; window_end.Give a demo, for a tumble window, window size is 10 MINUTE, which window would be assigned to for a record with timestamp 2021-06-30 00:00:04? offset is '-16 MINUTE',Â Â the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '-6 MINUTE', the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '-4 MINUTE', the record assigns to windowÂ [2021-06-29 23:56:00, 2021-06-30 00:06:00) offset is '0', the record assigns to window [2021-06-30 00:00:00, 2021-06-30 00:10:00) offset is '4 MINUTE',Â the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00) offset is '6 MINUTE,Â the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00) offset is '16 MINUTE',Â the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00)(4)Â We could find that, some window offset parameters may have same effect on the alignment of windows, in the above case,Â Â '-16 MINUTE' /'-6 MINUTE'/'4 MINUTE' have same effect on a tumble window with '10 MINUTE'Â  size.(5) Window offset is only used to change the alignment of Windows, it has no effect on watermark.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.TumblingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.HoppingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.CumulativeWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlTumbleTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlHopTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlCumulateTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="2305" opendate="2015-7-1 00:00:00" fixdate="2015-7-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documenation about Storm compatibility layer</summary>
      <description>Storm compatibility layer is currently no documented at the project web site.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormBoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.excamation.ExclamationTopology.java</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug id="23055" opendate="2021-6-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Window TVF offset</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  <bug id="23073" opendate="2021-6-21 00:00:00" fixdate="2021-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix space handling in Row CSV timestamp parser</summary>
      <description>FLINK-21947 Added support for TIMESTAMP_LTZ in the CSV format by replacing java.sql.Timestamp.valueOf with java.time.LocalDateTime.parse. Timestamp.valueOf internally calls `trim()` on the string before parsing while LocalDateTime.parse does not. This caused a breaking change where the CSV format can no longer parse timestamps of CSV's with spaces after the delimiter. We should manually re-add the call to trim to revert the behavior.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="23075" opendate="2021-6-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python API for enabling ChangelogStateBackend</summary>
      <description>After FLINK-22678, two APIs ```enableChangelogStateBackend``` and ```isChangelogStateBackendEnabled``` have been added. The corresponding interfaces should be added to python API.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="23097" opendate="2021-6-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Queryable state (rocksdb) with TM restart end-to-end test&amp;#39; fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19310&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529&amp;l=12333SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.Jun 22 14:04:55 MapState has 22 entriesJun 22 14:04:56 TaskManager 422719 killed.Jun 22 14:04:56 Number of running task managers 1 is not yet 0.Jun 22 14:05:00 Number of running task managers 1 is not yet 0.Jun 22 14:05:04 Number of running task managers 1 is not yet 0.Jun 22 14:05:08 Number of running task managers has reached 0.Jun 22 14:05:08 Latest snapshot count was 42Jun 22 14:05:09 Starting taskexecutor daemon on host fv-az68-17.Jun 22 14:05:09 Number of running task managers 0 is not yet 1.Jun 22 14:05:13 Number of running task managers has reached 1.Jun 22 14:05:15 Job (5b515e0f9168e338d1645bf2e9f92820) is running.Jun 22 14:05:15 Starting to wait for completion of 18 checkpointsJun 22 14:05:15 13/18 completed checkpointsJun 22 14:05:17 13/18 completed checkpointsJun 22 14:05:19 17/18 completed checkpointsJun 22 14:05:21 17/18 completed checkpointsSLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.Jun 22 14:05:24 after: 40Jun 22 14:05:24 An error occurredJun 22 14:05:24 [FAIL] Test script contains errors.Jun 22 14:05:24 Checking of logs skipped.Jun 22 14:05:24 Jun 22 14:05:24 [FAIL] 'Queryable state (rocksdb) with TM restart end-to-end test' failed after 0 minutes and 50 seconds! Test exited with exit code 1Jun 22 14:05:24 14:05:24 ##[group]Environment Information</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-queryable-state-test.src.main.java.org.apache.flink.streaming.tests.queryablestate.QsStateProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23104" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-statebackend-changelog does not build with scala 2.12</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19335&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&amp;l=4868</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23107" opendate="2021-6-23 00:00:00" fixdate="2021-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate deduplicate rank from rank functions</summary>
      <description>SELECT * FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY d ORDER BY e DESC) AS rownum from T) WHERE rownum=1Actually above sql is a deduplicate rank instead of a normal rank. We should separate the implementation for optimize the deduplicate rank and reduce bugs.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug id="23129" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When cancelling any running job of multiple jobs in an application cluster, JobManager shuts down</summary>
      <description>I have a jar with two jobs, both executeAsync() from the same main method. I execute the main method in an Application Mode cluster. When I cancel one of the two jobs, both jobs will stop executing.I would expect that the JobManager shuts down once all jobs submitted from an application are finished.If this is a known limitation, we should document it.2021-06-23 21:29:53,123 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job first job (18181be02da272387354d093519b2359) switched from state RUNNING to CANCELLING.2021-06-23 21:29:53,124 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from RUNNING to CANCELING.2021-06-23 21:29:53,141 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from CANCELING to CANCELED.2021-06-23 21:29:53,144 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b23592021-06-23 21:29:53,145 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job first job (18181be02da272387354d093519b2359) switched from state CANCELLING to CANCELED.2021-06-23 21:29:53,145 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job 18181be02da272387354d093519b2359.2021-06-23 21:29:53,147 INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down2021-06-23 21:29:53,150 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job 18181be02da272387354d093519b2359 reached terminal state CANCELED.2021-06-23 21:29:53,152 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job first job(18181be02da272387354d093519b2359).2021-06-23 21:29:53,155 INFO org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [c35b64879d6b02d383c825ea735ebba0].2021-06-23 21:29:53,159 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b23592021-06-23 21:29:53,159 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job first job(18181be02da272387354d093519b2359)..2021-06-23 21:29:53,160 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 18181be02da272387354d093519b2359 from the resource manager.2021-06-23 21:29:53,225 INFO org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application CANCELED:java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$4(ApplicationDispatcherBootstrap.java:304) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252] at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252] at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) [?:1.8.0_252] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) [?:1.8.0_252] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252] at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) [?:1.8.0_252] at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.OnComplete.internal(Future.scala:264) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.OnComplete.internal(Future.scala:261) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] ... 42 moreCaused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] ... 42 more2021-06-23 21:29:53,238 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting StandaloneApplicationClusterEntryPoint down with application status CANCELED. Diagnostics null.2021-06-23 21:29:53,239 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.2021-06-23 21:29:53,257 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/flink-web-a0d034d2-da2b-4d72-9ece-ec00c9ae032b/flink-web-ui2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://localhost:8081 lost leadership2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics null.2021-06-23 21:29:53,307 INFO org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.2021-06-23 21:29:53,308 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job second job(e4ff65c30754648cf114232c07ef903e).2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job e4ff65c30754648cf114232c07ef903e reached terminal state SUSPENDED.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.2021-06-23 21:29:53,309 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job second job (e4ff65c30754648cf114232c07ef903e) switched from state RUNNING to SUSPENDED.org.apache.flink.util.FlinkException: Scheduler is being stopped. at org.apache.flink.runtime.scheduler.SchedulerBase.closeAsync(SchedulerBase.java:604) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:962) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:926) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:398) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2021-06-23 21:29:53,311 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from RUNNING to CANCELING.2021-06-23 21:29:53,312 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -&gt; Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from CANCELING to CANCELED.2021-06-23 21:29:53,313 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution b08fac5184817c72f73a0b3fff0afbd3.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job e4ff65c30754648cf114232c07ef903e.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job e4ff65c30754648cf114232c07ef903e has been suspended.2021-06-23 21:29:53,314 INFO org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [30b64fc00bc2c8e83e80567e4f984ae9].2021-06-23 21:29:53,315 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job second job(e4ff65c30754648cf114232c07ef903e)..2021-06-23 21:29:53,318 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopped dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.2021-06-23 21:29:53,323 INFO org.apache.flink.runtime.blob.BlobServer [] - Stopped BLOB server at 0.0.0.0:614982021-06-23 21:29:53,323 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2021-06-23 21:29:53,326 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2021-06-23 21:29:53,331 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2021-06-23 21:29:53,331 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2021-06-23 21:29:53,332 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2021-06-23 21:29:53,332 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2021-06-23 21:29:53,348 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2021-06-23 21:29:53,348 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2021-06-23 21:29:53,359 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2021-06-23 21:29:53,366 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2021-06-23 21:29:53,366 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 0.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="23131" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove scala from plugin parent-first patterns</summary>
      <description>In order to load akka and it's scala version through a separate classloader we need to remove scala from the parent-first patterns for plugins.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23149" opendate="2021-6-25 00:00:00" fixdate="2021-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Java code splitter for code generation</summary>
      <description>In this first step we introduce a Java code splitter for the generated Java code. This splitter comes into effect after the original Java code is generated, hoping to solve the 64KB problem in one shot.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.DeclarationRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.DeclarationRewriter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.member.expected.TestNotRewriteStaticMember.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.member.code.TestNotRewriteStaticMember.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.MemberFieldRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.MemberFieldRewriter.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23184" opendate="2021-6-29 00:00:00" fixdate="2021-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException Assignment conversion not possible from type "int" to type "short"</summary>
      <description>CREATE TABLE MySink ( `a` SMALLINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath')CREATE TABLE database8_t0 ( `c0` SMALLINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath11')CREATE TABLE database8_t1 ( `c0` SMALLINT, `c1` TINYINT) WITH ( 'connector' = 'filesystem', 'format' = 'testcsv', 'path' = '$resultPath22')INSERT OVERWRITE database8_t0(c0) VALUES(cast(22424 as SMALLINT))INSERT OVERWRITE database8_t1(c0, c1) VALUES(cast(-17443 as SMALLINT), cast(97 as TINYINT))insert into MySinkSELECT database8_t0.c0 AS ref0 FROM database8_t0, database8_t1 WHERE CAST ((- (database8_t0.c0)) AS BOOLEAN)After running that , you will get the errors:2021-06-29 19:39:27org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:440) at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'BatchExecCalc$4536' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:43) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:600) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:540) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:171) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:547) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:536) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ... 14 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:106) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:76) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 17 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 53, Column 26: Assignment conversion not possible from type "int" to type "short" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790) at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2779) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:103) ... 23 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23188" opendate="2021-6-30 00:00:00" fixdate="2021-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unsupported function definition: IFNULL. Only user defined functions are supported as inline functions</summary>
      <description>CREATE TABLE database0_t0(c0 FLOAT) WITH (Â  'connector' = 'filesystem',Â  'path' = 'hdfs:///tmp/database0_t0.csv',Â  'format' = 'csv');INSERT OVERWRITE database0_t0(c0) VALUES(0.40445197);SELECT database0_t0.c0 AS ref0 FROM database0_t0 WHEREÂ ((IFNULL(database0_t0.c1, database0_t0.c1)) IS NULL);The errors:"&lt;ExceptionÂ onÂ serverÂ side: org.apache.flink.table.api.TableException:Â UnsupportedÂ functionÂ definition:Â IFNULL.Â OnlyÂ userÂ definedÂ functionsÂ areÂ supportedÂ asÂ inlineÂ functions. Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.lambda$createInlineFunctionName$0(BridgingUtils.java:81) Â atÂ java.util.Optional.orElseThrow(Optional.java:290) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.createInlineFunctionName(BridgingUtils.java:78) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingUtils.createName(BridgingUtils.java:58) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.&lt;init&gt;(BridgingSqlFunction.java:76) Â atÂ org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.of(BridgingSqlFunction.java:116) Â atÂ org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.convert(FunctionDefinitionConvertRule.java:65) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71) Â atÂ org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter$1.toRexNode(ExpressionConverter.java:247) Â atÂ java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) Â atÂ java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) Â atÂ java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) Â atÂ java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) Â atÂ java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) Â atÂ java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) Â atÂ java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.toRexNodes(ExpressionConverter.java:240) Â atÂ org.apache.flink.table.planner.expressions.converter.DirectConvertRule.lambda$convert$0(DirectConvertRule.java:220) Â atÂ java.util.Optional.map(Optional.java:215) Â atÂ org.apache.flink.table.planner.expressions.converter.DirectConvertRule.convert(DirectConvertRule.java:217) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97) Â atÂ org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71) Â atÂ org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.lambda$convertExpressionToRexNode$0(PushFilterIntoSourceScanRuleBase.java:73) Â atÂ java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) Â atÂ java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) Â atÂ java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) Â atÂ java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) Â atÂ java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) Â atÂ java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) Â atÂ java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.convertExpressionToRexNode(PushFilterIntoSourceScanRuleBase.java:73) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.resolveFiltersAndCreateTableSourceTable(PushFilterIntoSourceScanRuleBase.java:116) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:95) Â atÂ org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:70) Â atÂ org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) Â atÂ org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) Â atÂ org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) Â atÂ org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) Â atÂ org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) Â atÂ org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) Â atÂ org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.Iterator$class.foreach(Iterator.scala:891) Â atÂ scala.collection.AbstractIterator.foreach(Iterator.scala:1334) Â atÂ scala.collection.IterableLike$class.foreach(IterableLike.scala:72) Â atÂ scala.collection.AbstractIterable.foreach(Iterable.scala:54) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.immutable.Range.foreach(Range.scala:160) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) Â atÂ scala.collection.Iterator$class.foreach(Iterator.scala:891) Â atÂ scala.collection.AbstractIterator.foreach(Iterator.scala:1334) Â atÂ scala.collection.IterableLike$class.foreach(IterableLike.scala:72) Â atÂ scala.collection.AbstractIterable.foreach(Iterable.scala:54) Â atÂ scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) Â atÂ scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) Â atÂ org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) Â atÂ scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) Â atÂ org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) Â atÂ org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:93) Â atÂ org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:310) Â atÂ org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:172) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.lambda$executeQueryInternal$0(SelectOperation.java:183) Â atÂ com.ververica.flink.table.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:130) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.executeQueryInternal(SelectOperation.java:182) Â atÂ com.ververica.flink.table.gateway.operation.SelectOperation.execute(SelectOperation.java:82) Â atÂ com.ververica.flink.table.gateway.operation.executor.OneByOneOperationExecutor.execute(OneByOneOperationExecutor.java:57) Â atÂ com.ververica.flink.table.gateway.rest.session.Session.lambda$runStatement$1(Session.java:115) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.lambda$wrapWithHadoopUsernameIfNeeded$0(EnvironmentUtil.java:57) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:65) Â atÂ com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:56) Â atÂ com.ververica.flink.table.gateway.rest.session.Session.runStatement(Session.java:114) Â atÂ com.ververica.flink.table.gateway.rest.handler.StatementExecuteHandler.handleRequest(StatementExecuteHandler.java:83) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:85) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:184) Â atÂ com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:76) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) Â atÂ org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) Â atÂ org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) Â atÂ org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) Â atÂ org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) Â atÂ org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) Â atÂ org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) Â atÂ java.lang.Thread.run(Thread.java:834) EndÂ ofÂ exceptionÂ onÂ serverÂ side&gt;"</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23192" opendate="2021-6-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move connector/format option classes into a common package</summary>
      <description>For built-in connectors, we need to refactor their corresponding *Options classes to â¦ be located in a common package</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatSerializationSchema.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatOptions.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatDeserializationSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.BlackHoleConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.PrintSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.BlackHoleSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DataGeneratorMapper.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.SequenceGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenVisitorBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGeneratorContainer.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptionsUtil.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BlackHoleTableSinkFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23201" opendate="2021-7-1 00:00:00" fixdate="2021-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The check on alignmentDurationNanos seems to be too strict</summary>
      <description>The check on alignmentDurationNanos seems to be too strict at the line:https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointMetrics.java#L74This caused a job to fail when doing stop-with-savepoint. But doing savepoint only without stop does not seem to be impacted by this.</description>
      <version>1.12.2,1.14.0,1.13.1</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23208" opendate="2021-7-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Late processing timers need to wait 1ms at least to be fired</summary>
      <description>The problem is from the codes below:public static long getProcessingTimeDelay(long processingTimestamp, long currentTimestamp) { // delay the firing of the timer by 1 ms to align the semantics with watermark. A watermark // T says we won't see elements in the future with a timestamp smaller or equal to T. // With processing time, we therefore need to delay firing the timer by one ms. return Math.max(processingTimestamp - currentTimestamp, 0) + 1;}Assuming a Flink job creates 1 timer per millionseconds, and is able to consume 1 timer/ms. Here is what will happen: Timestmap1(1st ms): timer1 is registered and will be triggered on Timestamp2. Timestamp2(2nd ms): timer2 is registered and timer1 is triggered Timestamp3(3rd ms): timer3 is registered and timer1 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer2, and timer2 will be triggered on Timestamp4(wait 1ms at least) Timestamp4(4th ms): timer4 is registered and timer2 is triggered Timestamp5(5th ms): timer5 is registered and timer2 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer3, and timer3 will be triggered on Timestamp6(wait 1ms at least)As we can see here, the ability of the Flink job is consuming 1 timer/ms, but it's actually able to consume 0.5 timer/ms. And another problem is that we cannot observe the delay from the lag metrics of the source(Kafka). Instead, what we can tell is that the moment of output is much later than expected. I've added a metrics in our inner version, we can see the lag of the timer triggering keeps increasing: In another word, we should never let the late processing timer wait 1ms, I think a simple change would be as below:return Math.max(processingTimestamp - currentTimestamp, -1) + 1;</description>
      <version>1.11.0,1.11.3,1.13.0,1.14.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23214" opendate="2021-7-2 00:00:00" fixdate="2021-8-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>Make ShuffleMaster a cluster level shared service</summary>
      <description>This ticket tries to make ShuffleMaster a cluster level shared service which makes it consistent with the ShuffleEnvironment at the TM side.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerSharedServicesBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23223" opendate="2021-7-2 00:00:00" fixdate="2021-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When flushAlways is enabled the subpartition may lose notification of data availability</summary>
      <description>When the flushAways is enabled (namely set buffer timeout to 0), there might be cases like: The subpartition emit an event which blocks the channel The subpartition produce more records. However, this records would not be notified since isBlocked = true. When the downstream tasks resume the subpartition later, the subpartition would only mark isBlocked to false. For local input channels although it tries to add the channel if isAvailable = true, but this check would not pass since flushRequest = false.Â One case for this issue isÂ https://issues.apache.org/jira/browse/FLINK-22085Â which uses LocalInputChannel.</description>
      <version>1.11.3,1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="23232" opendate="2021-7-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink tox check fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3&amp;l=23069Jul 03 00:07:56 ============tox checks... [FAILED]============Jul 03 00:07:56 Process exited with EXIT CODE: 1.Jul 03 00:07:56 Trying to KILL watchdog (3140)./__w/2/s/tools/ci/watchdog.sh: line 100: 3140 Terminated watchdogJul 03 00:07:56 Searching for .dump, .dumpstream and related files in '/__w/2/s'The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.datastream.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="23233" opendate="2021-7-5 00:00:00" fixdate="2021-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=c2734c79-73b6-521c-e85a-67c7ecae9107&amp;l=9382Jul 03 01:37:31 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.415 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCaseJul 03 01:37:31 [ERROR] testOperatorEventLostWithReaderFailure(org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase) Time elapsed: 3.623 s &lt;&lt;&lt; FAILURE!Jul 03 01:37:31 java.lang.AssertionError: expected:&lt;[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]&gt; but was:&lt;[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]&gt;Jul 03 01:37:31 at org.junit.Assert.fail(Assert.java:88)Jul 03 01:37:31 at org.junit.Assert.failNotEquals(Assert.java:834)Jul 03 01:37:31 at org.junit.Assert.assertEquals(Assert.java:118)Jul 03 01:37:31 at org.junit.Assert.assertEquals(Assert.java:144)Jul 03 01:37:31 at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.runTest(OperatorEventSendingCheckpointITCase.java:254)Jul 03 01:37:31 at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure(OperatorEventSendingCheckpointITCase.java:143)Jul 03 01:37:31 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jul 03 01:37:31 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jul 03 01:37:31 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Jul 03 01:37:31 at java.lang.reflect.Method.invoke(Method.java:498)Jul 03 01:37:31 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)Jul 03 01:37:31 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Jul 03 01:37:31 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)Jul 03 01:37:31 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Jul 03 01:37:31 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Jul 03 01:37:31 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)Jul 03 01:37:31 at org.junit.rules.RunRules.evaluate(RunRules.java:20)Jul 03 01:37:31 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)Jul 03 01:37:31 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)Jul 03 01:37:31 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)</description>
      <version>1.14.0,1.12.3,1.13.1</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.EventReceivingTasks.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.Runnables.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="2324" opendate="2015-7-7 00:00:00" fixdate="2015-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework partitioned state storage</summary>
      <description>Partitioned states are currently stored per-key in statehandles. This is alright for in-memory storage but is very inefficient for HDFS. The logic behind the current mechanism is that this approach provides a way to repartition a state without fetching the data from the external storage and only manipulating handles.We should come up with a solution that can achieve both.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.OperatorStateHandle.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointingITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.WrapperStateHandle.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.StreamOperatorState.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.PartitionedStreamOperatorState.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.LazyStateStore.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.EagerStateStore.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StatefulStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PartitionedStateStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="23240" opendate="2021-7-5 00:00:00" fixdate="2021-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19872&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&amp;l=10186Jul 04 22:17:29 [ERROR] Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 91.407 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCaseJul 04 22:17:29 [ERROR] testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase) Time elapsed: 31.356 s &lt;&lt;&lt; ERROR!Jul 04 22:17:29 java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.runJobAndGetExternalizedCheckpoint(ResumeCheckpointManuallyITCase.java:303)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:275)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(ResumeCheckpointManuallyITCase.java:215)Jul 04 22:17:29 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jul 04 22:17:29 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jul 04 22:17:29 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Jul 04 22:17:29 at java.base/java.lang.reflect.Method.invoke(Method.java:566)Jul 04 22:17:29 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Jul 04 22:17:29 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Jul 04 22:17:29 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Jul 04 22:17:29 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Jul 04 22:17:29 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Jul 04 22:17:29 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Jul 04 22:17:29 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Jul 04 22:17:29 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Jul 04 22:17:29 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Jul 04 22:17:29 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Jul 04 22:17:29 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)Jul 04 22:17:29 at org.junit.rules.RunRules.evaluate(RunRules.java:20)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Jul 04 22:17:29 at org.junit.runners.Suite.runChild(Suite.java:128)Jul 04 22:17:29 at org.junit.runners.Suite.runChild(Suite.java:27)Jul 04 22:17:29 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Jul 04 22:17:29 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Jul 04 22:17:29 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Jul 04 22:17:29 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)Jul 04 22:17:29 Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.Jul 04 22:17:29 at com.sun.proxy.$Proxy30.cancelJob(Unknown Source)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.lambda$cancelJob$7(MiniCluster.java:716)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:758)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.cancelJob(MiniCluster.java:715)Jul 04 22:17:29 at org.apache.flink.client.program.MiniClusterClient.cancel(MiniClusterClient.java:83)Jul 04 22:17:29 ... 46 moreJul 04 22:17:29 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-1806874751]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)Jul 04 22:17:29 at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)Jul 04 22:17:29 at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)Jul 04 22:17:29 at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)Jul 04 22:17:29 at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)Jul 04 22:17:29 at java.base/java.lang.Thread.run(Thread.java:834)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23255" opendate="2021-7-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JUnit 5 jupiter and vintage engine</summary>
      <description>Add dependencies for JUnit 5 jupiter for supporting JUnit 5 tests, and vintage engine for supporting test cases in JUnit 4 style</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23271" opendate="2021-7-6 00:00:00" fixdate="2021-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeException: while resolving method &amp;#39;booleanValue&amp;#39; in class class java.math.BigDecimal</summary>
      <description>Sql :CREATE TABLE database3_t0(c0 DECIMAL , c1 SMALLINT ) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///tmp/database3_t0.csv', 'format' = 'csv' );INSERT OVERWRITE database8_t0(c0, c1) VALUES(2113554022, cast(-22975 as SMALLINT)), (1570419395, cast(-26858 as SMALLINT)), (-1569861129, cast(-20143 as SMALLINT));SELECT database8_t0.c0 AS ref0 FROM database8_t0 WHERE CAST (0.10915913549909961 AS BOOLEAN;After excuting the sql, you will find the error:java.lang.RuntimeException: while resolving method 'booleanValue' in class class java.math.BigDecimal at org.apache.calcite.linq4j.tree.Expressions.call(Expressions.java:424) at org.apache.calcite.linq4j.tree.Expressions.call(Expressions.java:435) at org.apache.calcite.linq4j.tree.Expressions.unbox(Expressions.java:1453) at org.apache.calcite.adapter.enumerable.EnumUtils.convert(EnumUtils.java:398) at org.apache.calcite.adapter.enumerable.EnumUtils.convert(EnumUtils.java:326) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateCast(RexToLixTranslator.java:538) at org.apache.calcite.adapter.enumerable.RexImpTable$CastImplementor.implementSafe(RexImpTable.java:2450) at org.apache.calcite.adapter.enumerable.RexImpTable$AbstractRexCallImplementor.genValueStatement(RexImpTable.java:2894) at org.apache.calcite.adapter.enumerable.RexImpTable$AbstractRexCallImplementor.implement(RexImpTable.java:2859) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.visitCall(RexToLixTranslator.java:1084) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.visitCall(RexToLixTranslator.java:90) at org.apache.calcite.rex.RexCall.accept(RexCall.java:174) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.visitLocalRef(RexToLixTranslator.java:970) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.visitLocalRef(RexToLixTranslator.java:90) at org.apache.calcite.rex.RexLocalRef.accept(RexLocalRef.java:75) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate(RexToLixTranslator.java:237) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translate(RexToLixTranslator.java:231) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateList(RexToLixTranslator.java:818) at org.apache.calcite.adapter.enumerable.RexToLixTranslator.translateProjects(RexToLixTranslator.java:198) at org.apache.calcite.rex.RexExecutorImpl.compile(RexExecutorImpl.java:90) at org.apache.calcite.rex.RexExecutorImpl.compile(RexExecutorImpl.java:66) at org.apache.calcite.rex.RexExecutorImpl.reduce(RexExecutorImpl.java:128) at org.apache.calcite.rex.RexSimplify.simplifyCast(RexSimplify.java:2101) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:326) at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:287) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:262) at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:224) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:93) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:310) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:172) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1658) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:747) at org.apache.flink.table.api.internal.StatementSetImpl.execute(StatementSetImpl.java:100) at org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.testTableXiaojin(TableSourceITCase.scala:436) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: java.lang.NoSuchMethodException: java.math.BigDecimal.booleanValue() at java.lang.Class.getMethod(Class.java:1786) at org.apache.calcite.linq4j.tree.Expressions.call(Expressions.java:421) ... 112 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="23277" opendate="2021-7-6 00:00:00" fixdate="2021-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changelog backend doesn&amp;#39;t apply TTL after recovery</summary>
      <description>Upon recovery, changelog backend requests states to apply changes.TTL config is not available at this moment, so states are created regardless of TTL config.One solution is to serialize TTL config along with metadata (in changelog).Note: values are already serialized as TTL values and serializers as TTL seralizersCaused by: java.lang.ClassCastException: org.apache.flink.runtime.state.ttl.TtlValue cannot be cast to org.apache.flink.table.data.RowData at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:129) at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:228) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:428) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:691) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:646) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:657) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:630) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:748)(doesn't affect test stability as changelog backend is currently disabled in tests)</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.KvStateChangeLoggerImplTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.restore.ChangelogBackendLogApplier.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PriorityQueueStateChangeLoggerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.KvStateChangeLoggerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.AbstractStateChangeLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="23279" opendate="2021-7-6 00:00:00" fixdate="2021-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable changelog backend in tests</summary>
      <description>FLINK-21448 adds the capability (test randomization), but it can't be turned on as there are some test failures: FLINK-23276, FLINK-23277, FLINK-23278 (should be enabled after those bugs fixed)..</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateHandleStreamImpl.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23287" opendate="2021-7-7 00:00:00" fixdate="2021-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create user document for Window Join in SQL</summary>
      <description>Create user document for Window Join in SQL</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  <bug id="23290" opendate="2021-7-7 00:00:00" fixdate="2021-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast &amp;#39;(LZ *3&amp;#39; as boolean, get a null</summary>
      <description>CREATE TABLE database5_t0(c0 VARCHAR , c1 BIGINT ) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///tmp/database5_t0.csv', 'format' = 'csv');INSERT OVERWRITE database5_t0(c0, c1) VALUES('(LZ *3', 2135917226)SELECT database5_t0.c0 AS ref0 FROM database5_t0 WHERE CAST (database5_t0.c0 AS BOOLEAN)After excuting that, you will get the error:Caused by: java.lang.NullPointerException at BatchExecCalc$20.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:319) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:414) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23297" opendate="2021-7-7 00:00:00" fixdate="2021-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Protobuf to 3.17.3</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our Protobuf dependency to version 3.17.3.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23308" opendate="2021-7-8 00:00:00" fixdate="2021-7-8 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Performance regression on 06.07</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&amp;ben=twoInputMapSink&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=onhttp://codespeed.dak8s.net:8000/timeline/?ben=readFileSplit&amp;env=2</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
    </fixedFiles>
  </bug>
  <bug id="23309" opendate="2021-7-8 00:00:00" fixdate="2021-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the finish bundle logic in PyFlink</summary>
      <description>Whenever the `bundle size` or `bundle time` is reached, the data in the input buffer needs to be sent from the jvm to the pvm, and then waits for the pym to be processed and sent back to the jvm to send all the results to the downstream operator, which leads to a large delay in current implementation of main thread triggering `finish bundle`, especially when it is a small size event as small messages are hard to processed in pipeline.We need to move the logic of `finish bundle` from the main thread to the asynchronous sending thread, so as to avoid the problem of the main thread being blocked.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.stream.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.stream.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="23345" opendate="2021-7-11 00:00:00" fixdate="2021-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to the next version of Python `requests` when released</summary>
      <description>Hello Maintainers,Â I am a PMC member of Apache Airflow, and I wanted to give you a bit of heads-up with rather important migration to the upcoming version of `requests` library in your Python release.Â Since you are using `requests` library in your project (at least indirectly via apache-beam), you are affected.As discussed at length in https://issues.apache.org/jira/browse/LEGAL-572Â we found out that the 'chardet` library used by `requests` library was a mandatory dependency to requests and since it has LGPL licence, we should not release any Apache Software with it.Â Since then (and since in Airflow we rely on requests heavily) we have been working with the requests maintainers and "charset-normalizer" maintainer to make it possible to replace `chardet` with MIT-licensed `charset-normalizer` instead so that requests library can be used in Python releases by Apache projects.This was a bumpy road but finally the PR by ashÂ has been merged: https://github.com/psf/requests/pull/5797Â and we hope soon a new version of requests library will be released.Â This is just a heads-up. I will let you know when it is released, but I have a kind requests as well - I might ask the maintainers to release a release candidate of requests and maybe you could help to test it before it is released, that would be some re-assurance for the maintainers of requests who are very concerned about stability of their releases.Let me know if you need any more information and whether you would like to help in testing the candidate when it is out.</description>
      <version>1.14.0</version>
      <fixedVersion>1.10.4,1.14.0,1.13.3,1.11.7,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="23367" opendate="2021-7-13 00:00:00" fixdate="2021-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testKeyGroupedInternalPriorityQueue does not dispose ChangelogDelegateEmbeddedRocksDB properly, and fails the test</summary>
      <description>The set of `testKeyGroupedInternalPriorityQueue` for `ChangelogDelegateEmbeddedRocksDBStateBackendTest` does not dispose rocksdb properly. It seems it creates one more than needed column family in the `ChangelogDelegateEmbeddedRocksDBStateBackendTest` case comparing to `EmbeddedRocksDBStateBackend` // ... continue with the ones created by Flink... for (RocksDbKvStateInfo kvStateInfo : kvStateInformation.values()) { RocksDBOperationUtils.addColumnFamilyOptionsToCloseLater( columnFamilyOptions, kvStateInfo.columnFamilyHandle); IOUtils.closeQuietly(kvStateInfo.columnFamilyHandle); }</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.StateChangeLoggingIterator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyGroupedPriorityQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="23389" opendate="2021-7-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>AWS Glue Schema Registry JSON support for Apache Flink</summary>
      <description>AWS Glue Schema Registry client library has recently released a new version (v1.1.1) which highlights JSON support.Â This request is to add the new data format support to launch an integration for Apache Flink with the latest AWS Glue Schema Registry.Â </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.glue.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.src.main.resources.avro.user.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.src.main.java.org.apache.flink.glue.schema.registry.test.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.src.main.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryExampleTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.src.main.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="23395" opendate="2021-7-15 00:00:00" fixdate="2021-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Okhttp to 3.14.9</summary>
      <description>We currently use 3 different version of Okhttp, which are partially lagging behind the last 3.X version by quite a bit.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23399" opendate="2021-7-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a performance benchmark for statebackend rescaling</summary>
      <description>We notice that rescaling is not covered in the current state benchmark, so we'd like to introduce a benchmark to test performance of state backend restore durign rescaling in flink-benchmark.The benchmark process is:(1) generate some states,(2) change the parallelism of the operator, and restore from these states generate before.The implementation of this benchmark is based on RocksIncrementalCheckpointRescalingTest, and AverageTime is used to measure the rescaling performance on each subtask.Â And this benchmark does not conflict with `RocksIncrementalCheckpointRescalingBenchmarkTest` inÂ  PR(#14893).Â Compare with `RocksIncrementalCheckpointRescalingBenchmarkTest`, this benchmark supports testing rescaling on different state backends, and has a finer granularity.</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23407" opendate="2021-7-16 00:00:00" fixdate="2021-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mechanism to document enums used for ConfigOptions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.TextElement.java</file>
      <file type="M">docs.layouts.shortcodes.generated.stream.pipeline.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.influxdb.reporter.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.BackendSwitchSpecs.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.TimersSavepointITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SqlClientOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ResultMode.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.rocksdb.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.sql.client.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23408" opendate="2021-7-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wait for a checkpoint completed after finishing a task</summary>
      <description>Before finishing a task we should wait for a checkpoint issued after finish() to commit all pending transactions created from the finish() method.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapperTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.CollectingDataOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointType.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.NoOpResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloater.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.PullingAsyncDataInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InputSelectable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InputSelection.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sort.MultiInputSortingDataInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sort.SortingDataInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.DataInputStatus.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.MultipleInputSelectionHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinishedOnRestoreSourceInput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.sort.CollectionDataInput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.sort.LargeSortingDataInputITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.InputGateWithMetrics.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockIndexedInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionDirectTransferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.EndOfUserRecordsEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="23409" opendate="2021-7-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>CrossITCase fails with "NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20548&amp;view=logs&amp;j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&amp;t=5360d54c-8d94-5d85-304e-a89267eb785a&amp;l=10074Jul 16 09:21:37 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)Jul 16 09:21:37 at akka.actor.Actor$class.aroundReceive(Actor.scala:517)Jul 16 09:21:37 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)Jul 16 09:21:37 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)Jul 16 09:21:37 at akka.actor.ActorCell.invoke(ActorCell.scala:561)Jul 16 09:21:37 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)Jul 16 09:21:37 at akka.dispatch.Mailbox.run(Mailbox.scala:225)Jul 16 09:21:37 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)Jul 16 09:21:37 ... 4 moreJul 16 09:21:37 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeoutJul 16 09:21:37 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)Jul 16 09:21:37 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)Jul 16 09:21:37 ... 31 moreJul 16 09:21:37 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeoutJul 16 09:21:37 at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)Jul 16 09:21:37 ... 24 moreJul 16 09:21:37 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 msJul 16 09:21:37 ... 25 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="23446" opendate="2021-7-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor SQL Client end to end tests</summary>
      <description>Remove useage of the YAML in SQL Client end to end tests</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka.sql.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23447" opendate="2021-7-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lz4-java to 1.8</summary>
      <description>Bump lz4 to the latest version for bug fixes and performance improvements.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-test.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="23460" opendate="2021-7-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a global flag for enabling/disabling final checkpoints</summary>
      <description>We should have a feature toggle for the final checkpoint story.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23462" opendate="2021-7-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the abfs documentation to chinese</summary>
      <description>Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  <bug id="23463" opendate="2021-7-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace &lt;div&gt; tags with ShortCodes</summary>
      <description>In FLINK-22922, we migrate Flink website to hugo. At the moment, most of the div tag in user doc is no long take effect. We need to replace them with the ShortCodes.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.docs.dev.dataset.iterations.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.custom.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.dataset.iterations.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="23476" opendate="2021-7-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot deployments are broken</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20855&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7[ERROR] 'dependencies.dependency.version' for com.typesafe.akka:akka-testkit_2.11:jar must be a valid version but is '${akka.version}'. @ line 1212, column 15</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23484" opendate="2021-7-23 00:00:00" fixdate="2021-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add benchmarks for the ChangelogStateBackend</summary>
      <description>http://codespeed.dak8s.net:8000/(similar to existing RocksDB or Heap)Should test several configurations (varying number of task per TM, barching options, etc.).Besides of adding a benchmark, update (or better re-write) the script for regression detection.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmarkTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmarkBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.state.benchmark.StateBackendBenchmarkUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateDiscardTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="23486" opendate="2021-7-23 00:00:00" fixdate="2021-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics for the Changelog uploader</summary>
      <description>https://docs.google.com/document/d/1k5WkWIYzs3n3GYQC76H9BLGxvN3wuq7qUHJuBPR9YX0/edit?usp=sharingÂ </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFsUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.RetryingExecutor.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageFactory.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorage.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.BatchingStateChangeUploader.java</file>
    </fixedFiles>
  </bug>
  <bug id="23493" opendate="2021-7-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug id="23497" opendate="2021-7-26 00:00:00" fixdate="2021-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table-planner does not compile on scala 2.12</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20961&amp;view=logs&amp;j=b9f58bb2-ed4a-500b-bef9-cc3cf2248e69&amp;t=e6d8efc2-861e-5470-71ae-bbaad6c133d32021-07-26T09:10:26.7655809Z [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-table-planner_2.12 ---2021-07-26T09:10:26.8567767Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/java:-1: info: compiling2021-07-26T09:10:26.8568668Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/scala:-1: info: compiling2021-07-26T09:10:26.8571934Z [INFO] Compiling 903 source files to /__w/1/s/flink-table/flink-table-planner/target/classes at 16272906268562021-07-26T09:10:31.0803565Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:32: error: object parsing is not a member of package util2021-07-26T09:10:31.0804449Z [ERROR] import _root_.scala.util.parsing.combinator.{JavaTokenParsers, PackratParsers}2021-07-26T09:10:31.0804904Z [ERROR] ^2021-07-26T09:10:31.1130420Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:59: error: not found: type JavaTokenParsers2021-07-26T09:10:31.1131245Z [ERROR] object PlannerExpressionParserImpl extends JavaTokenParsers2...</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23512" opendate="2021-7-27 00:00:00" fixdate="2021-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for illegal modifications of JobGraph with partially finished operators</summary>
      <description>Besides the fully finished operators, we also would like to disable inserting new operators before the partially finished operators: If keyed state is used and discarded after the tasks get finished in the first run, then if we received new records target at these keys, the result would be not right. Similarly, for normal operator subtasks, if new records are emitted and they relies on the discarded states, the result would also be confused.Â Thus we would first disable such cases.Â </description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointMetadataTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlan.java</file>
    </fixedFiles>
  </bug>
  <bug id="23531" opendate="2021-7-29 00:00:00" fixdate="2021-8-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow skip all change log for row-time deduplicate mini-batch</summary>
      <description>current we keep all change log for row-time deduplicate mini-batch,we need buffer all record on heap.In some cases, users do not need all change logs; In this way, the memory pressure and the calculation pressure of downstream operators can be reduced</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.RowTimeDeduplicateFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
    </fixedFiles>
  </bug>
  <bug id="23532" opendate="2021-7-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Unify stop-with-savepoint w drain and w/o drain</summary>
      <description>The code paths for stop-with-savepoint --drain and w/o drain are different after FLINK-23408. We should unify the two and in both cases we should wait for the savepoint in afterInvoke and close sources before triggering the last checkpoint.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.BoundedSourceITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.MultipleInputSelectionHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.DataInputStatus.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.StopWithSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestEventSource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.EndOfData.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.PullingAsyncDataInput.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.InputGateWithMetrics.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordOrEventCollectingResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.FinishedOperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockIndexedInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapperTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23534" opendate="2021-7-29 00:00:00" fixdate="2021-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Banned dependencies in flink-statebackend-changelog</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&amp;l=34482[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-statebackend-changelog ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.14-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.14-SNAPSHOTFound Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.apache.flink:flink-scala_2.11:jar:1.14-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-dstl-dfs_2.11:jar:1.14-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23544" opendate="2021-7-29 00:00:00" fixdate="2021-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window TVF Supports session window</summary>
      <description>Window TVF would support SESSION window in the following case: SESSION Window TVF followed by Window Aggregate, in this case SESSION window TVF would be pulled up into WindowAggregate, so Window assigner would happen in WindowAggrgeateÂ Â Â Note, SESSION window TVF only works in limited cases currently, the following user cases is not supported yet: SESSION WINDOW TVF followed by Window JOIN SESSION WINDOW TVF followed by Window RANK**BESIDES, SESSION window Aggregate does not support the following performance improvement yet:Â  Â  Â 1. Split Distinct AggregationÂ  Â  Â 2. Local-global AggregationÂ  Â  Â 3. Mini-batch Aggregate</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.SessionWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlSessionTableFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.window.WindowTableFunctionOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLocalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedWindowAggregateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.logical.WindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="23546" opendate="2021-7-29 00:00:00" fixdate="2021-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stop-cluster.sh produces warning on macOS 11.4</summary>
      <description>Since FLINK-17470, we are stopping daemons with a timeout, to SIGKILL them if they are not gracefully stopping.I noticed that this mechanism causes warnings on macOS:â°robertâ/tmp/flink-1.14-SNAPSHOTâ±ââ» ./bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host MacBook-Pro-2.localdomain.Starting taskexecutor daemon on host MacBook-Pro-2.localdomain.â°robertâ/tmp/flink-1.14-SNAPSHOTâ±ââ» ./bin/stop-cluster.shStopping taskexecutor daemon (pid: 50044) on host MacBook-Pro-2.localdomain.tail: illegal option -- -usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]Stopping standalonesession daemon (pid: 49812) on host MacBook-Pro-2.localdomain.tail: illegal option -- -usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23559" opendate="2021-7-30 00:00:00" fixdate="2021-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Randomize periodic materialisation interval in tests</summary>
      <description>FLINK-21448 adds the capability of test randomization.It's already used for the changelog backend (FLINK-23279).Â FLINK-21357 adds periodic materialization; the default interval is 10m which is likely too high for tests (so materialization isn't triggered).This interval should be randomized/reduced;Â Depends on FLINK-23170.Â </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="23560" opendate="2021-7-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 29.07.2021</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/?ben=remoteFilePartition&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=uncompressedMmapPartition&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=compressedFilePartition&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=tupleKeyBy&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=arrayKeyBy&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=uncompressedFilePartition&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=sortedTwoInput&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=sortedMultiInput&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2(And potentially other benchmarks)</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.ThroughputCalculatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.throughput.ThroughputCalculator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.TimerGauge.java</file>
    </fixedFiles>
  </bug>
  <bug id="23570" opendate="2021-8-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation lists incorrect scala suffixes</summary>
      <description>Some of the maven dependencies in the documentation seem to have some problems and cannot be used directly.Page: DataStreamÂ Connectors -&gt; File Sink/Streaming FIle SinkÂ </description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="23571" opendate="2021-8-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The internal query-start options missed when convert exec graph to transformation</summary>
      <description>The internal query-start configuration options is missed when convert exec graph to transformation, please see:// org.apache.flink.table.planner.delegation.PlannerBase translateToPlan(execGraph: ExecNodeGraph)</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23594" opendate="2021-8-3 00:00:00" fixdate="2021-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test_from_and_to_data_stream_event_time failed</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21337&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug id="23611" opendate="2021-8-4 00:00:00" fixdate="2021-9-4 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21439&amp;view=logs&amp;j=245e1f2e-ba5b-5570-d689-25ae21e5302f&amp;t=e7f339b2-a7c3-57d9-00af-3712d4b15354&amp;l=28959</description>
      <version>1.14.0,1.12.5</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23612" opendate="2021-8-4 00:00:00" fixdate="2021-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SELECT ROUND(CAST(1.2345 AS FLOAT), 1) cannot compile</summary>
      <description>Run this SQLÂ SELECT ROUND(CAST(1.2345 AS FLOAT), 1) and the following exception will be thrown:java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$2' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75) at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:108) at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759) at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699) at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1702) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:833) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1301) at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106) at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest2(CalcITCase.scala:1618) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) ... 74 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ... 76 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ... 79 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 36, Column 23: Assignment conversion not possible from type "double" to type "float" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790) at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2779) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ... 85 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23614" opendate="2021-8-4 00:00:00" fixdate="2021-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The resulting scale of TRUNCATE(DECIMAL, ...) is not correct</summary>
      <description>Run the following SQLSELECT TRUNCATE(123.456, 2), TRUNCATE(123.456, 0), TRUNCATE(123.456, -2), TRUNCATE(CAST(123.456 AS DOUBLE), 2), TRUNCATE(CAST(123.456 AS DOUBLE), 0), TRUNCATE(CAST(123.456 AS DOUBLE), -2)The result is123.450123.000100.000123.45123.0100.0It seems that the resulting scale of TRUNCATE(DECIMAL, ...) is the same as that of the input decimal.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23615" opendate="2021-8-4 00:00:00" fixdate="2021-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of the TRUNCATE SQL function is not correct</summary>
      <description>The documentation of the TRUNCATE SQL function states that "E.g. 42.324.truncate(2) to 42.34", which should be "42.32".Some period in the document also lacks trailing spaces.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="23616" opendate="2021-8-4 00:00:00" fixdate="2021-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to chain the Python DataStream operators as much as possible</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunctionInfo.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonConfigUtilTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.settings.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.ProtoUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.OneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractTwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.functions.python.DataStreamPythonFunctionInfo.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.Constants.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.input.handler.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23644" opendate="2021-8-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve maven warnings for duplicate dependencies/plugins</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23647" opendate="2021-8-5 00:00:00" fixdate="2021-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointStressITCase crashed on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21539&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=4855When testing DFS changelog implementation in FLINK-23279 and enabling it for all tests,UnalignedCheckpointStressITCase crashed with the following exception[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.433 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase[ERROR] runStressTest(org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase) Time elapsed: 17.663 s &lt;&lt;&lt; ERROR!java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit7860347244680665820/435237 d57439f2ceadfedba74dadd6fa/chk-16 at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88) at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104) at java.util.Iterator.forEachRemaining(Iterator.java:115) at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546) at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.discoverRetainedCheckpoint(UnalignedCheckpointStressITCase.java:288) at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runAndTakeExternalCheckpoint(UnalignedCheckpointStressITCase.java:261) at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest(UnalignedCheckpointStressITCase.java:157) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java :384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)Caused by: java.nio.file.NoSuchFileException: /tmp/junit7860347244680665820/435237d57439f2ceadfedba74 dadd6fa/chk-16 at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144) at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) at java.nio.file.Files.readAttributes(Files.java:1737) at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219) at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276) at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372) at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:84)The referred checkpoint 16 was aborted and scheduled for deletion.But the test does not wait for it to complete and proceeds to file listing.I think this problem is also present in UnalignedCheckpointRescaleITCase (FLINK-22197) and probably in CoordinatedSourceRescaleITCase(FLINK-23577).Patch to demonstrate it: https://github.com/rkhachatryan/flink/tree/f23647-demoCorresponding failure</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23652" opendate="2021-8-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-179: Expose Standardized Operator Metrics</summary>
      <description>This ticket is about implementing FLIP-179. It will some metrics out-of-the-box for sources/sinks and supports connector developers to easily implement some standardized metrics.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.util.InterceptingOperatorMetricGroup.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.TestSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.sink.Sink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingSplitEnumeratorContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingReaderContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.lib.NumberSequenceSourceTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReaderContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockRecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestFinishedOnRestoreStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.async.RichAsyncFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ChainingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.RichAsyncFunction.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.TestTaskContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedOperatorsMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.LogicalScopeProvider.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.test.java.org.apache.flink.metrics.util.TestMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroup.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
      <file type="M">flink-runtime.src.test.resources.META-INF.services.org.apache.flink.metrics.reporter.MetricReporterFactory</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestRuntimeContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RuntimeContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.RuntimeUDFContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.CollectionExecutor.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.functions.util.RuntimeUDFContextTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.RichInputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.RichOutputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.FlatMapOperatorCollectionTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.InnerJoinOperatorBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.MapOperatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.OuterJoinOperatorBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.PartitionMapOperatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.GenericDataSinkBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.GenericDataSourceBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.CoGroupOperatorCollectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.InnerJoinOperatorBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.ReduceOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CepRuntimeContext.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CepRuntimeContextTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapWrapperOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointRuntimeContext.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.metric.FlinkMetricContainerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.AbstractIterativeTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.OperatorScopeFormat.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.TaskScopeFormat.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.TaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.DistributedRuntimeUDFContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="23658" opendate="2021-8-6 00:00:00" fixdate="2021-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup JMX reporter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="23659" opendate="2021-8-6 00:00:00" fixdate="2021-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup Prometheus reporter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTaskScopeTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23662" opendate="2021-8-6 00:00:00" fixdate="2021-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Scala code in flink-rpc-akka to Java</summary>
      <description>flink-rpc-akka contains a small amount of Scala code, which we could port to Java.Since Scala is less common in the Flink code base than in the past this would make the code easier to maintain for new people. It would also simplify the build system (no scala plugins), reduce the dependency tree (no scala test) and improve build times.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.scala.org.apache.flink.runtime.rpc.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.scala.akka.actor.RobustActorSystemTest.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.org.apache.flink.runtime.rpc.akka.RemoteAddressExtension.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.org.apache.flink.runtime.rpc.akka.CustomSSLEngineProvider.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.org.apache.flink.runtime.rpc.akka.AkkaUtils.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.akka.dispatch.PriorityThreadsDispatcherPrerequisites.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.akka.dispatch.PriorityThreadsDispatcher.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.akka.dispatch.PriorityThreadFactory.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.scala.akka.actor.RobustActorSystem.scala</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23663" opendate="2021-8-6 00:00:00" fixdate="2021-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce state size in ChangelogNormalize through filter push down</summary>
      <description>ChangelogNormalize is an expensive stateful operation as it stores data for each key. Filters are generally not pushed through a ChangelogNormalize node which means that users have no possibility to at least limit the key space. Pushing filters like a &lt; 10 into a source like upsert-kafka that is emitting +I&amp;#91;key1, a=9&amp;#93; and -D&amp;#91;key1, a=10&amp;#93;, is problematic as the deletion will be filtered and leads to wrong results. But limiting the filter push down to key space should be safe.Furthermore, it seems the current implementation is also wrong as it pushes any kind of filter through ChangelogNormalize but only if the source implements filter push down.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23686" opendate="2021-8-9 00:00:00" fixdate="2021-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaSource metric "commitsSucceeded" should count per-commit instead of per-partition</summary>
      <description>Currently if a successful offset commit includes multiple topic partition (let's say 4), the counter will increase by 4 instead of 1</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetricsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="23698" opendate="2021-8-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass watermarks in finishedOnRestore tasks</summary>
      <description>After merging FLINK-23541 there is a bug on restore finished tasks that we are loosing an information that max watermark has been already emitted. As task is finished, it means no new watermark will be ever emitted, while downstream tasks (for example two/multiple input tasks) would be deadlocked waiting for a watermark from an already finished input.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestFinishedOnRestoreStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23699" opendate="2021-8-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The code comment&amp;#39;s reference is wrong for the function isUsingFixedMemoryPerSlot</summary>
      <description>The code is as follow.Â USE_MANAGED_MEMORY should be FIX_PER_SLOT_MEMORY_SIZE./** Gets whether the state backend is configured to use a fixed amount of memory shared between all RocksDB instances (in all tasks and operators) of a slot. See {@link * RocksDBOptions#USE_MANAGED_MEMORY} for details. */public boolean isUsingFixedMemoryPerSlot() { return fixedMemoryPerSlot != null;}</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23708" opendate="2021-8-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When task is finishedOnRestore Operators shouldn&amp;#39;t be used</summary>
      <description>When task is finishedOnRestore Operators shouldn't be used, invoked or even initialised. Currently at least a couple of methods are being invoked like: endInput() getMetricGroup()</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.FinishedOperatorChain.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskExecutionDecorationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  <bug id="23710" opendate="2021-8-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move sink to org.apache.kafka.conntor.kafka.sink package</summary>
      <description>The FLIP-27 source for kafka is already placed underÂ org.apache.kafka.conntor.kafka.source. We should also relocate the FLIP-143 to keep it consistent and ease the deprecation of the old connector</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.TransactionToAbortCheckerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.TransactionIdFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriterStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaCommittableSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaRecordSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.TransactionsToAbortChecker.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.TransactionalIdFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriterStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriterState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaTransactionLog.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaRecordSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaCommittableSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.KafkaCommittable.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.sink.DefaultKafkaSinkContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23715" opendate="2021-8-11 00:00:00" fixdate="2021-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for reading fields that do not exist in Parquet files</summary>
      <description>In a production environment, it is often encountered that users add fields to the hive table, but do not refresh the data of the historical partition. Therefore, if the new field is not in the historical partition file, an error will be reported when reading the historical partition.General users would expect that if there is no such field, then fill in null and return.The current flink Parquet format does not support this function.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="23722" opendate="2021-8-11 00:00:00" fixdate="2021-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 Tests fail on AZP: Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</summary>
      <description>E2E and integration tests fail withAug 11 09:11:32 Caused by: com.amazonaws.SdkClientException: Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.Aug 11 09:11:32 at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:462)Aug 11 09:11:32 at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)Aug 11 09:11:32 at com.amazonaws.client.builder.AwsSyncClientBuilder.build(AwsSyncClientBuilder.java:46)Aug 11 09:11:32 at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.buildAmazonS3Client(DefaultS3ClientFactory.java:144)Aug 11 09:11:32 at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:96)Aug 11 09:11:32 at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:753)Aug 11 09:11:32 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:446)Aug 11 09:11:32 ... 44 morehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21884&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.java</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="23728" opendate="2021-8-11 00:00:00" fixdate="2021-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>state bootstrapping fails with new state backend factory stack</summary>
      <description>The state processor API should force checkpoint storage to be FileSystemCheckpointStorage, irregardless of what state backend is configured.Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.SnapshotUtils.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapWrapperOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.KeyedStateBootstrapOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.BroadcastStateBootstrapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23742" opendate="2021-8-13 00:00:00" fixdate="2021-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_keyed_co_process test failed in py36 and py37</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22020&amp;view=logs&amp;j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&amp;t=b4612f28-e3b5-5853-8a8b-610ae894217a</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="23750" opendate="2021-8-13 00:00:00" fixdate="2021-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Window Top-N after Windowing TVF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
    </fixedFiles>
  </bug>
  <bug id="23753" opendate="2021-8-13 00:00:00" fixdate="2021-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about Python DataStream API chaining optimization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="23755" opendate="2021-8-13 00:00:00" fixdate="2021-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the default value of table.dynamic-table-options.enabled to true</summary>
      <description>The below SQL will throw an exception:"SELECT * FROM kafka_table /*+ OPTIONS('group.id'='new_id') */".Before, we introduced the feature of dynamic table options, but in order to avoid user abuse, we turned it off by default, so the above SQL will throw exceptions.When modifying the value of table.dynamic-table-options.enabled to true, the SQL can work.After so many versions, we think this feature can be provided to users, and it is also relied on by many users, so we open it by default.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.hint.OptionsHintTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.catalog.CatalogViewITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.hints.md</file>
    </fixedFiles>
  </bug>
  <bug id="23757" opendate="2021-8-13 00:00:00" fixdate="2021-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JSON_EXISTS / JSON_VALUE methods in pyFlink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.expression.completeness.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
    </fixedFiles>
  </bug>
  <bug id="2376" opendate="2015-7-17 00:00:00" fixdate="2015-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testFindConnectableAddress sometimes fails on Windows because of the time limit</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.NetUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2377" opendate="2015-7-17 00:00:00" fixdate="2015-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AbstractTestBase.deleteAllTempFiles sometimes fails on Windows</summary>
      <description>This is probably another file closing issue. (that is, Windows won't delete open files, as opposed to Linux)I have encountered two concrete tests so far where this actually appears: CsvOutputFormatITCase and CollectionSourceTest.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23770" opendate="2021-8-13 00:00:00" fixdate="2021-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-147: Unable to recover after source fully finished</summary>
      <description>When running one of the IT cases from https://github.com/apache/flink/pull/16773Â I see the following failure:Â 10194 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Trying to recover from a global failure.org.apache.flink.util.FlinkRuntimeException: Can not restore vertex Source: Custom Source -&gt; Timestamps/Watermarks(cbc357ccb763df2852fee8c4fc7d55f2) which contain both finished and unfinished operators at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$VerticesFinishedCache.calculateIfFinished(CheckpointCoordinator.java:1651) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$VerticesFinishedCache.lambda$getOrUpdate$0(CheckpointCoordinator.java:1631) ~[classes/:?] at java.util.HashMap.computeIfAbsent(HashMap.java:1127) ~[?:1.8.0_271] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$VerticesFinishedCache.getOrUpdate(CheckpointCoordinator.java:1629) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.validateFinishedOperators(CheckpointCoordinator.java:1674) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1577) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateToSubtasks(CheckpointCoordinator.java:1438) ~[classes/:?] at org.apache.flink.runtime.scheduler.SchedulerBase.restoreState(SchedulerBase.java:398) ~[classes/:?] at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasks(DefaultScheduler.java:317) ~[classes/:?] at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$null$2(DefaultScheduler.java:287) ~[classes/:?] at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) ~[?:1.8.0_271] at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701) ~[?:1.8.0_271] at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_271] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_1bc30f88-029c-4db2-8df5-833082f3d1a5.jar:1.14-SNAPSHOT] at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_271] at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067) [?:1.8.0_271] at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703) [?:1.8.0_271] at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172) [?:1.8.0_271]The graph has several sources, only one of which is fully finished (i.e. all subtasks).All sources have setUidHash set.The latter I think causes the problem:VerticesFinishedCache.checkOperatorFinished uses a hashmap of opertor states, keyed by operator ID. It prefers user-defined ID falling back to a generated one.However, the map seems to be always keyed by generated ID.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.VertexFinishedStateChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="23776" opendate="2021-8-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 14.08.2021 in FLIP-27</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=mapRebalanceMapSink.F27_UNBOUNDED&amp;env=2git ls 7b60a964b1..7f3636f6b47f3636f6b4f [2 days ago] [FLINK-23652][connectors] Adding common source metrics. [Arvid Heise]97c8f72b813 [3 months ago] [FLINK-23652][connectors] Adding common sink metrics. [Arvid Heise]48da20e8f88 [3 months ago] [FLINK-23652][test] Adding InMemoryMetricReporter and using it by default in MiniClusterResource. [Arvid Heise]63ee60859ca [3 months ago] [FLINK-23652][core/metrics] Extract Operator(IO)MetricGroup interfaces and expose them in RuntimeContext [Arvid Heise]5d5e39b614b [2 days ago] [refactor][connectors] Only use MockSplitReader.Builder for instantiation. [Arvid Heise]b927035610c [3 months ago] [refactor][core] Extract common context creation in CollectionExecutor [Arvid Heise]</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.TestingSourceOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceMetricsITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.java</file>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalSourceReaderMetricGroup.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23791" opendate="2021-8-16 00:00:00" fixdate="2021-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable RocksDB log again</summary>
      <description>FLINK-15068 disabled the RocksDB's local LOG due to previous RocksDB cannot limit the local log files.After we upgraded to newer RocksDB version, we can then enable RocksDB log again.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23798" opendate="2021-8-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using reflection to get filter when partition filter is enabled</summary>
      <description>FLINK-20496 introduce partitioned index &amp; filter to Flink. However, RocksDB only support new full format of filter in this feature, and we need to replace previous filter if user enabled. Previous implementation use reflection to get the filter and we could use API to get that after upgrading to newer version.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23808" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bypass operators when advanceToEndOfEventTime for both legacy and new source tasks</summary>
      <description>Currently when end of data processing, the sources would advance to max watermark and emit MAX_WATERMARK. However, currently the output used is the mainOperatorOutput, namely the output of the source operator. If the source operator is chained with other operators and the tasks are finished on restore, there would be problems since the following operators should be skipped, but some operators still have actions when the watermark is advanced, like WatermarkAssignerOperator, which might cause problems.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.FinishedOperatorChain.java</file>
    </fixedFiles>
  </bug>
  <bug id="23809" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Respect the finished flag when extracting operator states due to skip in-flight data</summary>
      <description>Currently when using a flag to skip in-flight data will cause task to loose it's finished on restore state.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FullyFinishedOperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23818" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about tgz files support for python archives</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="23827" opendate="2021-8-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ModifiedMonotonicity inference for some node</summary>
      <description>ModifiedMonotonicity handler do not handle some node properly, such asÂ IntermediateTableScan, Deduplicate and LookupJoin.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23832" opendate="2021-8-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for batch mode in StreamTableEnvironment</summary>
      <description>The DataStream API Integration page needs an update.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="23845" opendate="2021-8-18 00:00:00" fixdate="2021-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown</summary>
      <description>see https://issues.apache.org/jira/browse/FLINK-20691Â .Â Â whatever the problem has always existed, we should avoid other guys met it</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23864" opendate="2021-8-18 00:00:00" fixdate="2021-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The document for Pulsar Source</summary>
      <description>The new Pulsar source has been merged into flink master branch. We need a detailed documentation for how to use it on flink.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23868" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobExecutionResult printed even if suppressSysout is on</summary>
      <description>Environments prints job execution results to stdout by default and provides a flag `suppressSysout` to disable the behavior. This flag is useful when submitting jobs through REST API or other programmatic approaches. However,Â JobExecutionResult is still printed when this flag is on, which looks like a bug to me.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StreamContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="2387" opendate="2015-7-21 00:00:00" fixdate="2015-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test for live accumulators in Streaming</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="23871" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher should handle finishing job exception when recover</summary>
      <description>The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.Â  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again.Â I think we should deal with theÂ  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23877" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Embedded Pulsar broker backend for testing</summary>
      <description>The pulsar connector use TestContainers for all the pulsar related tests. We should have a lightweight embedded pulsar backend for performing all the Unit tests.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarContainerOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarContainerEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarContainerContextFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarContainerContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.ConfigurationDataCustomizer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarJsonUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23884" opendate="2021-8-20 00:00:00" fixdate="2021-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the concurrent problem between triggering savepoint with drain and finish</summary>
      <description>Currently when triggering stop-with-savepoint --drain, we would like to first finish the task and then trigger a savepoint. However, currently the implement does not ensures triggering get successful before the task get finished and close the mailbox, which might cause the savepoint fail.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="23898" opendate="2021-8-20 00:00:00" fixdate="2021-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log levels not mapped correctly to akka log levels</summary>
      <description>WARN should be mapped to WARNING.TRACE should be mapped to DEBUG.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23902" opendate="2021-8-20 00:00:00" fixdate="2021-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="23906" opendate="2021-8-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase akka.ask.timeout for tests using the MiniCluster</summary>
      <description>We have seen over the last couple of weeks/months an increased number of test failures because of TimeoutException that were triggered because the akka.ask.timeout was exceeded. The reason for this was that on our CI infrastructure it can happen that there are pauses of more than 10s (not sure about the exact reason) or our infrastructure simply being slow. In order to harden all tests relying on the MiniCluster I propose to increase the akka.ask.timeout to 5 minutes if nothing else has been configured.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23907" opendate="2021-8-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type Migration: introducing primitive functional interfaces</summary>
      <description>Hey!We are a collaborative group of researchers from JetBrains Research and Oregon State University, and we are testing our data-driven plugin, which is based on the IntelliJ's Type Migration framework and adjusts it using custom structural-replace templates that express the adaptations required to perform the type changes.I want to apply several type changes using it and open the PR, thus introducing primitive functional interfaces in order to prevent unnecessary boxing (like BooleanSupplier instead Supplier&lt;Boolean&gt;, OptionalInt instead of Optional&lt;Integer&gt;, Predicate&lt;T&gt; instead of Function&lt;T, Boolean&gt;, etc.), since it can affect the performance of the code (Effective Java, Items 44, 61).The patch itself is already prepared (because it is done automatically using the plugin), so I guess I will need to open this ticket, receive your approval, and then open the PR?It would help us a lot to evaluate the usefulness of our approach!Thank you in advance!</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliInputView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.SharedResourcesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.SharedResources.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.DataSetMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="23912" opendate="2021-8-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary "Clearing resource requirements of job"</summary>
      <description>The ResourceManager will print the log each time it receives an empty resource declaration. For deduplication, we need to: At JM side, skip decreasing empty resources. At SlotManager side, does not log if it is already empty.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.ResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="23914" opendate="2021-8-23 00:00:00" fixdate="2021-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make connector testing framework more verbose on test failure</summary>
      <description>Currently testing framework doesn't provide enough debugging if test case fails. We need to add more logs on test failure to reveal more information for debugging.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.TestDataMatchersTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.IteratorWithCurrentTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.utils.TestDataMatchers.java</file>
    </fixedFiles>
  </bug>
  <bug id="23923" opendate="2021-8-23 00:00:00" fixdate="2021-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log command when starting python processes</summary>
      <description>To ease debugging we should log the command used to start python processes.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23929" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining optimization doesn&amp;#39;t handle properly for transformations with multiple outputs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23962" opendate="2021-8-25 00:00:00" fixdate="2021-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateKind trait is not propagated properly in changeLog inference for DAG optimizing</summary>
      <description>For sql jobs with multi-sinks, the plan is divided into relNode blocks, changeLog mode should be also inferred among blocks. Currently, updateKind trait is not propagated properly from parent block to child blocks for the following pattern.Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â -&gt; block3 block0 -&gt; block1 -&gt; block4 Â  Â  Â  Â  Â  Â  -&gt; block2Â In the above example, if block3 requires UB and block2,Â block4 do not require UB, block1 only contains Calc node.For Agg in block0, UB should be emitted, but the updateKind for block0 is inferred asÂ ONLY_UPDATE_AFTER.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23965" opendate="2021-8-25 00:00:00" fixdate="2021-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E do not execute locally on MacOS</summary>
      <description>After FLINK-21346, the e2e tests are no longer executing locally on MacOS. The problem seems to be that the e2e configure a log directory that does not exist and this fails starting a Flink cluster.I suggest to change the directory to the old directory FLINK_DIR/log instead of FLINK_DIR/logs.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23969" opendate="2021-8-25 00:00:00" fixdate="2021-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test Pulsar source end 2 end</summary>
      <description>Write a test application using Pulsar Source and execute it in distributed fashion. Check fault-tolerance by crashing and restarting a TM.Ideally, we test different subscription modes and sticky keys in particular.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRange.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23983" opendate="2021-8-26 00:00:00" fixdate="2021-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JVM crash when running RocksDBStateBackend tests</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&amp;view=logs&amp;j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&amp;t=125e07e7-8de0-5c6c-a541-a567415af3ef&amp;l=11131You would need to compare the mvn logs "Running xxx" with "Test run xxx in xxx" to find out the unfinished test.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23995" opendate="2021-8-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect: `insert overwrite table partition if not exists` will throw exception when tablename is like &amp;#39;database.table&amp;#39;</summary>
      <description>when run such hive sqlÂ insert overwrite table default.dest2 partition (p1=1,p2='static') if not exists select x from srcÂ it will throw exceptionCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found default</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23999" opendate="2021-8-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support evaluating individual window table-valued function in planner</summary>
      <description>Currently, window table-valued function has to be used with other window operation, such as window aggregate, window topN and window join.Â In the ticket, we aim to support evaluating individual window table-valued function in planner.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowDeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.StreamOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.WindowTableFunctionUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.SimplifyWindowTableFunctionRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecWindowTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="24019" opendate="2021-8-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separately package Scala-reliant modules</summary>
      <description>Bundle all Scala-reliant modules (flink-scala, flink-streaming-scala, flink-scala-shell) into a separate jar, containing Scala-exclusive dependencies (e.g., Scala itself, Scala extension of Chill). This jar will be added to lib/ by default, but can be removed by users to get a Scala-free experience.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scala</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24020" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate HTTP requests before custom netty handers are getting the data</summary>
      <description>Custom netty handlers can do authentication (amongst other possibilities).This requires that the handlers are getting the whole HttpRequest content and not just partial data.At the moment it's not implemented this way which ends-up in flaky behaviour.Namely sometimes for example History server responds properly (when the request fits into one netty chunk) but sometimes authentication fails (when the request split into multiple netty chunks).</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24021" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential job unrecoverable due to Network failure</summary>
      <description>Now we use zk to do leader election andÂ retrieval for HA. And we register a fatalError handler in leaderElectionService and leaderRetrievalService to let jobManager or taskManager process exit at the time of some unexpected error.But we don't do this at the time of curatorFrameworkClient#start in ZookeeperUtils. This may lead to some unexpected error like :Â  ZookeeperUtils start curator client, but failed by network loss, this will not throw exception now, because we do not register an error handler. The network recover when master begin do leader election, so this will success The leaderRetrieval begin to work by get_data, but this will not be executed, because the curator client start failed in phase 1.Â So I think we should register a error handler in phase1 , so that we can fail fast.Â Â </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="24036" opendate="2021-8-28 00:00:00" fixdate="2021-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24038" opendate="2021-8-28 00:00:00" fixdate="2021-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DispatcherResourceManagerComponent fails to deregister application if no leading ResourceManager</summary>
      <description>With FLINK-21667 we introduced a change that can cause the DispatcherResourceManagerComponent to fail when trying to stop the application. The problem is that the DispatcherResourceManagerComponent needs a leading ResourceManager to successfully execute the stop/deregister application call. If this is not the case, then it will fail fatally. In the case of multiple standby JobManager processes it can happen that the leading ResourceManager runs somewhere else.I do see two possible solutions:1. Run the leader election process for the whole JobManager process2. Move the registration/deregistration of the application out of the ResourceManager so that it can be executed w/o a leader</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesCheckpointRecoveryFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionEvent.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="24049" opendate="2021-8-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description></description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug id="24051" opendate="2021-8-30 00:00:00" fixdate="2021-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make consumer.group-id optional for KafkaSource</summary>
      <description>For most of the users it is not necessary to generate a group-id and the source itself can provide a meaningful group-id during startup.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="24065" opendate="2021-8-31 00:00:00" fixdate="2021-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade the TwoPhaseCommitSink to support empty transaction after finished</summary>
      <description>In https://issues.apache.org/jira/browse/FLINK-23473 for the TwoPhaseCommitSink, we would not create new transactions after finished to avoid we have transactions left after job finished. However, since with the current implementation of the TwoPhaseCommitSink, we would have to write the transactions into the state for each checkpoint, and the state does not support null transaction now, thus there would be NullPointerException in this case.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkStateSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="24083" opendate="2021-8-31 00:00:00" fixdate="2021-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The result isn&amp;#39;t as expected when the result type is generator of string for Python UDTF</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
    </fixedFiles>
  </bug>
  <bug id="24085" opendate="2021-8-31 00:00:00" fixdate="2021-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The `versioned table` and `Timezone` pages missed the first class subject</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.versioned.tables.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.timezone.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.timezone.md</file>
    </fixedFiles>
  </bug>
  <bug id="24097" opendate="2021-9-1 00:00:00" fixdate="2021-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the streaming check in StreamTableEnvironment in PyFlink</summary>
      <description>Since it has supported to DataStream batch mode in StreamTableEnvironment in FLINK-20897, it should also work in PyFlink. Currently there are a few checks in the Python code and we should remove them.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="24099" opendate="2021-9-1 00:00:00" fixdate="2021-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation links to point to nightlies.apache.org</summary>
      <description>The documentation is being migrated to nightlies.apache.org . We need to update all links in the documentation accordingly.We will first update things on Flink side and see how it goes. Once everything works, we will migrate flink-web as well.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.create.snapshot.branch.sh</file>
      <file type="M">README.md</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help-command.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.AbstractBroadcastStateTransformation.java</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.examples.table.word.count.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.tumble.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.sliding.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.session.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.over.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.process.json.data.with.udf.py</file>
      <file type="M">flink-python.pyflink.examples.table.process.json.data.py</file>
      <file type="M">flink-python.pyflink.examples.table.pandas.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.examples.table.multi.sink.py</file>
      <file type="M">flink-python.pyflink.examples.table.mixing.use.of.datastream.and.table.py</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.relational.EmptyFieldsCountAccumulator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DynamicPropertiesUtil.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.layouts.partials.docs.inject.content-before.html</file>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.release-notes.flink-1.9.md</file>
      <file type="M">docs.content.release-notes.flink-1.8.md</file>
      <file type="M">docs.content.release-notes.flink-1.7.md</file>
      <file type="M">docs.content.release-notes.flink-1.13.md</file>
      <file type="M">docs.content.release-notes.flink-1.11.md</file>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh..index.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.9.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.8.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.7.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.13.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.11.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.config.toml</file>
    </fixedFiles>
  </bug>
  <bug id="24120" opendate="2021-9-2 00:00:00" fixdate="2021-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document MALLOC_ARENA_MAX as workaround for glibc memory leak</summary>
      <description>My task will do a savepoint every hour, so every hour will do a savepoint. From the memory monitoring, it can be seen that the memory of each hour will soar up, although the memory will drop a little later, but from every hour From the point of view of the memory peak on the whole point, the memory continues to rise little by little, and eventually rises to the limited memory, which will lead to being killed by k8SÂ Â </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
    </fixedFiles>
  </bug>
  <bug id="24126" opendate="2021-9-2 00:00:00" fixdate="2021-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use increment of bytes-consumed-total for updating numBytesIn in KafkaSource</summary>
      <description></description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="24130" opendate="2021-9-2 00:00:00" fixdate="2021-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RowDataSerializerTest fails on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23376&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=30176Sep 02 10:12:18 [ERROR] testSerializedCopyAsSequence Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE!Sep 02 10:12:18 java.lang.AssertionError: Exception in test: Row arity of input element does not match serializers.Sep 02 10:12:18 at org.junit.Assert.fail(Assert.java:89)Sep 02 10:12:18 at org.apache.flink.api.common.typeutils.SerializerTestBase.testSerializedCopyAsSequence(SerializerTestBase.java:429)Sep 02 10:12:18 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Sep 02 10:12:18 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Sep 02 10:12:18 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Sep 02 10:12:18 at java.lang.reflect.Method.invoke(Method.java:498)Sep 02 10:12:18 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Sep 02 10:12:18 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Sep 02 10:12:18 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Sep 02 10:12:18 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Sep 02 10:12:18 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Sep 02 10:12:18 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Sep 02 10:12:18 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Sep 02 10:12:18 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Sep 02 10:12:18 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Sep 02 10:12:18 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Sep 02 10:12:18 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Sep 02 10:12:18 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Sep 02 10:12:18 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Sep 02 10:12:18 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Sep 02 10:12:18 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Sep 02 10:12:18 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)Sep 02 10:12:18 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)Sep 02 10:12:18 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)Sep 02 10:12:18 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)Sep 02 10:12:18 at java.util.Iterator.forEachRemaining(Iterator.java:116)Sep 02 10:12:18 at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)Sep 02 10:12:18 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)Sep 02 10:12:18 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)Sep 02 10:12:18 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)Sep 02 10:12:18 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)Sep 02 10:12:18 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:116)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:199)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)Sep 02 10:12:18 at org.apache.flink.api.common.typeutils.SerializerTestBase$SerializerRunner.run(SerializerTestBase.java:580)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sink.SinkUpsertMaterializerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24138" opendate="2021-9-3 00:00:00" fixdate="2021-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automated architectural tests</summary>
      <description>See ML thread:Â https://lists.apache.org/thread.html/r35b679f0b0d83be8a4912dcd2155e28b316f476547ae5dab601bda65%40%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24151" opendate="2021-9-3 00:00:00" fixdate="2021-9-3 01:00:00" resolution="Done">
    <buginformation>
      <summary>KafkaSink fails with setMaxConcurrentCheckpoints being enabled</summary>
      <description>We experienced a RuntimeException in a test run for FLINK-23850 :java.lang.RuntimeException: Failed to send data to Kafka: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception. at org.apache.flink.connector.kafka.sink.KafkaWriter.checkErroneous(KafkaWriter.java:263) ~[flink-sql-connector-kafka_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.connector.kafka.sink.KafkaWriter.write(KafkaWriter.java:178) ~[flink-sql-connector-kafka_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:161) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at StreamExecCalc$6.processElement(Unknown Source) ~[?:?] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:196) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:36) ~[flink-sql-connector-kafka_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:27) ~[flink-sql-connector-kafka_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:141) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:341) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:490) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:789) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:741) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.Test job executed: Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, true); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config); env.setRuntimeMode(RuntimeExecutionMode.STREAMING); env.setRestartStrategy(RestartStrategies.fixedDelayRestart(20, 2000)); env.enableCheckpointing(10000, CheckpointingMode.EXACTLY_ONCE); env.getCheckpointConfig().setMaxConcurrentCheckpoints(2); env.setParallelism(6); final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); tableEnv.createTable("T1", TableDescriptor.forConnector("kafka") .schema(Schema.newBuilder() .column("pk", DataTypes.STRING().notNull()) .column("x", DataTypes.STRING().notNull()) .build()) .option("topic", "flink-23850-in1") .option("properties.bootstrap.servers", FLINK23850Utils.BOOTSTRAP_SERVERS) .option("value.format", "csv") .option("scan.startup.mode", "earliest-offset") .build()); final Table resultTable = tableEnv.sqlQuery( "SELECT " + "T1.pk, " + "'asd', " + "'foo', " + "'bar' " + "FROM T1"); tableEnv.createTable("T4", TableDescriptor.forConnector("kafka") .schema(Schema.newBuilder() .column("pk", DataTypes.STRING().notNull()) .column("some_calculated_value", DataTypes.STRING()) .column("pk1", DataTypes.STRING()) .column("pk2", DataTypes.STRING()) .build()) .option("topic", "flink-23850-out") .option("properties.bootstrap.servers", FLINK23850Utils.BOOTSTRAP_SERVERS) .option("value.format", "csv") .option("sink.delivery-guarantee", "exactly-once") .option("sink.transactional-id-prefix", "flink-23850") .option("scan.startup.mode", "earliest-offset") .build()); resultTable.executeInsert("T4");</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24155" opendate="2021-9-3 00:00:00" fixdate="2021-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="24161" opendate="2021-9-6 00:00:00" fixdate="2021-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not stop the job with savepoint while a task is finishing</summary>
      <description>When stop the job with savepoint, if there is a task is finishing, the action will be timeout.Testing job: https://github.com/KarmaGYZ/flink/blob/test-147/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/wordcount/WordCount.javaFlink conf:state.savepoints.dir: file:///tmp/flink-savepointsstate.backend: rocksdbstate.backend.incremental: truestate.checkpoints.dir: file:///tmp/flink-ckp/execution.checkpointing.aligned-checkpoint-timeout: 30 sexecution.checkpointing.interval: 5 staskmanager.numberOfTaskSlots: 2execution.checkpointing.checkpoints-after-tasks-finish.enabled: trueHow to reproduce:bin/flink run -d -p 4 examples/streaming/WordCount.jar# while one task is finishingbin/flink stop $JOB_IDClient log:------------------------------------------------------------ The program finished with the following exception:org.apache.flink.util.FlinkException: Could not stop with a savepoint job "e139a2eba7f8dc0b07fab65e84421ee4". at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:581) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:1002) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:569) at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1069) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132) at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:579) ... 6 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="24167" opendate="2021-9-6 00:00:00" fixdate="2021-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split HeartbeatTarget to HeartbeatReceiver and HeartbeatSender</summary>
      <description>Currently, HeartbeatTarget is responsible for the receiveHeartbeat and requestHeartbeat. But many components just need one of them and let the not needed interface implements with unsupportedOperationFuture. This makes the implementation not so clear. Can we split the HeartbeatTarget to HeartbeatReceiver and HeartbeatSender.The HeartbeatReceiver only cares about the logic when receive a heartbeat.The HeartbeatSender only cares about the logic of send heartbeat.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="24168" opendate="2021-9-6 00:00:00" fixdate="2021-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime type is not correct for windowTableFunction or OverAggregate follows after Match because the output type does not updated after input rowtime attribute changed from rowtime to rowtime_ltz</summary>
      <description>Rowtime type is not correct for windowTableFunction or OverAggregate on Match because the output type does not updated after input rowtime attribute changed from rowtime to rowtime_ltz in `RelTimeIndicator`.The bug could be reproduced by the following two cases:@Testdef testWindowTVFOnMatchRecognizeOnRowtimeLTZ(): Unit = { val sqlQuery = s""" |SELECT | * |FROM Ticker |MATCH_RECOGNIZE ( | PARTITION BY symbol | ORDER BY ts_ltz | MEASURES | A.price as price, | A.tax as tax, | MATCH_ROWTIME() as matchRowtime | ONE ROW PER MATCH | PATTERN (A) | DEFINE | A AS A.price &gt; 0 |) AS T |""".stripMargin val table = util.tableEnv.sqlQuery(sqlQuery) util.tableEnv.registerTable("T", table) val sqlQuery1 = s""" |SELECT * |FROM TABLE(TUMBLE(TABLE T, DESCRIPTOR(matchRowtime), INTERVAL '3' second)) |""".stripMargin util.verifyRelPlanWithType(sqlQuery1)}@Testdef testOverWindowOnMatchRecognizeOnRowtimeLTZ(): Unit = { val sqlQuery = s""" |SELECT | * |FROM Ticker |MATCH_RECOGNIZE ( | PARTITION BY symbol | ORDER BY ts_ltz | MEASURES | A.price as price, | A.tax as tax, | MATCH_ROWTIME() as matchRowtime | ONE ROW PER MATCH | PATTERN (A) | DEFINE | A AS A.price &gt; 0 |) AS T |""".stripMargin val table = util.tableEnv.sqlQuery(sqlQuery) util.tableEnv.registerTable("T", table) val sqlQuery1 = """ |SELECT | symbol, | price, | tax, | matchRowtime, | SUM(price) OVER ( | PARTITION BY symbol ORDER BY matchRowtime RANGE UNBOUNDED PRECEDING) as price_sum |FROM T """.stripMargin util.verifyRelPlanWithType(sqlQuery1)}Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.MatchUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.java</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.match.recognize.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="24170" opendate="2021-9-6 00:00:00" fixdate="2021-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building a fresh clone with latest Maven fails</summary>
      <description>Building with the latest Maven fails during the highest-dir goal:Â {{Cannot find a single highest directory for this project set. First two candidates directories don't share a common root. }}Â As suggested on this SO answer https://stackoverflow.com/questions/3084629/finding-the-root-directory-of-a-multi-module-maven-reactor-project, the directory-of goal works better</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24172" opendate="2021-9-6 00:00:00" fixdate="2021-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API join documentation for Java has missing end quote after table name</summary>
      <description>The table api join documentation has missing ending quote after table name:Â Table left = tableEnv.from("MyTable).select($("a"), $("b"), $("c"));Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="24186" opendate="2021-9-7 00:00:00" fixdate="2021-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable single rowtime column check for collect/print</summary>
      <description>As seen inÂ FLINK-23751, the single rowtime column check can occur also during collecting and printing which is not important there as watermarks as not used.The exception is also misleading as it references a DataStream:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.TableException: Found more than one rowtime field: [bidtime, window_time] in the query when insert into 'default_catalog.default_database.Unregistered_Collect_Sink_8'.Please select the rowtime field that should be used as event-time timestamp for the DataStream by casting all other fields to TIMESTAMP.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.CollectDynamicSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24197" opendate="2021-9-7 00:00:00" fixdate="2021-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job submission can fail with: "RestClientException: [File upload failed.]"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23672&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=070ff179-953e-5bda-71fa-d6599415701c&amp;l=11040Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'StreamingFileSinkProgram'. at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2056) at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:137) at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1917) at FileSinkProgram.main(FileSinkProgram.java:105) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ... 8 moreCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$11(RestClusterClient.java:433) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:373) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.rest.util.RestClientException: [File upload failed.] at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ... 4 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24199" opendate="2021-9-7 00:00:00" fixdate="2021-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose StreamExecutionEnvironment#configure in Python API</summary>
      <description>There are certain parameters that can be configured only through the underlying configuration of StreamExecutionEnvironment e.g. (execution.checkpointing.checkpoints-after-tasks-finish.enabled).We should be able to set those in the Python API.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="24212" opendate="2021-9-8 00:00:00" fixdate="2021-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>kerberos krb5.conf file is mounted as empty directory, not the expected file</summary>
      <description>From FLINK-18971ï¼we can mount kerberos krb5 conf file to pod with path /etc/krb5.confï¼however if the krb5 conf file is not named krb5.conf (e.g named mykrb5.conf)ï¼the mount path /etc/krb5.conf in pod will be an empty directory, not a file that we expect.root@mykrb5-conf-test-6dd5c76f87-vfwh5:/# ls /etc/krb5.conf/ -latotal 8drwxrwxrwx 2 root root 4096 Sep 8 10:42 .drwxr-xr-x 1 root root 4096 Sep 8 10:42 ..Â Â The reason is that, the codeÂ  in KerberosMountDecrator#decroateFlinkPod, we create the deployment like this:Â ... volumeMounts: - mountPath: /etc/krb5.conf name: my-krb5conf-volume subPath: krb5.conf ... volumes: - configMap: defaultMode: 420 items: - key: mykrb5.conf path: mykrb5.conf name: my-krb5conf name: my-krb5conf-volumepath value should be set to const value "krb5.conf", not the file name that user provide (path: mykrb5.conf).Â we can use the yaml description file attachment to reproduce the problem.Â Â mykrb5conf.yamlÂ </description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24213" opendate="2021-9-8 00:00:00" fixdate="2021-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock in QueryableState Client</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&amp;l=15476 Found one Java-level deadlock:Sep 08 11:12:50 =============================Sep 08 11:12:50 "Flink Test Client Event Loop Thread 0":Sep 08 11:12:50 waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),Sep 08 11:12:50 which is held by "main"Sep 08 11:12:50 "main":Sep 08 11:12:50 waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),Sep 08 11:12:50 which is held by "Flink Test Client Event Loop Thread 0"</description>
      <version>1.14.0,1.12.6,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="2422" opendate="2015-7-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web client is showing a blank page if "Meta refresh" is disabled in browser</summary>
      <description>A user reported via the Flink IRC channel that Firefox was showing only a blank page instead of the web client.We should add a link to that page as well, so that users can click it if the redirect doesn't work.Workaround: browse to launch.html directly.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.resources.web-docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="24232" opendate="2021-9-9 00:00:00" fixdate="2021-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archiving of suspended jobs prevents breaks subsequent archive attempts</summary>
      <description>To archive a job we write a file that uses the job ID as the name. Since suspended jobs are handled like other terminal jobs they are also being archived.When that job then later resumes any attempt to archive the job on termination will fail because an archive already exists.The simplest option is to add a suffix if an archive already exists, like "_1".</description>
      <version>1.14.0,1.13.1,1.12.5</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="24235" opendate="2021-9-9 00:00:00" fixdate="2021-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only support reporter factories for instantiation</summary>
      <description>Metric reporters can currently be instantiated in one of 2 ways:a) the reporter class is loaded via reflectionb) the reporter factory is loaded via reflection/ServiceLoader (aka, plugins)All reporters provided by Flink use the factory approach, and it is preferable because it supports plugins. The plugin approach also has been available 1.11, and I think it's fair to remove the old approach by now.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.JobManagerMetricsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.ReporterSetup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.InterceptInstantiationViaReflection.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.InstantiateViaFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="24246" opendate="2021-9-10 00:00:00" fixdate="2021-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump PulsarClient to 2.9.1 with better transaction management</summary>
      <description>Pulsar 2.9.1 has been released, the hack for getting TxnID from Pulsar Transaction instance could be removed after bumping flink-connector-pulsar's pulsar-client-all to 2.9.1.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarTransactionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24266" opendate="2021-9-13 00:00:00" fixdate="2021-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log improvement for aborting checkpoint due to tasks are finishing</summary>
      <description>When checkpoints are aborted due to tasks are finishing when triggering, 1. There is not log in the JM side, which might cause confusion.2. When the mail is rejected, the exception description for CHECKPOINT_DECLINED_TASK_CLOSING: "Checkpoint was declined (task's operators partially closed)", does not fully match the implementation now.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24274" opendate="2021-9-13 00:00:00" fixdate="2021-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong parameter order in documentation of State Processor API</summary>
      <description>Wrong order of parameters path and stateBackend in example code ofÂ State Processor Api # modifying-savepointsÂ Â </description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="24275" opendate="2021-9-13 00:00:00" fixdate="2021-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow idempotent job cancellation</summary>
      <description>As a user of Flink, I want to be able to cancel a job from an external system in a fault-tolerant way without guessing if the job has already been cancelled.Â Currently, the cancel endpoint (PATCH /jobs/:jobid?mode=cancel) will return a 404 if the job is already cancelled. This makes it hard to detect if the job truly doesn't exist, or if it is already in the desired state.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationMessageParameters.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestRestHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.resourcemanager.AbstractResourceManagerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractAggregatingMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobIdsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskAttemptHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractJobVertexHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.HandlerRequest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.StopWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.AbstractJobManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24276" opendate="2021-9-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary info in Loopback mode</summary>
      <description>If the job runs in loopback mode, it will print unnecessary info `apache_beam.typehints.native_type_compatibility - INFO - Using Any for unsupported type: typing.SequenceT` in the console. We need to remove this confusing info.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.worker.pool.service.py</file>
    </fixedFiles>
  </bug>
  <bug id="24281" opendate="2021-9-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate all existing tests to new Kafka Sink</summary>
      <description>The FlinkKafkaProducer is deprecated since 1.14 but a lot of existing tests are still using.We should replace it with the KafkaSink because it completely subsumes it.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilder.java</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.java.org.apache.flink.schema.registry.test.TestAvroConsumerConfluent.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24282" opendate="2021-9-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaRecordSerializationSchema TopicSelector is not serializable</summary>
      <description>To dynamically calculate the outgoing topic we allow passing a lambda. Unfortunately, it is currently not marked as serializable hence theÂ following code fails in during closure cleaning when used within a job.Â KafkaRecordSerializationSchema.builder() .setTopic(topic) .setValueSerializationSchema(serSchema) .setPartitioner(partitioner) .build())Â </description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="24283" opendate="2021-9-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector won&amp;#39;t use given hash ranges in Key_Shared mode</summary>
      <description>Pulsar broker will keep the old consumer select if the consumer has been closed. This would lead to the sticky key range won't take effect.We should use a sticky hash range when seeking the initial position in the source enumerator.https://github.com/apache/pulsar/pull/12035</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24292" opendate="2021-9-15 00:00:00" fixdate="2021-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flink&amp;#39;s Kafka examples to use KafkaSink</summary>
      <description></description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.KafkaEventsGeneratorJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test.src.main.java.org.apache.flink.streaming.kafka.test.KafkaExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="24300" opendate="2021-9-15 00:00:00" fixdate="2021-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleInputOperator is running much more slowly in TPCDS</summary>
      <description>When we are running TPCDS with release 1.14 we find that the job with MultipleInputOperatorÂ is running much more slowly than before. With a binary search among the commits, we find that the issue may be introduced by FLINK-23408.Â At the commitÂ 64570e4c56955713ca599fd1d7ae7be891a314c6, the jobÂ in TPCDS runs normally, as the image below illustrates:At the commit e3010c16947ed8da2ecb7d89a3aa08dacecc524a, the job q2.sql gets stuck for a pretty long time (longer than half an hour), as the image below illustrates:The detail of the job is illustrated below:The job uses a MultipleInputOperator with one normal input and two chained FileSource. It has finished reading the normal input and start to read the chained source. Each chained source has one source data fetcher.We capture the jstack of the stuck tasks and attach the file below. From the jstack.txtÂ we can see the main thread is blocked on waiting for the lock, and the lock is held by a source data fetcher. The source data fetcher is still running but the stack keeps on CompletableFuture.cleanStack.This issue happens in a batch job. However, from where it get blocked, it seems also affects the streaming jobs.For the reference, the code of TPCDS we are running is located at https://github.com/ververica/flink-sql-benchmark/tree/dev.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24305" opendate="2021-9-16 00:00:00" fixdate="2021-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="24308" opendate="2021-9-16 00:00:00" fixdate="2021-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate KafkaSink docs to chinese</summary>
      <description>With https://issues.apache.org/jira/browse/FLINK-23664 only the English documentation was updated. We also have to update the Chinese docs.</description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="24310" opendate="2021-9-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink onÂ this pageÂ has a bug:if (bufferedElements.size() == threshold) {It should be &gt;=Â instead ofÂ ==Â , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold.Â </description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="24315" opendate="2021-9-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot rebuild watcher thread while the K8S API server is unavailable</summary>
      <description>In native k8s integration, Flink will try to rebuild the watcher thread if the API server is temporarily unavailable. However, if the jitter is longer than the web socket timeout, the rebuilding of the watcher will timeout and Flink cannot handle the pod event correctly.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="24317" opendate="2021-9-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description></description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug id="24318" opendate="2021-9-17 00:00:00" fixdate="2021-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="24331" opendate="2021-9-18 00:00:00" fixdate="2021-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PartiallyFinishedSourcesITCase fails with "No downstream received 0 from xxx;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24287&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798&amp;l=10945Sep 18 02:21:08 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 224.44 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCaseSep 18 02:21:08 [ERROR] test[complex graph SINGLE_SUBTASK, failover: true, strategy: region] Time elapsed: 28.807 s &lt;&lt;&lt; FAILURE!Sep 18 02:21:08 java.lang.AssertionError: No downstream received 0 from 00000000000000000000000000000003[0]; received: {0=OperatorFinished 00000000000000000000000000000007/0, 1=OperatorFinished 00000000000000000000000000000007/1, 2=OperatorFinished 00000000000000000000000000000007/2, 3=OperatorFinished 00000000000000000000000000000007/3}Sep 18 02:21:08 at org.junit.Assert.fail(Assert.java:89)Sep 18 02:21:08 at org.junit.Assert.assertTrue(Assert.java:42)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.lambda$checkDataFlow$1(TestJobDataFlowValidator.java:96)Sep 18 02:21:08 at java.util.HashMap.forEach(HashMap.java:1289)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:94)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:62)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:139)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestEventSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="24334" opendate="2021-9-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration kubernetes.flink.log.dir not working</summary>
      <description>After FLINK-21128, kubernetes.flink.log.dir could not take effect.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24336" opendate="2021-9-18 00:00:00" fixdate="2021-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink TableEnvironment executes the SQL randomly MalformedURLException with the configuration for &amp;#39;pipeline.classpaths&amp;#39;</summary>
      <description>When I run flink client to submit a python based workflow, I got the MalformedURLException like this:https://gist.github.com/is/faabafc7f8750f3f3161fbb6517ed6ffAfter some debug work, I found the problem is related with TableEvneriontment.execute_sql. The root cause is TableEvenriontment._add_jars_to_j_env_config in pyflink/table/TableEnverionment.py.```if j_configuration.containsKey(config_key): for url in j_configuration.getString(config_key, "").split(";"): jar_urls_set.add(url)```In our case, pipeline.classpaths was set by empty list valuefrom FromProgramOption, so the upper code block willintroduce a empty string ("") into pipeline.classpaths, for example"a.jar;b.jar;;c.jar", and it will cause the according exception.Another problem, the order of string set in python is notdeterminate, so ";".join(jar_urls_set) does NOT keep theclasspaths order. The list is more suiteable in this case.</description>
      <version>1.14.0,1.13.2,1.14.1</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="24342" opendate="2021-9-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filesystem sink does not escape right bracket in partition name</summary>
      <description>How to reproduce the problemIn the following code snippet filesystem sink creates a partition named "{date}" and writes value "1" to file.create table sink ( val int, part string) partitioned by (part) with ( 'connector' = 'filesystem', 'path' = '/tmp/sink', 'format' = 'csv');insert into sink values (1, '{date}');Expected behaviorEscaped "{" and "}" in partition name$ ls /tmp/sink/part=%7Bdate%7DActual behaviorEscaped only "{" in partition name$ ls /tmp/sink/part=%7Bdate}</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PartitionPathUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PartitionPathUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24355" opendate="2021-9-22 00:00:00" fixdate="2021-10-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Expose the flag for enabling checkpoints after tasks finish in the Web UI</summary>
      <description>We should present the value of execution.checkpointing.checkpoints-after-tasks-finish.enabled in the Web UI.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="24366" opendate="2021-9-23 00:00:00" fixdate="2021-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary/misleading error message about failing restores when tasks are already canceled.</summary>
      <description>The following line is logged in all cases where the restore operation fails. The check whether the task is canceled comes only after that line.The fix would be to move the log line to after the check.Exception while restoring my-stateful-task from alternative (1/1), will retry while more alternatives are available.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="24376" opendate="2021-9-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Operator name in OperatorCoordinator should not use chained name</summary>
      <description>Currently the operator name passed to CoordinatedOperatorFactory#getCoordinatorProvider is a chained operator name (e.g. Source -&gt; Map) instead of the name of coordinating operator, which might be misleading.Â </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.7,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24380" opendate="2021-9-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink should handle the state transition of the pod from Pending to Failed</summary>
      <description>In K8s, there is five phases in pod's lifecycle: Pending, Running, Secceeded, Failed and Unknown. Currently, Flink does not handle the state transition of the pod from Pending to Failed. If a pod failed from Pending by `OutOfCPU` or `OutOfMem`, it will never be released and Flink keep waiting for it.To fix this issue, Flink should terminate the pod in Failed phase proactively.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  <bug id="24382" opendate="2021-9-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RecordsOut metric for sinks is inaccurate</summary>
      <description>Currently, the metric is computed on the operator level and it is assumed that every record flowing into the sink also generates one outgoing record.This is often not reasonable because the sinks can transform incoming records into multiple outgoing records, thus the metric should be implemented by the sink implementors and not be reasoned by the framework.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.FileSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24387" opendate="2021-9-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a JSON_STRING</summary>
      <description>We should consider adding a JSON_STRING function which can simply convert some (supported) type directly into a JSON representation, e.g. the equivalent of doingJSON_QUERY(JSON_ARRAY(x), '$.[0]')</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlJsonUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonObjectCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonArrayCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificInputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24389" opendate="2021-9-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDescription() in `CatalogTableImpl` should check null</summary>
      <description>```@Overridepublic Optional&lt;String&gt; getDescription() {Â  Â  return Optional.of(getComment());}```If the table comment is not set, then `getDescription` will throw NullPointerExceptionÂ https://github.com/apache/flink/blame/5b9e7882207357120717966d8bf7efd53c53ede5/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogTableImpl.java#L69Â </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogTableImpTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="24393" opendate="2021-9-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for all currently supported cast combinations</summary>
      <description>Currently there are only tests to check feasibility of casting between various types but not actual tests verifying the conversion. It would be nice to have them first before addressing the various bugs and missing functionality of CAST.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24394" opendate="2021-9-28 00:00:00" fixdate="2021-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor scalar function testing infrastructure to allow testing multiple columns</summary>
      <description>With the casting tests, to verify already supported conversion coming up soon, executing a table pipeline for each one of them is expensive. Allow the execution of multiple test cases as multiple columns of a table in a single execution to speed up the process.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.StringFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MiscFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CoalesceFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24407" opendate="2021-9-30 00:00:00" fixdate="2021-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector chinese document link to Pulsar document location incorrectly.</summary>
      <description>Pulsar connector chinese document link to Pulsar document location incorrectly.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="24408" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>org.codehaus.janino.InternalCompilerException: Compiling "StreamExecValues$200": Code of method "nextRecord(Ljava/lang/Object;)Ljava/lang/Object;" of class "StreamExecValues$200" grows beyond 64 KB</summary>
      <description>I build a large SQL in application, and meet the issue "Code of method methodÂ  grows beyond 64 KB". This bug should be fixed refer to #FLINK-22903.Â Â java.lang.RuntimeException:Â CouldÂ notÂ instantiateÂ generatedÂ classÂ 'StreamExecValues$200'Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:60)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:35)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:84)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;CausedÂ by:Â org.apache.flink.util.FlinkRuntimeException:Â org.apache.flink.api.common.InvalidProgramException:Â TableÂ programÂ cannotÂ beÂ compiled.Â ThisÂ isÂ aÂ bug.Â PleaseÂ fileÂ anÂ issue.Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â ...Â 6Â moreCausedÂ by:Â org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException:Â org.apache.flink.api.common.InvalidProgramException:Â TableÂ programÂ cannotÂ beÂ compiled.Â ThisÂ isÂ aÂ bug.Â PleaseÂ fileÂ anÂ issue.Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â ...Â 6Â moreCausedÂ by:Â org.apache.flink.api.common.InvalidProgramException:Â TableÂ programÂ cannotÂ beÂ compiled.Â ThisÂ isÂ aÂ bug.Â PleaseÂ fileÂ anÂ issue.Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â ...Â 6Â moreCausedÂ by:Â org.codehaus.janino.InternalCompilerException:Â CompilingÂ "StreamExecValues$200":Â CodeÂ ofÂ methodÂ "nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"Â ofÂ classÂ "StreamExecValues$200"Â growsÂ beyondÂ 64Â KBÂ Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â ...Â 6Â moreCausedÂ by:Â org.codehaus.janino.InternalCompilerException:Â CodeÂ ofÂ methodÂ "nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"Â ofÂ classÂ "StreamExecValues$200"Â growsÂ beyondÂ 64Â KBÂ Â Â Â atÂ org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.CodeContext.write(CodeContext.java:925)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12291)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.referenceThis(UnitCompiler.java:10103)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4488)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$10000(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4437)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4396)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$QualifiedThisReference.accept(Java.java:4407)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileContext2(UnitCompiler.java:4336)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4273)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4268)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4268)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4264)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileContext(UnitCompiler.java:4264)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5661)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5145)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$Block.accept(Java.java:2779)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:1529)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2434)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2181)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$2400(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1500)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$SwitchStatement.accept(Java.java:3391)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)Â &amp;#91;flink-dist_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)Â &amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â atÂ org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)Â ~&amp;#91;flink-table_2.11-1.14.0.jar:1.14.0&amp;#93;Â Â Â Â ...Â 6Â moreÂ </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.JavaCodeSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2441" opendate="2015-7-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Introduce an OpInfo object on the python side</summary>
      <description>All information required to construct on operation are currently saved in a plain dictionary on the python side, whose fields are generally accessed using a variety of string constants.so right now you find lines like: op_info[_Fields.KEYS] = keysThe following shortcomings exist in the current system: There is no central place to define default values. This is done all over the place, to some extent in a redundant way, It produces fairly long code, is surprisingly cumbersome to write.Instead i would like to add a separate OperationInfo object. This code be a special dictionary with preset values for each field, but due to points 2 and 3, Id prefer having an attribute for every field. the resulting code would look like this:op_info.keys = keysisn't that lovely.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
    </fixedFiles>
  </bug>
  <bug id="24410" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Confluent Platform OSS version in end-to-end tests</summary>
      <description>Flink uses Confluent Platform OSS/community edition 5.0.0, which doesn't exist in a Scala 2.12 version. We should bump the used version to at least 5.2.x.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24431" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] EAGER registration strategy does not work when job fails over</summary>
      <description>BackgroundThe EFO Kinesis connector will register and de-register stream consumers based on the configured registration strategy. When EAGER is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.SolutionThe proposal is to not deregister the stream consumer when EAGER is used. The documentation should be updated to reflect this.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="24437" opendate="2021-10-1 00:00:00" fixdate="2021-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unhandled exception handler from CuratorFramework before closing it</summary>
      <description>With FLINK-24021 we add an unhandled exception handler to the started CuratorFramework. In order to avoid that shutting down the CuratorFramework causes triggering of this handler, we should unregister it before closing the CuratorFramework instance.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24445" opendate="2021-10-4 00:00:00" fixdate="2021-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RPC System packaging to package phase</summary>
      <description>mvn compile/test currently fails because the copying of the flink-rpc-akka jar is done in the generate-sources phase.We should move this copying to the packaging phase.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24461" opendate="2021-10-6 00:00:00" fixdate="2021-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableResult#print() should use internal data types</summary>
      <description>The collector used by TableResult#print() should use internal data types</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.CollectDynamicSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PrintUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TimestampStringUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CollectModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.InsertResultIterator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.CollectResultProvider.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TestTableResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliChangelogResultView.java</file>
    </fixedFiles>
  </bug>
  <bug id="24462" opendate="2021-10-6 00:00:00" fixdate="2021-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor casting rules in a similar fashion to DataStructureConverter</summary>
      <description>The goal of this issue is to reorganize CAST logic in rules, similarly to DataStructureConverter. This makes the casting rules easier to debug and extend, allows us to reuse some of the rules for https://issues.apache.org/jira/browse/FLINK-21456 without duplicating any code and simplifies/cleanups the code generator code base. These rules can be reused in the context of https://issues.apache.org/jira/browse/FLINK-24385 as well.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.CompileUtilsTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedClass.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.CompileUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24467" opendate="2021-10-7 00:00:00" fixdate="2021-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set min and max buffer size even if the difference less than threshold</summary>
      <description>Right now, we apply a new buffer size only if it differs from the old buffer size more than the configured threshold but if the old buffer size is close to the max or min value less than this threshold we are always stuck on this value. For example, if we have the old buffer size 22k and our threshold is 50% then the value which we can apply should 33k but this is impossible because the max value is 32k so once we calculate the buffer size to 22k it is impossible to increase it.The suggestion is to apply the changes every time when we calculate the new value to min or max size and the old value was different.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloaterTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloater.java</file>
    </fixedFiles>
  </bug>
  <bug id="24476" opendate="2021-10-7 00:00:00" fixdate="2021-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename all ElasticSearch to Elasticsearch (without camel case)</summary>
      <description>Elasticsearch is a trademark and service mark. It's incorrect to use CamelCase: it's not two words, nor is the internal capital S part of the brand.Where possible, we should use the single word without an internal capital S, especially in user documentation.(Luckily, I don't believe there are any user-facing APIs with incorrect capitalization.)</description>
      <version>1.14.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.release-notes.flink-1.6.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.6.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="2448" opendate="2015-7-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>registerCacheFile fails with MultipleProgramsTestbase</summary>
      <description>When trying to register a file using a constant name an expection is thrown saying the file was already cached.This is probably because the same environment is reused, and the cacheFile entries are not cleared between runs.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.CollectionTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="24481" opendate="2021-10-8 00:00:00" fixdate="2021-12-8 01:00:00" resolution="Done">
    <buginformation>
      <summary>Translate buffer debloat documenation to chinese</summary>
      <description>It needs to translate the documentation of the buffer debloat to chinese. The original documentation was introduced here - https://issues.apache.org/jira/browse/FLINK-23458</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.tm.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="24516" opendate="2021-10-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modernize Maven Archetype</summary>
      <description>The maven archetypes used by many to start their first Flink application do not reflect the project's current state.Â Issues: They still bundle the DataSet API and recommend it for batch processing The JavaDoc recommends deprecated APIsÂ </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2453" opendate="2015-7-31 00:00:00" fixdate="2015-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update POM to use Java7 as the source and target version</summary>
      <description>This sub task is created to track effort to update POM files to move to Java7</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">docs.setup.cluster.setup.md</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-avro.pom.xml</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24538" opendate="2021-10-14 00:00:00" fixdate="2021-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten fails with NPE</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25020&amp;view=logs&amp;j=f2b08047-82c3-520f-51ee-a30fd6254285&amp;t=3810d23d-4df2-586c-103c-ec14ede6af00&amp;l=7573Oct 13 22:26:04 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.355 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTestOct 13 22:26:04 [ERROR] testLeaderShouldBeCorrectedWhenOverwritten Time elapsed: 1.138 s &lt;&lt;&lt; ERROR!Oct 13 22:26:04 java.lang.NullPointerExceptionOct 13 22:26:04 at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten(ZooKeeperLeaderElectionTest.java:434)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.SettableLeaderRetrievalServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingRetrievalBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24541" opendate="2021-10-14 00:00:00" fixdate="2021-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConfigurationUtils#assembleDynamicConfigsStr should consider special characters</summary>
      <description>Without quoting, some special characters will be misunderstood by shell, e.g. ';' used in list type options.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24550" opendate="2021-10-14 00:00:00" fixdate="2021-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not access job information from a standby jobmanager UI</summary>
      <description>One can not access the "running jobs" section (if a job is running) or if the job is completed it can not access the job page. Moreover the overview section does not work in the standby manager if a job is running. The active jobmanager UI works just fine.2021-10-14 15:45:11,483 ERROR org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler [] - Unhandled exception.java.util.concurrent.CancellationException: null at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2263) ~[?:1.8.0_231] at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInternal(DefaultExecutionGraphCache.java:98) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInfo(DefaultExecutionGraphCache.java:67) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.handleRequest(AbstractExecutionGraphHandler.java:81) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:83) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:195) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist_2.12-1.14.0.jar:1.14.0] at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_231] at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:238) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:71) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.12-1.14.0.jar:1.14.0] at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.12-1.14.0.jar:1.14.0] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_231]It seems to be working just fine in 1.13.2Reported in the ML: https://lists.apache.org/thread.html/r69646f1c943846ed07f9ff80232c8d0cea31222191354871f914484c%40%3Cuser.flink.apache.org%3E</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.ContextClassLoadingSettingTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2460" opendate="2015-8-1 00:00:00" fixdate="2015-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReduceOnNeighborsWithExceptionITCase failure</summary>
      <description>I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="24600" opendate="2021-10-20 00:00:00" fixdate="2021-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate 99th percentile displayed in checkpoint summary</summary>
      <description>Â flink checkpoints page has two p99 which is duplicated</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="24603" opendate="2021-10-20 00:00:00" fixdate="2021-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add E2E test</summary>
      <description>Add an e2e test, where the job uses some Scala libraries (not Scala types!). The test must remove the flink-scala jar from the lib/ directory beforehand, and manually add some other currently unsupported Scala version (e.g., 2.13 or even 3.X) to the lib/ directory.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.JobSubmission.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkResourceSetup.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug id="24608" opendate="2021-10-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24612" opendate="2021-10-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka test container creates a large amount of logs</summary>
      <description>When we use a testcontainer setup we try to forward all container STDOUT logs to the surrounding test logger. Unfortunately, Kafka loggers are by default writing a large number of logs because some of the internal loggers are defaulting to TRACE logging.A good example is this test job https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25084&amp;view=logs&amp;j=32a18cd8-d404-5807-996d-abcee436b891where one of the test was stuck and the generated artifact is ~25GB. This makes debugging very hard because the file is hard to parse.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24614" opendate="2021-10-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add array,map,row types support for parquet vectorized reader</summary>
      <description>Add array,map,row types support for parquet vectorized reader</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.columnar.vector.heap.AbstractHeapVector.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.ParquetWriterUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetDecimalVector.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ColumnBatchFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="24627" opendate="2021-10-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JUnit 4 rules to JUnit5 extensions</summary>
      <description>We have to use junit5 extensions to replace the existed junit4 rules in order to change tests to junit5 in flink. There are some generic rules that should be provided in advance.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24631" opendate="2021-10-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoiding directly use the labels as selector for deployment and service</summary>
      <description>We create deployment use the pod selector directly from labels, which is not necessary and may cause problem when some user label value have changed (may be by third-party system). This may lead to dangling pod or service can not select pods.Â I suggest to use minimal and stable flink internal selectors to select the JobManager pod like app=xxx, component=jobmanager and service, taskmanager pod and so on.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24634" opendate="2021-10-25 00:00:00" fixdate="2021-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java 11 profile should target JDK 8</summary>
      <description>Thee java11 profile currently targets Java 11. This was useful because we saw that doing so reveals additional issues that are not detected when building for Java 8. The end goal was to ensure a smooth transition once we switch.However, this has adverse effects on developer productivity.If you happen to switch between Java versions (for example, because of separate environments, or because certain features require Java 8), then you can easily run into UnsupportedVersionErrors when attempting to use Java 8 with Java 11 bytecode.IntelliJ also picks up on this and automatically sets the language level to 11, which means that it will readily allow you to use Java 11 exclusive APIs that will fail on CI later on.To remedy this I propose to split the profile.The java11 profile will pretty much stay as is, except that it is targeting java 8. The value proposition of this profile is being able to build Flink for Java 8 with Java 11.A new explicitly-opt-in java11-target profile then sets the target version to Java 11, which we will use on CI. This profile will ensure that we can readily switch to Java 11 as the target in the future.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24635" opendate="2021-10-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up flink-examples</summary>
      <description>The Flink DataStream examples have a number of deprecation warnings. These are some of the first things new users look at and we should be showing best practices.Â </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.test.scala.org.apache.flink.streaming.scala.examples.StreamingExamplesITCase.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.StreamingExamplesITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.twitter.TwitterExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.join.WindowJoin.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.join.WindowJoin.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.socket.SocketWindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.socket.SocketWindowWordCount.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.connectors.ChangelogSocketExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.async.AsyncIOExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.async.AsyncIOExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.sideoutput.SideOutputExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.iteration.IterateExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.iteration.IterateExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="24639" opendate="2021-10-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve assignment of Kinesis shards to subtasks</summary>
      <description>The default assigner of Kinesis shards to Flink subtasks simply takes the hashCode() of the StreamShardHandle (an integer), which is then interpreted modulo the number of subtasks. This basically does random-ish but deterministic assignment of shards to subtasks.However, this can lead to some subtasks getting several times the number of shards as others. To prevent those unlucky subtasks from being overloaded, the overall Flink cluster must be over-provisioned, so that each subtask has more headroom to handle any over-assignment of shards.We can do better here, at least if Kinesis is being used in a common way. Each record sent to a Kinesis stream has a particular hash key in the range [0, 2^128), which is used to determine which shard gets used; each shard has an assigned range of hash keys. By default Kinesis assigns each shard equal fractions of the hash-key space. And when you scale up or down using UpdateShardCount, it tries to maintain equal fractions to the extent possible. Also, a shard's hash key range is fixed at creation; it can only be replaced by new shards, which split it, or merge it.Given the above, one way to assign shards to subtasks is to do a linear mapping from hash-keys in range [0, 2^128) to subtask indices in [0, nSubtasks). For the 'coordinate' of each shard we pick the middle of the shard's range, to ensure neither subtask 0 nor subtask (n-1) is assigned too many.However this will probably not be helpful for Kinesis users that don't randomly assign partition or hash keys to Kinesis records. The existing assigner is probably better for them.I ran a simulation of the default shard assigner versus some alternatives, using shards taken from one of our Kinesis streams; results attached. The measure I used I call 'overload' and it measures how many times more shards the most heavily-loaded subtask has than is necessary. (DEFAULT is the default assigner, Sha256 is similar to the default but with a stronger hashing function, ShardId extracts the shard number from the shardId and uses that, and HashKey is the one I describe above.)Patch is at: https://github.com/apache/flink/compare/master...john-karp:uniform-shard-assigner?expand=1</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="24648" opendate="2021-10-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ceil, floor for some timeunit return wrong results or fail with CompileException</summary>
      <description>There are issues1. for TIMESTAMP WITHOUT TIMEZONE and DATE it returns wrong result for queriesselect ceil(timstamp'2020-26-10 12:12:12' to decade);select ceil(timstamp'2020-26-10 12:12:12' to century);select ceil(timstamp'2020-26-10 12:12:12' to millennium);same for FLOOR and DATE2. for TIMESTAMP WITH TIMEZONE it throws exception below.Expected for the queryselect floor(date '2021-10-07' to decade) as floor_decade, ceil(date '2021-10-07' to decade) as ceil_decade, floor(date '2021-10-07' to century) as floor_century, ceil(date '2021-10-07' to century) as ceil_century, floor(date '2021-10-07' to millennium) as floor_millennium, ceil(date '2021-10-07' to millennium) as ceil_millennium;is floor_decade ceil_decade floor_century ceil_century floor_millennium ceil_millennium 2020-01-01 2030-01-01 2001-01-01 2101-01-01 2001-01-01 3001-01-01based on PostgreSQL&amp;#91;1&amp;#93; and Vertica&amp;#91;2&amp;#93; defibitions And for both the definition is the sameDECADE - The year field divided by 10CENTURY - The first century starts at 0001-01-01 00:00:00 AD, although they did not know it at the time. This definition applies to all Gregorian calendar countries. There is no century number 0, you go from -1 century to 1 century.MILLENNIUM - The millennium number, where the first millennium is 1 and each millenium starts on 01-01-y001. For example, millennium 2 starts on 01-01-1001.from&amp;#91;1&amp;#93; https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT&amp;#91;2&amp;#93; https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Date-Time/DATE_PART.htm[ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 57, Column 0: No applicable constructor/method found for actual parameters "org.apache.flink.table.data.TimestampData, org.apache.flink.table.data.TimestampData"; candidates are: "public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, java.math.BigDecimal)", "public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, int)", "public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, java.math.BigDecimal)", "public static short org.apache.calcite.runtime.SqlFunctions.ceil(short, short)", "public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal)", "public static double org.apache.calcite.runtime.SqlFunctions.ceil(double)", "public static float org.apache.calcite.runtime.SqlFunctions.ceil(float)", "public static byte org.apache.calcite.runtime.SqlFunctions.ceil(byte, byte)", "public static long org.apache.calcite.runtime.SqlFunctions.ceil(long, long)", "public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, int)" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792) at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2779) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:187) at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.&lt;init&gt;(RegularOperatorChain.java:63) at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:675) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:661) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:574) at java.base/java.lang.Thread.run(Thread.java:834)</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24657" opendate="2021-10-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metric of the total real size of input/output buffers queue</summary>
      <description>Right now we have the metric of the length of input/output buffers queue but since buffer debloater has been introduced this metric is not always helpful because the real size of each buffer can be different. So it is an idea to add a new metric that shows the total size of buffers in the queue.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.NettyShuffleMetricFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="24658" opendate="2021-10-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Debug logs for buffer size calculation</summary>
      <description>Since the buffer debloater recalculates buffer size based on several different parameters. It makes sense to add debug logging to print all of them in case of necessary debugging.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.BufferDebloaterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.throughput.BufferDebloater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="24662" opendate="2021-10-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24667" opendate="2021-10-27 00:00:00" fixdate="2021-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Channel state writer would fail the task directly if meeting exception previously</summary>
      <description>Currently, if channel state writer come across exception when closing a file, such as meet exception during SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable, it will exit the loop. However, in the following channelStateWriter#abort it will throw exception directlyï¼switched from RUNNING to FAILED with failure cause: java.io.IOException: java.lang.RuntimeException: unable to send request to worker at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:228) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkPartitionRequestQueueInitialized(RemoteInputChannel.java:735) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:204) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:651) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:626) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:612) at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109) at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:149) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:98) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:798) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:585)This is not expected as checkpoint failure should not lead to task failover each time.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2467" opendate="2015-8-3 00:00:00" fixdate="2015-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example WordCountStorm.jar is not packaged correctly</summary>
      <description>After moving storm compatibility to flink-contrib, WordCountStorm example is not package correctly any more and the jar is not usable.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.assembly.word-count-storm.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24676" opendate="2021-10-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24678" opendate="2021-10-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the metric name of map state contains latency</summary>
      <description>Current metric name of map state contains is mapStateContainsAllLatency which should be mapStateContainsLatency.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingMapState.java</file>
    </fixedFiles>
  </bug>
  <bug id="24686" opendate="2021-10-28 00:00:00" fixdate="2021-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Make doc clear on AsyncFunction::timeout() overriding</summary>
      <description>Sometimes, a user overrides AsyncFunction::timeout() with an empty method or with only logging code. This causes the timeout does not signal back to the framework and job stuck especially when using orderedWait(). Opening this Jira to make the doc clear on this.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  <bug id="24689" opendate="2021-10-29 00:00:00" fixdate="2021-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add log file&amp;#39;s time info in loglist</summary>
      <description>Flink web UI support show all log file info in log dir, but loglist view only has file name and file size now. When I search and locate problem, I don't now which one log file that I should open(for example: taskmanager.log rotated and retained multi history log file)If there is a time info (for example mtime) in loglist's view, it will guide me to choose correct log file and analyse problem exactly and quickly, or I must open every log file inefficientlyIf no one made this improvement , I will be happy to fix it.Â Â Â </description>
      <version>1.12.2,1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.LogListInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.LogInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug id="24695" opendate="2021-10-29 00:00:00" fixdate="2021-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update how to configure unaligned checkpoints in the documentation</summary>
      <description>It looks like we don't have a code example how to enabled unaligned checkpoints anywhere in the docs. That should be fixed.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  <bug id="24697" opendate="2021-10-29 00:00:00" fixdate="2021-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka table source cannot change the auto.offset.reset setting for &amp;#39;group-offsets&amp;#39; startup mode</summary>
      <description>Because Flink 1.13 SQL does not use the new Source API in FLIP-27, the behavior to start from group offsets in flink 1.13 will use the kafka 'auto.offset.reset' default value(latest), when theÂ 'auto.offset.reset' configuration is not set in table options. But in flink 1.13 we could change the behavior by settingÂ 'auto.offset.reset' to other values. See the methodÂ setStartFromGroupOffsets in the class FlinkKafkaConsumerBase.Flink 1.14 uses the new Source API, but we have no ways to change the default 'auto.offset.reset' value when useÂ 'group-offsets' startup mode. In DataStream API, we could change it by `kafkaSourceBuilder.setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy))`.So we need the way to change auto offset reset configuration.The design is that when 'auto.offset.reset' is set, theÂ 'group-offsets' startup mode will use the provided auto offset reset strategy, or else 'none' reset strategy in order to be consistent with the DataStream API.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkAssertions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="24699" opendate="2021-10-29 00:00:00" fixdate="2021-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move scalastyle execution to validation phase</summary>
      <description>For some reason the scalstyle plugin by default runs in the verify phase.I propose to move it to the validate phase where other source QA plugins are run, which also makes it easier to run it locally.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24714" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate partition columns for ResolvedCatalogTable</summary>
      <description>Currently, partition columns are not validated and might not exist in the schema.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogBaseTableResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="24715" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.0</summary>
      <description>Flink uses multiple com.fasterxml.jackson components in different Flink modules. We should update these to the latest versionExample (one or more used in ElasticSearch connector, Kinesis, FS Hadoop/Presto, AVRO, Python, Table API etc) com.fasterxml.jackson.core:jackson-core:2.12.1 com.fasterxml.jackson.core:jackson-databind:2.12.1 com.fasterxml.jackson.core:jackson-annotations:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.12.1 com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.12.1Could all be updated to: com.fasterxml.jackson.core:jackson-core:2.13.0 com.fasterxml.jackson.core:jackson-databind:2.13.0 com.fasterxml.jackson.core:jackson-annotations:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.13.0 com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.13.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="24717" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push down partitions before filters</summary>
      <description>Currently, we push filters before partitions. This means that a applyFilters needs to have partition logic to extract the partition predicate. Furthermore, if a applyFilters consumes all filters (no remaining predicates), the applyPartitions is never called.We should execute the PushPartitionIntoTableSourceScanRule first and check for side effects of this change.See org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest#testMetadataProjectionWithoutProjectionPushDownWhenNotSupportedAndNoneSelected for an example of using the new test infrastructure.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.PartitionableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24718" opendate="2021-11-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update AVRO dependency to 1.11.1</summary>
      <description>Update org.apache.avro:avro from 1.10 to 1.11</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24719" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Postgres test dependencies to 0.13.4</summary>
      <description>Update Postgress and Postgress test dependencies to the latest version</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24720" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-runtime-web dependency from flink-tests</summary>
      <description>For some reason flink-tests has a test dependency on flink-runtime-web and it's test-jar.From what I can tell however these are entirely unused, and removing said dependency neither fails any test nor results in any additional test to be skipped.I propose to remove these dependencies to simplify the dependency tree. This means that developers only have to worry about the UI being built when they build flink-dist/flink-docs. It also removes a bottleneck for parallel builds.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24724" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Later">
    <buginformation>
      <summary>Update japicmp jaxb dependencies</summary>
      <description>Update com.sun.xml.bind:jaxb-impl and com.sun.xml.bind:jaxb-core to the latest available minor version. This means an upgrade from the 2.3.0 version (released August 2017) to 3.0.2 (released August 2021)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24725" opendate="2021-11-1 00:00:00" fixdate="2021-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Cython to the latest version</summary>
      <description>Update Cython from 0.29.16 to 0.29.24</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24733" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data loss in pulsar source when using shared mode</summary>
      <description>Descriptionï¼ Noticed that when using Flink-connector-pulsar with Shared mode (default mode), some messages are lost. Detailsï¼ When a topic partition does not receive message for more than 10s (pulsar.source.maxFetchTime), the next coming message will be dropped by source.In stream applications, this situation which there is no data for more than 10s is very common. So this bug will cause data loss.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="24739" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State requirements for Flink&amp;#39;s application mode in the documentation</summary>
      <description>If I am not mistaken, then Flink won't ship jars when using the application mode because it assumes the jars to be on the classpath. If this is true, then we should make this requirement a bit more prominent in the deployment documentation because currently it is only subtly hinted at.Alternatively, we could enable Flink to ship the user code jars to its components also when using the application mode.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="24740" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24742" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="24766" opendate="2021-11-4 00:00:00" fixdate="2021-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ceiling/flooring dates to day return wrong results</summary>
      <description>Query to reproduceselect ceil(date '2021-11-04' to day) as `ceil`, floor(date '2021-11-04' to day) as `floor`;givesceil floor8525-03-02 1970-01-01expectedceil floor2021-11-05 2021-11-05</description>
      <version>1.14.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.TimeFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24777" opendate="2021-11-4 00:00:00" fixdate="2021-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Processed (persisted) in-flight data description miss on Monitoring Checkpointing page</summary>
      <description>Processed (persisted) in-flight data description is missed, we need to merge fromÂ Processed (persisted) in-flight data and Persisted) in-flight dataÂ </description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.content.zh.docs.ops.monitoring.checkpoint.monitoring.md</file>
    </fixedFiles>
  </bug>
  <bug id="24779" opendate="2021-11-4 00:00:00" fixdate="2021-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Numeric casting logic to CastRule</summary>
      <description>For more details, check the parent issue</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.IfCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.UpcastToBigIntCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.IdentityCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.AbstractNullAwareCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.AbstractExpressionCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ExpressionCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="24784" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable state.backend.latency-track.state-name-as-variable by default</summary>
      <description>This could help improve the usablility of state access latency.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingStateConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.StateBackendOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.latency.tracking.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24785" opendate="2021-11-5 00:00:00" fixdate="2021-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate RocksDB&amp;#39;s log under flink log directory by default</summary>
      <description>Previously, RocksDB's log locates at its own DB folder, which makes the debuging RocksDB not so easy. We could let RocksDB's log stay in Flink's log directory by default.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24786" opendate="2021-11-5 00:00:00" fixdate="2021-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce RocksDB&amp;#39;s statistics related options</summary>
      <description>Currently, Flink only support RocksDB's native metrics of property related. However, such metrics cannot help on performance tunning, we could introduce statistics related metrics to help debug performance related problem.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitor.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.native.metric.configuration.html</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="24787" opendate="2021-11-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more details of state latency tracking documentation</summary>
      <description>Current documentation only tells how to enable or configure state latency tracking related options. We could add more details of state specific descriptions.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="24789" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalStateException with CheckpointCleaner being closed already</summary>
      <description>We experienced a failure of OperatorCoordinatorSchedulerTest in our VVP Fork of Flink. The finegrained_resource_management test run failed with an non-0 exit code:Nov 01 17:19:12 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (default-test) on project flink-runtime: There are test failures.Nov 01 17:19:12 [ERROR] Nov 01 17:19:12 [ERROR] Please refer to /__w/1/s/flink-runtime/target/surefire-reports for the individual test results.Nov 01 17:19:12 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.Nov 01 17:19:12 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Nov 01 17:19:12 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter6007815607334336440.jar /__w/1/s/flink-runtime/target/surefire 2021-11-01T16-51-51_363-jvmRun2 surefire6448660128033443499tmp surefire_4131168043975619749001tmpNov 01 17:19:12 [ERROR] Error occurred in starting fork, check output in logNov 01 17:19:12 [ERROR] Process Exit Code: 239Nov 01 17:19:12 [ERROR] Crashed tests:Nov 01 17:19:12 [ERROR] org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTestNov 01 17:19:12 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Nov 01 17:19:12 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter6007815607334336440.jar /__w/1/s/flink-runtime/target/surefire 2021-11-01T16-51-51_363-jvmRun2 surefire6448660128033443499tmp surefire_4131168043975619749001tmpNov 01 17:19:12 [ERROR] Error occurred in starting fork, check output in logNov 01 17:19:12 [ERROR] Process Exit Code: 239Nov 01 17:19:12 [ERROR] Crashed tests:Nov 01 17:19:12 [ERROR] org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTestNov 01 17:19:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)Nov 01 17:19:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)It looks like the testSnapshotAsyncFailureFailsCheckpoint caused it even though finishing successfully due to a fatal error when shutting down the cluster:17:07:27,264 [ Checkpoint Timer] ERROR org.apache.flink.util.FatalExitExceptionHandler [] - FATAL: Thread 'Checkpoint Timer' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: CheckpointsCleaner has already been closed at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:626) ~[classes/:?] at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_292] at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_292] at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:814) [?:1.8.0_292] at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_292] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: CheckpointsCleaner has already been closed at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292] ... 8 moreCaused by: java.lang.IllegalStateException: CheckpointsCleaner has already been closed at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.14-stream-SNAPSHOT.jar:1.14-stream-SNAPSHOT] at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.incrementNumberOfCheckpointsToClean(CheckpointsCleaner.java:105) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.cleanup(CheckpointsCleaner.java:87) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.cleanCheckpoint(CheckpointsCleaner.java:62) ~[classes/:?] at org.apache.flink.runtime.checkpoint.PendingCheckpoint.dispose(PendingCheckpoint.java:573) ~[classes/:?] at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:551) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1939) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1926) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:910) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:875) ~[classes/:?] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$6(CheckpointCoordinator.java:614) ~[classes/:?] at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292] at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292] ... 8 more</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.concurrent.FutureUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24798" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="24816" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump com.rabbitmq:amqp-client to v5.13.1</summary>
      <description>Bump com.rabbitmq:amqp-client:5.9.0 to com.rabbitmq:amqp-client:5.13.1List of fixes for the RabbitMQ client: https://github.com/rabbitmq/rabbitmq-java-client/releases</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
    </fixedFiles>
  </bug>
  <bug id="24820" opendate="2021-11-8 00:00:00" fixdate="2021-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples in documentation for value1 IS DISTINCT FROM value2 are wrong</summary>
      <description>Currently it is stated in docs for value1 IS DISTINCT FROM value2E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.In fact they return opposite values.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24848" opendate="2021-11-9 00:00:00" fixdate="2021-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error codes for savepoint handlers</summary>
      <description>We currently return a 500 Server Error in pretty much all error scenarios. We can improve this a fair bit, like returning 404 if no operation is known for a given key, or 409 Conflict when attempting to trigger an operation that has already failed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.UnknownOperationKeyException.java</file>
    </fixedFiles>
  </bug>
  <bug id="24858" opendate="2021-11-10 00:00:00" fixdate="2021-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TypeSerializer version mismatch during eagerly restore</summary>
      <description>Currently, some of our TypeSerializer snapshots assume information about the binary layout of the actual data rather than only holding information about the TypeSerialzer.Multiple users ran into this problem i.e.https://lists.apache.org/thread/4q5q7wp0br96op6p7f695q2l8lz4wfzxThis manifest itself when state is restored egarly (for example an operator state) but, for example a user doesn't register the state on their intializeState/open,* and then a checkpoint happens.The result is that we will have elements serialized according to an old binary layout, but our serializer snapshot declares a new version which indicates that the elements are written with a new binary layout.The next restore will fail.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.RowSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
      <file type="M">docs.content.release-notes.flink-1.14.md</file>
    </fixedFiles>
  </bug>
  <bug id="24859" opendate="2021-11-10 00:00:00" fixdate="2021-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new File formats</summary>
      <description>The project recently introduced new formats: BulkFormat and StreamFormat interfaces.Â There are already implementations of these formats: hive, parquet, orc and textLine formats that need to be documented.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.filesystem.LimitableBulkFormatTest.java</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowFileInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcColumnarRowFileInputFormat.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.wordcount.WordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.wordcount.WordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.WindowWordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveCompactReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.reader.TextLineFormat.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSource.java</file>
      <file type="M">docs.content.docs.connectors.datastream.hybridsource.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.hybridsource.md</file>
    </fixedFiles>
  </bug>
  <bug id="24861" opendate="2021-11-10 00:00:00" fixdate="2021-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to disable caching missing key for lookup cache</summary>
      <description>Ideally, in case of cache miss for a key, or with null value fetch for key, key shouldn't be cached</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="24869" opendate="2021-11-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-core should be provided in flink-file-sink-common</summary>
      <description>As example flink-connector-files brings flink-core with compile scope via flink-file-sink-common.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-file-sink-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24875" opendate="2021-11-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed e2e tests across 2 profiles</summary>
      <description>It's that time of the year again :/</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24876" opendate="2021-11-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="24880" opendate="2021-11-12 00:00:00" fixdate="2021-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error messages "OverflowError: timeout value is too large" shown when executing PyFlink jobs</summary>
      <description>The following exception shown when executing PyFlink jobs according to the demo show in https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/python/datastream/intro_to_datastream_api/ Â  Â Â Common Structure of Python DataStream API ProgramsÂ Â Exception in thread Thread-15:Traceback (most recent call last):Â  File "D:\soft\py\lib\threading.py", line 932, in _bootstrap_innerÂ  Â  self.run()Â  File "C:\Users\wangdonglin\AppData\Roaming\Python\Python38\site-packages\apache_beam\runners\worker\data_plane.py", line 218, in runÂ  Â  while not self._finished.wait(next_call - time.time()):Â  File "D:\soft\py\lib\threading.py", line 558, in waitÂ  Â  signaled = self._cond.wait(timeout)Â  File "D:\soft\py\lib\threading.py", line 306, in waitÂ  Â  gotit = waiter.acquire(True, timeout)OverflowError: timeout value is too large</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.stream.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
    </fixedFiles>
  </bug>
  <bug id="24884" opendate="2021-11-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink flame graph webui bug</summary>
      <description>i can not compile success when i port the flame graph feature to our low version of flink.but it is success in the high version of flink Â </description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
    </fixedFiles>
  </bug>
  <bug id="24889" opendate="2021-11-12 00:00:00" fixdate="2021-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL Client should print corrently multisets</summary>
      <description>Probably the easiest way to reproduce is CREATE TABLE flink_multiset_example ( m multiset&lt;BIGINT&gt; ) WITH ( 'connector' = 'datagen' );select * from flink_multiset_example;I think it relates to https://issues.apache.org/jira/browse/FLINK-21456</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="24928" opendate="2021-11-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve type-safety in Flink UI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.test.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-duration.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-date.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.checkpoint-badge.checkpoint-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.checkpoint-badge.checkpoint-badge.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.backpressure-badge.backpressure-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.backpressure-badge.backpressure-badge.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.resize.resize.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.svg-container.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.svg-container.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.safe-any.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.task-badge.task-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.flame-graph.flame-graph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.graph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.dagre.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.plan.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.overview.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-timeline.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-subtask.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-overview.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-exception.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-backpressure.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-accumulators.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.jar.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-badge.job-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.status.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.overview.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.jar.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.deepFind.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-bytes.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.utils.isNil.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-detail.job-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.chart.job-overview-drawer-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.timeline.job-timeline.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.statistic.overview-statistic.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="24954" opendate="2021-11-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reset read buffer request timeout on buffer recycling for sort-shuffle</summary>
      <description>Currently, the read buffer request timeout implementation of sort-shuffle is a little aggressive. As reported in the mailing list: https://lists.apache.org/thread/bd3s5bqfg9oxlb1g1gg3pxs3577lhf88. The TimeoutException may be triggered if there is data skew and the downstream task is slow. Actually, we can further improve this case by reseting the request timeout on buffer recycling.</description>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="24958" opendate="2021-11-19 00:00:00" fixdate="2021-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct the example and link for temporal table function documentation</summary>
      <description>correct the example and link for temporal table function documentation</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.joins.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.joins.md</file>
    </fixedFiles>
  </bug>
  <bug id="24963" opendate="2021-11-19 00:00:00" fixdate="2021-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the tail separator when outputting yarn queue names</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="24964" opendate="2021-11-19 00:00:00" fixdate="2021-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade frontend-maven-plugin to 1.11.0</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our frontend-maven-plugin to 1.11.0.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24998" opendate="2021-11-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SIGSEGV in Kryo / C2 CompilerThread on Java 17</summary>
      <description>While running our tests on CI with Java 17 they failed infrequently with a SIGSEGV error.All occurrences were related to Kryo and happened in the C2 CompilerThread.Current thread (0x00007f1394165c00): JavaThread "C2 CompilerThread0" daemon [_thread_in_native, id=470077, stack(0x00007f1374361000,0x00007f1374462000)]Current CompileTask:C2: 14251 6333 4 com.esotericsoftware.kryo.io.Input::readString (127 bytes)The full error is attached to the ticket. I can also provide the core dump if needed.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="25014" opendate="2021-11-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table to DataStream conversion, wrong field order</summary>
      <description>It seems that in some cases, the field reordering as describe in the relevant part of the docs does not seem to work properly. Given the following DDLcreate table if not exists masterdata( facility text, shortcode text, sks text, sksnumber integer, rdspp text, manufacturer text, facilitytype text, controls text, serial integer, powerkw double precision, hubheight double precision, rotorheight integer, latitude double precision, longitude double precision, elevation double precision); which should map to this POJO:public static class MasterData { public String controls; public Double elevation; public String facility; public String facilityType; public Double hubHeight; public Double latitude; public Double longitude; public String manufacturer; public Double powerKw; public String rdsPp; public Long rotorHeight; public Long serial; public String shortcode; public String sks; public Long sksNumber;} I register the database using JdbcCatalog like this:Â JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog("cat", catalog); tableEnv.useCatalog("cat"); and if I try to create a table with either "SELECT * FROM masterdata" or viatableEnv.from("masterdata"); It will bail out with an exception similar toException in thread "main" org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'cat.postgres.Unregistered_DataStream_Sink_1' do not match.Cause: Incompatible types for sink column 'elevation' at position 1.Query schema: [facility: STRING, shortcode: STRING, sks: STRING, sksnumber: INT, rdspp: STRING, manufacturer: STRING, facilitytype: STRING, controls: STRING, serial: INT, powerkw: DOUBLE, hubheight: DOUBLE, rotorheight: INT, latitude: DOUBLE, longitude: DOUBLE, elevation: DOUBLE]Sink schema: Â [controls: STRING, elevation: DOUBLE, facility: STRING, facilityType: STRING, hubHeight: DOUBLE, latitude: DOUBLE, longitude: DOUBLE, manufacturer: STRING, powerKw: DOUBLE, rdsPp: STRING, rotorHeight: BIGINT, serial: BIGINT, shortcode: STRING, sks: STRING, sksNumber: BIGINT] If i explicitly set the order of the columns in the SELECT like this:tableEnv.sqlQuery("SELECT elevation,facility,latitude,longitude,manufacturer,serial from masterdata");it works. In the debugger I can see that "queryFields" and "sinkField" in the call to DynamicSinkUtils.validateSchemaAndApplyImplicitCast() are not aligned, i.e. the order of the fields in those two lists are not the same, hence the exception.Here is the full code for reproducing:import org.apache.flink.connector.jdbc.catalog.JdbcCatalog;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import org.junit.Test;public class FieldReorderTest { @Test public void testFieldReordering() throws Exception { String name = "cat"; String defaultDatabase = "postgres"; String username = "cat"; String password = "1234"; String baseUrl = "jdbc:postgresql://cat.postgres.database.azure.com:5432"; var env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog("cat", catalog); tableEnv.useCatalog("cat"); var table = tableEnv.from("masterdata"); table.printSchema(); tableEnv.toDataStream(table, MasterData.class).print(); env.execute(); Â  Â  Â  // this works // tableEnv.sqlQuery("SELECT controls,elevation,facility,facilitytype,hubheight,latitude,longitude," + // "manufacturer,powerkw,rdspp,rotorheight,serial,shortcode,sks,sksnumber from masterdata"); Â  Â  } public static class MasterData { public String controls; public Double elevation; public String facility; public String facilityType; public Double hubHeight; public Double latitude; public Double longitude; public String manufacturer; public Double powerKw; public String rdsPp; public Long rotorHeight; public Long serial; public String shortcode; public String sks; public Long sksNumber; }} Â Â </description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaTranslatorTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.SchemaTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25016" opendate="2021-11-23 00:00:00" fixdate="2021-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump netty to 4.1.70</summary>
      <description>But Netty to the latest version for general improvements and a proper for FLINK-24197.</description>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25022" opendate="2021-11-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API</summary>
      <description>If a job is submitted using the REST API's /jars/:jarid/run endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like Flink-DispatcherRestEndpoint-thread-*.If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the ChildFirstClassloader of the job and thus leaking that.We saw this for the jsoniter scala library at the JM which creates ThreadLocal instances but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code.Â There are a few workarounds a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances.Â A proper fix should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's main() method and once the job is submitted exit that thread and destroy all thread locals along with it.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarMessageParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="25051" opendate="2021-11-25 00:00:00" fixdate="2021-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port raw &lt;-&gt; binary logic to CastRule</summary>
      <description>More details on the parent task</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25052" opendate="2021-11-25 00:00:00" fixdate="2021-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port row to row cast logic to CastRule</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.AbstractCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.TableAssertions.java</file>
    </fixedFiles>
  </bug>
  <bug id="25053" opendate="2021-11-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="25079" opendate="2021-11-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Globally introduce assertj assertions for tests</summary>
      <description>As discussed on the dev@ mailing list, we advertise assertj instead of Hamcrest in the future.https://lists.apache.org/thread/33t7hz8w873p1bc5msppk65792z08rgt</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ValueDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeFactoryMock.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeTestingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeMergingTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecksTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeDuplicatorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalCommonTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.TypeInferenceExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ClassDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.ProjectedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.JoinedRowDataTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25080" opendate="2021-11-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move FutureUtils to flink-core</summary>
      <description>The same applies to the RetryStrategy implementation tests.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FixedRetryStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ExponentialBackoffRetryStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ConjunctFutureTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestTaskBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestExternalHandlersITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientMultipartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FutureUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolInteractionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerSharedServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfigurationBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25085" opendate="2021-11-29 00:00:00" fixdate="2021-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a scheduled thread pool in Endpoint and close it when the endpoint is stopped</summary>
      <description>Add a dedicated thread pool in Endpoint to schedule tasks that have a long delay such as PhysicalSlotRequestBulkCheckerImpl, heatbeat checker and some other timeout checker in JM/TM/RM. Job should shut down the thread pool and all the pending tasks will be removed when it terminates.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ThrowingScheduledFuture.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredComponentMainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.FencedRpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="25091" opendate="2021-11-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12ã1.13ã1.14 ããã&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="25096" opendate="2021-11-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make errors happened during JobMaster initialization accessible through the exception history</summary>
      <description>Currently we are using flink version 1.13.2 and as per the flink documentation we should get all exceptions through exceptions api in exceptionHistory tag. While running few scenarios we observed that the below two exceptions are not coming in exceptionHistory tag but are coming in root-exception tag.Exception 1 - caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'C:\Users\abc\Documents\checkpoints\a737088e21206281db87f6492bcba074' on file system 'file'.Exception 2 - Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/mnt/c/Users/abc/Documents/checkpoints/a737088e21206281db87f6492bcba074/chk-144.Please find the attachment for the logs of above exceptions.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.exceptionhistory.RootExceptionHistoryEntry.java</file>
    </fixedFiles>
  </bug>
  <bug id="25109" opendate="2021-11-30 00:00:00" fixdate="2021-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jline3 to 3.21.0</summary>
      <description>There is a number improvements in a new jline which could be used in this FLIPe.g. rgb support in style (could be used for prompts and highlighting)line numbers support in prompt continuationautopairing, display hints during completion</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25123" opendate="2021-12-1 00:00:00" fixdate="2021-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve expression description in SQL operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowDeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.FilterableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveSingleAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CorrelateSortToRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalIntervalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ValuesTest.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25128" opendate="2021-12-1 00:00:00" fixdate="2021-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize table modules and introduce flink-table-planner-loader</summary>
      <description>For more details, see https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.icu4j</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.janino</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonArrayAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonObjectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.tablefunctions.ReplicateRows.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25129" opendate="2021-12-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs to use flink-table-planner-loader instead of flink-table-planner</summary>
      <description>For more details https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.content.docs.dev.configuration.testing.md</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.docs.dev.configuration.maven.md</file>
      <file type="M">docs.content.docs.dev.configuration.connector.md</file>
      <file type="M">docs.content.docs.dev.configuration.advanced.md</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.libs.gelly.overview.md</file>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.docs.dev.datastream..index.md</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.connectors.table.upsert-kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.rabbitmq.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pubsub.md</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.cassandra.md</file>
    </fixedFiles>
  </bug>
  <bug id="25143" opendate="2021-12-2 00:00:00" fixdate="2021-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ITCase for Generalized incremental checkpoints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointFormatITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="25160" opendate="2021-12-3 00:00:00" fixdate="2021-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make doc clear: tolerable-failed-checkpoints counts consecutive failures</summary>
      <description>According to the code, tolerable-failed-checkpoints counts the consecutive failures. We should make this clear in the doc configÂ </description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25161" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependency for japicmp-maven-plugin</summary>
      <description>compiliation with jdk 17 fails like belowupdate of jaxb-impl to 2.3.1 helpsjava.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain) at java.base/java.security.AccessController.doPrivileged(AccessController.java:573) at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.&lt;clinit&gt;(Injector.java:197) at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81) at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125) at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.&lt;init&gt;(TransducedAccessor.java:235) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175) at com.sun.xml.bind.v2.runtime.property.AttributeProperty.&lt;init&gt;(AttributeProperty.java:91) at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108) at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.&lt;init&gt;(ClassBeanInfoImpl.java:181) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:331) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662) at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119) at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70) at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866) at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149) at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25162" opendate="2021-12-3 00:00:00" fixdate="2021-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink : Connectors : Hive fails with VectorizedRowBatch not found</summary>
      <description>While compiling with jdk17[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-connector-hive_2.12: Compilation failure[ERROR] flink/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveInputFormat.java:[168,17] cannot access org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch[ERROR] class file for org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch not found[ERROR] [ERROR] -&gt; [Help 1][ERROR]</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="25163" opendate="2021-12-4 00:00:00" fixdate="2021-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more options for rocksdb state backend to make configuration more flexible</summary>
      <description>Now flink has less options than the configurations what Rocksdb can set. We can see many function in the org.rocksdb.DBOptions that can influence its behavior(e.g rocksdb background threads).It make us do less when we want to do some thing to tuning Rocksdb. In my opinion, there are at least there options: maxBackgroundFlushes, it can define the background flush threads. default 1. maxBackgroundCompactions, it can define the background compaction threads. default 1. maxBackgroundJobs, it can define the background threads. default 2.setIncreaseParallelism (the most we can do for the rocksdb backend background threads) seems like can do little. It can't change the flush threads. I think it's necessary to make rocksdb configuration flexible.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25183" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize changelog normalize for the managed table upsert mode</summary>
      <description>The upsert mode of managed table preserves the complete delete message and avoids normalization for the following downstream operators: Upsert sink: Upsert sink only requires upsert inputs without UPDATE_BEFORE. Join: Join for unique inputs will store records by unique key. It can work without Â UPDATE_BEFORE.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalChangelogNormalize.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestManagedTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25204" opendate="2021-12-7 00:00:00" fixdate="2021-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spell one more as in the DataStream interval join docs</summary>
      <description>Spell one more as in the DataStream interval join documentBoth the lower and upper bound can be either negative or positive as long as as the lower bound is always smaller or equal to the upper bound.=&gt;Both the lower and upper bound can be either negative or positive as long as the lower bound is always smaller or equal to the upper bound.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.joining.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.joining.md</file>
    </fixedFiles>
  </bug>
  <bug id="25222" opendate="2021-12-8 00:00:00" fixdate="2021-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove NetworkFailureProxy used for Kafka connector tests</summary>
      <description>Recently the number of Kafka connector tests either hitting a timeout due to blocked networking or corrupted network responses increased significantly.Â We think it is caused by our custom network failure implementation since all the tests are for the legacy FlinkKafkaProducer or FlinkKafkaConsumer we can safely remove them because we will not add more features to this connector, to increase the overall stability.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.networking.NetworkFailuresProxyTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.networking.NetworkFailuresProxy.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.networking.NetworkFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25227" opendate="2021-12-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparing the equality of the same (boxed) numeric values returns false</summary>
      <description>Add the following test case to TableEnvironmentITCase to reproduce this bug.@Testdef myTest(): Unit = { val data = Seq( Row.of( java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000), java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000)) ) tEnv.executeSql( s""" |create table T ( | a int, | b int, | c int, | d int |) with ( | 'connector' = 'values', | 'bounded' = 'true', | 'data-id' = '${TestValuesTableFactory.registerData(data)}' |) |""".stripMargin) tEnv.executeSql("select greatest(a, b) = greatest(c, d) from T").print()}The result is false, which is obviously incorrect.This is caused by the generated java code:public class StreamExecCalc$8 extends org.apache.flink.table.runtime.operators.TableStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$8( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output, org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception { this.references = references; this.setup(task, config, output); if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) { ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue(); int field$0; boolean isNull$0; int field$1; boolean isNull$1; int field$3; boolean isNull$3; int field$4; boolean isNull$4; boolean isNull$6; boolean result$7; isNull$3 = in1.isNullAt(2); field$3 = -1; if (!isNull$3) { field$3 = in1.getInt(2); } isNull$0 = in1.isNullAt(0); field$0 = -1; if (!isNull$0) { field$0 = in1.getInt(0); } isNull$1 = in1.isNullAt(1); field$1 = -1; if (!isNull$1) { field$1 = in1.getInt(1); } isNull$4 = in1.isNullAt(3); field$4 = -1; if (!isNull$4) { field$4 = in1.getInt(3); } out.setRowKind(in1.getRowKind()); java.lang.Integer result$2 = field$0; boolean nullTerm$2 = false; if (!nullTerm$2) { java.lang.Integer cur$2 = field$0; if (isNull$0) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (!nullTerm$2) { java.lang.Integer cur$2 = field$1; if (isNull$1) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (nullTerm$2) { result$2 = null; } java.lang.Integer result$5 = field$3; boolean nullTerm$5 = false; if (!nullTerm$5) { java.lang.Integer cur$5 = field$3; if (isNull$3) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (!nullTerm$5) { java.lang.Integer cur$5 = field$4; if (isNull$4) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (nullTerm$5) { result$5 = null; } isNull$6 = nullTerm$2 || nullTerm$5; result$7 = false; if (!isNull$6) { result$7 = result$2 == result$5; } if (isNull$6) { out.setNullAt(0); } else { out.setBoolean(0, result$7); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}You can see that line 137 compares two boxed Integer types with == instead of .equals, which causes this problem.In older Flink versions where the return types of cast functions are also boxed types, casting strings to numeric values are also affected by this bug.Currently for a quick fix we can rewrite the generated code. But for a long term solution we shouldn't use boxed types as internal data structures.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.14.5,1.15.0,1.16.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25228" opendate="2021-12-9 00:00:00" fixdate="2021-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink-table-test-utils</summary>
      <description>Introduce a package to ship test utilities for formats, connectors and end users.This package should provide: Assertions for data types, logical types and internal data structures. Test cases for formats and connnectorsThe end goal is to remove the test-jar planner dependency in formats and connectors and replace it with this package, so formats and connectors can then just depend on table-planner-loader.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.TableAssertions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.StringDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.RowDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeAssert.java</file>
    </fixedFiles>
  </bug>
  <bug id="25252" opendate="2021-12-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Kafka E2E tests on Java 11</summary>
      <description>The Java Kafka E2E tests are currently not run on Java 11. We should check what the actual issue is and whether it can be resolved (e.g., by a Kafka server version bump):</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25274" opendate="2021-12-13 00:00:00" fixdate="2021-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ResolvedSchema in DataGen instead of TableSchema</summary>
      <description>TableSchema is deprecated It is recommended to use ResolvedSchema and Schema in TableSchema javadoc</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="25277" opendate="2021-12-13 00:00:00" fixdate="2021-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce explicit shutdown signalling between TaskManager and JobManager</summary>
      <description>We need to introduce shutdown signalling between TaskManager and JobManager for fast &amp; graceful shutdown in reactive scheduler mode.In Flink 1.14 and earlier versions, the JobManager tracks the availability of a TaskManager using a hearbeat. This heartbeat is by default configured with an interval of 10 seconds and a timeout of 50 seconds &amp;#91;1&amp;#93;. Hence, the shutdown of a TaskManager is recognized only after about 50-60 seconds. This works fine for the static scheduling mode, where a TaskManager only disappears as part of a cluster shutdown or a job failure. However, in the reactive scheduler mode (FLINK-10407), TaskManagers are regularly added and removed from a running job. Here, the heartbeat-mechanisms incurs additional delays.To remove these delays, we add an explicit shutdown signal from the TaskManager to the JobManager.Â &amp;#91;1&amp;#93;https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#heartbeat-timeout</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriverTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerRunnerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25282" opendate="2021-12-13 00:00:00" fixdate="2021-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move runtime dependencies from table-planner to table-runtime</summary>
      <description>There are several runtime dependencies (e.g. functions used in codegen) that are shipped by table-planner and calcite-core. We should move these dependencies to runtime</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlJsonUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.LikeCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonValueCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.SqlLikeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="25287" opendate="2021-12-14 00:00:00" fixdate="2021-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor connector testing framework interfaces</summary>
      <description>A refactor in connector testing framework interfaces is required to support more test scenarios and cases such as sinks and Table / SQL API.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroBulkFormatITCase.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaSingleTopicExternalContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaMultipleTopicExternalContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.UnorderedSourceTestSuiteBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.KeyedPulsarPartitionDataWriter.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.SharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.KeySharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.FailoverSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.ExclusiveSubscriptionContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContextFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicTemplateContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25288" opendate="2021-12-14 00:00:00" fixdate="2021-2-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add savepoint and metric cases in DataStream source suite of connector testing framework</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaSourceExternalContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.ConnectorTestConstants.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="25289" opendate="2021-12-14 00:00:00" fixdate="2021-2-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add DataStream sink test suite in connector testing framework</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.MetricQuerier.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.external.sink.DataStreamSinkExternalContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25295" opendate="2021-12-14 00:00:00" fixdate="2021-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Log4j to 2.16.0</summary>
      <description>Log4j 2.16.0 has been released https://lists.apache.org/thread/d6v4r6nosxysyq9rvnr779336yf0woz4This version removes message lookups and disables JNDI by default and results in a hardening of the default behaviour and configuration. Just to be clear, this dependency upgrade is not required to fix CVE-2021-44228. That has already been covered by https://issues.apache.org/jira/browse/FLINK-25240</description>
      <version>None</version>
      <fixedVersion>1.11.6,1.12.7,1.13.5,1.14.2,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25329" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement of execution graph store in flink session cluster for jobs</summary>
      <description>Flink session cluster uses files to store info of jobs after they reach termination with `FileExecutionGraphInfoStore`, each job will generate one file. When the cluster executes many small jobs concurrently, there will be many disk related operations, which will1&gt; Increase the CPU usage of `Dispatcher`2&gt; Decrease the performance of the jobs in the cluster.We hope to improve the disk operations in `FileExecutionGraphInfoStore` to increase the performance of session cluster, or support memory store.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStore.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25331" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow testcontainer tests to run on Java 17</summary>
      <description>Tests using testcontainers for Flink are currently locked to Java 8.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkImageBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="25385" opendate="2021-12-20 00:00:00" fixdate="2021-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden function serialization in JSON plan</summary>
      <description>Similar to FLINK-25230, we should revisit how functions are serialized into the JSON plan. No legacy in plan No Java serialization in plan Consider config option regarding catalog objects Helpful exceptions for the unsupported cases Use function version 1 for all functions as a first step</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ContextResolvedTableSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ResolvedCatalogTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.CatalogManagerMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.BuiltInSqlOperator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.typeutils.SymbolUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.typeutils.SymbolUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
    </fixedFiles>
  </bug>
  <bug id="25386" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden table serialization in JSON plan</summary>
      <description>Similar to previous subtasks, we should revisit the JSON plan according to FLIP-190: Consider config option regarding catalog objects Helpful exceptions for the unsupported cases</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.UnionJsonPlanTest.jsonplan.testUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testOverwrite.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestDynamicTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.RexNodeExpression.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.CatalogTableSpecBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.TemporalTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.PlannerMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.jsonplan.testGetJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testChangelogSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
    </fixedFiles>
  </bug>
  <bug id="25388" opendate="2021-12-20 00:00:00" fixdate="2021-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Identify topology changing config options for all StreamExec nodes</summary>
      <description>Add an annotation to very ExecNode that we currently support Identify consumed exec config options</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeMetadata.java</file>
    </fixedFiles>
  </bug>
  <bug id="25391" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Updating existing table factories for mutable table options</summary>
      <description>Update all existing factories for FLINK-25390.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.json.md</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.AbstractFileSystemTable.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro-confluent.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBase.java</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.csv.md</file>
    </fixedFiles>
  </bug>
  <bug id="25392" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new STATEMENT SET syntax</summary>
      <description>Support:EXECUTE STATEMENT SETBEGIN INSERT INTO pageview_pv_sink SELECT page_id, count(1) FROM clicks GROUP BY page_id; INSERT INTO pageview_uv_sink SELECT page_id, count(distinct user_id) FROM clicks GROUP BY page_id;END;EXPLAIN STATEMENT SETBEGIN INSERT INTO pageview_pv_sink SELECT page_id, count(1) FROM clicks GROUP BY page_id; INSERT INTO pageview_uv_sink SELECT page_id, count(distinct user_id) FROM clicks GROUP BY page_id;END;This time we should add this to the SQL parser. We need to figure out a solution for the interactive SQL Client.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStatementSetExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.StatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.EndStatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.BeginStatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="25418" opendate="2021-12-22 00:00:00" fixdate="2021-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dir_cache is specified in the flink task. When there is no network, you will still download the python third-party library</summary>
      <description>Specified in Python code set_python_requirements(requirements_cache_dir=dir_cache)During task execution, priority will be given to downloading Python third-party packages from the networkï¼Can I directly use the python package in the cache I specify when I specify the cache value and don't want the task task to download the python package from the network</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonEnvironmentManagerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="25608" opendate="2022-1-11 00:00:00" fixdate="2022-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add stability annotations to metrics classes</summary>
      <description>With the introduction of architectural tests and the exposure of the metrics API to user-facing components the metrics classes also need proper annotations.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.View.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.SimpleCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.Scheduled.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.MetricReporterFactory.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.MetricReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.InterceptInstantiationViaReflection.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.InstantiateViaFactory.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricConfig.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Metric.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MeterView.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Meter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.LogicalScopeProvider.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.HistogramStatistics.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Histogram.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.SplitEnumeratorMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.SourceReaderMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.SinkWriterMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.OperatorMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.OperatorIOMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.OperatorCoordinatorMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Gauge.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Counter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.CharacterFilter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
    </fixedFiles>
  </bug>
  <bug id="2561" opendate="2015-8-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync Gelly Java and Scala APIs</summary>
      <description>There is some functionality and tests missing from the Gelly Scala API. This should be added, together with documentation, a completeness test and some usage examples.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.SingleSourceShortestPathsData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.ConnectedComponentsDefaultData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.ConnectedComponents.java</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.GraphMetrics.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphOperationsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.pom.xml</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GraphMetrics.java</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.ReduceOnNeighborMethodsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.ReduceOnEdgesMethodsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.MapVerticesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.MapEdgesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.JoinWithVerticesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.JoinWithEdgesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphMutationsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.DegreesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.VertexToTuple2Map.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.Tuple3ToEdgeMap.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.Tuple2ToVertexMap.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.EdgeToTuple3Map.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.Graph.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25611" opendate="2022-1-11 00:00:00" fixdate="2022-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove CoordinatorExecutorThreadFactory thread creation guards</summary>
      <description>The CoordinatorExecutorThreadFactory of the SourceCoordinator checks that only a single thread is active and that no new thread can be created if the previous one failed.Neither of these guards work properly. If a runnable in the ThreadPoolExecutor fails then it actually uses the worker thread of the failed runnable to spawn a new worker. This means that at the time the second thread is created the previous thread is still alive, and the exception that caused the failure hasn't even been propagated to the threads exception handler.As these guards do not work, and to boot result in the actual failure causes being hidden (like in FLINK-24855), we should remove them.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="25638" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default write buffer size of sort-shuffle to 16M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default write buffer size of sort-shuffle to 16M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BlockingShuffleITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25639" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default read buffer size of sort-shuffle to 64M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default read buffer size of sort-shuffle to 64M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25645" opendate="2022-1-13 00:00:00" fixdate="2022-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnsupportedOperationException would thrown out when hash shuffle by a field with array type</summary>
      <description>Currently array type is not supported as hash shuffle key because CodeGen does not support it yet. An unsupportedOperationException would thrown out when hash shuffle by a field with array type,</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.HashFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25646" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document buffer debloating issues with high parallelism</summary>
      <description>According to last benchmarks, there are some problems with buffer debloat when job has high parallelism. The high parallelism means the different value from job to job but in general it is more than 200. So it makes sense to document that problem and propose the solution - increasing the number of buffers.</description>
      <version>1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="25650" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document unaligned checkpoints performance limitations (larger records/flat map/timers/...)</summary>
      <description>The unaligned checkpoint can be delayed if the current record is consumed too long(because it is too large or it is the flat map etc.). Which can be pretty confused. So it makes sense to document this limitation to give the user understanding of this situation.</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  <bug id="26132" opendate="2022-2-14 00:00:00" fixdate="2022-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always serialize anonymous table</summary>
      <description>Currently, we throw an exception for anonymous tables if CatalogPlanCompilation.IDENTIFIER is set. However, we don't consider anonymous tables as catalog table and should always try to serialize them.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ResolvedCatalogTableSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ContextResolvedTableSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSpecBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ResolvedCatalogTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ContextResolvedTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ContextResolvedTableJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.CompiledPlan.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="26506" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="26607" opendate="2022-3-11 00:00:00" fixdate="2022-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>There are multiple MAX_LONG_VALUE value errors in pyflink code</summary>
      <description>There are multiple MAX_LONG_VALUE values sys. In pyflink code maxsizeMAX_LONG_VALUE = sys.maxsizemaxsizeÂ attribute of theÂ sysÂ module fetches the largest value aÂ variable of data typeÂ Py_ssize_tÂ **Â can store. It is the Python platformâs pointerÂ thatÂ dictates the maximum size of lists and strings in Python. The size value returned by maxsize depends on the platform architecture: 32-bit:Â the value will be 2^31 â 1, i.e.Â 2147483647 64-bit:Â the value will be 2^63 â 1, i.e.Â 9223372036854775807</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.functions.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.window.window.operator.py</file>
      <file type="M">flink-python.pyflink.datastream.window.py</file>
    </fixedFiles>
  </bug>
  <bug id="26842" opendate="2022-3-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-python Scala dependency</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingSinks.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingFunctions.scala</file>
      <file type="M">flink-python.src.test.scala.org.apache.flink.table.legacyutils.legacyTestingDescriptors.scala</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27026" opendate="2022-4-2 00:00:00" fixdate="2022-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle plugin</summary>
      <description>Newer versions of the checkstyle plugin allow running checkstyle:check without requiring dependency resolution. This allows it to be used in a fresh environment.</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27027" opendate="2022-4-2 00:00:00" fixdate="2022-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get rid of oddly named mvn-${sys mvn.forkNumber}.log files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="27108" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="27223" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State access doesn&amp;#39;t work as expected when cache size is set to 0</summary>
      <description>For the following job:import jsonimport loggingimport sysfrom pyflink.common import Types, Configurationfrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.util.java_utils import get_j_env_configurationif __name__ == '__main__': logging.basicConfig(stream=sys.stdout, level=logging.INFO, format="%(message)s") env = StreamExecutionEnvironment.get_execution_environment() config = Configuration( j_configuration=get_j_env_configuration(env._j_stream_execution_environment)) config.set_integer("python.state.cache-size", 0) env.set_parallelism(1) # define the source ds = env.from_collection( collection=[ (1, '{"name": "Flink", "tel": 123, "addr": {"country": "Germany", "city": "Berlin"}}'), (2, '{"name": "hello", "tel": 135, "addr": {"country": "China", "city": "Shanghai"}}'), (3, '{"name": "world", "tel": 124, "addr": {"country": "USA", "city": "NewYork"}}'), (4, '{"name": "PyFlink", "tel": 32, "addr": {"country": "China", "city": "Hangzhou"}}') ], type_info=Types.ROW_NAMED(["id", "info"], [Types.INT(), Types.STRING()]) ) # key by ds = ds.map(lambda data: (json.loads(data.info)['addr']['country'], json.loads(data.info)['tel'])) \ .key_by(lambda data: data[0]).sum(1) ds.print() env.execute()The expected result should be:('Germany', 123)('China', 135)('USA', 124)('China', 167)However, the actual result is:('Germany', 123)('China', 135)('USA', 124)('China', 32)</description>
      <version>1.14.0</version>
      <fixedVersion>1.14.5,1.15.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="27224" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop redundant flink.forkCountTestPackage property</summary>
      <description>This property has been identical to flink.forkCount for as long as I can remember. We should be able to get rid of it.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="27225" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant reuseForks settings</summary>
      <description>A number of modules set reuseForks without it having any effect because of it being enabled by default and/or the module not containing unit tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27251" opendate="2022-4-14 00:00:00" fixdate="2022-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timeout aligned to unaligned checkpoint barrier in the output buffers of an upstream subtask</summary>
      <description>After FLINK-23041, the downstream task can be switched UC when currentTime - triggerTime &gt; timeout. But the downstream task still needs wait for all barriers of upstream.Â If the back pressure is serve, the downstream task cannot receive all barrier within CP timeout, causes CP to fail.Â Can we support upstream Task switching from Aligned to UC? It means that when the barrier cannot be sent from the output buffer to the downstream task within the execution.checkpointing.aligned-checkpoint-timeout, the upstream task switches to UC and takes a snapshot of the data before the barrier in the output buffer.Â Hi akalashnikov , please help take a look in your free time, thanks a lot.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputChannelTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.MockChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="27252" opendate="2022-4-14 00:00:00" fixdate="2022-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove surefire fork options from connector-hive</summary>
      <description>Cleanup of unnecessary settings, that will also slightly speed up testing.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27253" opendate="2022-4-14 00:00:00" fixdate="2022-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from connector-cassandra</summary>
      <description>With the recent improvements around the cassandra test stability we can clean up some technical debt.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27544" opendate="2022-5-9 00:00:00" fixdate="2022-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="28288" opendate="2022-6-29 00:00:00" fixdate="2022-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support decode and encode built-in function in the Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31170" opendate="2023-2-21 00:00:00" fixdate="2023-1-21 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The spelling error of the document word causes sql to fail to execute</summary>
      <description>The spelling error of the document word causes sql to fail to execute</description>
      <version>1.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
