<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="5406" opendate="2017-1-4 00:00:00" fixdate="2017-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add normalization phase for predicate logical plan rewriting between decorrelate query phase and volcano optimization phase</summary>
      <description>Normalization phase is for predicate logical plan rewriting and is independent of cost module. The rules in normalization phase do not need to repeatedly applied to different logical plan which is different to volcano optimization phase. And the benefit of normalization phase is to reduce the running time of volcano planner.ReduceExpressionsRule can apply various simplifying transformations on RexNode trees. Currently, there are two transformations:1) Constant reduction, which evaluates constant subtrees, replacing them with a corresponding RexLiteral2) Removal of redundant casts, which occurs when the argument into the cast is the same as the type of the resulting cast expressionthe above transformations do not depend on the cost module, so we can move the rules in ReduceExpressionsRule from DATASET_OPT_RULES/DATASTREAM_OPT_RULES to DataSet/DataStream Normalization Rules.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.AggregationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6367" opendate="2017-4-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support custom header settings of allow origin</summary>
      <description>`jobmanager.web.access-control-allow-origin`: Enable custom access control parameter for allow origin header, default is `*`.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="6988" opendate="2017-6-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Apache Kafka 0.11 connector</summary>
      <description>Kafka 0.11 (it will be released very soon) add supports for transactions. Thanks to that, Flink might be able to implement Kafka sink supporting "exactly-once" semantic. API changes and whole transactions support is described in KIP-98.The goal is to mimic implementation of existing BucketingSink. New FlinkKafkaProducer011 would upon creation begin transaction, store transaction identifiers into the state and would write all incoming data to an output Kafka topic using that transaction on `snapshotState` call, it would flush the data and write in state information that current transaction is pending to be committed on `notifyCheckpointComplete` we would commit this pending transaction in case of crash between `snapshotState` and `notifyCheckpointComplete` we either abort this pending transaction (if not every participant successfully saved the snapshot) or restore and commit it.</description>
      <version>1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ProducerITCase.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011Tests.java</file>
    </fixedFiles>
  </bug>
  <bug id="7005" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimization steps are missing for nested registered tables</summary>
      <description>Tables that are registered (implicitly or explicitly) do not pass the first three optimization steps: decorrelate convert time indicators normalize the logical planE.g. this has the wrong plan right now:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val table1 = tEnv.sql(s"""SELECT 1 + 1 FROM $table""") // not optimizedval table2 = tEnv.sql(s"""SELECT myrt FROM $table1""")val results = table2.toAppendStream[Row]</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7011" opendate="2017-6-27 00:00:00" fixdate="2017-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Instable Kafka testStartFromKafkaCommitOffsets failures on Travis</summary>
      <description>Example:https://s3.amazonaws.com/archive.travis-ci.org/jobs/246703474/log.txt?X-Amz-Expires=30&amp;X-Amz-Date=20170627T065647Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170627/us-east-1/s3/aws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=dbfc90cfc386fef0990325b54ff74ee4d441944687e7fdaa73ce7b0c2b2ec0eaIn general, the test testStartFromKafkaCommitOffsets implementation is a bit of an overkill. Before continuing with the test, it writes some records just for the sake of committing offsets to Kafka and waits for some offsets to be committed (which leads to the instability), whereas we can do that simply using the test base's OffsetHandler.</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7025" opendate="2017-6-28 00:00:00" fixdate="2017-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using NullByteKeySelector for Unbounded ProcTime NonPartitioned Over</summary>
      <description>Currently we added `Cleanup State` feature. But It not work well if we enabled the stateCleaning on Unbounded ProcTime NonPartitioned Over window, Because in `ProcessFunctionWithCleanupState` we has using the keyed state.So, In this JIRA. I'll change the `Unbounded ProcTime NonPartitioned Over` to `partitioned Over` by using NullByteKeySelector. OR created a `NonKeyedProcessFunctionWithCleanupState`. But I think the first way is simpler. What do you think? Fabian Hueske</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedNonPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7026" opendate="2017-6-28 00:00:00" fixdate="2017-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add shaded asm dependency</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.maven.checkstyle.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-recipes.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DependencyVisitor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzerUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.TaggedValue.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.NestedMethodAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMFrame.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="703" opendate="2014-6-9 00:00:00" fixdate="2014-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use complete element as join key.</summary>
      <description>In some situations such as semi-joins it could make sense to use a complete element as join key. Currently this can be done using a key-selector function, but we could offer a shortcut for that.This is not an urgent issue, but might be helpful.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/703Created by: fhueskeLabels: enhancement, java api, user satisfaction, Milestone: Release 0.6 (unplanned)Created at: Thu Apr 17 23:40:00 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.JoinOperatorTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.JoinITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupingTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.CoGroupOperatorTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.CoGroupITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.unfinishedKeyPairOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.postpass.JavaApiPostPass.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GroupCombineOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CoGroupOperatorBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7030" opendate="2017-6-28 00:00:00" fixdate="2017-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build with scala-2.11 by default</summary>
      <description>As proposed recently on the dev mailing list.I propose to switch to Scala 2.11 as a default and to have a Scala 2.10 build profile. Now it is the other way around. The reason for that is poor support for build profiles in Intellij, I was unable to make it work after I added Kafka 0.11 dependency (Kafka 0.11 dropped support for Scala 2.10).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">docs.setup.building.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="7044" opendate="2017-6-29 00:00:00" fixdate="2017-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods to the client API that take the stateDescriptor.</summary>
      <description></description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.AbstractQueryableStateITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.QueryableStateClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="7051" opendate="2017-6-30 00:00:00" fixdate="2017-11-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Bump up Calcite version to 1.14</summary>
      <description>This is an umbrella issue for all tasks that need to be done once Apache Calcite 1.14 is released.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.AggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.GeneratedAggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetFinalAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.catalog.ExternalCatalogSchema.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7058" opendate="2017-6-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-scala-shell unintended dependencies for scala 2.11</summary>
      <description>Activation of profile scala-2.10 in `flink-scala-shell` and `flink-scala` do not work as intended. &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;scala-2.10&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;!scala-2.11&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalamacros&lt;/groupId&gt; &lt;artifactId&gt;quasiquotes_2.10&lt;/artifactId&gt; &lt;version&gt;${scala.macros.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;jline&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;activation&gt;&lt;/activation&gt;This activation IMO have nothing to do with `-Pscala-2.11` profile switch used in our build. "properties" are defined by `-Dproperty` switches. As far as I understand that, those additional dependencies would be added only if nobody defined property named `scala-2.11`, which means, they would be added only if switch `-Dscala-2.11` was not used, so it seems like those dependencies were basically added always. This quick test proves that I'm correct:$ mvn dependency:tree -pl flink-scala | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.11 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.10 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compileregardless of the selected profile those dependencies are always there.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7061" opendate="2017-7-2 00:00:00" fixdate="2017-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix quantifier range starting from 0</summary>
      <description>Currently, there is a bug in quantifier range implementation that for times(0, m), it will match m+1 events at most.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.pattern.PatternTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.TimesRangeITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
    </fixedFiles>
  </bug>
  <bug id="7062" opendate="2017-7-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the basic functionality of MATCH_RECOGNIZE</summary>
      <description>In this JIRA, we will support the basic functionality of MATCH_RECOGNIZE in Flink SQL API which includes the support of syntax MEASURES, PATTERN and DEFINE. This would allow users write basic cep use cases with SQL like the following example:SELECT T.aid, T.bid, T.cidFROM MyTableMATCH_RECOGNIZE ( MEASURES A.id AS aid, B.id AS bid, C.id AS cid PATTERN (A B C) DEFINE A AS A.name = 'a', B AS B.name = 'b', C AS C.name = 'c') AS T</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CepITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternFlatSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.MatchUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.ConvertToRow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.Indenter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7063" opendate="2017-7-3 00:00:00" fixdate="2017-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test instability in OperatorStateBackendTest.testSnapshotAsyncCancel</summary>
      <description>OperatorStateBackendTest.testSnapshotAsyncCancel seems to be instable and sometimes fails:testSnapshotAsyncCancel(org.apache.flink.runtime.state.OperatorStateBackendTest) Time elapsed: 0.036 sec &lt;&lt;&lt; ERROR!java.util.concurrent.ExecutionException: java.io.IOException: Stream closed. at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:206) at org.apache.flink.runtime.state.OperatorStateBackendTest.testSnapshotAsyncCancel(OperatorStateBackendTest.java:636)Caused by: java.io.IOException: Stream closed. at org.apache.flink.runtime.util.BlockerCheckpointStreamFactory$1.write(BlockerCheckpointStreamFactory.java:95) at java.io.DataOutputStream.writeInt(DataOutputStream.java:197) at org.apache.flink.core.io.VersionedIOReadableWritable.write(VersionedIOReadableWritable.java:40) at org.apache.flink.runtime.state.OperatorBackendSerializationProxy.write(OperatorBackendSerializationProxy.java:65) at org.apache.flink.runtime.state.DefaultOperatorStateBackend$1.performOperation(DefaultOperatorStateBackend.java:255) at org.apache.flink.runtime.state.DefaultOperatorStateBackend$1.performOperation(DefaultOperatorStateBackend.java:233) at org.apache.flink.runtime.io.async.AbstractAsyncIOCallable.call(AbstractAsyncIOCallable.java:72) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)logs: https://s3.amazonaws.com/archive.travis-ci.org/jobs/248822546/log.txt?X-Amz-Expires=30&amp;X-Amz-Date=20170703T092940Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170703/us-east-1/s3/aws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f468cd238236d7038a1e12086dd4a0e3ba538d93c883790d180e4c63b973a5f2 https://transfer.sh/MHawk/17392.5.tar.gz</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.async.AsyncStoppableTaskWithCallback.java</file>
    </fixedFiles>
  </bug>
  <bug id="7069" opendate="2017-7-3 00:00:00" fixdate="2017-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Catch exceptions for each reporter separately</summary>
      <description>The metric system can be effectively disabled by a reporter that throws exceptions whenever it is notified of adding metrics.The reason is that the catching of exceptions isn't granular enough, as this peace of psude code shows:addMetric(metric): try for reporter in reporters: reporter.addMetric(metric) metricQueryService.addMetric(metric) catch (e) logError(e) If a reporter throws an exception we never even attempt the other reporters, not notify the MQS which disabled metrics in the WebUI.</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="7107" opendate="2017-7-5 00:00:00" fixdate="2017-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start Yarn session via start-up script</summary>
      <description>The yarn-session.sh shell script should be adapted to be able to also start a Flip-6 Yarn session cluster (YarnSessionClusterEntrypoint).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7108" opendate="2017-7-5 00:00:00" fixdate="2017-7-5 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement Session cluster entry point</summary>
      <description>Implement a Yarn session cluster entry point.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptorV2.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="7123" opendate="2017-7-7 00:00:00" fixdate="2017-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support timesOrMore in CEP</summary>
      <description>The CEP API should provide API such as timesOrMore(#ofTimes) for quantifier {n,}.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="7124" opendate="2017-7-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to rescale JobGraph on JobManager</summary>
      <description>In order to support dynamic scaling, the JobManager has to be able to change the parallelism of the submitted JobGraph. This basically entails that we can change the parallelism settings on the cluster side.We already have the functionality that we can change the parallelism if it was set to ExecutionConfig.PARALLELISM_AUTO_MAX. Therefore, I think the task is mostly about making sure that things really properly work when requesting a parallelism change.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRescalingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7133" opendate="2017-7-7 00:00:00" fixdate="2017-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Elasticsearch version interference</summary>
      <description>At least two users have encountered problems with shading in the Elasticsearch connector: https://lists.apache.org/thread.html/b5bc1f690dc894ea9a8b69e82c89eb89ba6dfc2fec2588d2ccacee2c@%3Cuser.flink.apache.org%3E https://lists.apache.org/thread.html/2356670d168f61c20e34611e3c4aeb9c9b3f959f23a9833f631da1ba@%3Cuser.flink.apache.org%3EThe problem seems to be (quote from the second mail):I've found out the source of the problem when I build flink locally.elastic-search base depends on (by default) ES version 1.7.1 that depends onasm 4.1 and that version is shaded to elasticsearch-base-jar. I tried to setelasticsearch.version property in Maven to 5.1.2 (the same as elasticsearch5connector) but then elasticsearch-base does not compile:[ERROR] Failed to execute goalorg.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile(default-testCompile) on project flink-connector-elasticsearch-base_2.11:Compilation failure[ERROR]/home/adebski/Downloads/flink-release-1.3.1/flink-connectors/flink-connector-elasticsearch-base/src/test/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBaseTest.java:[491,92]no suitable constructor found forBulkItemResponse(int,java.lang.String,org.elasticsearch.action.ActionResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.DocWriteResponse)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.DocWriteResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.bulk.BulkItemResponse.Failure)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.bulk.BulkItemResponse.Failure)To me, it seems like we have to get rid of the "base" package and have two completely separate packages.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7143" opendate="2017-7-10 00:00:00" fixdate="2017-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition assignment for Kafka consumer is not stable</summary>
      <description>Important Notice: Upgrading jobs from 1.2.x exhibits no known problems. Jobs from 1.3.0 and 1.3.1 with incorrect partition assignments cannot be automatically fixed by upgrading to Flink 1.3.2 via a savepoint, because the upgraded version would resume the wrong partition assignment from the savepoint. A workaround is to assign a different uuid to the Kafka source (so the offsets won't be resumed from the savepoint) and let it start from the latest offsets committed to Kafka instead. Note that this may violate exactly-once semantics and introduce some duplicates, because Kafka's committed offsets are not guaranteed to be 100% up date date with Flink's internal offset tracking. To maximize the alignment between the offsets in Kafka and those tracked by Flink, we suggest to abort the 1.3.x job via the "cancel with savepoint" command (https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html#cancel-job-with-savepoint) during the upgrade process.Original Issue DescriptionWhile deploying Flink 1.3 release to hundreds of routing jobs, we found some issues with partition assignment for Kafka consumer. some partitions weren't assigned and some partitions got assigned more than once.Here is the bug introduced in Flink 1.3. protected static void initializeSubscribedPartitionsToStartOffsets(...) { ... for (int i = 0; i &lt; kafkaTopicPartitions.size(); i++) { if (i % numParallelSubtasks == indexOfThisSubtask) { if (startupMode != StartupMode.SPECIFIC_OFFSETS) { subscribedPartitionsToStartOffsets.put(kafkaTopicPartitions.get(i), startupMode.getStateSentinel()); } ... }The bug is using array index i to mod against numParallelSubtasks. if the kafkaTopicPartitions has different order among different subtasks, assignment is not stable cross subtasks and creates the assignment issue mentioned earlier. fix is also very simple, we should use partitionId to do the mod if (kafkaTopicPartitions.get&amp;#40;i&amp;#41;.getPartition() % numParallelSubtasks == indexOfThisSubtask). That would result in stable assignment cross subtasks that is independent of ordering in the array.marking it as blocker because of its impact.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscovererTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7147" opendate="2017-7-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support greedy quantifier in CEP</summary>
      <description>Greedy quantifier will try to match the token as many times as possible. For example, for pattern a b* c (skip till next is used) and inputs a b1 b2 c, if the quantifier for b is greedy, it will only output a b1 b2 c.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.StateTransition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="7170" opendate="2017-7-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix until condition when the contiguity is strict</summary>
      <description>When the contiguity is STRICT, the method extendWithUntilCondition is not correct.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.UntilConditionITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="7174" opendate="2017-7-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump dependency of Kafka 0.10.x to the latest one</summary>
      <description>We are using pretty old Kafka version for 0.10. Besides any bug fixes and improvements that were made between 0.10.0.1 and 0.10.2.1, it 0.10.2.1 version is more similar to 0.11.0.</description>
      <version>1.2.1,1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7181" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/operators/*</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.NamesTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UdfOperatorUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.OperatorTranslation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Operator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.KeyFunctions.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.join.JoinType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.join.JoinOperatorSetsBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.IterativeDataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Grouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FlatMapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FilterOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIterationResultSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIteration.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CustomUnaryOperation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupRawOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.BulkIterationResultSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.AggregateOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7182" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesPrecedenceTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SelectByFunctionsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.ClosureCleanerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SemanticPropUtil.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SelectByMinFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SelectByMaxFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SampleWithFraction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SampleInPartition.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SampleInCoordinator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.IdPartitioner.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.GroupReduceIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FunctionAnnotation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FormattingMapper.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FlatMapIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FirstReducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7183" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/aggregation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.UnsupportedAggregationTypeException.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.SumAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MinAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MaxAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AvgAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.Aggregations.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AggregationFunctionFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AggregationFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7184" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/hadoop</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.wrapper.HadoopDummyReporter.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.wrapper.HadoopDummyProgressable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.utils.HadoopUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormatBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormatBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.utils.HadoopUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormatBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.common.HadoopOutputFormatCommonBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.common.HadoopInputFormatCommonBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7185" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/io</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.TypeSerializerFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.TextInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.RowCsvInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrimitiveInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.FromElementsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CollectionInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.TypeSerializerOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.TupleCsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.TextValueInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.TextOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.TextInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.SplitDataProperties.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.RowCsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.PrintingOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.PrimitiveInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.PojoCsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.ParallelIteratorInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.LocalCollectionOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.IteratorInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.DiscardingOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CollectionInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="7187" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/sca</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerExamplesTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzerUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.TaggedValue.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.NestedMethodAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMFrame.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.CodeErrorException.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.CodeAnalyzerException.java</file>
    </fixedFiles>
  </bug>
  <bug id="7188" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/summarize</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.SummaryAggregatorFactoryTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.StringValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.StringSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.ShortValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.ShortSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.LongValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.LongSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.IntegerValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.IntegerSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.FloatValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.FloatSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.DoubleValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.DoubleSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.CompensatedSumTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.BooleanValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.BooleanSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.AggregateCombineHarness.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.StringColumnSummary.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.ObjectColumnSummary.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.NumericColumnSummary.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.ColumnSummary.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.BooleanColumnSummary.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.ValueSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.TupleSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.SummaryAggregatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.StringSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.ShortSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.ObjectSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.NumericSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.LongSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.IntegerSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.FloatSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.DoubleSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.CompensatedSum.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.BooleanSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.Aggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7189" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/utils</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.RequiredParametersTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.AbstractParameterToolTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.RequiredParametersException.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.RequiredParameters.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.ParameterTool.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.OptionType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.Option.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7190" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/*</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.TypeExtractionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.MultipleInvokationsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionWithBroadcastVariableTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionIterationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionAccumulatorsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.ReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.InnerJoinOperatorBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.CoGroupOperatorCollectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.io.SerializedFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.io.SequentialFormatTestBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.LocalEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironmentFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.CollectionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="7191" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/operators/translation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.ReduceTranslationTests.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DistinctTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DeltaIterationTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.CoGroupSortTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.AggregateTranslationTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.WrappingFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TwoKeyExtractingMapper.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleWrappingCollector.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleUnwrappingJoiner.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleUnwrappingIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleRightUnwrappingJoiner.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleLeftUnwrappingJoiner.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.Tuple3WrappingCollector.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.Tuple3UnwrappingIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.RichCombineToGroupCombineWrapper.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingSortedReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingSortedGroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingGroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanFilterOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.KeyRemovingMapper.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.KeyExtractingMapper.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.CombineToGroupCombineWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="7192" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle flink-java/test/operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.SortPartitionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.RightOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.PartitionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.OperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.LeftOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupCombineOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.FullOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.FirstNOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DataSinkTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.AggregateOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7202" opendate="2017-7-15 00:00:00" fixdate="2017-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split supressions for flink-core, flink-java, flink-optimizer per package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-optimizer.xml</file>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">tools.maven.suppressions-core.xml</file>
      <file type="M">flink-optimizer.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7227" opendate="2017-7-19 00:00:00" fixdate="2017-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OR expression with more than 2 predicates is not pushed into a TableSource</summary>
      <description>It seems that RexNodeToExpressionConverter cannot handle OR expressions with more than 2 predicates. Therefore the expression is not pushed into a FilterableTableSource.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TestFilterableTableSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7228" opendate="2017-7-19 00:00:00" fixdate="2017-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden HistoryServerStaticFileHandlerTest</summary>
      <description>We can harden the test to use a free port instead of the hard-coded 8081.</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7231" opendate="2017-7-19 00:00:00" fixdate="2017-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SlotSharingGroups are not always released in time for new restarts</summary>
      <description>In the case where there are not enough resources to schedule the streaming program, a race condition can lead to a sequence of the following errors:java.lang.IllegalStateException: SlotSharingGroup cannot clear task assignment, group still has allocated resources.This eventually recovers, but may involve many fast restart attempts before doing so.The root cause is that slots are not cleared before the next restart attempt.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="7258" opendate="2017-7-24 00:00:00" fixdate="2017-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalArgumentException in Netty bootstrap with large memory state segment size</summary>
      <description>In NettyBootstrap we configure the low and high watermarks in the following order:bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, config.getMemorySegmentSize() + 1);bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, 2 * config.getMemorySegmentSize());When the memory segment size is higher than the default high water mark, this throws an `IllegalArgumentException` when a client tries to connect. Hence, this unfortunately only happens during runtime when a intermediate result is requested. This doesn't fail the job, but logs a warning and ignores the failed configuration attempt, potentially resulting in degraded performance because of a lower than expected watermark.A simple fix is to first configure the high water mark and only then configure the low watermark.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyServerLowAndHighWatermarkTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.netty.KvStateServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7266" opendate="2017-7-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t attempt to delete parent directory on S3</summary>
      <description>Currently, every attempted release of an S3 state object also checks if the "parent directory" is empty and then tries to delete it.Not only is that unnecessary on S3, but it is prohibitively expensive and for example causes S3 to throttle calls by the JobManager on checkpoint cleanup.The FileState must only attempt parent directory cleanup when operating against real file systems, not when operating against object stores.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="7294" opendate="2017-7-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mesos.resourcemanager.framework.role not working</summary>
      <description>I am using the above said setting in flink-conf.yamle.g.mesos.resourcemanager.framework.role: mesos_role_tasksI see a flink-scheduler registered in mesos/frameworks tab with above said role.But the scheduler fails to launch any tasks inspite of getting resource-offers from mesos-agents with correct role.The error seen is:2017-07-28 13:23:00,683 INFO org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManager - Mesos task taskmanager-03768 failed, with a TaskManager in launch or registration. State: TASK_ERROR Reason: REASON_TASK_INVALID (Task uses more resources cpus(\*):1; mem(\*):1024; ports(\*):[4006-4007] than available cpus(mesos_role_tasks):7.4; mem(mesos_role_tasks):45876; ports(mesos_role_tasks):[4002-4129, 4131-4380, 4382-4809, 4811-4957, 4959-4966, 4968-4979, 4981-5049, 31000-31196, 31198-31431, 31433-31607, 31609-32000]; ephemeral_storage(mesos_role_tasks):37662; efs_storage(mesos_role_tasks):8.79609e+12; disk(mesos_role_tasks):5115)The request is made for resources with * role. We do not have mesos running anywhere with * role. Thus task manager never come up. Am I missing any configuration?I am using flink version 1.3.1</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
    </fixedFiles>
  </bug>
  <bug id="7309" opendate="2017-7-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException in CodeGenUtils.timePointToInternalCode() generated code</summary>
      <description>The code generated by CodeGenUtils.timePointToInternalCode() will cause a NullPointerException when SQL table field type is `TIMESTAMP` and the field value is `null`.Example for reproduce:object StreamSQLExample { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) // null field value val orderA: DataStream[Order] = env.fromCollection(Seq( Order(null, "beer", 3))) tEnv.registerDataStream("OrderA", orderA, 'ts, 'product, 'amount) val result = tEnv.sql("SELECT * FROM OrderA") result.toAppendStream[Order].print() env.execute() } case class Order(ts: Timestamp, product: String, amount: Int)}In the above example, timePointToInternalCode() will generated some statements like this:... long result$1 = org.apache.calcite.runtime.SqlFunctions.toLong((java.sql.Timestamp) in1.ts()); boolean isNull$2 = (java.sql.Timestamp) in1.ts() == null;...so, the NPE will happen when in1.ts() is null.</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7357" opendate="2017-8-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HOP_START() HOP_END() does not work when using HAVING clause with GROUP BY HOP window</summary>
      <description>The following SQL does not compile:invalid_having_hop_start_sqlSELECT c AS k, COUNT(a) AS v, HOP_START(rowtime, INTERVAL '1' MINUTE, INTERVAL '1' MINUTE) AS windowStart, HOP_END(rowtime, INTERVAL '1' MINUTE, INTERVAL '1' MINUTE) AS windowEnd FROM T1 GROUP BY HOP(rowtime, INTERVAL '1' MINUTE, INTERVAL '1' MINUTE), c HAVING SUM(b) &gt; 1While individually keeping HAVING clause or HOP_START field compiles and runs without issue.more details: https://github.com/apache/flink/compare/master...walterddr:having_does_not_work_with_hop_start_end</description>
      <version>1.3.1</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7491" opendate="2017-8-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink SQL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.MapTypeInfo.java</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="7658" opendate="2017-9-20 00:00:00" fixdate="2017-2-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink TABLE API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="8897" opendate="2018-3-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9031" opendate="2018-3-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet Job result changes when adding rebalance after union</summary>
      <description>A user reported this issue on the user mailing list.I am using Flink 1.3.1 and I have found a strange behavior on running the following logic: Read data from file and store into DataSet&lt;POJO&gt; Split dataset in two, by checking if "field1" of POJOs is empty or not, so that the first dataset contains only elements with non empty "field1", and the second dataset will contain the other elements. Each dataset is then grouped by, one by "field1" and other by another field, and subsequently reduced. The 2 datasets are merged together by union. The final dataset is written as json.What I was expected, from output, was to find only one element with a specific value of "field1" because: Reducing the first dataset grouped by "field1" should generate only one element with a specific value of "field1". The second dataset should contain only elements with empty "field1". Making an union of them should not duplicate any record.This does not happen. When i read the generated jsons i see some duplicate (non empty) values of "field1". Strangely this does not happen when the union between the two datasets is not computed. In this case the first dataset produces elements only with distinct values of "field1", while second dataset produces only records with empty field "value1".The user has not enable object reuse.Later he reports that the problem disappears when he injects a rebalance() after a union resolves the problem. I had a look at the execution plans for both cases (attached to this issue) but could not identify a problem.Hence I assume, this might be an issue with the runtime code but we need to look deeper into this. The user also provided an example program consisting of two classes which are attached to the issue as well.   </description>
      <version>1.3.1</version>
      <fixedVersion>1.3.4,1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.UnionReplacementTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.GraphCreatingVisitor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.Optimizer.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.BinaryUnionNode.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
