<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10791" opendate="2018-11-5 00:00:00" fixdate="2018-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide end-to-end test for Kafka 0.11 connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10801" opendate="2018-11-6 00:00:00" fixdate="2018-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sql client integrate elasticsearch connector test failure</summary>
      <description>It usually reports : FAIL SQL Client Elasticsearch Upsert: Output hash mismatch. Got 6187222e109ee9222e6b2f117742070c, expected 982cb32908def9801e781381c1b8a8db.head hexdump of actual:0000000 { \n " h i t s " : { \n 0000010 " t o t a l " : 3 , \n0000020 " m a x _ s c o r e " 0000030 : 1 . 0 , \n " h i t s0000040 " : [ \n { \n 0000050 " _ i n d e x " :0000060 " m y _ u s e r s " , \n 0000070 " _ t y p e " : "0000080 u s e r " , \n "0000090 _ i d " : " 1 _ B o b "00000a0 , \n " _ s c o r00000b0 e " : 1 . 0 , \n 00000bathe actual hash means : { "hits" : { "total" : 3, "max_score" : 1.0, "hits" : [ { "_index" : "my_users", "_type" : "user", "_id" : "1_Bob ", "_score" : 1.0, "_source" : { "user_id" : 1, "user_name" : "Bob ", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "22_Alice", "_score" : 1.0, "_source" : { "user_id" : 22, "user_name" : "Alice", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "42_Greg ", "_score" : 1.0, "_source" : { "user_id" : 42, "user_name" : "Greg ", "user_count" : 3 } } ] }}the expected hash code means : { "hits" : { "total" : 3, "max_score" : 1.0, "hits" : [ { "_index" : "my_users", "_type" : "user", "_id" : "1_Bob ", "_score" : 1.0, "_source" : { "user_id" : 1, "user_name" : "Bob ", "user_count" : 2 } }, { "_index" : "my_users", "_type" : "user", "_id" : "22_Alice", "_score" : 1.0, "_source" : { "user_id" : 22, "user_name" : "Alice", "user_count" : 1 } }, { "_index" : "my_users", "_type" : "user", "_id" : "42_Greg ", "_score" : 1.0, "_source" : { "user_id" : 42, "user_name" : "Greg ", "user_count" : 3 } } ] }}It seems that the user count for "Bob" is off by 1.The speculation is due to the premature acquisition of aggregated statistics from Elasticsearch. </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10921" opendate="2018-11-18 00:00:00" fixdate="2018-5-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Prioritize shard consumers in Kinesis Consumer by event time</summary>
      <description>Shard consumer threads currently emit records directly. In order to align shards by event time, decouple shard consumer threads and emitter with a queue, as described in &amp;#91;1&amp;#93;.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/ac41718246ad8f6098efaf7dbf5f7182d60abdc473e8bf3c96ef5968@%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="10922" opendate="2018-11-19 00:00:00" fixdate="2018-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor the placement of the Flink Kafka connector end to end test module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.RollingAdditionMapper.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaExampleUtil.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaEventSchema.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.KafkaEvent.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.src.main.java.org.apache.flink.streaming.examples.kafka.base.CustomWatermarkExtractor.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka011Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka011.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka011-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka010-test.src.main.scala.org.apache.flink.streaming.scala.kafka.test.Kafka010Example.scala</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka010-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10932" opendate="2018-11-19 00:00:00" fixdate="2018-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initial flink-kubernetes module with empty implementation</summary>
      <description>Initial the skeleton module to start native kubernetes integration. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10951" opendate="2018-11-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable enforcing of YARN container virtual memory limits in tests</summary>
      <description>DescriptionThe Jepsen YARN tests sporadically fail because TM containers are exceeding their virtual memory limits:Closing TaskExecutor connection container_1541436244107_0001_01_000005 because: Container [pid=32403,containerID=container_1541436244107_0001_01_000005] is running beyond virtual memory limits. Current usage: 970.2 MB of 2 GB physical memory used; 4.2 GB of 4.2 GB virtual memory used. Killing container.By default YARN enforces a virtual memory limit of 2.1 times the requested physical memory. However, in my experiments, the virtual memory of a JVM process running the ClusterEntryPoint (without submitting job) is already in the region of 3.3 GB. Hence, the virtual memory enforcement should be disabled.Acceptance Criteria yarn.nodemanager.vmem-check-enabled is false in yarn-site.xml</description>
      <version>1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10992" opendate="2018-11-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10995" opendate="2018-11-23 00:00:00" fixdate="2018-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Copy intermediate serialization results only once for broadcast mode</summary>
      <description>The emitted records from operator would be firstly serialized into intermediate bytes array in RecordSerializer, then copy the intermediate results into target buffers for different sub partitions.  For broadcast mode, the same intermediate results would be copied as many times as the number of sub partitions, and this would affect the performance seriously in large scale jobs.We can copy to only one target buffer which would be shared by all the sub partitions to reduce the overheads. For emitting latency marker in broadcast mode, we should flush the previous shared target buffers first, and then request a new buffer for the target sub partition to send latency marker.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.FileChannelBoundedDataTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.IteratorWrappingTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionAvailabilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferConsumer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11012" opendate="2018-11-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce abstract superclass for filesystem IT cases</summary>
      <description>IT cases for filesystems are somewhat similar as they are using write/read cycles and querying meta data like directory listing. We could avoid code duplication now and for the future by introducing an abstract superclass for those it cases that outlines the tests.</description>
      <version>1.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.test.java.org.apache.flink.fs.osshadoop.HadoopOSSFileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">docs.ops.deployment.oss.md</file>
    </fixedFiles>
  </bug>
  <bug id="11042" opendate="2018-11-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testFlinkKafkaProducerFailTransactionCoordinatorBeforeNotify is an invalid test</summary>
      <description>main point of testFlinkKafkaProducerFailTransactionCoordinatorBeforeNotify is to fail transaction coordinator (by using kafkaProducer.getTransactionCoordinatorId(); ) and we expect that this will cause failure of Flink job. However that's not always the case. Maybe because transaction coordinator can be re-elected before KafkaProducer even notices it or for whatever the reason, sometimes the failure is not happening.Because of a bug in the test, if failure hasn't happened, the test will not fail.Generally speaking this test is invalid and should be dropped.</description>
      <version>1.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="11126" opendate="2018-12-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter out AMRMToken in the TaskManager credentials</summary>
      <description>Currently, Flink JobManager propagates its storage tokens to TaskManager to meet the requirement of YARN log aggregation (see FLINK-6376). But in this way the AMRMToken is also included in the TaskManager credentials, which could be potentially insecure. We should filter out AMRMToken before setting the tokens to TaskManager's container launch context.</description>
      <version>1.6.2,1.6.4,1.7.0,1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11144" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Tests runnable on Java 9</summary>
      <description>When using Java 9, tests should run when invoking mvn clean install -DfastAcceptance criteria: Project compiles with mvn clean install -Dfast -DskipTests Tests are runnable but may not pass</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11154" opendate="2018-12-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Netty to 4.1.32</summary>
      <description>Notable changes since 4.1.24 (currently used in Flink 1.6-1.7): big (performance, feature set) improvements for using openSSL based SSL engine (useful for FLINK-9816) allow multiple shaded versions of the same netty artifact (as long as the shaded prefix is different) Ensure ByteToMessageDecoder.Cumulator implementations always release. Don't re-arm timerfd each epoll_wait Use a non-volatile read for ensureAccessible() whenever possible to reduce overhead and allow better inlining. Do not fail on runtime when an older version of Log4J2 is on the classpath Fix leak and corruption bugs in CompositeByteBuf Add support for TLSv1.3 Harden ref-counting concurrency semantics bug fixes Java 9-12 related fixes</description>
      <version>None</version>
      <fixedVersion>shaded-6.0,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.AbstractByteBufTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11161" opendate="2018-12-14 00:00:00" fixdate="2018-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to import java packages in scala-shell</summary>
      <description></description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11191" opendate="2018-12-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception in code generation when ambiguous columns in MATCH_RECOGNIZE</summary>
      <description>Query:SELECT *FROM TickerMATCH_RECOGNIZE ( PARTITION BY symbol, price ORDER BY proctime MEASURES A.symbol AS symbol, A.price AS price PATTERN (A) DEFINE A AS symbol = 'a') AS Tthrows a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. I've also created a calcite ticket to fix it on calcite's side: CALCITE-2747</description>
      <version>1.7.0,1.8.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11207" opendate="2018-12-20 00:00:00" fixdate="2018-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="11235" opendate="2018-12-31 00:00:00" fixdate="2018-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector leaks threads if no connection could be established</summary>
      <description>elasticsearch transport sink init steps1, create client thread2, try to check every host:port3, if each host:port is unreachable, while throw RuntimeExceptionbut, because of throw RuntimeException, the client can not close, so causing thread leaktransport client code```TransportClient transportClient = new PreBuiltTransportClient(settings);for (TransportAddress transport : ElasticsearchUtils.convertInetSocketAddresses(transportAddresses)) { transportClient.addTransportAddress(transport);}// verify that we actually are connected to a clusterif (transportClient.connectedNodes().isEmpty()) { throw new RuntimeException("Elasticsearch client is not connected to any Elasticsearch nodes!");}return transportClient;}```thread leakthread dump </description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
    </fixedFiles>
  </bug>
  <bug id="11262" opendate="2019-1-3 00:00:00" fixdate="2019-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump jython-standalone to 2.7.1</summary>
      <description>Due to security issue: https://ossindex.sonatype.org/vuln/7a4be7b3-74f5-4a9b-a24f-d1fd80ed6bbca</description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-libraries.flink-streaming-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-libraries.flink-streaming-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11288" opendate="2019-1-8 00:00:00" fixdate="2019-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add separate flink-ml module for building fat jar</summary>
      <description>Similar to sql-connectors the flink-ml fat jar , that is included in flink-dist, should be built in a separate module so that we can add proper licensing to it.</description>
      <version>1.7.1,1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11289" opendate="2019-1-8 00:00:00" fixdate="2019-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework example module structure to account for licensing</summary>
      <description>Some of our example bundle additional dependencies (like kafka). The example jars (that are deployed to maven central and included in flink-dist) should have proper licensing.Otherwise we would have to disable deployment of these jars, and keep the flink-dist NOTICE file in sync with example dependencies manually.One way to do this would be to add a separate module for each jar, or at the very least for those example that bundle dependencies.</description>
      <version>1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-twitter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11293" opendate="2019-1-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis</summary>
      <description>The KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis.20:26:29.579 [ERROR] Errors: 20:26:29.579 [ERROR] KafkaITCase.testConcurrentProducerConsumerTopology:73-&gt;KafkaConsumerTestBase.runSimpleConcurrentProducerConsumerTopology:824-&gt;KafkaTestBase.deleteTestTopic:206-&gt;Object.wait:502-&gt;Object.wait:-2 Â» TestTimedOutThe solution might as simple as increasing the timeout.https://api.travis-ci.org/v3/job/476975725/log.txt</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="11391" opendate="2019-1-18 00:00:00" fixdate="2019-6-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce ShuffleMaster in Job Master</summary>
      <description>Implement PartitionShuffleDescriptor for covering necessary abstract info. Implement ShuffleDeploymentDecriptor generated from PartitionShuffleDecriptor. Define ShuffleMaster interface and create a simple implementation on JM side which relies on currently implemented NetworkEnviroment on TM side. Define ShuffleManager interface for creating ShuffleMaster. Introduce a Flink configuration option for ShuffleManager implementation. Default value for it could be &lt;none&gt; which serves as a feature flag at the moment to use current code paths.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FinalizeOnMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.PartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="11422" opendate="2019-1-24 00:00:00" fixdate="2019-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add builder for creating StreamTask mocks</summary>
      <description></description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamSourceContextIdleDetectionTests.java</file>
    </fixedFiles>
  </bug>
  <bug id="11427" opendate="2019-1-24 00:00:00" fixdate="2019-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protobuf parquet writer implementation</summary>
      <description>Right now only ParquetAvroWriters exist to create ParquetWriterFactory. We want to implement a protobuf ParquetProtoWriters to create ParquetWriterFactory.  I am happy to submit a PR if this approach sounds good . </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="11442" opendate="2019-1-29 00:00:00" fixdate="2019-1-29 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Upgrade OSS SDK Version</summary>
      <description>Upgrade oss sdk version to exclude org.json dependency.&amp;#91;INFO&amp;#93; +- com.aliyun.oss:aliyun-sdk-oss:jar:3.1.0:compile&amp;#91;INFO&amp;#93; | +- org.apache.httpcomponents:httpclient:jar:4.5.3:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.apache.httpcomponents:httpcore:jar:4.4.6:compile&amp;#91;INFO&amp;#93; | +- org.jdom:jdom:jar:1.1:compile&amp;#91;INFO&amp;#93; | +- com.sun.jersey:jersey-json:jar:1.9:compile&amp;#91;INFO&amp;#93; | | +- org.codehaus.jettison:jettison:jar:1.1:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; stax:stax-api:jar:1.0.1:compile&amp;#91;INFO&amp;#93; | | +- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; javax.xml.bind:jaxb-api:jar:2.2.2:compile&amp;#91;INFO&amp;#93; | | | +- javax.xml.stream:stax-api:jar:1.0-2:compile&amp;#91;INFO&amp;#93; | | | &amp;#45; javax.activation:activation:jar:1.1:compile&amp;#91;INFO&amp;#93; | | +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.3:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.codehaus.jackson:jackson-xc:jar:1.8.3:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-core:jar:3.4.0:compile&amp;#91;INFO&amp;#93; | | &amp;#45; org.json:json:jar:20170516:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-ram:jar:3.0.0:compile&amp;#91;INFO&amp;#93; | +- com.aliyun:aliyun-java-sdk-sts:jar:3.0.0:compile&amp;#91;INFO&amp;#93; | &amp;#45; com.aliyun:aliyun-java-sdk-ecs:jar:4.2.0:compile The license of org.json:json:jar:20170516:compile is JSON License, which cannot be included.https://www.apache.org/legal/resolved.html#json</description>
      <version>1.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11469" opendate="2019-1-30 00:00:00" fixdate="2019-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="11518" opendate="2019-2-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add partition related catalog APIs</summary>
      <description>To support partitions, we need to introduce additional APIs on top of FLINK-11474.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableWritableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11519" opendate="2019-2-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add function related catalog APIs</summary>
      <description>This is to support functions (UDFs) with related to catalog.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableWritableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="11535" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client jar does not contain table-api-java</summary>
      <description>The SQL Client jar does not package flink-table-api-java.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11585" opendate="2019-2-12 00:00:00" fixdate="2019-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prefix matching in ConfigDocsGenerator can result in wrong assignments</summary>
      <description>There are some cases where the key-prefix matching does not work as intended: given the prefixes "a.b" and "a.b.c.d", then an option with a key "a.b.c.X" will be assigned to the default groups instead of "a.b" given a prefix "a.b", an option "a.c.b" will be matched to that group instead of the default</description>
      <version>1.6.3,1.7.1,1.8.0</version>
      <fixedVersion>1.6.4,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1159" opendate="2014-10-13 00:00:00" fixdate="2014-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case style anonymous functions not supported by Scala API</summary>
      <description>In Scala it is very common to define anonymous functions of the following form{case foo: Bar =&gt; foobar(foo)case _ =&gt; throw new RuntimeException()}These case style anonymous functions are not supported yet by the Scala API. Thus, one has to write redundant code to name the function parameter.What works is the following pattern, but it is not intuitive for someone coming from Scala:dataset.map{ _ match{ case foo:Bar =&gt; ... }}</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.JoinedStreams.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedStreams.scala</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.java8.md</file>
      <file type="M">docs.apis.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="1160" opendate="2014-10-13 00:00:00" fixdate="2014-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify record readers to allow publishing events to specific inputs (instead of broadcast) in the union record reader</summary>
      <description>In order to do efficient record acknowledgments for fault tolerance we need to modify record readers to allow publishing events to specific inputs in the union record reader. Currently only broadcasting is supported for all inputs which is unnecessary for the acknowledgements and poses significant network overhead.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.ReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.AbstractUnionRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.AbstractSingleGateRecordReader.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.CoRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="1161" opendate="2014-10-13 00:00:00" fixdate="2014-12-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Modify streaming api type extractor classes to allow java 8 lambdas</summary>
      <description>The classes used for type information extraction from user defined functions in the streaming API currently don't support java 8 lambdas. This issue can be easily solved in most cases by updating the used methods to match recent developments in the batch API.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.serialization.TypeSerializationTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.ProjectTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.TypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.ProjectTypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.ObjectTypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.FunctionTypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.CombineTypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.ClassTypeWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.OutputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.InputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.CoStreamVertex.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.ProjectInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.StreamProjection.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.StreamJoinOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SplitDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.GroupedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="11634" opendate="2019-2-16 00:00:00" fixdate="2019-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "State Backends" page into Chinese</summary>
      <description>doc locates in flink/docs/dev/stream/state/state_backens.md</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="11636" opendate="2019-2-16 00:00:00" fixdate="2019-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "State Schema Evolution" into Chinese</summary>
      <description>doc locates in flink/docs/dev/stream/state/schema_evolution.mdhttps://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.schema.evolution.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="11637" opendate="2019-2-16 00:00:00" fixdate="2019-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Checkpoints" page into Chinese</summary>
      <description>doc locates in flink/docs/ops/state/checkpoints.md</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="1164" opendate="2014-10-13 00:00:00" fixdate="2014-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uninformative error message when having an identity iteration with the Scala API</summary>
      <description>If one implements an identity mapper with the Scala API, then one gets a strange error message which might be very confusing for the user: Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: java.lang.Exception: Failed to deploy the task PartialSolution (BulkIteration (Bulk Iteration)) (1/8) - execution #0 to slot SubSlot 1 (0bbe935796159bfe18abca6f1b32769a (0) - ALLOCATED/ALIVE): java.lang.Exception: The number of writers created in 'registerInputOutput()' is different than the number of connected outgoing edges in the job graph.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.BulkIterationNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="11653" opendate="2019-2-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add configuration to enforce custom UID&amp;#39;s on datastream</summary>
      <description>Current best practice when deploying Flink applications to production is to set a custom UID, using DataStream#uid, so jobs can resume from savepoints even if they job graph has been modified. Flink should contain a configuration that can allow users to fail submission if their program contains an operator without a custom UID; enforcing best practices similarly to #disableGenericTypes.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.graph.StreamingJobGraphGeneratorNodeHashTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug id="11665" opendate="2019-2-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink fails to remove JobGraph from ZK even though it reports it did</summary>
      <description>We recently have seen the following issue with Flink 1.5.5:Given Flink Job ID 1d24cad26843dcebdfca236d5e3ad82a: 1- A job is activated successfully and the job graph added to ZK:Added SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null) to ZooKeeper.2- Job is deactivated, Flink reports that the job graph has been successfully removed from ZK and the blob is deleted from the blob server (in this case S3):Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.3- JM is later restarted, Flink for some reason attempts to recover the job that it reported earlier it has removed from ZK but since the blob has already been deleted the JM goes into a crash loop. The only way to recover it manually is to remove the job graph entry from ZK:Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null). andorg.apache.flink.fs.s3presto.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 1BCDFD83FC4546A2), S3 Extended Request ID: OzZtMbihzCm1LKy99s2+rgUMxyll/xYmL6ouMvU2eo30wuDbUmj/DAWoTCs9pNNCLft0FWqbhTo= (Path: s3://blam-state-staging/flink/default/blob/job_1d24cad26843dcebdfca236d5e3ad82a/blob_p-c51b25cc0b20351f6e32a628bb6e674ee48a273e-ccfa96b0fd795502897c73714185dde3)My question is under what circumstances would this happen? this seems to happen very infrequently but since the consequence is severe (JM crash loop) we'd like to understand how it would happen.This all seems a little similar to https://issues.apache.org/jira/browse/FLINK-9575 but that issue is reported fixed in Flink 1.5.2 and we are already on Flink 1.5.5</description>
      <version>1.5.5,1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDispatcherRunnerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerImplNGFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="11668" opendate="2019-2-20 00:00:00" fixdate="2019-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow sources to advance time to max watermark on checkpoint.</summary>
      <description>This is needed for the TERMINATE case. It will allow the sources to inject the MAX_WATERMARK before the barrier that will trigger the last savepoint. This will fire any registered event-time timers and flush any state associated with these timers, e.g. windows.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceExternalCheckpointTriggerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.RestoreStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11670" opendate="2019-2-20 00:00:00" fixdate="2019-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SUSPEND/TERMINATE calls to REST API</summary>
      <description>Exposes the SUSPEND/TERMINATE functionality to the user through the REST API.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.util.FakeClusterClient.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.MiniClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.StopOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="11679" opendate="2019-2-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Blink SQL planner and runtime modules</summary>
      <description>As mentioned in FLIP-32, we will create separate modules while performing the Blink SQL merge. As part of this issue we will create flink-table-planner-blink and flink-table-runtime-blink modules.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11686" opendate="2019-2-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce spaces around lambdas</summary>
      <description>Enforce spaces around lambdas so that ()-&gt;x would be rejected. We commonly add spaces around lambdas, might as well enforce it properly.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="11713" opendate="2019-2-21 00:00:00" fixdate="2019-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy mode from documentation</summary>
      <description>Legacy mode cannot be used anymore but docs still mention it:mode: Execution mode of Flink. Possible values are legacy and new. In order to start the legacy components, you have to specify legacy (DEFAULT: new). </description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="11743" opendate="2019-2-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sticky E2E tests failed on travis</summary>
      <description>https://travis-ci.org/apache/flink/builds/497743828Running 'Local recovery and sticky scheduling end-to-end test'==============================================================================TEST_DATA_DIR: /home/travis/build/apache/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-02822890481Flink dist directory: /home/travis/build/apache/flink/flink/flink-dist/target/flink-1.8-SNAPSHOT-bin/flink-1.8-SNAPSHOTRunning local recovery test with configuration:       parallelism: 4       max attempts: 10       backend: rocks       incremental checkpoints: true       kill JVM: trueStarting zookeeper daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.Starting HA cluster with 1 masters.used deprecated key `jobmanager.heap.mb`, please replace with key `jobmanager.heap.size`Starting standalonesession daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.used deprecated key `taskmanager.heap.mb`, please replace with key `taskmanager.heap.size`Starting taskexecutor daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Started TM watchdog with PID 25095.Starting execution of programProgram execution finishedJob with JobID d5eec8ff1a3f52d89d9c0a14d26afb5f has finished.Job Runtime: 1698 msFAILURE: Found 15 failed attempt(s) for local recovery of correctly scheduled task(s).</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestorePrepareResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.BackendBuildingException.java</file>
    </fixedFiles>
  </bug>
  <bug id="11745" opendate="2019-2-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TTL end-to-end test restores from the savepoint after the job cancelation</summary>
      <description>The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1176" opendate="2014-10-19 00:00:00" fixdate="2014-2-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add operator for global reduce/aggregations</summary>
      <description>Currently streaming reduce operators (like .reduce or .window(..).reduce, sum, etc.) only allow local reduces/aggregations. While one can achieve global aggregation by setting the parallelism of these operators to 1 it can cause a serious bottleneck in the program. For window/batch reduces and aggregations a global reducer can be added, with parallelism 1, to further reduce partial results. This global reduce option could be introduced as an optional boolean parameter for the intended operators.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.join.WindowJoin.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowingExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.socket.SocketTextStreamWordCount.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.ml.IncrementalLearningSkeleton.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.WindowInvokableTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowInvokableTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CloneableTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CloneableEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowReduceInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.windowing.WindowMerger.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowGroupReduceInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="11779" opendate="2019-2-28 00:00:00" fixdate="2019-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI ignores -m parameter if high-availability is ZOOKEEPER</summary>
      <description>DescriptionThe CLI will ignores the host/port provided by the -m parameter if high-availability: ZOOKEEPER is configured in flink-conf.yamlExpected behavior TBD: either document this behavior or give precedence to -m</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
      <file type="M">docs..includes.generated.common.host.port.section.html</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FallbackYarnSessionCli.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.AbstractCustomCommandLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11781" opendate="2019-2-28 00:00:00" fixdate="2019-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject "DISABLED" as value for yarn.per-job-cluster.include-user-jar</summary>
      <description>DescriptionSetting yarn.per-job-cluster.include-user-jar: DISABLED in flink-conf.yaml is not supported (anymore). Doing so will lead to the job jar not being on the system classpath, which is mandatory if Flink is deployed in job mode. The job will never run.Expected behaviorDocumentation should reflect that setting yarn.per-job-cluster.include-user-jar: DISABLED does not work.  </description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DelegatingConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="11804" opendate="2019-3-4 00:00:00" fixdate="2019-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure the CloseableRegistry used in backend builder is registered with task</summary>
      <description>In FLINK-10043 we have moved the restore process into backend builder, but the CloseableRegistry used in building the backend instance is not registered with the task, which may lead to resource leak when task is canceled before restore complete.</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSpyWrapperStateBackend.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapStateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackendBuilder.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.StubStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="11822" opendate="2019-3-5 00:00:00" fixdate="2019-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Flink metadata handlers</summary>
      <description>Calcite has defined various metadata handlers(e.g. RowCoun, Selectivity and provided default implementation(e.g. RelMdRowCount, RelMdSelectivity). However, the default implementation can't completely meet our requirements, e.g. some of its logic is incomplete，and some RelNodes are not considered.There are two options to meet our requirements:option 1. Extends from default implementation, overrides method to improve its logic, add new methods for new RelNode. The advantage of this option is we just need to focus on the additions and modifications. However, its shortcomings are also obvious: we have no control over the code of non-override methods in default implementation classes especially when upgrading the Calcite version.option 2. Extends from metadata handler interfaces, reimplement all the logic. Its shortcomings are very obvious, however we can control all the code logic that's what we want.so we choose option 2!In this jira, all Flink metadata handles will be introduced, including calcite builtin metadata handlers:FlinkRelMdPercentageOriginalRow,FlinkRelMdNonCumulativeCost,FlinkRelMdCumulativeCost,FlinkRelMdRowCount,FlinkRelMdSize,FlinkRelMdSelectivity,FlinkRelMdDistinctRowCoun,FlinkRelMdPopulationSize,FlinkRelMdColumnUniqueness,FlinkRelMdUniqueKeys,FlinkRelMdDistribution,and flink extented metadata handlers:FlinkRelMdColumnInterval,FlinkRelMdFilteredColumnInterval,FlinkRelMdColumnNullCount,FlinkRelMdColumnOriginNullCount,FlinkRelMdUniqueGroups,FlinkRelMdModifiedMonotonicity</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelOptClusterFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.package.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalIntersect.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalMinus.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalOverWindow.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkHepProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkVolcanoProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecOverWindowAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkRelOptTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.trait.FlinkRelDistribution.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankProcessStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.RankITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11827" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce DataFormatConverters to convert internal data format and java format</summary>
      <description>We have introduced a new internal data format, but if user interact with Source, sink and Udx, they prefer to use the traditional Java data format.So we introduce DataFormat Converters to convert the internal efficient binary data format into more general Java data format when source, sink and UDX are used.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArray.java</file>
    </fixedFiles>
  </bug>
  <bug id="11836" opendate="2019-3-6 00:00:00" fixdate="2019-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update NOTICE-binary and licenses-binary for Flink 1.8.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.collect.license.files.sh</file>
    </fixedFiles>
  </bug>
  <bug id="11838" opendate="2019-3-6 00:00:00" fixdate="2019-1-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Create RecoverableWriter for GCS</summary>
      <description>GCS supports the resumable upload which we can use to create a Recoverable writer similar to the S3 implementation:https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-uploadAfter using the Hadoop compatible interface: https://github.com/apache/flink/pull/7519We've noticed that the current implementation relies heavily on the renaming of the files on the commit: https://github.com/apache/flink/blob/master/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopRecoverableFsDataOutputStream.java#L233-L259This is suboptimal on an object store such as GCS. Therefore we would like to implement a more GCS native RecoverableWriter</description>
      <version>1.8.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="11856" opendate="2019-3-8 00:00:00" fixdate="2019-3-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce BinaryHashTable to batch table runtime</summary>
      <description>Once we have BinaryRow, we should also introduce the hash table which stored all rows in binary format. It has multiple benefits if we knows the binary format about the build side row and probe side row. For example, we can directly compare on byte arrays when we want to check if build key and probe key equals. We can also eliminate lots of deserialize cost when we want to get a row instance from some binary data.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.sort.TestMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.AbstractRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.MemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.ListMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.BinaryKVInMemorySortBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.BinaryIndexedSortable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.AbstractPagedOutputView.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="11857" opendate="2019-3-8 00:00:00" fixdate="2019-3-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce BinaryExternalSorter and BufferedKVExternalSorter to batch table runtime</summary>
      <description>We need a sorter to take full advantage of the high performance of the Binary format.For Sort, Sort Aggregation, SortMergeJoin: we need a BinaryExternalSorter to sort BinaryRows.For Hash Aggregation：We store the data in HashMap in KeyValue format. When memory is not enough, we spill all the data in memory onto disk and degenerate it into Sort Aggregation. So we need a BufferedKVExternalSorter to write the data that already in memory to disk, and then carry out Sort Merge.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedBlockChannelReader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.BlockCompressionFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.AbstractPagedOutputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.AbstractPagedInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.HeaderlessChannelReaderInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.ChannelReaderInputViewIterator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ShortComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.LongComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.IntComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CharComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ByteComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BooleanComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11858" opendate="2019-3-8 00:00:00" fixdate="2019-3-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce block compressor/decompressor for batch table runtime</summary>
      <description>Introduce block compressor/decompressor API to support future compression of the spilled data in batch table runtime. The compressor/decompressor works on block one at a time. A block is a piece of continuous data stored either in `byte[]` or `ByteBuffer`. We will first support lz4 compression algorithm.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11892" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port conflict when running nightly end-to-end tests</summary>
      <description>When I do the end-to-end check according to `https://github.com/apache/flink/tree/master/flink-end-to-end-tests`. I got the follows problem:1. Executed command， and the message of console as follows:FLINK_DIR=/Users/jincheng/work/FlinkRelease/1.8/flink-1.8.0/flink-dist/target/flink-1.8.0-bin/flink-1.8.0export FLINK_DIRsh flink-end-to-end-tests/run-nightly-tests.sh......Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.Dispatcher REST endpoint is up.Job (180a3cfc35d549417e5807520d7402f9) is running.Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...2. Log info:2019-03-13 07:56:33,670 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator         - Remoting shut down.2019-03-13 07:56:33,673 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)Caused by: java.net.BindException: Could not start actor system on any port in port range 6123at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:172)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)3. environmentMacOS: 10.14.3 Java version "1.8.0_151"jinchengsunjcs-iMac:~ jincheng$ echo $0-bash</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="11896" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce stream physical nodes</summary>
      <description>This issues aims to introduce flink stream physical RelNode, such as StreamExecCalc, StreamExecExchange, StreamExecExpand etc.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11898" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support code generation for all Blink built-in functions and operators</summary>
      <description>Support code generation for built-in functions and operators. FLINK-11788 has supported some of the operators. This issue is aiming to complement the functions and operators supported in Flink SQL.This should inlclude: CONCAT, LIKE, SUBSTRING, UPPER, LOWER, and so on.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryStringTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.util.SegmentsUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.TypeConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.InternalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.ThreadLocalCache.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.CompileUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.util.HashUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.LazyBinaryFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BoxedWrapperRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryRowWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryGeneric.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArrayWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.RowTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ArrayTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.MathFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.LiteralTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11899" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce parquet ColumnarRow split reader</summary>
      <description>Parquet ColumnarRow split reader is introduced to read parquet data in batches.When returning each row of data, instead of actually retrieving each field, we use BaseRow's abstraction to return a Columnar Row-like view.This will greatly improve the downstream filtered scenarios, so that there is no need to access redundant fields on the filtered data.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.TestUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetTableSource.java</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11918" opendate="2019-3-14 00:00:00" fixdate="2019-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecated Window and Rename it to GroupWindow</summary>
      <description>OverWindow and Window are confusing in the API, and mentioned that we want to rename it to GroupWindow for many times.  So,  here just a suggestion, how about Deprecated the Window in release-1.8, since we should create a new RC2 for release 1.8. If we do not do that the Window will keep existing for almost half a year. I create this JIRA, and link to release-1.8 vote mail thread, ask RM's options. If all of you do not agree, I'll close the JIRA, otherwise, we can open the new PR for Depercated the window. What do you think?</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.GroupWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.windows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="11950" opendate="2019-3-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing dependencies in NOTICE file of flink-dist.</summary>
      <description>Add Missing dependencies in NOTICE file of flink-dist. </description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="11980" opendate="2019-3-20 00:00:00" fixdate="2019-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve efficiency of iterating KeySelectionListener on notification</summary>
      <description>KeySelectionListener was introduced for incremental TTL state cleanup as a driver of the cleanup process. Listeners are notified whenever the current key in the backend is set (i.e. for every event). The current implementation of the collection that holds the listener is a HashSet, iterated via `forEach` on each key change. This method comes with the overhead of creating temporaray objects, e.g. iterators, on every invocation and even if there is no listener registered. We should rather use an ArrayList with for-loop iteration in this hot code path to i) minimize overhead and ii) minimize costs for the very likely case that there is no listener at all.</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="12013" opendate="2019-3-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support calc and correlate in blink</summary>
      <description>1.Support filter and projection in calc, e.g.: select a + b, c from T where a &gt; 5;2.Support udf in calc.3.Support udtf in correlate.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.RowType.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.InternalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12021" opendate="2019-3-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let ResultConjunctFuture return future results in same order as futures</summary>
      <description>The ResultConjunctFuture should return future results in the same order as the futures were specified. The advantage would be that we maintain the same output order as the input order if the input has been specified with a special ordering.This should also fix a current performance regression where we don't maintain the topological ordering when deploying tasks. Due to this, we need to make an additional result partition lookup which adds additional latency. The problem has occurred due to FLINK-10431 which changes the interleaving how the slot futures are completed.</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.LocalTaskManagerLocation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ConjunctFutureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12064" opendate="2019-3-29 00:00:00" fixdate="2019-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDBKeyedStateBackend snapshot uses incorrect key serializer if reconfigure happens during restore</summary>
      <description>As titled, in current RocksDBKeyedStateBackend we use keySerializer rather than keySerializerProvider.currentSchemaSerializer(), which is incorrect. The issue is not revealed in existing UT since current cases didn't check snapshot after state schema migration.This is a regression issue caused by the FLINK-10043 refactoring work.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendMigrationTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="12075" opendate="2019-4-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not able to submit jobs on YARN when there&amp;#39;s a firewall</summary>
      <description>If there is a firewall around the YARN cluster and the machine, submitting flink job it is unpractical because new flink clusters start up with random ports for REST communication. FLINK-5758 should've fixed this. But it seems FLINK-11081 either undid the changes or did not implement this. The relevant code is changed in FLINK-11081 (https://github.com/apache/flink/commit/730eed71ef3f718d61f85d5e94b1060844ca56db#diff-487838863ab693af7008f04cb3359be3R102)  </description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="12076" opendate="2019-4-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for simple group aggregate on batch</summary>
      <description>To distinguish Window aggregate and Over aggregate, we call the `GROUP BY` aggregate as group aggregate.This issue only involves batch group aggregate, stream group aggregate will be finished in new issue due to retraction derivation.complex group aggregate(including GROUP SETS and DISTINCT) will also be finished in new issue.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.ValuesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.ValuesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12077" opendate="2019-4-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce HashJoinOperator and LongHashJoinGenerator to blink</summary>
      <description>Introduce HashJoinOperator to support all join type.Introduce LongHashJoinGenerator to do some performance optimization when key is long using LongHybridHashTable.HashJoinOperator and LongHashJoinOperator can be used in both shuffle hash join and broadcast hash join.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedProjection.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12079" opendate="2019-4-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for join on batch</summary>
      <description>Currently, there are 3 types of batch physical join nodes: BatchExecHashJoin, BatchExecSortMergeJoin and BatchExecNestedLoopJoin. This issue aims to add rules to convert logical join to appropriate physical join. a logical join can be converted to BatchExecHashJoin if there exists at least one equal-join condition and ShuffleHashJoin or BroadcastHashJoin are enabled, can be converted to BatchExecSortMergeJoin if there exists at least one equal-join condition and SortMergeJoin is enabled, can be converted to BatchExecNestedLoopJoin if NestedLoopJoin is enabled or one of join input sides returns at most a single row.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.package.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12122" opendate="2019-4-8 00:00:00" fixdate="2019-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spread out tasks evenly across all available registered TaskManagers</summary>
      <description>With Flip-6, we changed the default behaviour how slots are assigned to TaskManagers. Instead of evenly spreading it out over all registered TaskManagers, we randomly pick slots from TaskManagers with a tendency to first fill up a TM before using another one. This is a regression wrt the pre Flip-6 code.I suggest to change the behaviour so that we try to evenly distribute slots across all available TaskManagers by considering how many of their slots are already allocated.</description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotSelectionStrategyTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.LegacySchedulerBatchSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolInteractionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolCoLocationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.PreviousAllocationSlotSelectionStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.LocationPreferenceSlotSelectionStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.LocationPreferenceSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PreviousAllocationSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs..includes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12147" opendate="2019-4-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>InfluxDB throws errors if metric value is NaN/infinity</summary>
      <description>when use influxDB store metric datas, and metric kafka performance. There has some POSITIVE_INFINITY or NEGATIVE_INFINITY   data in the values, but influxDB no support it, so it alway some error.</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12159" opendate="2019-4-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable YarnMiniCluster integration test under non-secure mode</summary>
      <description>Currently if third party want to use flink for integration under yarn mode, it has to be secure mode, it doesn't make sense. We should also allow it under non-secure mode.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="12168" opendate="2019-4-12 00:00:00" fixdate="2019-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support e2e limit, sortLimit, rank, union in blink batch</summary>
      <description>Support limit and add limit it cases to blink batch.Support sortLimit and add sortLimit it cases to blink batch.Support rank and add rank it cases to blink batch.Support union and add union it cases to blink batch.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.OuterJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.util.BaseRowTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12171" opendate="2019-4-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The network buffer memory size should not be checked against the heap size on the TM side</summary>
      <description>Currently when computing the network buffer memory size on the TM side in TaskManagerService#calculateNetworkBufferMemory`(version 1.8 or 1.7) or NetworkEnvironmentConfiguration#calculateNewNetworkBufferMemory(master), the computed network buffer memory size is checked to be less than `maxJvmHeapMemory`. However, in TM side, maxJvmHeapMemory stores the maximum heap memory (namely -Xmx) . With the above process, when TM starts, -Xmx is computed in RM or in taskmanager.sh with (container memory - network buffer memory - managed memory),  thus the above checking implies that the heap memory of the TM must be larger than the network memory, which seems to be not necessary. This may cause TM to use more memory than expected. For example, for a job who has a large network throughput, uses may configure network memory to 2G. However, if users want to assign 1G to heap memory, the TM will fail to start, and user has to allocate at least 2G heap memory (in other words, 4G in total for the TM instead of 3G) to make the TM runnable. This may cause resource inefficiency. Therefore, I think the network buffer memory size also need to be checked against the total memory instead of the heap memory on the TM  side: Checks that networkBufFraction &lt; 1.0. Compute the total memory by ( jvmHeapNoNet / (1 - networkBufFraction)). Compare the network buffer memory with the total memory.This checking is also consistent with the similar one done on the RM side.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-dist.src.test.java.org.apache.flink.dist.TaskManagerHeapSizeCalculationJavaBashTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12185" opendate="2019-4-13 00:00:00" fixdate="2019-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for join on stream</summary>
      <description>This issue aims to supports generating optimized plan for join on stream. The query will be converted to window join if join condition contains window bounds, otherwise will be converted to normal join.e.g.Queries similar to the following should be window join:SELECT t1.a, t2.b FROM MyTable t1 JOIN MyTable2 t2 ON t1.a = t2.a AND t1.proctime BETWEEN t2.proctime - INTERVAL '1' HOUR AND t2.proctime + INTERVAL '1' HOUR</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12200" opendate="2019-4-15 00:00:00" fixdate="2019-4-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>[Table API] Support UNNEST for MAP types</summary>
      <description>In case if the input dataset has the following schema :Row(a: Integer, b: Long, c: Map&lt;String, String&gt;)I would like to have the ability to execute the SQL query like:SELECT a, k, v FROM src, UNNEST(c) as m (k,v)Currently, the UNNEST operator is supported only for ARRAY and MULTISET I would like to propose adding the support of UNNEST functionality for MAP types.</description>
      <version>1.7.2,1.7.3,1.8.0,1.8.1</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12248" opendate="2019-4-18 00:00:00" fixdate="2019-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support e2e over window in blink batch</summary>
      <description>Supporting all major over window operations</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.RowUnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.RowUnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.RowSlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.OffsetOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.dataview.DataViewUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankLikeAggFunctionBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.DenseRankAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="12250" opendate="2019-4-18 00:00:00" fixdate="2019-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite assembleNewPartPath to let it return a new PartPath</summary>
      <description>While debugging some code, I've noticed assembleNewPartPath does not really return a new path. Also rewrote the code a bit so the mutable inProgressPart is changed in a single place</description>
      <version>1.8.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="12253" opendate="2019-4-18 00:00:00" fixdate="2019-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup a class hierarchy for the new logical type system</summary>
      <description>Setup a new class hierarchy around LogicalType in table-common.The classes implement the types listed in the table of FLIP-37.The classes won't be connected to the API yet.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.NullType.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalTypeVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12254" opendate="2019-4-18 00:00:00" fixdate="2019-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the new type system through the API</summary>
      <description>Exposes the new type system through API methods.Introduces new methods, adds converters for backwards-compatibility, and deprecates old methods.Adds checks to types that are not supported by the legacy planner.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.ProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.NestedFieldsProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedFieldMapping.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.expressions.rules.VerifyNoUnresolvedExpressionsRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.lookups.FieldReferenceLookup.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowEmitStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.FieldReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LocalReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.Expression.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TypeLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSchemaValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSchemaTest.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliResultView.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.descriptors.LiteralValueValidator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.TimeIntervalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowTableAggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSinkBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TableSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.CollectTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.CsvTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationExpressionsUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.TableOperationTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.expressions.ExpressionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ExpandColumnFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AliasOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12277" opendate="2019-4-20 00:00:00" fixdate="2019-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for catalog</summary>
      <description>Add documentation for unified catalog APIs and catalogs.The ticket for corresponding Chinese documentation is FLINK-13086</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="12283" opendate="2019-4-22 00:00:00" fixdate="2019-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API does not allow non-static inner class as UDF</summary>
      <description>See details here https://lists.apache.org/thread.html/9ecec89ba1225dbd6b3ea2466a910ad9685a42a4672b449f6ee13565@%3Cuser.flink.apache.org%3E</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.CallExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.ScalarFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AggregateFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="12285" opendate="2019-4-22 00:00:00" fixdate="2019-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in SavepointITCase and SavepointMigrationTestBase</summary>
      <description>The tests in SavepointITCase and SavepointMigrationTestBase do not cancel running jobs before exit. It will cause exceptions in {{TaskExecutor}}s and unreleased memory segments. Succeeding tests may fail due to insufficient amount of memory.The problem is caused by cancelling {{TaskExecutor}}s with running tasks. Another issue caused by the reason can be seen in FLINK-11343. Maybe we can find a more dedicated method to cancel those {{TaskExecutor}}s still having running tasks.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="12296" opendate="2019-4-23 00:00:00" fixdate="2019-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data loss silently in RocksDBStateBackend when more than one operator(has states) chained in a single task</summary>
      <description>As the mail list said&amp;#91;1&amp;#93;, there may be a problem when more than one operator chained in a single task, and all the operators have states, we'll encounter data loss silently problem.Currently, the local directory we used is like below../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1/(state), if more than one operator chained in a single task, and all the operators have states, then all the operators will share the same local directory(because the vertext_id is the same), this will lead a data loss problem.  The path generation logic is below:// LocalRecoveryDirectoryProviderImpl.java@Overridepublic File subtaskSpecificCheckpointDirectory(long checkpointId) { return new File(subtaskBaseDirectory(checkpointId), checkpointDirString(checkpointId));}@VisibleForTestingString subtaskDirString() { return Paths.get("jid_" + jobID, "vtx_" + jobVertexID + "_sti_" + subtaskIndex).toString();}@VisibleForTestingString checkpointDirString(long checkpointId) { return "chk_" + checkpointId;}&amp;#91;1&amp;#93; http://mail-archives.apache.org/mod_mbox/flink-user/201904.mbox/%3Cm2ef5tpfwy.wl-ningshi2@gmail.com%3E</description>
      <version>1.6.3,1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.CheckpointStreamWithResultProviderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12299" opendate="2019-4-23 00:00:00" fixdate="2019-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject negative auto watermark intervals</summary>
      <description>In any scenario, `autoWatermarkInterval` should not be less than or equal to zero.First of all, this does not correspond to the meaning of `autoWatermarkInterval`.Second, in the case where `autoWatermarkInterval` is less than 0, we will not be able to register ourselves in `TimestampsAndPeriodicWatermarksOperator#open`, which will result in the water level of this stream being kept at the lowest level.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="12306" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the name of variable "log" to upper case "LOG"</summary>
      <description>Change the name of variable "log" from lower case to upper case "LOG" to correspond to other class files.</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadValidatorUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12319" opendate="2019-4-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverFlowError in cep.nfa.sharedbuffer.SharedBuffer</summary>
      <description> I wrote a simple SourceFunction that creats Events in a loop.The CEP pattern is very simple:      final Pattern&lt;Event, ?&gt; failurePattern =              Pattern.&lt;Event&gt;begin("5 or more failures", AfterMatchSkipStrategy.skipPastLastEvent())                      .subtype(LoginEvent.class)                      .where(                              new IterativeCondition&lt;LoginEvent&gt;() {                                  @Override                                  public boolean filter(LoginEvent value, Context&lt;LoginEvent&gt; ctx) throws Exception {                                      return value.get("type").equals("failed");                                  }                              })                      .times(5)                      .next("1 or more successes")                      .subtype(LoginEvent.class)                      .where(                              new IterativeCondition&lt;LoginEvent&gt;() {                                  @Override                                  public boolean filter(LoginEvent value, Context&lt;LoginEvent&gt; ctx) throws Exception {                                      return value.get("type").equals("success");                                  }                              })                      .times(1)                      .within(Time.milliseconds(20)); After about 100k Events, Flink aborts with this stacktrace: Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)    at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:630)    at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:123)    at org.classdump.alerts.FlinkCep.brute_force_login(FlinkCep.java:263)    at org.classdump.alerts.FlinkCep.main(FlinkCep.java:41)Caused by: java.lang.StackOverflowError    at org.apache.flink.runtime.state.heap.HeapMapState.get(HeapMapState.java:85)    at org.apache.flink.runtime.state.UserFacingMapState.get(UserFacingMapState.java:47)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:339)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)[...] This happens with version 1.8.0, 1.7.2, 1.6.4Version 1.5.6 does not have this issue.Seems to be related to FLINK-9418  </description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBufferAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12321" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports sub-plan reuse</summary>
      <description>Many query plans have duplicated sub-plan, and their computing logic is identical. So we could reuse duplicated sub-plans to reduce computing. Digest of RelNode is an identifier to distinguish RelNode, and the digest of sub-plan contains all inner-nodes' digests. we could use the digest of sub-plan to find duplicated sub-plan.e.g. WITH r AS (SELECT a FROM x LIMIT 10)SELECT r1.a FROM r r1, r r2 WHERE r1.a = r2.athe physical plan after sub-plan reuse:Calc(select=[a])+- HashJoin(joinType=[InnerJoin], where=[=(a, a0)], select=[a, a0], isBroadcast=[true], build=[right]) :- Exchange(distribution=[hash[a]]) : +- Calc(select=[a], reuse_id=[1]) : +- Limit(offset=[0], fetch=[10], global=[true]) : +- Exchange(distribution=[single]) : +- Limit(offset=[0], fetch=[10], global=[false]) : +- TableSourceScan(table=[[x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c]) +- Exchange(distribution=[broadcast]) +- Reused(reference_id=[1])sub-plan: Calc-Limit-Exchange-Limit-TableSourceScan is reused.For batch job, reused node might lead to a deadlock when a HashJoin or NestedLoopJoin have same reused inputs. The probe side of HashJoin could start to read data only after build side has finished. If there is no full dam (DamBehavior#FULL_DAM) operators (e.g. Sort, HashAggregate) in probe side, the data will be blocked by probe side. In this case, we could set Exchange node (if it does not exist, add new one) as BATCH mode to break up the deadlock.(The Exchange node is a full dam operator now)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelFieldCollationUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.trait.FlinkRelDistribution.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12323" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy ActorGateway implementations</summary>
      <description>Remove the legacy ActorGateway based implementations.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProviderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayPartitionProducerStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateRegistryListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayKvStateLocationOracle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayGlobalAggregateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
    </fixedFiles>
  </bug>
  <bug id="1234" opendate="2014-11-11 00:00:00" fixdate="2014-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hadoop2 profile default</summary>
      <description>As per mailing list discussion.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.generate.specific.pom.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">tools.change-version</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-addons.pom.xml</file>
      <file type="M">flink-addons.flink-yarn.pom.xml</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">docs.building.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12342" opendate="2019-4-26 00:00:00" fixdate="2019-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn Resource Manager Acquires Too Many Containers</summary>
      <description>In currently implementation of YarnFlinkResourceManager, it starts to acquire new container one by one when get request from SlotManager. The mechanism works when job is still, say less than 32 containers. If the job has 256 container, containers can't be immediately allocated and appending requests in AMRMClient will be not removed accordingly. We observe the situation that AMRMClient ask for current pending request + 1 (the new request from slot manager) containers. In this way, during the start time of such job, it asked for 4000+ containers. If there is an external dependency issue happens, for example hdfs access is slow. Then, the whole job will be blocked without getting enough resource and finally killed with SlotManager request timeout.Thus, we should use the total number of container asked rather than pending request in AMRMClient as threshold to make decision whether we need to add one more resource request.</description>
      <version>1.6.4,1.7.2,1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12343" opendate="2019-4-26 00:00:00" fixdate="2019-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow set file.replication in Yarn Configuration</summary>
      <description>Currently, FlinkYarnSessionCli upload jars into hdfs with default 3 replications. From our production experience, we find that 3 replications will block big job (256 containers) to launch, when the HDFS is slow due to big workload for batch pipelines. Thus, we want to make the factor customizable from FlinkYarnSessionCli by adding an option.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12386" opendate="2019-5-1 00:00:00" fixdate="2019-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support mapping BinaryType, VarBinaryType, CharType, VarCharType, and DecimalType between Flink and Hive in HiveCatalog</summary>
      <description>Flink right now doesn't have a 1:1 data type mapping for Hive's CHAR, VARCHAR, DECIMAL, MAP, STRUCT. Needs to add the mapping once FLIP-37 &amp;#91;rework Flink type system&amp;#93; is finished</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12388" opendate="2019-5-2 00:00:00" fixdate="2019-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the production readiness checklist</summary>
      <description>The production readiness checklist has grown organically over the years, and while it provides valuable information, the content does not flow cohesively as it has been worked on by a number of users. We should improve the overall structure and readability of the checklist and also update any outdated information.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug id="1239" opendate="2014-11-12 00:00:00" fixdate="2014-7-12 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Fix iteration example getting stuck with large input</summary>
      <description>When running the streaming iteration example with buffer timeout set to 0 (meaning the StreamRecorWriter gets flushed after every emit in every task), the iteration gets stuck at flushing the output after emitting a record. This happens only on larger number of inputs (eg. 1000 record to iterate on).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.iteration.IterateExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="12390" opendate="2019-5-2 00:00:00" fixdate="2019-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fully migrate to asm6</summary>
      <description>We currently use a mix of asm5/asm6, let's migrate completely so we can drop a dependency and remove a module from flink-shaded.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DependencyVisitor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12391" opendate="2019-5-2 00:00:00" fixdate="2019-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeout to transfer.sh</summary>
      <description>The upload to transfer.sh frequently does not work, but causes the build to timeout, like seen here: https://api.travis-ci.org/v3/job/527020650/log.txt</description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12399" opendate="2019-5-3 00:00:00" fixdate="2019-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FilterableTableSource does not use filters on job run</summary>
      <description>As discussed on the mailing list, there appears to be a bug where a job that uses a custom FilterableTableSource does not keep the filters that were pushed down into the table source. More specifically, the table source does receive filters via applyPredicates, and a new table source with those filters is returned, but the final job graph appears to use the original table source, which does not contain any filters.I attached a minimal example program to this ticket. The custom table source is as follows: public class CustomTableSource implements BatchTableSource&lt;Model&gt;, FilterableTableSource&lt;Model&gt; { private static final Logger LOG = LoggerFactory.getLogger(CustomTableSource.class); private final Filter[] filters; private final FilterConverter converter = new FilterConverter(); public CustomTableSource() { this(null); } private CustomTableSource(Filter[] filters) { this.filters = filters; } @Override public DataSet&lt;Model&gt; getDataSet(ExecutionEnvironment execEnv) { if (filters == null) { LOG.info("==== No filters defined ===="); } else { LOG.info("==== Found filters ===="); for (Filter filter : filters) { LOG.info("FILTER: {}", filter); } } return execEnv.fromCollection(allModels()); } @Override public TableSource&lt;Model&gt; applyPredicate(List&lt;Expression&gt; predicates) { LOG.info("Applying predicates"); List&lt;Filter&gt; acceptedFilters = new ArrayList&lt;&gt;(); for (final Expression predicate : predicates) { converter.convert(predicate).ifPresent(acceptedFilters::add); } return new CustomTableSource(acceptedFilters.toArray(new Filter[0])); } @Override public boolean isFilterPushedDown() { return filters != null; } @Override public TypeInformation&lt;Model&gt; getReturnType() { return TypeInformation.of(Model.class); } @Override public TableSchema getTableSchema() { return TableSchema.fromTypeInfo(getReturnType()); } private List&lt;Model&gt; allModels() { List&lt;Model&gt; models = new ArrayList&lt;&gt;(); models.add(new Model(1, 2, 3, 4)); models.add(new Model(10, 11, 12, 13)); models.add(new Model(20, 21, 22, 23)); return models; }} When run, it logs15:24:54,888 INFO com.klaviyo.filterbug.CustomTableSource - Applying predicates15:24:54,901 INFO com.klaviyo.filterbug.CustomTableSource - Applying predicates15:24:54,910 INFO com.klaviyo.filterbug.CustomTableSource - Applying predicates15:24:54,977 INFO com.klaviyo.filterbug.CustomTableSource - ==== No filters defined ====which appears to indicate that although filters are getting pushed down, the final job does not use them.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TestFilterableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushProjectIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetTableSource.java</file>
      <file type="M">flink-connectors.flink-orc.src.main.java.org.apache.flink.orc.OrcTableSource.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSource.java</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="12401" opendate="2019-5-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support incremental emit under AccRetract mode for non-window streaming FlatAggregate on Table API</summary>
      <description>As described in Flip-29, there are two output modes for non-window streaming flatAggregate. One is emitting with full values, the other is emitting with incremental values. FLINK-10977 supports the former one, this jira is going to support the latter one.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupTableAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunction.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="12402" opendate="2019-5-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make validation error message for CallExpression more user friendly</summary>
      <description>Currently, the error message for CallExpression validation may not display the root cause which may confuse our users. Take the following test as an example: @Test def testSimpleSelectAllWithAs(): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) StreamITCase.testResults = mutable.MutableList() val ds = StreamTestData.getSmall3TupleDataStream(env).toTable(tEnv, 'a, 'b, 'c) .select('a, 'b.log as 'b, 'c) val results = ds.toAppendStream[Row] results.addSink(new StreamITCase.StringSink[Row]) env.execute() val expected = mutable.MutableList( "1,1,Hi", "2,2,Hello", "3,2,Hello world") assertEquals(expected.sorted, StreamITCase.testResults.sorted) }The error message is:org.apache.flink.table.api.ValidationException: Invalid arguments [log(b), 'b'] for function: asFrom the error message, it shows there is something wrong with the `as` function. However, the root cause is the log function can only accept a double parameter while b is a long number.To make it more user friendly, it would be better to display the root cause error message.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="12409" opendate="2019-5-6 00:00:00" fixdate="2019-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds from_elements in TableEnvironment</summary>
      <description>This is a convenient method to create a table from a collection of elements. It works as follows:1) Serializes the python objects to a local file2) Loads the file in Java and deserializes the data to Java objects</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-python.pyflink.table.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.set.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.print.schema.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.distinct.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.py4j</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.python.PythonUtilTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.python.PythonUtil.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.python.PythonGatewayServer.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink.bat</file>
    </fixedFiles>
  </bug>
  <bug id="12461" opendate="2019-5-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Workaround">
    <buginformation>
      <summary>Document binary compatibility situation with Scala beyond 2.12.8</summary>
      <description>When using 1.8 with Scala 2.12.8 and trying to parse a scala.Map (not a java.util.Map), I get a: java.lang.ClassNotFoundException: scala.math.Ordering$$anon$9To reproduce: start with the Scala Maven archetype (org.apache.flink:flink-quickstart-scala:1.8.0) in the POM, set the scala.version to 2.12.8 and the scala.binary.version to 2.12 in StreamingJob, add: env.fromElements[Map&amp;#91;String, Int&amp;#93;]()It works with Scala 2.12.7 (well, without putting anything in the job, it fails with "No operators defined in streaming topology", which is expected).I suspect this is linked to the binary incompatiblity of 2.12.8 with 2.12.7 (see the release note of 2.12.8), so compiling Flink with 2.12.8 instead of 2.12.7 might be enough (although it might stop working with 2.12.7?)</description>
      <version>1.8.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="12479" opendate="2019-5-10 00:00:00" fixdate="2019-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make InputProcessors and StreamIterationHead non-blocking</summary>
      <description>After decomposing the monolith loops in the previous sub-task, we need to change the implementations of certain actions, e.g. in {{InputProcessor}}s to be non-blocking, so that they will not stall the mailbox processing in the future.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskSelectiveReadingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputSelectableProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12487" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules to rewrite expression and merge calc</summary>
      <description>This issue aims to introduce planner rules to rewrite expression and merge calc, rules include:1. ConvertToNotInOrInRule, that converts a cascade of predicates to IN or NOT_IN,e.g. converts predicate (x = 1 OR x = 2 OR x = 3 OR x = 4) AND y = 5 to predicate x IN (1, 2, 3, 4) AND y = 5 converts predicate (x &lt;&gt; 1 AND x &lt;&gt; 2 AND x &lt;&gt; 3 AND x &lt;&gt; 4) AND y = 5 to predicate x NOT IN (1, 2, 3, 4) AND y = 52. RewriteCoalesceRule, that rewrites Coalesce to Case When3. FlinkCalcMergeRule, that is copied from Calcite CalcMergeRule, and it will simplify the merged program</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12490" opendate="2019-5-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce NetworkInput class</summary>
      <description>NetworkInput will take care of records deserialisation and barrier alignment. It will be used by InputProcessors (One, TwoInput and TwoInputSelectable) to access buffers from InputGates. </description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12501" opendate="2019-5-13 00:00:00" fixdate="2019-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AvroTypeSerializer does not work with types generated by avrohugger</summary>
      <description>The main problem is that the code in SpecificData.createSchema() tries to reflectively read the SCHEMA$ field, that is normally there in Avro generated classes. However, avrohugger generates this field in a companion object, which the reflective Java code will therefore not find.This is also described in these ML threads: https://lists.apache.org/thread.html/5db58c7d15e4e9aaa515f935be3b342fe036e97d32e1fb0f0d1797ee@%3Cuser.flink.apache.org%3E https://lists.apache.org/thread.html/cf1c5b8fa7f095739438807de9f2497e04ffe55237c5dea83355112d@%3Cuser.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12508" opendate="2019-5-14 00:00:00" fixdate="2019-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand Documentation on Testing DataStream API programs</summary>
      <description>The documentation under ApplicationDevelopment -&gt; Streaming -&gt; Testing should be extended by a more detailed discussion of Flink's TestHarnesses as well as the `MiniClusterWithClientResource`.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
    </fixedFiles>
  </bug>
  <bug id="12566" opendate="2019-5-21 00:00:00" fixdate="2019-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove row interval type</summary>
      <description>The row interval type just adds additional complexity and prevents SQL queries from supporting count windows. A regular BIGINT type is sufficient to represent a count.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.overOffsets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.OverWindowResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.RowIntervalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12568" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement OutputFormat to write Hive tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12590" opendate="2019-5-22 00:00:00" fixdate="2019-1-22 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Replace http links in documentation</summary>
      <description></description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.misc.CollectionExecutionExample.java</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
      <file type="M">flink-dist.src.main.flink-bin.README.txt</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.zh.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.zh.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug id="12617" opendate="2019-5-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StandaloneJobClusterEntrypoint should default to random JobID for non-HA setups</summary>
      <description></description>
      <version>1.8.0</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="12703" opendate="2019-6-1 00:00:00" fixdate="2019-7-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce metadata handlers on SEMI/ANTI join and lookup join</summary>
      <description>In FLINK-11822, we have introduced all Flink metadata handlers, several RelNode s (e.g. look up join) have not be implemented. So this issue aims to introduce all metadata handlers on SEMI/ANTI join and lookup join.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroupsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPopulationSizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPercentageOriginalRowsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnOriginNullCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCountTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnOriginNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12715" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-1.2.1 build is broken</summary>
      <description>Some Hive util methods used are not compatible between Hive-2.3.4 and Hive-1.2.1.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12716" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an interactive shell for Python Table API</summary>
      <description>We should add an interactive shell for the Python Table API. It will have the similar functionality like the Scala Shell.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="12717" opendate="2019-6-4 00:00:00" fixdate="2019-5-4 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add windows support for PyFlink</summary>
      <description>The aim of this JIRA is to allow Python users to develop PyFlink programs in windows. Users should be able to run a simple PyFlink program in the IDE(via minicluster) for debugging purposes.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-python.src.main.resources.pyflink-udf-runner.sh</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.ResourceUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonGatewayServer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonFunctionFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.python.callback.server.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pyflink.gen.protos.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.state.backend.py</file>
    </fixedFiles>
  </bug>
  <bug id="12723" opendate="2019-6-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds a wiki page about setting up a Python Table API development environment</summary>
      <description>We should add a wiki page showing how to set up a Python Table API development environment to help contributors who are interested in the Python Table API to join in easily.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.ide.setup.zh.md</file>
      <file type="M">docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="12726" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ANY type serialization</summary>
      <description>Every logical type needs to be string serializable. In old versions we used Java serialization logic for it. Since an any type has no type information anymore but just type serializer, we can use the snapshot to write out an any type into properties in a backward compatible way.However, the current serialization logic is wrong.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.AnyType.java</file>
    </fixedFiles>
  </bug>
  <bug id="12756" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate HiveCatalog from TypeInformation-based old type system to DataType-based new type system</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12758" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink-ml-lib module</summary>
      <description>The Jira introduces a new module "flink-ml-lib" under flink-ml-parent.The flink-ml-lib is planned in the roadmap in FLIP-39, as the code base of library implementations of FlinkML. This Jira only aims to create the module, and algorithms will be added in separate Jira in the future. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo]</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12763" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail job immediately if tasks’ resource needs can not be satisfied.</summary>
      <description></description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.StandaloneResourceManagerWithUUIDFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionJobVertexBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerFailUnfulfillableTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12765" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bookkeeping of available resources of allocated slots in SlotPool.</summary>
      <description>In this version, a task will always requests slot with its own resource need. If the resource need is less than the default slot resource, it will always be allocated to a default sized slot.  The extra resources in the slot leaves chances for other tasks within the same slot sharing group to fit in. To take these chance, SlotPool will maintain available resources of each allocated slot. Available resource of an allocated slot should always be the total resource of the slot minus resources of tasks already assigned onto the slot. In this way, the SlotPool would be able to determine whether another task can fit into the slot. If a task cannot fit into the slot, for slot sharing group the SlotPool should request another slot from the ResourceManager, and for colocation group it should fail the job.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotSelectionStrategyTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.LocationPreferenceSlotSelectionStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PreviousAllocationSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.LocationPreferenceSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolCoLocationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12766" opendate="2019-6-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Dynamically allocate TaskExecutor’s managed memory to slots.</summary>
      <description>This step is a temporal workaround for release 1.9 to meet the basic usability requirements of batch functions from Blink.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.ResourceSpecTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.resources.Resource.java</file>
    </fixedFiles>
  </bug>
  <bug id="12767" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user defined connectors/format</summary>
      <description>Currently, only built-in connectors such as FileSystem/Kafka/ES are supported and only built-in formats such as OldCSV/JSON/Avro/CSV/ are supported. We should also provide a convenient way for the connectors/formats that are not built-in supported.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12787" opendate="2019-6-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to specify directory in option -pyfs</summary>
      <description>Current only files can be specified in option `-pyfs`, we want to improve it allow also specify directories in option `-pyfs`. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonDriverTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonDriver.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="1279" opendate="2014-11-26 00:00:00" fixdate="2014-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default partitioning setting for low parallelism stream sources from forward to distribute</summary>
      <description>The default partitioning setting for all operators including stream sources are forward at the moment. In many cases sources have lower parallelism therefore the forward connection might cause that only a subset of the operators will receive data.Partitioning from source to the first operators should be changed to distribute data to all parallel instances of the first operators, when the source parallelism is lower.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.ForwardPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.DistributePartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.PrintTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.StreamPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.ShufflePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.GlobalPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.FieldsPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.DistributePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.IterativeDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="12801" opendate="2019-6-11 00:00:00" fixdate="2019-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set parallelism for batch SQL</summary>
      <description>       DataStream user can set parallelism by SingleOutputStreamOperator#setParallelism and DataStreamSink#setParallelism. But SQL users cannot set parallelism  to operators while compiled jobGraphs from SQL are usally complex.       Now we first set parallelism for batch SQL by config. We introduce two resourceSetting mode:       InferMode.NONE:  User can set parallelism to source, sink and other nodes separately.       InferMode.ONLY_SOURCE： Relative to  InferMode.NONE, source paralelism can be inferred by source row count.        We also introduce ShuffleStage to make adjacent operatos parallelism same that there is no data shuffle between them.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.exec.NodeResourceConfig.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12812" opendate="2019-6-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set resource profiles for task slots</summary>
      <description></description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12814" opendate="2019-6-12 00:00:00" fixdate="2019-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a traditional and scrolling view of result (non-interactive)</summary>
      <description>In table mode, we want to introduce a non-interactive view (so-called FinalizedResult), which submit SQL statements(DQLs) in attach mode with a user defined timeout, fetch results until the job finished/failed/timeout or interrupted by user(Ctrl+C), and output them in a non-interactive way (the behavior in change-log mode is under discussion)</description>
      <version>1.8.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TerminalUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="12815" opendate="2019-6-12 00:00:00" fixdate="2019-6-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports CatalogManager in blink planner</summary>
      <description>This issue aims to let blink planner support CatalogMananger which is what FLINK-11476 has done.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.CommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.nodes.resource.ExecNodeResourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SortTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.ModifiedMonotonicityTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.physical.stream.RetractionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.subquery.SubQueryAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.subquery.FlinkRewriteSubQueryRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.RewriteCoalesceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ReplaceMinusWithAntiJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ReplaceIntersectWithSemiJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ProjectSemiAntiJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ProjectPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.JoinDeriveNullFilterRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.JoinDependentConditionDerivationRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.JoinConditionTypeCoerceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.JoinConditionEqualityTransferRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkSemiAntiJoinProjectTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkSemiAntiJoinJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkSemiAntiJoinFilterTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkPruneEmptyRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLimit0RemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkJoinPushExpressionsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkCalcMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateOuterJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateInnerJoinTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.CalcPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.nodes.resource.ExecNodeResourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.calcite.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.java.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkRelOptTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.IntermediateRelTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.RelTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelShuttles.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.util.JavaScalaConversionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.TableStatsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1282" opendate="2014-11-26 00:00:00" fixdate="2014-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update layout of documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.footer.md</file>
      <file type="M">docs.css.custom.css</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug id="12820" opendate="2019-6-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ignoring null fields when writing to Cassandra</summary>
      <description>Currently, records which have null fields are written to their corresponding columns in Cassandra as null. Writing null is basically a 'delete' for Cassandra, it's useful if nulls should correspond to deletes in the data model, but nulls can also indicate a missing data or partial column update. In that case, we end up overwriting columns of existing record on Cassandra with nulls.  I believe it's already possible to ignore null values for POJO's with mapper options, as documented here:https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/cassandra.html#cassandra-sink-example-for-streaming-pojo-data-type But this is not possible when using scala tuples or case classes. Perhaps with a Cassandra sink configuration flag, null values can be unset using below option for tuples and case classes.https://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/BoundStatement.html#unset-int- Here is the equivalent configuration in spark-cassandra-connector;https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md#globally-treating-all-nulls-as-unset</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseConfig.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSink.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.AbstractCassandraTupleSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="12821" opendate="2019-6-12 00:00:00" fixdate="2019-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug that fix time quantifier can not be the last element of a pattern</summary>
      <description>Currently, exception "Greedy quantifiers are not allowed as the last element of a Pattern yet. Finish your pattern with either a simple variable or reluctant quantifier." will be thrown for patterns such as "a{2}". Actually greedy property is not meaningful for this kind of pattern.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12838" opendate="2019-6-13 00:00:00" fixdate="2019-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty-fy the network SSL setup</summary>
      <description>Refactor the SSL configuration done for Netty to have it more like the way Netty intends it to be: using its SslContextBuilder. This will make it much easier to set a different Netty SSL engine provider.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.SSLHandlerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12874" opendate="2019-6-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the semantics of zero length character strings</summary>
      <description>Zero-length character strings need special treatment as the SQL standard forbids to declare those type of character strings. For the type inference (e.g. determine the return type of a TRIM('')) it should be possible to return zero-lengths VARCHAR types. In any case, those type should not have a serializable string representation. Similar behavior is done in the Oracle system: SELECT DUMP(TRIM('')) AS tt FROM dual; returns NULL.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarCharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarBinaryType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.CharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.BinaryType.java</file>
    </fixedFiles>
  </bug>
  <bug id="12875" opendate="2019-6-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support converting input args of char, varchar, bytes, timestamp, date for Hive functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSimpleUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="12924" opendate="2019-6-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce basic type inference interfaces</summary>
      <description>This issue introduces basic interfaces for performing type inference. It includes validation of input types and deduction of accumulator or return types. Argument type inference (i.e. deducing a type for NULL) will be introduced in a later step. The interfaces replace the need for PlannerExpression.A rough design document can be found here but it needs some update. Introduction of annotations and basic type extraction will come later:https://docs.google.com/document/d/1RM_-XvD25AldUOl7xRSvA3sascWBa1-Jze3i5mvb7WU/edit#heading=h.kwyjplavecx4 </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.InternalFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
    </fixedFiles>
  </bug>
  <bug id="12957" opendate="2019-6-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix thrift and protobuf dependency examples in documentation</summary>
      <description>The examples in the docs are not up-to-date anymore and should be updated.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.custom.serializers.md</file>
    </fixedFiles>
  </bug>
  <bug id="12959" opendate="2019-6-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use BoundedInput and InputSelectable in blink</summary>
      <description>Now BoundedInput and InputSelectable are ready in runtime. Blink planner should use it instead of invoking endInput in close.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.sort.StreamSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.StreamSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.SortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.SortLimitOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.HashJoinType.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12981" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore NaN values in histogram&amp;#39;s percentile implementation</summary>
      <description>Histogram metrics use "long" values and therefore, there is no Double.NaN in DescriptiveStatistics' data and there is no need to cleanse it while working with it.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="12983" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace descriptive histogram&amp;#39;s storage back-end</summary>
      <description>DescriptiveStatistics relies on their ResizableDoubleArray for storing double values for their histograms. However, this is constantly resizing an internal array and seems to have quite some overhead.Additionally, we're not using SynchronizedDescriptiveStatistics which, according to its docs, we should. Currently, we seem to be somewhat safe because ResizableDoubleArray has some synchronized parts but these are scheduled to go away with commons.math version 4.Internal tests with the current implementation, one based on a linear array of twice the histogram size (and moving values back to the start once the window reaches the end), and one using a circular array (wrapping around with flexible start position) has shown these numbers using the optimised code from FLINK-10236, FLINK-12981, and FLINK-12982: only adding values to the histogramBenchmark Mode Cnt Score Error UnitsHistogramBenchmarks.dropwizardHistogramAdd thrpt 30 47985.359 ± 25.847 ops/msHistogramBenchmarks.descriptiveHistogramAdd thrpt 30 70158.792 ± 276.858 ops/ms--- with FLINK-10236, FLINK-12981, and FLINK-12982 ---HistogramBenchmarks.descriptiveHistogramAdd thrpt 30 75303.040 ± 475.355 ops/msHistogramBenchmarks.descrHistogramCircularAdd thrpt 30 200906.902 ± 384.483 ops/msHistogramBenchmarks.descrHistogramLinearAdd thrpt 30 189788.728 ± 233.283 ops/ms after adding each value, also retrieving a common set of metrics:Benchmark Mode Cnt Score Error UnitsHistogramBenchmarks.dropwizardHistogram thrpt 30 400.274 ± 4.930 ops/msHistogramBenchmarks.descriptiveHistogram thrpt 30 124.533 ± 1.060 ops/ms--- with FLINK-10236, FLINK-12981, and FLINK-12982 ---HistogramBenchmarks.descriptiveHistogram thrpt 30 251.895 ± 1.809 ops/msHistogramBenchmarks.descrHistogramCircular thrpt 30 301.068 ± 2.077 ops/msHistogramBenchmarks.descrHistogramLinear thrpt 30 234.050 ± 5.485 ops/ms</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="12984" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only call Histogram#getStatistics() once per set of retrieved statistics</summary>
      <description>In some occasions, Histogram#getStatistics() was called multiple times to retrieve different statistics. However, at least the Dropwizard implementation has some constant overhead per call and we should maybe rather interpret this method as returning a point-in-time snapshot of the histogram in order to get consistent values when querying them.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12990" opendate="2019-6-26 00:00:00" fixdate="2019-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type doesn&amp;#39;t consider the local TimeZone</summary>
      <description>Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13017" opendate="2019-6-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken and irreproducible dockerized docs build</summary>
      <description>The build tools around docs/docker seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:bash: /etc/bash_completion.d/git-prompt.sh: No such file or directorybash: __git_ps1: command not found/usr/bin/env: 'ruby.ruby2.5': No such file or directorybash: __git_ps1: command not foundReason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get builds which are independent from the host system (making them reproducible) not have the commands in the container affect the host</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13026" opendate="2019-6-28 00:00:00" fixdate="2019-7-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce JDBCLookupFunction</summary>
      <description>It is very common to store dimension information in JDBC databases. It is very useful to support JDBCLookupFunction, user can use this to lookup dimension fields in his streaming job.1.Introduce JDBCLookupFunction2.Introduce JDBCLookupTableSourceUser can use JDBCLookupFunction to table function, or user can use JDBCLookupTableSource to blink planner temporal table join.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUtils.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialect.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13028" opendate="2019-6-28 00:00:00" fixdate="2019-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move expression resolver to flink-table-api-java</summary>
      <description>Move the expression resolver to flink-table-api-java and replace the expression bridge used there.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ExpandColumnFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.FlattenCallRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.KeywordParseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilderImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.ExpressionBridge.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.SortOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.OperationTreeBuilderFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ColumnOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AliasOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.StarReferenceFlatteningRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.RuleExpressionVisitor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolverRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveFlattenCallRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ReferenceResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.QualifyBuiltInFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.OverWindowResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.LookupCallByNameRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveFlattenCallRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.OverWindowPartitionedOrdered.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ExpressionParser.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LocalOverWindow.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LookupCallResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.lookups.TableReferenceLookup.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.PlannerExpressionParser.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ResolvedExpressionDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationExpressionsUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.Count1AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.CountAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.IncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.IncrSumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.LeadLagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankLikeAggFunctionBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RowNumberAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.SingleValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.SumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.SumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.lookups.FieldReferenceLookup.java</file>
    </fixedFiles>
  </bug>
  <bug id="13045" opendate="2019-7-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Scala expression DSL to flink-table-api-scala</summary>
      <description>Moves the implicit conversions to the target module.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.package.scala</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.DataStreamConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.DataSetConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="13046" opendate="2019-7-1 00:00:00" fixdate="2019-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rename hive-site-path to hive-conf-dir to be consistent with standard name in Hive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogValidator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogDescriptor.java</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="13225" opendate="2019-7-11 00:00:00" fixdate="2019-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce type inference for hive functions in blink</summary>
      <description>See some conversation in https://github.com/apache/flink/pull/8920</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TypedFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.FlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DeferredTypeFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13227" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Asynchronous I/O for External Data Access" page into Chinese</summary>
      <description>The page url is https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/asyncio.htmlThe markdown file is located in flink/docs/dev/stream/operators/asyncio.md</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.asyncio.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13347" opendate="2019-7-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>should handle new JoinRelType(SEMI/ANTI) in switch case</summary>
      <description>Calcite 1.20 introduces SEMI &amp; ANTI to JoinRelType, blink planner &amp; flink planner should handle them in each switch case</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoinToCoProcessTranslator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13519" opendate="2019-7-31 00:00:00" fixdate="2019-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Connector sample code for Scala on version 6.x will not work</summary>
      <description>The Scala example in the documentation for the Elasticsearch Connector, version 6.x, will not work. The class ElasticsearchSinkFunction&amp;#91;String&amp;#93; requires a RuntimeContext and a RequestIndexer, which the example omits.Also, type needs to be in inverse quotes as it's a Scala keyword.It should look like the following:def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer) { val json = new java.util.HashMap[String, String] json.put("data", element) val rqst: IndexRequest = Requests.indexRequest .index("testarindex") .`type`("_doc") .source(json) indexer.add(rqst) } </description>
      <version>1.7.2,1.8.0,1.8.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="13538" opendate="2019-8-1 00:00:00" fixdate="2019-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give field names in deserializers thrown exceptions</summary>
      <description>Deserializers like JsonRowDeserializerSchema parse JSON strings according to a TypeInformation&lt;?&gt; object.Types mistakes can occur and it usually rise a IOException caused by a IllegalStateException. Here I try to parse "field":"blabla" described with Type.INT java.io.IOException: Failed to deserialize JSON object.    at org.apache.flink.formats.json.JsonRowDeserializationSchema.deserialize(JsonRowDeserializationSchema.java:97)    at com.dcbrain.etl.inputformat.JsonInputFormat.nextRecord(JsonInputFormat.java:96)    at com.dcbrain.etl.inputformat.JsonInputFormat.nextRecord(JsonInputFormat.java:1)    at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:192)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalStateException: Unsupported type information 'Integer' for node: "blabla"    at org.apache.flink.formats.json.JsonRowDeserializationSchema.convert(JsonRowDeserializationSchema.java:191)    at org.apache.flink.formats.json.JsonRowDeserializationSchema.convertRow(JsonRowDeserializationSchema.java:212)    at org.apache.flink.formats.json.JsonRowDeserializationSchema.deserialize(JsonRowDeserializationSchema.java:95)    ... 5 common frames omitted The message nor the exception objects contains reference to field causing this error which require time to inspect complex input data to find where the error really is.Could it be possible to improve messages or even Exceptions objects thrown by Serializers/Deserializers to get which field is responsible of the error please?JsonRowDeserializerSchema isn't the only one touched by such issues. This will allow to produce more useful logs to be read by users or administrators. All the best</description>
      <version>1.8.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.RowDataToJsonConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowDataToCsvConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDataDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RowDataToAvroConverters.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroToRowDataConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="13539" opendate="2019-8-1 00:00:00" fixdate="2019-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL do not support CSV tableFactory because CSV require format.fields</summary>
      <description>(Now DDL do not support key of properties contains number or "-".)And old csv validator require "format.fields.#.type". So there is an validation exception now.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsvValidator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.OldCsv.java</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="13540" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL do not support key of properties contains number or "-"</summary>
      <description>But many connector have key of properties contains number or "-", like kafka..So as long as we don't solve this problem, it's hard for users to use these connectors.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="13541" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State Processor Api sets the wrong key selector when writing savepoints</summary>
      <description>The state processor api is setting the wrong key selector for its StreamConfig when writing savepoints. It uses two key selectors internally that happen to output the same value for integer keys but not in general. Caused by: java.lang.RuntimeException: Exception occurred while setting the current key context. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setCurrentKey(AbstractStreamOperator.java:641) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement(AbstractStreamOperator.java:627) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement1(AbstractStreamOperator.java:615) at org.apache.flink.state.api.output.BoundedStreamTask.performDefaultAction(BoundedStreamTask.java:83) at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:140) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:378) at org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.mapPartition(BoundedOneInputStreamTaskRunner.java:76) at org.apache.flink.runtime.operators.MapPartitionDriver.run(MapPartitionDriver.java:103) at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504) at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:688) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:518) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:33) at org.apache.flink.contrib.streaming.state.RocksDBSerializedCompositeKeyBuilder.serializeKeyGroupAndKey(RocksDBSerializedCompositeKeyBuilder.java:159) at org.apache.flink.contrib.streaming.state.RocksDBSerializedCompositeKeyBuilder.setKeyAndKeyGroup(RocksDBSerializedCompositeKeyBuilder.java:96) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.setCurrentKey(RocksDBKeyedStateBackend.java:303) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setCurrentKey(AbstractStreamOperator.java:639) ... 12 more</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.BootstrapTransformationTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13807" opendate="2019-8-21 00:00:00" fixdate="2019-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink-avro unit tests fails if the character encoding in the environment is not default to UTF-8</summary>
      <description>On Flink release-1.8 branch:[ERROR] Tests run: 12, Failures: 4, Errors: 0, Skipped: 0, Time elapsed: 4.81 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest[ERROR] testSimpleAvroRead[Execution mode = CLUSTER](org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest) Time elapsed: 0.438 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Different elements in arrays: expected 2 elements and received 2files: [/tmp/junit5386344396421857812/junit6023978980792200274.tmp/4, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/2, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/1, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/3] expected: [{"name": "Alyssa", "favorite_number": 256, "favorite_color": null, "type_long_test": null, "type_double_test": 123.45, "type_null_test": null, "type_bool_test": true, "type_array_string": ["ELEMENT 1", "ELEMENT 2"], "type_array_boolean": [true, false], "type_nullable_array": null, "type_enum": "GREEN", "type_map": {"KEY 2": 17554, "KEY 1": 8546456}, "type_fixed": null, "type_union": null, "type_nested": {"num": 239, "street": "Baker Street", "city": "London", "state": "London", "zip": "NW1 6XE"}, "type_bytes": {"bytes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"}, "type_date": 2014-03-01, "type_time_millis": 12:12:12.000, "type_time_micros": 123456, "type_timestamp_millis": 2014-03-01T12:12:12.321Z, "type_timestamp_micros": 123456, "type_decimal_bytes": {"bytes": "\u0007?"}, "type_decimal_fixed": [7, -48]}, {"name": "Charlie", "favorite_number": null, "favorite_color": "blue", "type_long_test": 1337, "type_double_test": 1.337, "type_null_test": null, "type_bool_test": false, "type_array_string": [], "type_array_boolean": [], "type_nullable_array": null, "type_enum": "RED", "type_map": {}, "type_fixed": null, "type_union": null, "type_nested": {"num": 239, "street": "Baker Street", "city": "London", "state": "London", "zip": "NW1 6XE"}, "type_bytes": {"bytes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"}, "type_date": 2014-03-01, "type_time_millis": 12:12:12.000, "type_time_micros": 123456, "type_timestamp_millis": 2014-03-01T12:12:12.321Z, "type_timestamp_micros": 123456, "type_decimal_bytes": {"bytes": "\u0007?"}, "type_decimal_fixed": [7, -48]}] received: [{"name": "Alyssa", "favorite_number": 256, "favorite_color": null, "type_long_test": null, "type_double_test": 123.45, "type_null_test": null, "type_bool_test": true, "type_array_string": ["ELEMENT 1", "ELEMENT 2"], "type_array_boolean": [true, false], "type_nullable_array": null, "type_enum": "GREEN", "type_map": {"KEY 2": 17554, "KEY 1": 8546456}, "type_fixed": null, "type_union": null, "type_nested": {"num": 239, "street": "Baker Street", "city": "London", "state": "London", "zip": "NW1 6XE"}, "type_bytes": {"bytes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"}, "type_date": 2014-03-01, "type_time_millis": 12:12:12.000, "type_time_micros": 123456, "type_timestamp_millis": 2014-03-01T12:12:12.321Z, "type_timestamp_micros": 123456, "type_decimal_bytes": {"bytes": "\u0007??"}, "type_decimal_fixed": [7, -48]}, {"name": "Charlie", "favorite_number": null, "favorite_color": "blue", "type_long_test": 1337, "type_double_test": 1.337, "type_null_test": null, "type_bool_test": false, "type_array_string": [], "type_array_boolean": [], "type_nullable_array": null, "type_enum": "RED", "type_map": {}, "type_fixed": null, "type_union": null, "type_nested": {"num": 239, "street": "Baker Street", "city": "London", "state": "London", "zip": "NW1 6XE"}, "type_bytes": {"bytes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"}, "type_date": 2014-03-01, "type_time_millis": 12:12:12.000, "type_time_micros": 123456, "type_timestamp_millis": 2014-03-01T12:12:12.321Z, "type_timestamp_micros": 123456, "type_decimal_bytes": {"bytes": "\u0007??"}, "type_decimal_fixed": [7, -48]}] at org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest.after(AvroTypeExtractionTest.java:76)Comparing “expected” with “received”, there is really some question mark difference.For example, in “expected’, it’s"type_decimal_bytes": {"bytes": "\u0007?”}While in “received”, it’s "type_decimal_bytes": {"bytes": "\u0007??"}The environment I ran the unit tests on uses ANSI_X3.4-1968 I changed to "en_US.UTF-8" and the unit tests passed.</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.2,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13868" opendate="2019-8-27 00:00:00" fixdate="2019-9-27 01:00:00" resolution="Done">
    <buginformation>
      <summary>Job vertex add taskmanager id in rest api</summary>
      <description>In web, user want to see subtask run in which taskmanager. But now there is no taskmanager's id, user have to judge it by host and port. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="13893" opendate="2019-8-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 tests are failing due to missing jaxb dependency</summary>
      <description>The (hadoop/presto) S3 filesystems make use of javax.xml.bind but does not declare a dependency on jaxb-api and relies on the JDK containing this class. This is no longer the case on Java 11; we have to add it as a dependency and bundle it in the jar.16:20:12.975 [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.691 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase16:20:12.992 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 5.067 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)16:20:12.992 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.194 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NoClassDefFoundError&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)https://api.travis-ci.org/v3/job/577860512/log.txt</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13894" opendate="2019-8-29 00:00:00" fixdate="2019-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TaskExecutor log link to subtask view</summary>
      <description>As FLINK-13868 has added ResourceId in rest api of subatsk. So we can according to it add log link for Taskamanager which subtask  allocated.screenshot  </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-subtask.ts</file>
    </fixedFiles>
  </bug>
  <bug id="13941" opendate="2019-9-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent data-loss by not cleaning up small part files from S3.</summary>
      <description></description>
      <version>1.8.0,1.8.1,1.9.0</version>
      <fixedVersion>1.8.2,1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="1395" opendate="2015-1-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Jodatime support to Kryo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-dist.src.main.flink-bin.NOTICE</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTest.scala</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.LICENSE</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="13973" opendate="2019-9-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13995" opendate="2019-9-6 00:00:00" fixdate="2019-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix shading of the licence information of netty</summary>
      <description>The license filter isn't actually filtering anything. It should be META-INF/license/**.The first filter seems to be outdated btw.Multiple modules affected.&lt;filter&gt; &lt;artifact&gt;io.netty:netty&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/maven/io.netty/**&lt;/exclude&gt; &lt;!-- Only some of these licenses actually apply to the JAR and have been manuallyplaced in this module's resources directory. --&gt; &lt;exclude&gt;META-INF/license&lt;/exclude&gt; &lt;!-- Only parts of NOTICE file actually apply to the netty JAR and have been manuallycopied into this modules's NOTICE file. --&gt; &lt;exclude&gt;META-INF/NOTICE.txt&lt;/exclude&gt; &lt;/excludes&gt;&lt;/filter&gt;</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14004" opendate="2019-9-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define SourceReaderOperator to verify the integration with StreamOneInputProcessor</summary>
      <description>We already refactored the task input and output in runtime stack for considering the requirements of FLIP-27. In order to further verify that the new source could work well with the unified StreamOneInputProcessor in mailbox model, we define the SourceReaderOperator as task input and implement a unit test for passing through the whole process.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceReaderStreamTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14005" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.2.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14006" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java UDFs in Python API</summary>
      <description>Currently, user can not find out the doc for  how to using Java UDFs in Python API. So we should add the detail doc. In https://ci.apache.org/projects/flink/flink-docs-master/dev/table/udfs.html only describe how to using the APIs, but do not mention how to add the JARs to the class path.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="14007" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java user-defined source/sink in Python API</summary>
      <description>Currently, user can not find out the doc for how to using Java user-defined source/sink in Python API. So we should add the detail doc. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="14334" opendate="2019-10-7 00:00:00" fixdate="2019-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ElasticSearch docs refer to non-existent ExceptionUtils.containsThrowable</summary>
      <description></description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraFailureHandler.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.zh.md</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="14336" opendate="2019-10-7 00:00:00" fixdate="2019-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions in AsyncCheckpointRunnable#run are never logged</summary>
      <description>Exceptions that are thrown in AsyncCheckpointRunnable#run and subsequently being handled in AsyncCheckpointRunnable#handleExecutionException are never logged on the TaskExecutor side, and are only forwarded to the JobMaster.</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14398" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Further split input unboxing code into separate methods</summary>
      <description>In one of our production pipelines, we have a table with 1200+ columns.  At runtime, it failed due to a method inside the generated code exceeding 64kb when compiled to bytecode.After we investigated the generated code, it appeared that the map method inside a generated RichMapFunction was too long. See attached file (codegen.example.txt).In the problematic map method, result setters were correctly split into individual methods and did not have the largest footprint.However, there were also 1000+ input unboxing expressions inside reusableInputUnboxingExprs, which, individually were not trivial and when flattened linearly in the map function here, pushed the method size beyond 64kb in bytecode.We think it is worthwhile to split these input unboxing code snippets into individual methods.  We were able to verify, in our production environment, that splitting input unboxing code snippets into individual methods resolves the issue.  Would love to hear thoughts from the team and find the best path to fix it.</description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14445" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build failed when making sdist</summary>
      <description>From the description of error-log from building python module in travis, it seems invocation failed for sdist-make and then the phase of building python module exited.The instance log: https://api.travis-ci.com/v3/job/246710918/log.txt</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="14639" opendate="2019-11-7 00:00:00" fixdate="2019-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metrics User Scope docs refer to wrong class</summary>
      <description>Hi , i think it should be `MetricGroup{{#addGroup(String key, String value)}}` </description>
      <version>1.8.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="14954" opendate="2019-11-26 00:00:00" fixdate="2019-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="15080" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploy OSS filesystem to maven central</summary>
      <description>Just noticed that the OSS filesystem isn't being deployed.I see no reason why this is the case; we are deploying all other artifacts and it just makes it more difficult to access older versions (since you'd have to splice them out of a distribution).</description>
      <version>1.8.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16862" opendate="2020-3-30 00:00:00" fixdate="2020-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove example url in quickstarts</summary>
      <description>Our quickstarts define an example url http:/myorganization.org, which as it turns out is a real website.We should just remove tag.</description>
      <version>1.8.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17073" opendate="2020-4-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slow checkpoint cleanup causing OOMs</summary>
      <description>A user reported that he sees a decline in checkpoint cleanup speed when upgrading from Flink 1.7.2 to 1.10.0. The result is that a lot of cleanup tasks are waiting in the execution queue occupying memory. Ultimately, the JM process dies with an OOM.Compared to Flink 1.7.2, we introduced a dedicated ioExecutor which is used by the HighAvailabilityServices (FLINK-11851). Before, we use the AkkaRpcService thread pool which was a ForkJoinPool with a max parallelism of 64. Now it is a FixedThreadPool with as many threads as CPU cores. This change might have caused the decline in completed checkpoint discard throughput. This suspicion needs to be validated before trying to fix it!&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r390e5d775878918edca0b6c9f18de96f828c266a888e34ed30ce8494%40%3Cuser.flink.apache.org%3E</description>
      <version>1.7.3,1.8.0,1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RegionFailoverITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.CheckpointsUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SerializableRunnable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaningRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="17107" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17254" opendate="2020-4-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve the PyFlink documentation and examples to use SQL DDL for source/sink definition</summary>
      <description>Currently there are two ways to register a table sink/source in PyFlink table API:1) TableEnvironment.connect2) TableEnvironment.sql_updateI think it's better to provide documentation and examples on how to use 2) in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="17256" opendate="2020-4-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Suppport keyword arguments in the PyFlink Descriptor API</summary>
      <description>Keyword arguments is a very commonly used feature in Python. We should support it in the PyFlink Descriptor API to make the API more user friendly for Python users.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  <bug id="19009" opendate="2020-8-21 00:00:00" fixdate="2020-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong way to calculate the "downtime" metric</summary>
      <description>Currently the way to calculate the Flink system metric "downtime"  is not consistent with the description in the doc, now the downtime is actually the current timestamp minus the time timestamp when the job started.   But Flink doc (https://flink.apache.org/gettinghelp.html) obviously describes the time as the current timestamp minus the timestamp when the job failed. I believe we should update the code this metric as the Flink doc shows. The easy way to solve this is using the current timestamp to minus the latest uptime timestamp.</description>
      <version>1.7.2,1.8.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.metrics.DownTimeGauge.java</file>
    </fixedFiles>
  </bug>
  <bug id="1901" opendate="2015-4-16 00:00:00" fixdate="2015-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create sample operator for Dataset</summary>
      <description>In order to be able to implement Stochastic Gradient Descent and a number of other machine learning algorithms we need to have a way to take a random sample from a Dataset.We need to be able to sample with or without replacement from the Dataset, choose the relative or exact size of the sample, set a seed for reproducibility, and support sampling within iterations.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSetUtils.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.NOTICE</file>
      <file type="M">flink-dist.src.main.flink-bin.LICENSE</file>
    </fixedFiles>
  </bug>
  <bug id="19010" opendate="2020-8-21 00:00:00" fixdate="2020-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a system metric to show the checkpoint restore time</summary>
      <description>Now the system metric only shows the downtime when failure happens. It would be interesting to see the time to restore the checkpoint, so users can better understand the bottleneck of failure recovery.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="19173" opendate="2020-9-9 00:00:00" fixdate="2020-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Pandas Batch Group Aggregation Function Operator</summary>
      <description>Add Pandas Batch Group Aggregation Function Operator</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperatorBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamTwoInputPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonReduceFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="19300" opendate="2020-9-21 00:00:00" fixdate="2020-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timer loss after restoring from savepoint</summary>
      <description>While using heap-based timers, we are seeing occasional timer loss after restoring program from savepoint, especially when using a remote savepoint storage (s3). After some investigation, the issue seems to be related to this line in deserialization. When trying to check the VERSIONED_IDENTIFIER, the input stream may not guarantee filling the byte array, causing timers to be dropped for the affected key group.Should keep reading until expected number of bytes are actually read or if end of the stream has been reached. </description>
      <version>1.8.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.io.PostVersionedIOReadableWritableTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.IOUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.PostVersionedIOReadableWritable.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
