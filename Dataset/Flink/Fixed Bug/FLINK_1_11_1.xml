<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="16513" opendate="2020-3-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement API to persist channel state: checkpointing metadata</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OperatorSnapshotUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.RestoreStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PrioritizedOperatorSubtaskStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ResultSubpartitionStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.InputChannelStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateObjectCollection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.InputChannelInfo.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.splits.KeyGroupRangeInputSplit.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateHandleDummyUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.RoundRobinOperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PrioritizedOperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.OperatorStateInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16516" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Avoid codegen user-defined function for Python UDF</summary>
      <description>Currently we make use of codegen to generate PythonScalarFunction and PythonTableFunction, but it is unnecessary. We can directly create a static PythonScalarFunction and PythonTableFunction.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="1799" opendate="2015-3-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not support generic arrays</summary>
      <description>The Scala API does not support generic arrays at the moment. It throws a rather unhelpful error message ```InvalidTypesException: The given type is not a valid object array```.Code to reproduce the problem is given below:def main(args: Array[String]) { foobar[Double]}def foobar[T: ClassTag: TypeInformation]: DataSet[Block[T]] = { val tpe = createTypeInformation[Array[T]] null}</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="18182" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK in flink-connector-kinesis to include new region af-south-1</summary>
      <description>Current (1.11.1) version of flink-connector-kinesis is compiled against version 1.11.754 of the AWS SDK, which does not include the new af-south-1 (Cape Town) AWS region.Looking at the release notes for AWS SDK - this region is included from version 1.11.768 onwards.I'd be happy to try and create a PR for this.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18343" opendate="2020-6-17 00:00:00" fixdate="2020-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DEBUG logging for java e2e tests</summary>
      <description>Java e2e tests run with the default logging configuration, which only logs on INFO.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug id="18595" opendate="2020-7-14 00:00:00" fixdate="2020-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock during job shutdown</summary>
      <description>https://travis-ci.org/github/apache/flink/jobs/707843779Found one Java-level deadlock:============================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": waiting to lock monitor 0x00007f51f655e228 (object 0x00000000812b9180, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Flat Map -&gt; Sink: Unnamed (9/12)""Flat Map -&gt; Sink: Unnamed (9/12)": waiting to lock monitor 0x000055fb00bb4b88 (object 0x00000000812b9210, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641)."Java stack information for the threads listed above:==================================================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.b4511: No such processuffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.releaseAll(RemoteInputChannel.java:665) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:254) - locked &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:431) - locked &lt;0x0000000080ba2488&gt; (a java.lang.Object) at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:85) at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:901) at org.apache.flink.runtime.taskmanager.Task$$Lambda$434/985222953.run(Unknown Source) at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1370) at java.lang.Thread.run(Thread.java:748)"Flat Map -&gt; Sink: Unnamed (9/12)": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.addExclusiveBuffer(RemoteInputChannel.java:629) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.recycle(RemoteInputChannel.java:314) - locked &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.streaming.runtime.io.CachedBufferStorage.close(CachedBufferStorage.java:113) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.cleanup(CheckpointedInputGate.java:216) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.close(StreamTaskNetworkInput.java:208) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.close(StreamOneInputProcessor.java:82) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanup(StreamTask.java:298) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:555) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:480) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock.</description>
      <version>1.10.0,1.10.1,1.11.0,1.11.1</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.BufferManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18644" opendate="2020-7-20 00:00:00" fixdate="2020-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove obsolete doc for hive connector</summary>
      <description></description>
      <version>1.10.1,1.11.1</version>
      <fixedVersion>1.10.2,1.11.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="18660" opendate="2020-7-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump netty to 4.1.49</summary>
      <description>Bump netty to 4.1.49 for some security fixes.This also entails bumping netty-tcnative to 2.30.0 .</description>
      <version>None</version>
      <fixedVersion>shaded-12.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18673" opendate="2020-7-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling ROW() in a UDF results in UnsupportedOperationException</summary>
      <description>Given a UDF func that accepts a ROW(INT, STRING) as parameter, it cannot be called like this:SELECT func(ROW(a, b)) FROM twhile this worksSELECT func(r) FROM (SELECT ROW(a, b) FROM t) The exception returned is:org.apache.flink.table.api.ValidationException: SQL validation failed. nullwith an empty UnsupportedOperationException as cause.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="18678" opendate="2020-7-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector fails to create vector orc reader if user specifies incorrect hive version</summary>
      <description>Issue reported by user. User's Hive deployment is 2.1.1 and uses flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar in Flink lib. If user specifies Hive version as 2.1.1, then creating vectorized orc reader fails with exception:java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.orc.ReaderImpl cannot be cast to org.apache.orc.Reader at org.apache.flink.orc.shim.OrcShimV200.createReader(OrcShimV200.java:63) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:98) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcSplitReader.&lt;init&gt;(OrcSplitReader.java:73) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcColumnarRowSplitReader.&lt;init&gt;(OrcColumnarRowSplitReader.java:54) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0] at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:91) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0]......</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18694" opendate="2020-7-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unaligned checkpoint config to web ui</summary>
      <description>What is the purpose of the change Show in web ui if unaligned checkpoints are enabled.Brief change log Adds unaligned checkpoint config to REST endpoint, and web ui. https://github.com/apache/flink/pull/12962</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug id="18705" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debezium-JSON throws NPE when tombstone message is received</summary>
      <description>By default, Debezium will send two messages to Kafka for DELETE operation, one for delete message, the other for tombstone message (message value is null). However, debezium-json will throw NPE when processing such tombstone message. We should just skip such messages. As a workaround, we can diable tombstone on Debezium Connect tombstones.on.delete: false.</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="18708" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct</summary>
      <description>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="18725" opendate="2020-7-27 00:00:00" fixdate="2020-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Run Kubernetes test" failed with "30025: provided port is already allocated"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4901&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=3e8647c1-5a28-5917-dd93-bf78594ea994The Service "flink-job-cluster" is invalid: spec.ports[2].nodePort: Invalid value: 30025: provided port is already allocated</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.job-cluster-service.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="18726" opendate="2020-7-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT INTO specific columns</summary>
      <description>Currently Flink only supports insert into a table without specifying columns, but most database systems support insert into specific columns byINSERT INTO table_name(column1, column2, ...) ...The columns not specified will be filled with default values or NULL if no default value is given when creating the table.As Flink currently does not support default values when creating tables, we can fill the unspecified columns with NULL and throw exceptions if there are columns with NOT NULL constraints.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18730" opendate="2020-7-27 00:00:00" fixdate="2020-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="18731" opendate="2020-7-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18750" opendate="2020-7-29 00:00:00" fixdate="2020-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlValidatorException thrown when select from a view which contains a UDTF call</summary>
      <description>When executing such code: package com.example;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import org.apache.flink.table.functions.TableFunction;public class TestUTDF { public static class UDTF extends TableFunction&lt;String&gt; { public void eval(String input) { collect(input); } } public static void main(String[] args) { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create( env, EnvironmentSettings.newInstance().useBlinkPlanner().build()); tEnv.createTemporarySystemFunction("udtf", new UDTF()); tEnv.createTemporaryView("source", tEnv.fromValues("a", "b", "c").as("f0")); String udtfCall = "SELECT S.f0, T.f1 FROM source as S, LATERAL TABLE(udtf(f0)) as T(f1)"; System.out.println(tEnv.explainSql(udtfCall)); String createViewCall = "CREATE VIEW tmp_view AS" + udtfCall; tEnv.executeSql(createViewCall); System.out.println(tEnv.from("tmp_view").explain()); }}Such a SqlValidatorException would be thrown:  == Abstract Syntax Tree ==== Abstract Syntax Tree ==LogicalProject(f0=[$0], f1=[$1])+- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0}])   :- LogicalProject(f0=[AS($0, _UTF-16LE'f0')])   :  +- LogicalValues(tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]])   +- LogicalTableFunctionScan(invocation=[udtf($cor0.f0)], rowType=[RecordType(VARCHAR(2147483647) EXPR$0)])== Optimized Logical Plan ==Correlate(invocation=[udtf($cor0.f0)], correlate=[table(udtf($cor0.f0))], select=[f0,EXPR$0], rowType=[RecordType(CHAR(1) f0, VARCHAR(2147483647) EXPR$0)], joinType=[INNER])+- Calc(select=[f0])   +- Values(type=[RecordType(CHAR(1) f0)], tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]])== Physical Execution Plan ==Stage 1 : Data Source content : Source: Values(tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]]) Stage 2 : Operator content : Calc(select=[f0]) ship_strategy : FORWARD Stage 3 : Operator content : Correlate(invocation=[udtf($cor0.f0)], correlate=[table(udtf($cor0.f0))], select=[f0,EXPR$0], rowType=[RecordType(CHAR(1) f0, VARCHAR(2147483647) EXPR$0)], joinType=[INNER]) ship_strategy : FORWARDException in thread "main" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 4, column 14 to line 4, column 17: Column 'f0' not found in any table at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$ToRelContextImpl.expandView(FlinkPlannerImpl.scala:204) at org.apache.calcite.plan.ViewExpanders$1.expandView(ViewExpanders.java:52) at org.apache.flink.table.planner.catalog.SqlCatalogViewTable.convertToRel(SqlCatalogViewTable.java:58) at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.expand(ExpandingPreparingTable.java:59) at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.toRel(ExpandingPreparingTable.java:55) at org.apache.calcite.rel.core.RelFactories$TableScanFactoryImpl.createScan(RelFactories.java:533) at org.apache.calcite.tools.RelBuilder.scan(RelBuilder.java:1044) at org.apache.calcite.tools.RelBuilder.scan(RelBuilder.java:1068) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:349) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:152) at org.apache.flink.table.operations.CatalogQueryOperation.accept(CatalogQueryOperation.java:69) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:149) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:131) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:92) at org.apache.flink.table.operations.CatalogQueryOperation.accept(CatalogQueryOperation.java:69) at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.scala:165) at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:82) at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:80) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:80) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:43) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:654) at org.apache.flink.table.api.internal.TableImpl.explain(TableImpl.java:575) at com.example.TestUTDF.main(TestUTDF.java:39)Caused by: org.apache.calcite.runtime.CalciteContextException: From line 4, column 14 to line 4, column 17: Column 'f0' not found in any table at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089) at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:259) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5882) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1785) at org.apache.calcite.sql.SqlOperator.constructArgTypeList(SqlOperator.java:609) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:236) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303) at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ... 31 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'f0' not found in any table at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550) ... 72 more  After some debugging I think it is because the keyword "lateral" was discarded after executing the create view operation.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18769" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniBatch doesn&amp;#39;t work with FLIP-95 source</summary>
      <description>The following Table API streaming job is stuck when enabling mini batching StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings); // disable mini-batching completely to get a result Configuration tableConf = tableEnv.getConfig() .getConfiguration(); tableConf.setString("table.exec.mini-batch.enabled", "true"); tableConf.setString("table.exec.mini-batch.allow-latency", "5 s"); tableConf.setString("table.exec.mini-batch.size", "5000"); tableConf.setString("table.optimizer.agg-phase-strategy", "TWO_PHASE"); tableEnv.executeSql( "CREATE TABLE input_table (" + "location STRING, " + "population INT" + ") WITH (" + "'connector' = 'kafka', " + "'topic' = 'kafka_batching_input', " + "'properties.bootstrap.servers' = 'localhost:9092', " + "'format' = 'csv', " + "'scan.startup.mode' = 'earliest-offset'" + ")"); tableEnv.executeSql( "CREATE TABLE result_table WITH ('connector' = 'print') LIKE input_table (EXCLUDING OPTIONS)"); tableEnv .from("input_table") .groupBy($("location")) .select($("location").cast(DataTypes.CHAR(2)).as("location"), $("population").sum().as("population")) .executeInsert("result_table");I am using a pre-populated Kafka topic called kafka_batching_input with these elements:"Berlin",1"Berlin",2</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.MiniBatchIntervalInferRule.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18809" opendate="2020-8-4 00:00:00" fixdate="2020-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update internal aggregate functions to new type system</summary>
      <description>Imperative aggregate functions use the old type system for computation. After FLINK-15803 is completed, we can update those implementations.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstLastValueAggFunctionWithOrderTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.aggregation.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.typeutils.DataViewUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="18861" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18862" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LISTAGG throws BinaryRawValueData cannot be cast to StringData exception in runtime</summary>
      <description>1. Env：flinksql、 version 1.11.1，perjob mode2. Error：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData3、Job：(1) create a kafka table CREATE TABLE kafka( x String, y String )with( 'connector' = 'kafka', ...... )(2)create a view: CREATE VIEW view1 AS SELECT x, y, CAST(COUNT(1) AS VARCHAR) AS ct FROM kafka GROUP BY x, y(3) aggregate on the view: select x, LISTAGG(CONCAT_WS('=', y, ct), ',') AS lists FROM view1 GROUP BY xAnd then the exception is thrown：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringDataThe problem is that, there is no RawValueData in the query. The result type of count(1) should be bigint, not RawValueData. (4) If there is no aggregation, the job can run succefully. select x, CONCAT_WS('=', y, ct) from view1The detailed exception:java.lang.ClassCastException: org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.data.JoinedRowData.getString(JoinedRowData.java:139) ~[flink-table-blink_2.11-1.11.1.jar:?] at org.apache.flink.table.data.RowData.get(RowData.java:273) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:156) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:123) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:50) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:715) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:205) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1] at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:161) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:178) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:153) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [ad_features_auto-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [ad_features_auto-1.0-SNAPSHOT.jar:?]</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="18863" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support read_text_file() and print() interface for Python DataStream API.</summary>
      <description>Support print() and read_text_file() interface for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18864" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support key_by() operation for Python DataStream API</summary>
      <description>Support key_by() operation for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18867" opendate="2020-8-10 00:00:00" fixdate="2020-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generic table stored in Hive catalog is incompatible between 1.10 and 1.11</summary>
      <description>Generic table stored in 1.10 cannot be accessed in 1.11, because we changed how table schema is stored.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="18878" opendate="2020-8-10 00:00:00" fixdate="2020-8-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support dependency management for Python StreamExecutionEnvironment.</summary>
      <description>Add dependency management for StreamExecutionEnvironment when Users need to specified third party dependencies in their DataStream Job. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonConfigUtilTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18883" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support reduce() operation for Python KeyedStream.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18884" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add chaining strategy and slot sharing group interfaces for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18885" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add partitioning interfaces for Python DataStream API.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18886" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Kafka connectors for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug id="18888" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support execute_async for StreamExecutionEnvironment.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="18901" opendate="2020-8-12 00:00:00" fixdate="2020-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new type inference for SQL DDL of aggregate functions</summary>
      <description>Enables the new type inference in the SQL DDL.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.functions.UserDefinedFunctionHelperTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.functions.FunctionDefinitionUtilTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.FunctionDefinitionUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18906" opendate="2020-8-12 00:00:00" fixdate="2020-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support checkpointing with multiple input operator chained with sources</summary>
      <description>Generalize `CheckpointBarrierHandlers` plus hook in sources to `CheckpointBarrierHandlers`.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnalignerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnalignerCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.InputProcessorUtilTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.IndexedInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.CheckpointableInput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CreditBasedCheckpointBarrierAlignerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.AvailabilityProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMultipleInputSelectiveReadingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="18916" opendate="2020-8-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Operations" link(linked to dev/table/tableApi.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.user-guide.table.sql.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.sql.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18917" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Built-in Functions" link (linked to dev/table/functions/systemFunctions.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug id="1892" opendate="2015-4-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local job execution does not exit.</summary>
      <description>When using the LocalTezEnvironment to run a job from the IDE the job fails to exit after producing data. The following thread seems to run and not allow the job to exit:"Thread-31" #46 prio=5 os_prio=31 tid=0x00007fb5d2c43000 nid=0x5507 runnable &amp;#91;0x0000000127319000&amp;#93; java.lang.Thread.State: RUNNABLE at java.lang.Throwable.fillInStackTrace(Native Method) at java.lang.Throwable.fillInStackTrace(Throwable.java:783) locked &lt;0x000000076dfda130&gt; (a java.lang.InterruptedException) at java.lang.Throwable.&lt;init&gt;(Throwable.java:250) at java.lang.Exception.&lt;init&gt;(Exception.java:54) at java.lang.InterruptedException.&lt;init&gt;(InterruptedException.java:57) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:545) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.processRequest(LocalTaskSchedulerService.java:322) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.run(LocalTaskSchedulerService.java:316) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18926" opendate="2020-8-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Variables" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18930" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Hive Dialect" page of "Hive Integration" into Chinese</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18936" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation about user-defined aggregate functions</summary>
      <description>The documentation needs an update because all functions support the new type inference now.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18937" opendate="2020-8-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Setup" section to the "Installation" document</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="18943" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support CoMapFunction for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.dev.glibc.version.fix.h</file>
    </fixedFiles>
  </bug>
  <bug id="18944" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support JDBC connector for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug id="18945" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support CoFlatMap for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18947" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support partitionCustom() operation for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperatorBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18948" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end to end test for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18949" opendate="2020-8-14 00:00:00" fixdate="2020-8-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Streaming File Sink for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.serialization.schemas.py</file>
      <file type="M">flink-python.pyflink.common.serialization.schemas.py</file>
    </fixedFiles>
  </bug>
  <bug id="18950" opendate="2020-8-14 00:00:00" fixdate="2020-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Operations in Python DataStream API.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.index.zh.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18951" opendate="2020-8-14 00:00:00" fixdate="2020-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for dependency management in Python DataStream API.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18952" opendate="2020-8-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 10 minutes to DataStream API documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.data.types.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.data.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="18971" opendate="2020-8-17 00:00:00" fixdate="2020-10-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support to mount kerberos conf as ConfigMap and Keytab as Secrete</summary>
      <description>Currently, if user want to enable Kerberos Authentication, they need to build a custom image with keytab and krb5 conf file. To improve usability, we need to create a ConfigMap and a Secrete for krb5 conf and keytab when needed.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.overlays.Krb5ConfOverlayTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.overlays.Krb5ConfOverlay.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesTaskManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18978" opendate="2020-8-17 00:00:00" fixdate="2020-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support full table scan of key and namespace from statebackend</summary>
      <description>Support full table scan of keys and namespaces from the state backend. All operations assume the calling code already knows what namespace they are interested in interacting with.This is a prerequisite to support reading window operators with the state processor api because window panes are stored as additional namespace components.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.iterator.RocksStateKeysIterator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="18988" opendate="2020-8-18 00:00:00" fixdate="2020-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Continuous query with LATERAL and LIMIT produces wrong result</summary>
      <description>I was trying out the example queries provided in this blog post: https://materialize.io/lateral-joins-and-demand-driven-queries/ to check if Flink supports the same and found that the queries were translated and executed but produced the wrong result.I used the SQL Client and Kafka (running at kafka:9092) to store the table data. I executed the following statements:-- create cities tableCREATE TABLE cities ( name STRING NOT NULL, state STRING NOT NULL, pop INT NOT NULL) WITH ( 'connector' = 'kafka', 'topic' = 'cities', 'properties.bootstrap.servers' = 'kafka:9092', 'properties.group.id' = 'mygroup', 'scan.startup.mode' = 'earliest-offset', 'format' = 'json');-- fill cities tableINSERT INTO cities VALUES ('Los_Angeles', 'CA', 3979576), ('Phoenix', 'AZ', 1680992), ('Houston', 'TX', 2320268), ('San_Diego', 'CA', 1423851), ('San_Francisco', 'CA', 881549), ('New_York', 'NY', 8336817), ('Dallas', 'TX', 1343573), ('San_Antonio', 'TX', 1547253), ('San_Jose', 'CA', 1021795), ('Chicago', 'IL', 2695598), ('Austin', 'TX', 978908);-- execute querySELECT state, name FROM (SELECT DISTINCT state FROM cities) states, LATERAL ( SELECT name, pop FROM cities WHERE state = states.state ORDER BY pop DESC LIMIT 3 );-- resultstate name CA Los_Angeles NY New_York IL Chicago-- expected resultstate | name------+-------------TX    | DallasAZ    | PhoenixIL    | ChicagoTX    | HoustonCA    | San_JoseNY    | New_YorkCA    | San_DiegoCA    | Los_AngelesTX    | San_AntonioAs you can see from the query result, Flink computes the top3 cities over all states, not for every state individually. Hence, I assume that this is a bug in the query optimizer or one of the rewriting rules.There are two valid ways to solve this issue: Fixing the rewriting rules / optimizer (obviously preferred) Disabling this feature and throwing an exception</description>
      <version>1.11.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18989" opendate="2020-8-18 00:00:00" fixdate="2020-8-18 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>Write input and output channel state to separate files</summary>
      <description>To support sequential read of channel state (which would be more efficient),input and output channel state should be stored in separate files/handles.This will allow to keep in/out state reading code separate and run it in parallel. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMultipleInputSelectiveReadingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReader.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockIndexedInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerMassiveRandomTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.InputGateWithMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteRecoveredInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalRecoveredInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.RefCountingFSDataInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateStreamReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18998" opendate="2020-8-19 00:00:00" fixdate="2020-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No watermark is shown on Flink UI when ProcessingTime is used</summary>
      <description>As stated in the subject, no watermark is shown on Flink UI when ProcessingTime is used, see the attached screenshot. It is better to be more specific, like "Watermarks are only available if EventTime is used." </description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
    </fixedFiles>
  </bug>
  <bug id="19005" opendate="2020-8-20 00:00:00" fixdate="2020-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document JDBC drivers as source of Metaspace leaks</summary>
      <description>Hi !Im running a 1.11.1 flink cluster, where I execute batch jobs made with DataSet API.I submit these jobs every day to calculate daily data.In every execution, cluster's used metaspace increase by 7MB and its never released.This ends up with an OutOfMemoryError caused by Metaspace every 15 days and i need to restart the cluster to clean the metaspacetaskmanager.memory.jvm-metaspace.size is set to 512mbAny idea of what could be causing this metaspace grow and why is it not released ? =================================================== Summary ======================================================================================Case 1, reported by gestevez: Flink 1.11.1 Java 11 Maximum Metaspace size set to 512mb Custom Batch job, submitted daily Requires restart every 15 days after an OOM Case 2, reported by Echo Lee: Flink 1.11.0 Java 11 G1GC WordCount Batch job, submitted every second / every 5 minutes eventually fails TaskExecutor with OOMCase 3, reported by DaDaShen Flink 1.11.0 Java 11 WordCount Batch job, submitted every 5 seconds growing Metaspace, eventually OOM </description>
      <version>1.11.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  <bug id="1902" opendate="2015-4-17 00:00:00" fixdate="2015-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface reports false (the default) jobmanager.rpc.port on YARN</summary>
      <description>Running Flink as YARN session mode I was completely confused by the web interface reporting a false jobmanager.rpc.port (the default).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="1909" opendate="2015-4-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor streaming scala api to use returns for adding typeinfo</summary>
      <description>Currently the streaming scala api uses transform to pass the extracted type information instead of .returns. This leads to a lot of code duplication.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedDataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.TypeFillTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DiscretizedStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="19092" opendate="2020-8-29 00:00:00" fixdate="2020-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse comment on computed column failed</summary>
      <description>Parser Exception will be thrown when add comment on computed column,{{}}CREATE TABLE test ( `id` BIGINT, `age` INT COMMENT 'test comment', //PASS `nominal_age` as age + 1 COMMENT 'test comment' // FAIL) WITH ( 'connector' = 'values', 'data-id' = '$dataId') org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "COMMENT" at line 6, column 28.org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "COMMENT" at line 6, column 28.Was expecting one of:    ")" ...    "," ...    "." ...    "NOT" ...    "IN" ...    "&lt;" ...    "&lt;=" ...    "&gt;" ...    "&gt;=" ...    "=" ...    "&lt;&gt;" ...    "!=" ...    "BETWEEN" ...    "LIKE" ...    "SIMILAR" ...    "+" ...    "-" ...    "*" ...    "/" ...    "%" ...    "||" ...    "AND" ...    "OR" ...    "IS" ...    "MEMBER" ...    "SUBMULTISET" ...    "CONTAINS" ...    "OVERLAPS" ...    "EQUALS" ...    "PRECEDES" ...    "SUCCEEDS" ...    "IMMEDIATELY" ...    "MULTISET" ...    "[" ...    "FORMAT" ...     at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:659) at org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.createLookupTable(LookupJoinITCase.scala:96) at org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.before(LookupJoinITCase.scala:72) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered "COMMENT" at line 6, column 28.Was expecting one of:we should support it as the Flink doc&amp;#91;1&amp;#93; described:&lt;computed_column_definition&gt;: column_name AS computed_column_expression &amp;#91;COMMENT column_comment&amp;#93;  &amp;#91;1&amp;#93;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.MergeTableLikeUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.MergeTableLikeUtil.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="19109" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="19110" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="19121" opendate="2020-9-2 00:00:00" fixdate="2020-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid accessing HDFS frequently in HiveBulkWriterFactory</summary>
      <description>In HadoopPathBasedBulkWriter, getSize will invoke `FileSystem.exists` and `FileSystem.getFileStatus`, but it is invoked per record.There will be lots of visits to HDFS, may make HDFS pressure too high.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19131" opendate="2020-9-3 00:00:00" fixdate="2020-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add py38 support in PyFlink</summary>
      <description>At present, py39 has been released, and many open source projects have supported PY38, such as, beam, arrow, pandas, etc. so, would be great to support Py38 in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.metric.FlinkMetricContainerTest.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.metric.FlinkMetricContainer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.runners.python.beam.BeamDataStreamPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolThreadCache.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PooledByteBufAllocator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p21p0.io.netty.buffer.PoolArena.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.runners.fnexecution.state.GrpcStateService.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="19133" opendate="2020-9-3 00:00:00" fixdate="2020-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>User provided kafka partitioners are not initialized correctly</summary>
      <description>Reported in the ML: https://lists.apache.org/thread.html/r94275a7314d44154eb1ac16237906e0f097e8a9d8a5a937e8dcb5e85%40%3Cdev.flink.apache.org%3EIf a user provides a partitioner in combination with SerializationSchema it is not initialized correctly and has no access to the parallel instance index or number of parallel instances.</description>
      <version>1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="19170" opendate="2020-9-9 00:00:00" fixdate="2020-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parameter naming error</summary>
      <description></description>
      <version>1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19213" opendate="2020-9-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Chinese documentation</summary>
      <description>We should update the Chinese documentation with the changes introduced in FLINK-18802</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.avro-confluent.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="19224" opendate="2020-9-14 00:00:00" fixdate="2020-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an easy way to read window state</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.utils.SavepointTestBase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.ExistingSavepoint.java</file>
      <file type="M">docs.dev.libs.state.processor.api.zh.md</file>
      <file type="M">docs.dev.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="19229" opendate="2020-9-15 00:00:00" fixdate="2020-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce the PythonStreamGroupAggregateOperator for Python UDAF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.RowDataPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="19243" opendate="2020-9-15 00:00:00" fixdate="2020-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Snakeyaml to 1.27</summary>
      <description>CVE-2017-18640</description>
      <version>None</version>
      <fixedVersion>shaded-12.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19284" opendate="2020-9-18 00:00:00" fixdate="2020-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use Python UDF in the Java Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="19421" opendate="2020-9-26 00:00:00" fixdate="2020-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDAF in streaming mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonAggregateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunctionInfo.java</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="19434" opendate="2020-9-27 00:00:00" fixdate="2020-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add StreamJobGraphGenerator support for source chaining</summary>
      <description>Add changes around StreamJobGraphGenerator to generate job graph with source chaining, so that both DataStream and Table API can take advantage of source chaining.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ChainingStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19453" opendate="2020-9-29 00:00:00" fixdate="2020-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate old source and sink interfaces</summary>
      <description>Deprecate all interfaces and classes that are not necessary anymore with FLIP-95.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.tsextractors.TimestampExtractor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.RowtimeAttributeDescriptor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.ProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.PartitionableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.NestedFieldsProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LookupableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.LimitableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FilterableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.FieldComputer.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedRowtimeAttributes.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedProctimeAttribute.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedFieldMapping.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.PartitionableTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.OverwritableTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.StreamTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.InputFormatTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.UpsertStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.StreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.RetractStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.OutputFormatTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.AppendStreamTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.StreamTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BatchTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BatchTableSinkFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19472" opendate="2020-9-30 00:00:00" fixdate="2020-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement one input sorting DataInput</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.algorithm.configuration.html</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AbstractTwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sort.SortingDataInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AlgorithmOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.SpillingThread.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorter.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19473" opendate="2020-9-30 00:00:00" fixdate="2020-10-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement multi inputs sorting DataInput</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.sort.SortingDataInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.sort.SortingDataInputITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sort.SortingDataInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SortingBoundedInputITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AbstractTwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19474" opendate="2020-9-30 00:00:00" fixdate="2020-10-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a state backend that holds a single key at a time</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19475" opendate="2020-9-30 00:00:00" fixdate="2020-10-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a timer service that holds a single key at a time</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="19486" opendate="2020-10-1 00:00:00" fixdate="2020-10-1 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>expected: class org.apache.flink.runtime.state.KeyGroupsStateHandle, but found: class org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle</summary>
      <description>The reason why I reopen it is that: I can not resume from incremental checkpoint manually when I stop the following program by force(maybe  NOT supported officially?).My ExperienceStateBackEndresultNormal CheckpointFsStateBackendsucceedIncremental CheckpointRocksDBStateBackendfailedMy Other EffortContentResult①I hava subscribed to mailing listDavid Anderson has answered this several days ago. But he did not tell me how I can fix it.I guess he's very busy.②stackoverflow accountmy account is forbidden to ask questions.③alibaba Flink dingtalk groupno one can answer this question④I have read document carefullyI can NOT succeed in it⑤apologise for post it herehow can I make sure this question is a support instead of a bug or due to my own mistake?no successful example in Google/Baidu,am I right?⑥about less than 1000  people visit my failure record about this experiment.in vain Please help,thanks~ --------------------------------original posting content--------------------------------I want to do an experiment of"incremental checkpoint"informationcontentpom.xml[ is in the attachment]code https://paste.ubuntu.com/p/DpTyQKq6Vk/errorhttps://paste.ubuntu.com/p/49HRYXFzR2/ some of the above error is:Caused by: java.lang.IllegalStateException: Unexpected state handle type, expected: class org.apache.flink.runtime.state.KeyGroupsStateHandle, but found: class org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle  The steps for incremental experiment are:stepcontent①mvn clean scala:compile compile package②nc -lk 9999③flink run -c wordcount_increstate  datastream_api-1.0-SNAPSHOT.jarJob has been submitted with JobID df6d62a43620f258155b8538ef0ddf1b④input the following conents in nc -lk 9999before error error error error⑤flink run -s hdfs://Desktop:9000/tmp/flinkck/df6d62a43620f258155b8538ef0ddf1b/chk-22 -c StateWordCount datastream_api-1.0-SNAPSHOT.jarThen the above error happens.Please help,Thanks~! </description>
      <version>1.11.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapRestoreOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19667" opendate="2020-10-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="19736" opendate="2020-10-20 00:00:00" fixdate="2020-11-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement the `SinkTransformation`</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.TestSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeInformationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19737" opendate="2020-10-20 00:00:00" fixdate="2020-10-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce TableOperatorWrapperGenerator to translate transformation DAG in a multiple-input node to TableOperatorWrapper DAG</summary>
      <description>Transformation is not serializable, while StreamOperatorFactory is. We need to introduce another class (named TableOperatorWrapper) to store the information of a Transformation, and introduce a utility class (named TableOperatorWrapper) to convert the Transformation DAG to TableOperatorWrapper DAG.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExplainMultipleInput.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.MultipleInputRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19750" opendate="2020-10-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20642" opendate="2020-12-17 00:00:00" fixdate="2020-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce InternalRow to optimize Row used in Python UDAF</summary>
      <description>InternalRow is a cython classcdef enum InternalRowKind: INSERT = 0 UPDATE_BEFORE = 1 UPDATE_AFTER = 2 DELETE = 3cdef class InternalRow: cdef list values cdef InternalRowKind row_kind</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="2083" opendate="2015-5-22 00:00:00" fixdate="2015-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure high quality docs for FlinkML in 0.9</summary>
      <description>As defined in our vision for FlinkML, providing high-quality documentation is a primary goal for us.This issue concerns the docs that will be included in 0.9, and will track improvements and additions for the release.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.latex.commands.html</file>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.distance.metrics.md</file>
      <file type="M">docs.libs.ml.als.md</file>
    </fixedFiles>
  </bug>
  <bug id="20830" opendate="2021-1-3 00:00:00" fixdate="2021-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a type of HEADLESS_CLUSTER_IP for rest service type</summary>
      <description>Now we can choose ClusterIP or NodePort or LoadBalancer as rest service type. But in our internal kubernetes cluster, there is no kube-proxy, and ClusterIP mode rely on kube-proxy. So I think can we support another type of HEADLESS_CLUSTER_IP to directly talk to jobmanager pod? cc fly_in_gis</description>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2174" opendate="2015-6-5 00:00:00" fixdate="2015-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow comments in &amp;#39;slaves&amp;#39; file</summary>
      <description>Currently, each line in slaves in interpreded as a host name. Scripts should skip lines starting with '#'. Also allow for comments at the end of a line and skip empty lines.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster-streaming.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21844" opendate="2021-3-17 00:00:00" fixdate="2021-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not auto-configure maxParallelism when setting "scheduler-mode: reactive"</summary>
      <description>I believe we should not automatically change the maxParallelism when the "scheduler-mode" is set to "reactive", because: it magically breaks savepoint compatibility, when you switch between default and reactive scheduler mode the maximum parallelism is an orthogonal concern that in my opinion should not be mixed with the scheduler mode. The reactive scheduler should respect the maxParallelism, but it should not set/ change its default value.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitionerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.topology.BuildExecutionGraphBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphConstructionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.ReactiveModeUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.JobGraphJobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.MutableVertexAttemptNumberStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
    </fixedFiles>
  </bug>
  <bug id="22327" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE exception happens if it throws exception in finishBundle during job shutdown</summary>
      <description>Currently, if it throws exceptions in finishBundle during job shutdown, NPE exception may happen if time-based finish bundle is scheduled. It caused the actual exception isn't propagate. This makes users very difficult to trouble shot the problem.See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22329" opendate="2021-4-17 00:00:00" fixdate="2021-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing credentials in jobconf causes repeated authentication in Hive datasource</summary>
      <description>Related Flink code: https://github.com/apache/flink/blob/577113f0c339df844f2cc32b1d4a09d3da28085a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveSourceFileEnumerator.java#L107 In this getSplits method, it will call hadoop FileInputFormat's getSplits method. related hadoop code is here. Simple code is as follows// Hadoop FileInputFormatpublic InputSplit[] getSplits(JobConf job, int numSplits) throws IOException { StopWatch sw = new StopWatch().start(); FileStatus[] stats = listStatus(job); ......}protected FileStatus[] listStatus(JobConf job) throws IOException { Path[] dirs = getInputPaths(job); if (dirs.length == 0) { throw new IOException("No input paths specified in job"); } // get tokens for all the required FileSystems.. TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job); // Whether we need to recursive look into the directory structure ......} In listStatus method, it will obtain delegation tokens by calling  TokenCache.obtainTokensForNamenodes method. Howerver this method will give up to get delegation tokens when credentials in jobconf.So it's neccessary to inject current ugi credentials into jobconf. Besides, when Flink support delegation tokens directly without keytab(refer to this PR), TokenCache.obtainTokensForNamenodes will failed without this patch because of no corresponding credentials.    </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.JobConfUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22712" opendate="2021-5-19 00:00:00" fixdate="2021-6-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support accessing Row fields by attribute name in PyFlink Row-based Operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
    </fixedFiles>
  </bug>
  <bug id="22893" opendate="2021-6-6 00:00:00" fixdate="2021-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leader retrieval fails with NoNodeException</summary>
      <description>The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&amp;view=logs&amp;j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&amp;t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&amp;l=4382</description>
      <version>1.11.1,1.14.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
