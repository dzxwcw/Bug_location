<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="20188" opendate="2020-11-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description></description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="23192" opendate="2021-6-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move connector/format option classes into a common package</summary>
      <description>For built-in connectors, we need to refactor their corresponding *Options classes to … be located in a common package</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatSerializationSchema.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatOptions.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatDeserializationSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.BlackHoleConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.PrintSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.BlackHoleSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DataGeneratorMapper.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.SequenceGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenVisitorBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGeneratorContainer.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptionsUtil.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BlackHoleTableSinkFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23571" opendate="2021-8-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The internal query-start options missed when convert exec graph to transformation</summary>
      <description>The internal query-start configuration options is missed when convert exec graph to transformation, please see:// org.apache.flink.table.planner.delegation.PlannerBase translateToPlan(execGraph: ExecNodeGraph)</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24213" opendate="2021-9-8 00:00:00" fixdate="2021-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock in QueryableState Client</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&amp;l=15476 Found one Java-level deadlock:Sep 08 11:12:50 =============================Sep 08 11:12:50 "Flink Test Client Event Loop Thread 0":Sep 08 11:12:50 waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),Sep 08 11:12:50 which is held by "main"Sep 08 11:12:50 "main":Sep 08 11:12:50 waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),Sep 08 11:12:50 which is held by "Flink Test Client Event Loop Thread 0"</description>
      <version>1.14.0,1.12.6,1.13.3</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="24310" opendate="2021-9-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink on this page has a bug:if (bufferedElements.size() == threshold) {It should be &gt;= instead of == , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold. </description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="24608" opendate="2021-10-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24631" opendate="2021-10-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoiding directly use the labels as selector for deployment and service</summary>
      <description>We create deployment use the pod selector directly from labels, which is not necessary and may cause problem when some user label value have changed (may be by third-party system). This may lead to dangling pod or service can not select pods. I suggest to use minimal and stable flink internal selectors to select the JobManager pod like app=xxx, component=jobmanager and service, taskmanager pod and so on.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24667" opendate="2021-10-27 00:00:00" fixdate="2021-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Channel state writer would fail the task directly if meeting exception previously</summary>
      <description>Currently, if channel state writer come across exception when closing a file, such as meet exception during SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable, it will exit the loop. However, in the following channelStateWriter#abort it will throw exception directly：switched from RUNNING to FAILED with failure cause: java.io.IOException: java.lang.RuntimeException: unable to send request to worker at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:228) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkPartitionRequestQueueInitialized(RemoteInputChannel.java:735) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:204) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:651) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:626) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:612) at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109) at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:149) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:98) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:798) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:585)This is not expected as checkpoint failure should not lead to task failover each time.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="24678" opendate="2021-10-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the metric name of map state contains latency</summary>
      <description>Current metric name of map state contains is mapStateContainsAllLatency which should be mapStateContainsLatency.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingMapState.java</file>
    </fixedFiles>
  </bug>
  <bug id="24686" opendate="2021-10-28 00:00:00" fixdate="2021-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Make doc clear on AsyncFunction::timeout() overriding</summary>
      <description>Sometimes, a user overrides AsyncFunction::timeout() with an empty method or with only logging code. This causes the timeout does not signal back to the framework and job stuck especially when using orderedWait(). Opening this Jira to make the doc clear on this.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  <bug id="24739" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State requirements for Flink&amp;#39;s application mode in the documentation</summary>
      <description>If I am not mistaken, then Flink won't ship jars when using the application mode because it assumes the jars to be on the classpath. If this is true, then we should make this requirement a bit more prominent in the deployment documentation because currently it is only subtly hinted at.Alternatively, we could enable Flink to ship the user code jars to its components also when using the application mode.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="24740" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24742" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="24820" opendate="2021-11-8 00:00:00" fixdate="2021-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples in documentation for value1 IS DISTINCT FROM value2 are wrong</summary>
      <description>Currently it is stated in docs for value1 IS DISTINCT FROM value2E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.In fact they return opposite values.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24858" opendate="2021-11-10 00:00:00" fixdate="2021-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TypeSerializer version mismatch during eagerly restore</summary>
      <description>Currently, some of our TypeSerializer snapshots assume information about the binary layout of the actual data rather than only holding information about the TypeSerialzer.Multiple users ran into this problem i.e.https://lists.apache.org/thread/4q5q7wp0br96op6p7f695q2l8lz4wfzxThis manifest itself when state is restored egarly (for example an operator state) but, for example a user doesn't register the state on their intializeState/open,* and then a checkpoint happens.The result is that we will have elements serialized according to an old binary layout, but our serializer snapshot declares a new version which indicates that the elements are written with a new binary layout.The next restore will fail.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.RowSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
      <file type="M">docs.content.release-notes.flink-1.14.md</file>
    </fixedFiles>
  </bug>
  <bug id="24859" opendate="2021-11-10 00:00:00" fixdate="2021-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new File formats</summary>
      <description>The project recently introduced new formats: BulkFormat and StreamFormat interfaces. There are already implementations of these formats: hive, parquet, orc and textLine formats that need to be documented.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.filesystem.LimitableBulkFormatTest.java</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowFileInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcColumnarRowFileInputFormat.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.wordcount.WordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.wordcount.WordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.WindowWordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveCompactReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.reader.TextLineFormat.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSource.java</file>
      <file type="M">docs.content.docs.connectors.datastream.hybridsource.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.hybridsource.md</file>
    </fixedFiles>
  </bug>
  <bug id="24884" opendate="2021-11-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink flame graph webui bug</summary>
      <description>i can not compile success when i port the flame graph feature to our low version of flink.but it is success in the high version of flink  </description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.@types.d3-flame-graph.index.d.ts</file>
    </fixedFiles>
  </bug>
  <bug id="25022" opendate="2021-11-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API</summary>
      <description>If a job is submitted using the REST API's /jars/:jarid/run endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like Flink-DispatcherRestEndpoint-thread-*.If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the ChildFirstClassloader of the job and thus leaking that.We saw this for the jsoniter scala library at the JM which creates ThreadLocal instances but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code. There are a few workarounds a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances. A proper fix should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's main() method and once the job is submitted exit that thread and destroy all thread locals along with it.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarMessageParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="25053" opendate="2021-11-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="25085" opendate="2021-11-29 00:00:00" fixdate="2021-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a scheduled thread pool in Endpoint and close it when the endpoint is stopped</summary>
      <description>Add a dedicated thread pool in Endpoint to schedule tasks that have a long delay such as PhysicalSlotRequestBulkCheckerImpl, heatbeat checker and some other timeout checker in JM/TM/RM. Job should shut down the thread pool and all the pending tasks will be removed when it terminates.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ThrowingScheduledFuture.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredComponentMainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.FencedRpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="25096" opendate="2021-11-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make errors happened during JobMaster initialization accessible through the exception history</summary>
      <description>Currently we are using flink version 1.13.2 and as per the flink documentation we should get all exceptions through exceptions api in exceptionHistory tag. While running few scenarios we observed that the below two exceptions are not coming in exceptionHistory tag but are coming in root-exception tag.Exception 1 - caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'C:\Users\abc\Documents\checkpoints\a737088e21206281db87f6492bcba074' on file system 'file'.Exception 2 - Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/mnt/c/Users/abc/Documents/checkpoints/a737088e21206281db87f6492bcba074/chk-144.Please find the attachment for the logs of above exceptions.</description>
      <version>1.14.0,1.13.3</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.exceptionhistory.RootExceptionHistoryEntry.java</file>
    </fixedFiles>
  </bug>
  <bug id="25118" opendate="2021-12-1 00:00:00" fixdate="2021-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vertex index as prefix in vertex name</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25160" opendate="2021-12-3 00:00:00" fixdate="2021-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make doc clear: tolerable-failed-checkpoints counts consecutive failures</summary>
      <description>According to the code, tolerable-failed-checkpoints counts the consecutive failures. We should make this clear in the doc config </description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25161" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependency for japicmp-maven-plugin</summary>
      <description>compiliation with jdk 17 fails like belowupdate of jaxb-impl to 2.3.1 helpsjava.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain) at java.base/java.security.AccessController.doPrivileged(AccessController.java:573) at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.&lt;clinit&gt;(Injector.java:197) at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81) at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125) at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.&lt;init&gt;(TransducedAccessor.java:235) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175) at com.sun.xml.bind.v2.runtime.property.AttributeProperty.&lt;init&gt;(AttributeProperty.java:91) at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108) at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.&lt;init&gt;(ClassBeanInfoImpl.java:181) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:331) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662) at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119) at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70) at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866) at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149) at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25162" opendate="2021-12-3 00:00:00" fixdate="2021-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink : Connectors : Hive fails with VectorizedRowBatch not found</summary>
      <description>While compiling with jdk17[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-connector-hive_2.12: Compilation failure[ERROR] flink/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveInputFormat.java:[168,17] cannot access org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch[ERROR] class file for org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch not found[ERROR] [ERROR] -&gt; [Help 1][ERROR]</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="25163" opendate="2021-12-4 00:00:00" fixdate="2021-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more options for rocksdb state backend to make configuration more flexible</summary>
      <description>Now flink has less options than the configurations what Rocksdb can set. We can see many function in the org.rocksdb.DBOptions that can influence its behavior(e.g rocksdb background threads).It make us do less when we want to do some thing to tuning Rocksdb. In my opinion, there are at least there options: maxBackgroundFlushes, it can define the background flush threads. default 1. maxBackgroundCompactions, it can define the background compaction threads. default 1. maxBackgroundJobs, it can define the background threads. default 2.setIncreaseParallelism (the most we can do for the rocksdb backend background threads) seems like can do little. It can't change the flush threads. I think it's necessary to make rocksdb configuration flexible.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25173" opendate="2021-12-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce CatalogLock</summary>
      <description>Currently, only HiveCatalog can provide this catalog lock./** * An interface that allows source and sink to use global lock to some transaction-related things. */@Internalpublic interface CatalogLock extends Closeable {     /** Run with catalog lock. The caller should tell catalog the database and table name. */    &lt;T&gt; T runWithLock(String database, String table, Callable&lt;T&gt; callable) throws Exception;     /** Factory to create {@link CatalogLock}. */    interface Factory extends Serializable {        CatalogLock create();    }} And we need a interface to set lock to source&amp;sink by catalog:/** * Source and sink implement this interface if they require {@link CatalogLock}. This is marked as * internal. If we need lock to be more general, we can put lock factory into {@link * DynamicTableFactory.Context}. */@Internalpublic interface RequireCatalogLock {     void setLockFactory(CatalogLock.Factory lockFactory);} {{}}</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25176" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce "ALTER TABLE ... COMPACT" SQL syntax</summary>
      <description>Introduce "ALTER TABLE ... COMPACT" SQL Work with managed table</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.PartitioningSpec.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.UseCatalogOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableDropConstraintOperation.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="25178" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when managed table sink needs checkpointing</summary>
      <description>In the past, many users encountered the problem that the sink did not output because they did not open the checkpoint.For the managed table: The planner will throw an exception if the checkpoint is not turned on. (Later we can add public connector interface, including Filesystem, Hive, Iceberg, Hudi need it).</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2521" opendate="2015-8-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add automatic test name logging for tests</summary>
      <description>When running tests on travis the Flink components log to a file. This is helpful in case of a failed test to retrieve the error. However, the log does not contain the test name and the reason for the failure. Therefore it is difficult to find the log output which corresponds to the failed test.It would be nice to automatically add the test case information to the log. This would ease the debugging process big time.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.tuple.base.PairComparatorTestBase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.completeness.ScalaAPICompletenessTestBase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancellingTestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.CompilerTestBase.java</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.tuple.base.TuplePairComparatorTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.StringUtilsTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.SimpleStringUtilsTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.NumberSequenceIteratorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.InstantiationUtilTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.AbstractIDTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.ParserTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.UnmodifiableConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.GlobalConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.SequentialFormatTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25227" opendate="2021-12-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparing the equality of the same (boxed) numeric values returns false</summary>
      <description>Add the following test case to TableEnvironmentITCase to reproduce this bug.@Testdef myTest(): Unit = { val data = Seq( Row.of( java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000), java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000)) ) tEnv.executeSql( s""" |create table T ( | a int, | b int, | c int, | d int |) with ( | 'connector' = 'values', | 'bounded' = 'true', | 'data-id' = '${TestValuesTableFactory.registerData(data)}' |) |""".stripMargin) tEnv.executeSql("select greatest(a, b) = greatest(c, d) from T").print()}The result is false, which is obviously incorrect.This is caused by the generated java code:public class StreamExecCalc$8 extends org.apache.flink.table.runtime.operators.TableStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$8( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output, org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception { this.references = references; this.setup(task, config, output); if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) { ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue(); int field$0; boolean isNull$0; int field$1; boolean isNull$1; int field$3; boolean isNull$3; int field$4; boolean isNull$4; boolean isNull$6; boolean result$7; isNull$3 = in1.isNullAt(2); field$3 = -1; if (!isNull$3) { field$3 = in1.getInt(2); } isNull$0 = in1.isNullAt(0); field$0 = -1; if (!isNull$0) { field$0 = in1.getInt(0); } isNull$1 = in1.isNullAt(1); field$1 = -1; if (!isNull$1) { field$1 = in1.getInt(1); } isNull$4 = in1.isNullAt(3); field$4 = -1; if (!isNull$4) { field$4 = in1.getInt(3); } out.setRowKind(in1.getRowKind()); java.lang.Integer result$2 = field$0; boolean nullTerm$2 = false; if (!nullTerm$2) { java.lang.Integer cur$2 = field$0; if (isNull$0) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (!nullTerm$2) { java.lang.Integer cur$2 = field$1; if (isNull$1) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (nullTerm$2) { result$2 = null; } java.lang.Integer result$5 = field$3; boolean nullTerm$5 = false; if (!nullTerm$5) { java.lang.Integer cur$5 = field$3; if (isNull$3) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (!nullTerm$5) { java.lang.Integer cur$5 = field$4; if (isNull$4) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (nullTerm$5) { result$5 = null; } isNull$6 = nullTerm$2 || nullTerm$5; result$7 = false; if (!isNull$6) { result$7 = result$2 == result$5; } if (isNull$6) { out.setNullAt(0); } else { out.setBoolean(0, result$7); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}You can see that line 137 compares two boxed Integer types with == instead of .equals, which causes this problem.In older Flink versions where the return types of cast functions are also boxed types, casting strings to numeric values are also affected by this bug.Currently for a quick fix we can rewrite the generated code. But for a long term solution we shouldn't use boxed types as internal data structures.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.14.5,1.15.0,1.16.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25228" opendate="2021-12-9 00:00:00" fixdate="2021-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink-table-test-utils</summary>
      <description>Introduce a package to ship test utilities for formats, connectors and end users.This package should provide: Assertions for data types, logical types and internal data structures. Test cases for formats and connnectorsThe end goal is to remove the test-jar planner dependency in formats and connectors and replace it with this package, so formats and connectors can then just depend on table-planner-loader.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.TableAssertions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.StringDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.RowDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeAssert.java</file>
    </fixedFiles>
  </bug>
  <bug id="25252" opendate="2021-12-10 00:00:00" fixdate="2021-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Kafka E2E tests on Java 11</summary>
      <description>The Java Kafka E2E tests are currently not run on Java 11. We should check what the actual issue is and whether it can be resolved (e.g., by a Kafka server version bump):</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25278" opendate="2021-12-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure failed due to unable to transfer jar from confluent maven repo</summary>
      <description>Dec 12 00:46:45 [ERROR] Failed to execute goal on project flink-avro-confluent-registry: Could not resolve dependencies for project org.apache.flink:flink-avro-confluent-registry:jar:1.13-SNAPSHOT: Could not transfer artifact io.confluent:common-utils:jar:5.5.2 from/to confluent (https://packages.confluent.io/maven/): transfer failed for https://packages.confluent.io/maven/io/confluent/common-utils/5.5.2/common-utils-5.5.2.jar: Connection reset -&gt; [Help 1]Dec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Dec 12 00:46:45 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Dec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] For more information about the errors and possible solutions, please read the following articles:Dec 12 00:46:45 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionDec 12 00:46:45 [ERROR] Dec 12 00:46:45 [ERROR] After correcting the problems, you can resume the build with the commandDec 12 00:46:45 [ERROR] mvn &lt;goals&gt; -rf :flink-avro-confluent-registry https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27994&amp;view=logs&amp;j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&amp;t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699&amp;l=8812</description>
      <version>1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25329" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement of execution graph store in flink session cluster for jobs</summary>
      <description>Flink session cluster uses files to store info of jobs after they reach termination with `FileExecutionGraphInfoStore`, each job will generate one file. When the cluster executes many small jobs concurrently, there will be many disk related operations, which will1&gt; Increase the CPU usage of `Dispatcher`2&gt; Decrease the performance of the jobs in the cluster.We hope to improve the disk operations in `FileExecutionGraphInfoStore` to increase the performance of session cluster, or support memory store.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStore.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25331" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow testcontainer tests to run on Java 17</summary>
      <description>Tests using testcontainers for Flink are currently locked to Java 8.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkImageBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="25509" opendate="2022-1-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>FLIP-208: Add RecordEvaluator to dynamically stop source based on de-serialized records</summary>
      <description>This feature is needed to migrate applications which uses KafkaDeserializationSchema::isEndOfStream() from using FlinkKafkaConsumer to using KafkaSource.Please checkout https://cwiki.apache.org/confluence/display/FLINK/FLIP-208%3A+Add+RecordEvaluator+to+dynamically+stop+source+based+on+de-serialized+records for the motivation and the proposed changes.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="26909" opendate="2022-3-29 00:00:00" fixdate="2022-5-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Allow setting parallelism to -1 from CLI</summary>
      <description>When we start the job by command with args "-p $parallelism", the error is thrown with "The parallelism must be a positive number: -1".Since we can use AdaptiveBatch with config parallelism.default: -1, we should support it from Cli.</description>
      <version>None</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
