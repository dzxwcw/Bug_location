<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="16583" opendate="2020-3-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid classloader during pipeline creation</summary>
      <description>The end-to-end test SQLClientKafkaITCase.testKafka failed with18:13:02.425 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 32.246 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:11:46 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)18:13:02.425 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 34.539 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:12:21 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)https://api.travis-ci.org/v3/job/661535183/log.txt</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="23021" opendate="2021-6-17 00:00:00" fixdate="2021-7-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Check for illegal modifications of JobGraph with finished operators</summary>
      <description>Users might modify the job topology before restart for external checkpoint and savepoint. To overcome this issue, we would need to check if a fully finished operator has been added after a non-fully-finished operator. If so, we would throw exception to disallow this situation or re-mark the fully finished operator as alive. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27246" opendate="2022-4-14 00:00:00" fixdate="2022-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB</summary>
      <description>I think this bug should get fixed in https://issues.apache.org/jira/browse/FLINK-23007Unfortunately I spotted it on Flink 1.14.3java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$9211' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:198) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.&lt;init&gt;(RegularOperatorChain.java:63) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:666) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling "HashAggregateWithKeys$9211": Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.CodeContext.write(CodeContext.java:940) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:12282) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11941) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11926) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4408) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4274) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4461) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4403) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4120) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3957) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4448) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5004) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4417) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5057) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4409) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ParenthesizedExpression.accept(Java.java:4924) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 more</description>
      <version>1.14.3,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.code.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaCodeSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.IfStatementRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.CodeRewriterTestBase.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.JavaCodeSplitter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.IfStatementRewriter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29112" opendate="2022-8-26 00:00:00" fixdate="2022-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print the lookup join hint on the node `Correlate` in the origin RelNode tree for easier debuging</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29156" opendate="2022-8-31 00:00:00" fixdate="2022-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support LISTAGG in the Table API</summary>
      <description>Currently, LISTAGG  are not supported in Table API.table.group_by(col("a"))     .select(         col("a"),        call("LISTAGG", col("b"), ','))</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29231" opendate="2022-9-8 00:00:00" fixdate="2022-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink UDAF produces different results in the same sliding window</summary>
      <description>It seems that PyFlink udtaf produces different results in the same sliding window. It can be reproduced with the given code and input. It is not always happening but the possibility is relatively high.The incorrect output is the following: We can see that the output contains different `val_sum` at `window_time` 2022-01-01 00:01:59.999.</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29319" opendate="2022-9-16 00:00:00" fixdate="2022-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.32</summary>
      <description>This release fixes CVE-2022-39135, an XML External Entity (XEE) vulnerability that allows a SQL query to read the contents of files via the SQL functions EXISTS_NODE, EXTRACT_XML, XML_TRANSFORM or EXTRACT_VALUE.Coming 1 month after 1.31.0 with 19 issues fixed by 17 contributors, this release also replaces the ESRI spatial engine with JTS and proj4j, adds 65 spatial SQL functions including ST_Centroid, ST_Covers and ST_GeomFromGeoJSON, adds the CHAR SQL function, and improves the return type of the ARRAY and MULTISET functions.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.hint.NodeTypeHintPredicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Snapshot.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-calcite-bridge.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29322" opendate="2022-9-16 00:00:00" fixdate="2022-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose savepoint format on Web UI</summary>
      <description>Savepoint format is not exposed on the Web UI, thus users should remember how they triggered it.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug id="29363" opendate="2022-9-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web ui to fully redirect to other page</summary>
      <description>In a streaming platform system, web ui usually integrates with internal authentication and authorization system. Given the validation failed, the request needs to be redirected to a landing page. It does't work for AJAX request. It will be great to have the web ui configurable to allow auto full redirect.</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
    </fixedFiles>
  </bug>
  <bug id="29455" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OperatorIdentifier</summary>
      <description>Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowSavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadataV2.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowSavepointReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="29457" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a uid(hash) remapping function</summary>
      <description>Expose functionality for modifying the uid&amp;#91;hash&amp;#93; of a state.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorIdentifier.java</file>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="2946" opendate="2015-10-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add orderBy() to Table API</summary>
      <description>In order to implement a FLINK-2099 prototype that uses the Table APIs code generation facilities, the Table API needs a sorting feature.I would implement it the next days. Ideas how to implement such a sorting feature are very welcome. Is there any more efficient way instead of .sortPartition(...).setParallism(1)? Is it better to sort locally on the nodes first and finally sort on one node afterwards?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29498" opendate="2022-10-3 00:00:00" fixdate="2022-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Async I/O Retry Strategies Do Not Work for Scala AsyncDataStream API</summary>
      <description>We are using the async I/O to make HTTP calls and one of the features we wanted to leverage was the retries, so we pulled the newest commit: http://github.com/apache/flink/pull/19983 into our internal Flink fork.When I try calling the function AsyncDataStream.unorderedWaitWithRetry from the scala API I with a retry strategy from the java API I get an error as unorderedWaitWithRetry expects a scala retry strategy. The problem is that retry strategies were only implemented in java and not Scala in this PR: http://github.com/apache/flink/pull/19983. Here is some of the code to reproduce the error:import org.apache.flink.streaming.api.scala.AsyncDataStreamimport org.apache.flink.streaming.util.retryable.{AsyncRetryStrategies =&gt; JAsyncRetryStrategies}val javaAsyncRetryStrategy = new JAsyncRetryStrategies.FixedDelayRetryStrategyBuilder[Int](3, 100L) .build()val data = AsyncDataStream.unorderedWaitWithRetry( source, asyncOperator, pipelineTimeoutInMs, TimeUnit.MILLISECONDS, javaAsyncRetryStrategy)</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  <bug id="29558" opendate="2022-10-10 00:00:00" fixdate="2022-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use select count(*) from xxx; and get SQL syntax</summary>
      <description>Hi, I use flink sql to make kafka records to mysql.so I create these 2 tables in flink sql,here is the mysql ,and I created the table in mysql before I did the insert action in flink sql. CREATE TABLE mysql_MyUserTable ( id STRING, name STRING, age STRING, status STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://10.19.29.170:3306/fromflink152', 'table-name' = 'users', 'username' = 'root', 'password' = '******');In mysql, I created database "fromflink152" then created the table like this way CREATE TABLE `users` ( `id` varchar(64) NOT NULL DEFAULT '', `name` varchar(255) DEFAULT NULL, `age` varchar(255) DEFAULT NULL, `status` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) After executed insert sql,I found 'select * from mysql_MyUserTable' can get correct result,but ’select count(&amp;#42;) from mysql_MyUserTable‘ or ’select count(id) from mysql_MyUserTable‘ ,the collect job in flink app keep restarting again and again.The exception is: So I wonder which config that I missed about the table in flink or mysql side</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="29567" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert sink output metric names from numRecordsSend back to numRecordsOut</summary>
      <description>As discussed in the mailing list, all sink metrics with name “numXXXOut” defined in FLIP-33 are replace by “numXXXSend” in FLINK-26126 and FLINK-26492. Considering metric names are public APIs, this is a breaking change to end users and not backward compatible. We need to revert these metric names back. </description>
      <version>1.16.0,1.15.3</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalOperatorIOMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.SinkWriterMetricGroup.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.TestSinkInitContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29568" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary whitespace in request/response blocks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="29569" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of deprecated expand shortcode</summary>
      <description>The expand shortcode is deprecated; use &lt;details&gt; instead.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.assets..custom.scss</file>
    </fixedFiles>
  </bug>
  <bug id="29626" opendate="2022-10-13 00:00:00" fixdate="2022-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Akka to 2.6.20</summary>
      <description>Update Akka to the latest 2.6 version that's still under Apache license</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29644" opendate="2022-10-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reference Kubernetes operator from Flink Kubernetes deploy docs</summary>
      <description>Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="29812" opendate="2022-10-31 00:00:00" fixdate="2022-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Netty API usages</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.KeepAliveWrite.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29834" opendate="2022-11-1 00:00:00" fixdate="2022-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear static Jackson TypeFactory cache on CL release</summary>
      <description>The Jackson TypeFactory contains a singleton instance that is at times used by Jackson, potentially containing user-classes for longer than necessary.https://github.com/FasterXML/jackson-databind/issues/1363We could clear this cache whenever a user code CL is being released similar to what was done in BEAM-6460.</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="29849" opendate="2022-11-2 00:00:00" fixdate="2022-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Event time temporal join on an upsert source may produce incorrect execution plan</summary>
      <description>For current implementation, the execution plan is incorrect when do event time temporal join on an upsert source. There's two problems:1. for an upsert source, we should not add a ChangelogNormalize node under a temporal join input, or it will damage the versions of the version table. For versioned tables, we use a single-temporal mechanism which relies sequencial records of a same key to ensure the valid period of each version, so if the ChangelogNormalize was added then an UB message will be produced based on the previous UA or Insert message, and all the columns are totally same include event time, e.g., original upsert input+I (key1, '2022-11-02 10:00:00', a1)+U (key1, '2022-11-02 10:01:03', a2)the versioned data should be:v1 [~, '2022-11-02 10:00:00')v2 ['2022-11-02 10:00:00', '2022-11-02 10:01:03')after ChangelogNormalize's processing, will output:+I (key1, '2022-11-02 10:00:00', a1)-U (key1, '2022-11-02 10:00:00', a1)+U (key1, '2022-11-02 10:01:03', a2)versions are incorrect:v1 ['2022-11-02 10:00:00', '2022-11-02 10:00:00') // invalid periodv2 ['2022-11-02 10:00:00', '2022-11-02 10:01:03')2. semantically, a filter cannot be pushed into an event time temporal join, otherwise, the filter may also corrupt the versioned table</description>
      <version>1.16.0,1.15.3</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="2985" opendate="2015-11-6 00:00:00" fixdate="2015-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow different field names for unionAll() in Table API</summary>
      <description>The recently merged `unionAll` operator checks if the field names of the left and right side are equal. Actually, this is not necessary. The union operator in SQL checks only the types and uses the names of left side.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29914" opendate="2022-11-7 00:00:00" fixdate="2022-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaTableITCase.testKafkaSourceSink fails</summary>
      <description>This build failed due to an error in KafkaTableITCase.testKafkaSourceSink:Caused by: org.apache.kafka.common.errors.TimeoutException: Topic tstopic_avro not present in metadata after 60000 ms.</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.src.test.java.SqlClientITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.KafkaContainerClient.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaSourceExternalContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.testutils.KafkaSinkExternalContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29925" opendate="2022-11-8 00:00:00" fixdate="2022-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>table ui of configure value is strange</summary>
      <description>As shown in the figure below, when the configure value is very large, the ui of the table is a bit strange</description>
      <version>1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="30041" opendate="2022-11-16 00:00:00" fixdate="2022-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup conjars https mirror</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.google-mirror-settings.xml</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30386" opendate="2022-12-12 00:00:00" fixdate="2022-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column constraint lacks primary key not enforced check</summary>
      <description>Currently, only table constraint performs the enforced check. Not sure if it is by design or a bug.The following case can be reproduced on Flink 1.16.0, 1.15.3, and 1.15.2. I think the earlier version might also reveal it.Flink SQL&gt; create table T (f0 int not null primary key, f1 string) with ('connector' = 'datagen');[INFO] Execute statement succeed.Flink SQL&gt; explain select * from T;== Abstract Syntax Tree ==LogicalProject(f0=[$0], f1=[$1])+- LogicalTableScan(table=[[default_catalog, default_database, T]])== Optimized Physical Plan ==TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])== Optimized Execution Plan ==TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])Flink SQL&gt; create table S (f0 int not null, f1 string, primary key(f0)) with ('connector' = 'datagen');[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode</description>
      <version>1.16.0,1.15.2,1.15.3</version>
      <fixedVersion>1.17.0,hbase-3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.SqlConstraintValidator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30424" opendate="2022-12-15 00:00:00" fixdate="2022-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add source operator restore readerState log to distinguish split is from newPartitions or split state</summary>
      <description>When a job start firstly, we can find 'assignPartitions' from log。but if source recover from state, we can not distinguish the newPartitions is from timed discover thread or from reader task state.  We can add a helper log to distinguish and confirm the reader using split state in recover situation.  it's very useful for troubleshooting.  </description>
      <version>1.16.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="30425" opendate="2022-12-15 00:00:00" fixdate="2022-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize token receive side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdaterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30461" opendate="2022-12-20 00:00:00" fixdate="2022-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some rocksdb sst files will remain forever</summary>
      <description>In rocksdb incremental checkpoint mode, during file upload, if some files have been uploaded and some files have not been uploaded, the checkpoint is canceled due to checkpoint timeout at this time, and the uploaded files will remain. Impact: The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written.However only 50k files are available, the other 950k files should be cleaned up.Root cause:If an exception is thrown during the checkpoint async phase, flink will clean up metaStateHandle, miscFiles and sstFiles.However, when all sst files are uploaded, they are added together to sstFiles. If some sst files have been uploaded and some sst files are still being uploaded, and  the checkpoint is canceled due to checkpoint timeout at this time, all sst files will not be added to sstFiles. The uploaded sst will remain on hdfs.code linkSolution:Using the CloseableRegistry as the tmpResourcesRegistry. If the async phase is failed, the tmpResourcesRegistry will cleanup these temporary resources. POC code:https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1   </description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="30549" opendate="2023-1-3 00:00:00" fixdate="2023-1-3 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove flink-connector-aws-kinesis-firehose from Flink master branch</summary>
      <description>Remove: flink-connector-aws-kinesis-firehose flink-sql-connector-aws-kinesis-firehose Corresponding e2e tests </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.src.test.resources.send-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.test.KinesisFirehoseTableITTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.testutils.KinesisFirehoseTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.resources.log4j2.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.util.KinesisFirehoseConnectorOptionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.KinesisFirehoseConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseException.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.archunit-violations.a6cbd99c-b115-447a-8f19-43c1094db549</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.archunit-violations.54da9a7d-14d2-4632-a045-1dd8fc665c8f</file>
    </fixedFiles>
  </bug>
  <bug id="30631" opendate="2023-1-11 00:00:00" fixdate="2023-1-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Limit the max number of subpartitons consumed by each downstream task</summary>
      <description>In the current implementation(FLINK-25035), when the upstream vertex parallelism is much greater than the downstream vertex parallelism, it may lead to a large number of channels in the downstream tasks(for example, A -&gt; B, all to all edge, max parallelism is 1000. If parallelism of A is 1000, parallelism of B is decided to be 1, then the only subtask of B will consume 1000 * 1000 subpartitions), resulting in a large overhead for processing channels.In this ticket, we temporarily address this issue by limiting the max number of subpartitons consumed by each downstream task. The ultimate solution should be to support single channel consume multiple subpartitons.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="30683" opendate="2023-1-14 00:00:00" fixdate="2023-1-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Make adaptive batch scheduler as the default batch scheduler</summary>
      <description>Based on the FLIP-283, this issue mainly focuses on the first issue.This change proposes to make AdaptiveBatchScheduler as the default batch scheduler and user can use it without explicitly configuring it.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveBatchSchedulerITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testCountAggFunctionFallbackPlan.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testMaxAggFunctionFallbackPlan.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testMinAggFunctionFallbackPlan.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testSumAggFunctionFallbackPlan.out</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.ExecutionConfigTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.StreamPartitionerTestUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ForwardHashExchangeProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.join.AdaptiveHashJoinITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonGroupWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30684" opendate="2023-1-14 00:00:00" fixdate="2023-1-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Use the default parallelism as a flag for vertices that can automatically derive parallelism.</summary>
      <description>This change proposes to add a parallelismConfigured property as a flag to identify whether the parallelism of node is used "parallelism.default" or not. If the vertex's parallelismConfigured is true, the AdaptiveBatchScheduler will not automatically deciding parallelisms for it. Otherwise, AdaptiveBatchScheduler will automatically deciding parallelisms and use the "parallelism.default" as an alternative value for the "jobmanager.adaptive-batch-scheduler.max-parallelism".This change will make user do not need to configure "parallelism.default" as "-1" to automatically deciding parallelisms for vertices.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.CacheITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BatchShuffleITCaseBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.MatchRecognizeITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SourceTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.MultiInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.LegacySinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.CacheTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.AbstractTwoInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.AbstractOneInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.ReduceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.PhysicalTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.KeyedBroadcastStateTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.BroadcastStateTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.AbstractBroadcastStateTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedStreams.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.BroadcastConnectedStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertex.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpch.sh</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.shuffle.FlinkKafkaShuffle.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectAggITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="30685" opendate="2023-1-14 00:00:00" fixdate="2023-1-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support mark the transformations whose parallelism is infected by the input transformation</summary>
      <description>In order to chain operators together as much as possible, many downstream operators will use the parallelism of upstream input operators in the table planner.If some operators need to have their own defined parallelism, the parallelism will be explicitly set. Therefore, the operator that takes the parallelism of the upstream operator as its own parallelism should be automatically derived by the AdaptiveBatchScheduler.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecWindowTableFunction.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveBatchSchedulerITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMiniBatchAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDropUpdateBefore.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.FileSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.batch.BatchSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.StreamingSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.CoGroupedStreams.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.AbstractMultipleInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CacheTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CoFeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.FeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.MultipleInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TimestampsAndWatermarksTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.OutputFormatTableSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSource.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecDynamicFilteringDataCollector.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecScriptTransform.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExpand.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="30761" opendate="2023-1-20 00:00:00" fixdate="2023-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JVM asserts from leader election code</summary>
      <description>assert is not enabled in the test run. We should using Preconditions</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.ManualLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
    </fixedFiles>
  </bug>
  <bug id="30864" opendate="2023-2-1 00:00:00" fixdate="2023-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional pattern at the start of a group pattern not working</summary>
      <description>The optional pattern at the start of a group pattern turns out be "not optional", e.g.Pattern.&lt;String&gt;begin("A").next(Pattern.&lt;String&gt;begin("B").optional().next("C")).next("D")cannot match sequence "a1 c1 d1".</description>
      <version>1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="30905" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>doc generation fails with "concurrent map read and map write"</summary>
      <description>We experience a build failure in master (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a concurrent map read and map write within hugo:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=ddd6d61a-af16-5d03-2b9a-76a279badf98Start building sites … fatal error: concurrent map read and map writegoroutine 233 [running]:runtime.throw(0x23054e4, 0x21) /usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd) /usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5[...]</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30948" opendate="2023-2-7 00:00:00" fixdate="2023-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-avro-glue-schema-registry and flink-json-glue-schema-registry from Flink main repo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryAvroKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GlueSchemaRegistryJsonKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.3e77c07d-dfdb-4ec0-8e64-5fad5c651c72</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.dc78e80c-3bb3-45bb-87c1-b57472d6f45b</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.User.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.4703059b-4f06-41c9-9724-644e6d00584f</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.b99819a4-a946-475e-883f-963de77c7e57</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoder.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.Car.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31077" opendate="2023-2-15 00:00:00" fixdate="2023-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger checkpoint failed but it were shown as COMPLETED by rest API</summary>
      <description>Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed(here), the checkpoint is not written to the HA service and we can not failover from this checkpoint.</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31168" opendate="2023-2-21 00:00:00" fixdate="2023-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManagerHAProcessFailureRecoveryITCase failed due to job not being found</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46342&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=747432ad-a576-5911-1e2a-68c6bedc248a&amp;l=12706We see this build failure because a job couldn't be found:java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1061) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:958) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:942) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:235) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase$4.run(JobManagerHAProcessFailureRecoveryITCase.java:336)Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1056) ... 4 moreCaused by: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160) at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$2(AbstractSessionClusterExecutor.java:82) at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73) at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job 865dcd87f4828dbeb3d93eb52e2636b1 not found at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99) at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986) at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:252) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45) at akka.dispatch.OnComplete.internal(Future.scala:299) at akka.dispatch.OnComplete.internal(Future.scala:297) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (865dcd87f4828dbeb3d93eb52e2636b1) at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:840) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ... 5 more</description>
      <version>1.15.3,1.16.1,1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="32267" opendate="2023-6-6 00:00:00" fixdate="2023-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.18.3</summary>
      <description>Among others there are Fixes the issue of missing root cause in container launch TimeoutException (e.g. SSLHandshakeException) Make sure we don't hide exceptions from waitUntilContainerStartedalso full list is at https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.0https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.1https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.2https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.3 </description>
      <version>None</version>
      <fixedVersion>elasticsearch-4.0.0,1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32349" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support atomic for CREATE TABLE AS SELECT(CTAS) statement</summary>
      <description>For detailed information, see FLIP-305https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobStatusHook.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32351" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for call procedure</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  <bug id="32354" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute the call procedure operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.ProcedureITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.PlannerCallProcedureOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CallProcedureOperation.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeConvertContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.SqlToRexConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="32358" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CI may unintentionally use fallback akka loader</summary>
      <description>We have a fallback akka loader for developer convenience in the IDE, that is on the classpath of most modules. Depending on the order of jars on the classpath it can happen that the fallback loader appears first, which we dont want because it slows down the build and creates noisy logs.We can add a simple prioritization scheme to the rpc system loading to remedy that.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystem.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.FallbackAkkaRpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="32514" opendate="2023-7-3 00:00:00" fixdate="2023-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-309: Support using larger checkpointing interval when source is processing backlog</summary>
      <description>Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.ManuallyTriggeredScheduledExecutorService.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingSplitEnumeratorContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexEndOfDataListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32516" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse [CREATE OR ] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="32517" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlReplaceTableAs.java</file>
    </fixedFiles>
  </bug>
  <bug id="32518" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable atomicity for [CREATE OR] REPLACE table as statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.AtomicCtasITCaseBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsStaging.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.StagedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.StagedSinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ReplaceTableAsOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.execution.CtasJobStatusHook.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32519" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
