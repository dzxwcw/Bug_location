<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="25274" opendate="2021-12-13 00:00:00" fixdate="2021-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ResolvedSchema in DataGen instead of TableSchema</summary>
      <description>TableSchema is deprecated It is recommended to use ResolvedSchema and Schema in TableSchema javadoc</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="25312" opendate="2021-12-15 00:00:00" fixdate="2021-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive supports managed table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25313" opendate="2021-12-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable flink runtime web</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7UpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6UpsertTableSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6UpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.table.descriptors.ElasticsearchTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.table.descriptors.ElasticsearchValidator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.table.descriptors.Elasticsearch.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25362" opendate="2021-12-17 00:00:00" fixdate="2021-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect dependencies in Table Confluent/Avro docs</summary>
      <description>"Confluent Avro Format" is missing an explanation to also add the dependency to flink-avro have the confluent repository defined"Avro Format" should not show the maven dependency to flink-sql-avro but instead flink-avro</description>
      <version>1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.sql.download.table.html</file>
      <file type="M">docs.data.sql.connectors.yml</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro-confluent.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.avro-confluent.md</file>
    </fixedFiles>
  </bug>
  <bug id="25366" opendate="2021-12-17 00:00:00" fixdate="2021-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce BINARY/VARBINARY precision when outputing to a Sink</summary>
      <description>When a column is declared with BINARY/VARBINARY with a specific length, i.e. BINARY(10) a sink that would require following this precision strictly (like a relational DB) would throw errors if the records received exceed this limit. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25370" opendate="2021-12-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase SQL Connector Options &amp;#39;table-name&amp;#39; description</summary>
      <description>In document the HBase SQL Connector Options "table-name" description is "The name of HBase table to connect.". All demo of HBase SQL Connector document 'table-name' just give the 'tablename', it does't explain how to use the table in the specified namespace of HBase. So I think we need to add some description for this options</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="25372" opendate="2021-12-18 00:00:00" fixdate="2021-12-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add thread dump feature for jobmanager</summary>
      <description>Add thread dump feature for jobmanager in addition to the previous work on TM side: FLINK-14816. It is useful for debugging job submission and scheduling issues especially in OLAP scenarios.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerThreadDumpHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="25375" opendate="2021-12-19 00:00:00" fixdate="2021-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Log4j to 2.17.0</summary>
      <description>Log4j 2.17.0 has been released &amp;#91;1&amp;#93; This release contains the changes noted below: Address CVE-2021-45105. Require components that use JNDI to be enabled individually via system properties. Remove LDAP and LDAPS as supported protocols from JNDI.&amp;#91;1&amp;#93; https://github.com/apache/logging-log4j2/blob/6b1581901ba7a107cdc4a2208ecec03655722b44/RELEASE-NOTES.md#apache-log4j-2170-release-notes</description>
      <version>1.11.6,1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25415" opendate="2021-12-22 00:00:00" fixdate="2021-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement retrial on connections to Cassandra container</summary>
      <description>NoHostAvailableException is raised by Cassandra client under load while connecting to a cluster. It happens no matter the Cassandra backend we use: I saw that using Achilles test backend, cassandra daemon and testContainers. So we should implement a retrial on the connection.</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2544" opendate="2015-8-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some test cases using PowerMock fail with Java 8u20</summary>
      <description>I observed that some of the test cases using PowerMockRunner fail with Java 8u20 with the following error:java.lang.VerifyError: Bad &lt;init&gt; method call from inside of a branchException Details: Location: org/apache/flink/client/program/ClientTest$SuccessReturningActor.&lt;init&gt;()V @32: invokespecial Reason: Error exists in the bytecode Bytecode: 0x0000000: 2a4c 1214 b800 1a03 bd00 0d12 1bb8 001f 0x0000010: b800 254e 2db2 0029 a500 0e2a 01c0 002b 0x0000020: b700 2ea7 0009 2bb7 0030 0157 2a01 4c01 0x0000030: 4d01 4e2b 01a5 0008 2b4e a700 0912 32b8 0x0000040: 001a 4e2d 1234 03bd 000d 1236 b800 1f12 0x0000050: 32b8 003a 3a04 1904 b200 29a6 000a b800 0x0000060: 3c4d a700 0919 04c0 0011 4d2c b800 42b5 0x0000070: 0046 b1 Stackmap Table: full_frame(@38,{UninitializedThis,UninitializedThis,Top,Object[#13]},{}) full_frame(@44,{Object[#2],Object[#2],Top,Object[#13]},{}) full_frame(@61,{Object[#2],Null,Null,Null},{Object[#2]}) full_frame(@67,{Object[#2],Null,Null,Object[#15]},{Object[#2]}) full_frame(@101,{Object[#2],Null,Null,Object[#15],Object[#13]},{Object[#2]}) full_frame(@107,{Object[#2],Null,Object[#17],Object[#15],Object[#13]},{Object[#2]}) at java.lang.Class.getDeclaredConstructors0(Native Method) at java.lang.Class.privateGetDeclaredConstructors(Class.java:2658) at java.lang.Class.getConstructor0(Class.java:3062) at java.lang.Class.getDeclaredConstructor(Class.java:2165) at akka.util.Reflect$$anonfun$4.apply(Reflect.scala:86) at akka.util.Reflect$$anonfun$4.apply(Reflect.scala:86) at scala.util.Try$.apply(Try.scala:161) at akka.util.Reflect$.findConstructor(Reflect.scala:86) at akka.actor.NoArgsReflectConstructor.&lt;init&gt;(Props.scala:359) at akka.actor.IndirectActorProducer$.apply(Props.scala:308) at akka.actor.Props.producer(Props.scala:176) at akka.actor.Props.&lt;init&gt;(Props.scala:189) at akka.actor.Props$.create(Props.scala:99) at akka.actor.Props$.create(Props.scala:99) at akka.actor.Props.create(Props.scala) at org.apache.flink.client.program.ClientTest.shouldSubmitToJobClient(ClientTest.java:143) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:310) at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:88) at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:96) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:294) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:127) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:282) at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:86) at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:49) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:207) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:146) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:120) at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:33) at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:45) at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:118) at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:104) at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:53) at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:53) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)Affected tests are: ClientTest.shouldSubmitToJobClient, ClientTest.shouldSubmitToJobClientFails, DataSourceTaskTest, DataSinkTaskTest and ChainTaskTest. This list, however, is by no means complete.With the latest java version Java 8u51, the tests pass, though. The problem might be related to https://code.google.com/p/powermock/issues/detail?id=505 and to https://bugs.openjdk.java.net/browse/JDK-8051012I propose to either document this fact more prominently or to rewrite the affected tests so that they also run with Java 8u20.</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">docs.setup.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="25440" opendate="2021-12-24 00:00:00" fixdate="2021-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apache Pulsar Connector Document description error about &amp;#39;Starting Position&amp;#39;.</summary>
      <description>Starting Position description error.Start from the specified message time by Message&lt;byte[]&gt;.getEventTime().StartCursor.fromMessageTime(long)it should be 'Start from the specified message time by publishTime.'</description>
      <version>1.14.2</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.TimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.TimestampStartCursor.java</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="25445" opendate="2021-12-27 00:00:00" fixdate="2021-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskExecutor always creates local file for task even when local state store is not used</summary>
      <description>`TaskExecutor` will create file, check `localRecoveryEnabled` and load local state store for each task submission in method `localStateStoreForSubtask`. For batch jobs, the `localRecoveryEnabled` is always false, and there's no need to create these local files for task in `TaskExecutor`</description>
      <version>1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestLocalRecoveryConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.LocalRecoveryConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointLocalRecoveryProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="25446" opendate="2021-12-27 00:00:00" fixdate="2021-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid sanity check on read bytes on DataInputStream#read(byte[])</summary>
      <description>Current changelog related code would check the number of read bytes whether equal to target bytes:checkState(size == input.read(bytes));However, this is not correct as the java doc said: "An attempt is made to read as many as len bytes, but a smaller number may be read, possibly zero."</description>
      <version>1.14.2</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="25485" opendate="2021-12-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>JDBC connector implicitly append url options when use mysql</summary>
      <description>When we directly use buffer-flush options of the mysql sink (sink.buffer-flush.max-rows and sink.buffer-flush.interval) can not increase throughput.We must set 'rewriteBatchedStatements=true' into mysql jdbc url, for 'sink.buffer-flush' to take effect.Such as: 'jdbc:mysql://ip:3306/dbname?rewriteBatchedStatements=true'I want add a method appendUrlSuffix in MySQLDialect to auto set this jdbc option.Many users forget or don't know this option.Inspired by alibaba DataX.https://github.com/alibaba/DataX/blob/master/plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DataBaseType.java</description>
      <version>1.14.2</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.mysql.MySqlDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialect.java</file>
    </fixedFiles>
  </bug>
  <bug id="25488" opendate="2021-12-30 00:00:00" fixdate="2021-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using a pipe symbol as pair delimiter in STR_TO_MAP in combination with concatenation results in broken output</summary>
      <description>Reproducible using Flink Faker:-- Create source tableCREATE TABLE `customers` ( `identifier` STRING, `fullname` STRING, `postal_address` STRING, `residential_address` STRING) WITH ( 'connector' = 'faker', 'fields.identifier.expression' = '#{Internet.uuid}', 'fields.fullname.expression' = '#{Name.firstName} #{Name.lastName}', 'fields.postal_address.expression' = '#{Address.fullAddress}', 'fields.residential_address.expression' = '#{Address.fullAddress}', 'rows-per-second' = '1');-- Doesn't generate expected outputSELECT `identifier`, `fullname`, STR_TO_MAP('postal_address:' || postal_address || '|residential_address:' || residential_address,'|',':') AS `addresses`FROM `customers`;Output will look like:{=, A=null, C=null, D=null, L=null, O=null, P=null, S=null, T=null, _=null, =null, a=null, b=null, c=null, d=null, e=null, g=null, h=null, i=null, ,=null, l=null, -=null, m=null, .=null, n=null, o=null, p=null, q=null, 2=null, r=null, 3=null, s=null, 4=null, t=null, 5=null, u=null, 6=null, v=null, 7=null, w=null, 8=null, 9=null, |=null}When using:-- Output looks like expected when using a different separator SELECT `identifier`, `fullname`, STR_TO_MAP('postal_address:' || postal_address || ';residential_address:' || residential_address,';',':') AS `addresses`FROM `customers`;The output looks as expected:{postal_address=6654 Chong Meadows, East Lupita, CT 51702-8560, residential_address=Apt. 098 51845 Shields Fork, North Erikland, NV 10386}</description>
      <version>1.14.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="25492" opendate="2021-12-31 00:00:00" fixdate="2021-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changelog with InMemoryStateChangelogStorage cannot work well</summary>
      <description>while using ChangelogStataBackend with InMemoryStateChangelogStorage, the seq will be increased when append a record. The materialization will materilalize the record into delegated state backend. But after the materialization finished, the persist method still puts the record into InMemoryChangelogStateHandle which is incorrect.Just as the patch shows, ChangelogDelegateHashMapInMemoryTest will fail because when restore, the medata cannot be found and the record that should't be there is restored firstly.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogWriter.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="25530" opendate="2022-1-5 00:00:00" fixdate="2022-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pulsar source connector in Python DataStream API.</summary>
      <description>Flink have supported Pulsar source connector.https://issues.apache.org/jira/browse/FLINK-20726</description>
      <version>1.14.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25553" opendate="2022-1-6 00:00:00" fixdate="2022-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove MapR filesystem</summary>
      <description>Pending a positive outcome in the Dev mailing list https://lists.apache.org/thread/od2137fk5j1gq034sopj5n2th2w719w4 we can remove the MapR filesystem</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRNotInClassPathTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactoryTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.com.mapr.fs.MapRFileSystem.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.resources.META-INF.services.org.apache.flink.core.fs.FileSystemFactory</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactory.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">docs.content.docs.internals.filesystems.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.internals.filesystems.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="25576" opendate="2022-1-7 00:00:00" fixdate="2022-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.206</summary>
      <description>Flink uses com.h2database:h2 version 1.4.200, we should update this to 2.0.206</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25614" opendate="2022-1-11 00:00:00" fixdate="2022-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let LocalWindowAggregate be chained with upstream</summary>
      <description>When enabling two-phase aggregation (local-global) strategy for Window TVF, the physical plan is shown as follows:TableSourceScan -&gt; Calc -&gt; WatermarkAssigner -&gt; Calc|||| [FORWARD]||LocalWindowAggregate|||| [HASH]||GlobalWindowAggregate||||...We can let the `LocalWindowAggregate` node be chained with upstream operators in order to improve efficiency, just like the non-windowing counterpart `LocalGroupAggregate`. </description>
      <version>1.14.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25656" opendate="2022-1-14 00:00:00" fixdate="2022-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade software.amazon.glue:schema-registry-common and software.amazon.glue:schema-registry-serde dependency from 1.1.5 to 1.1.8</summary>
      <description>We should update the software.amazon.glue:schema-registry-common and software.amazon.glue:schema-registry-serde dependencies from 1.1.5 to 1.1.8 to be up to date with the latest version</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25657" opendate="2022-1-14 00:00:00" fixdate="2022-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade com.amazonaws:amazon-kinesis-client dependency from 1.14.1 to 1.14.7</summary>
      <description>We should update the com.amazonaws:amazon-kinesis-client dependency from 1.14.1 to 1.14.7</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25658" opendate="2022-1-14 00:00:00" fixdate="2022-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup Nexus npm cache</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25683" opendate="2022-1-18 00:00:00" fixdate="2022-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong result if table transfrom to DataStream then window process in batch mode</summary>
      <description>I have 5 line datas,i first need to transform current data with SQLthen mix current data and historical data which is batch get from hbasefor some special reason the program must run in batch modei think the correct result should be like this：(BOB,1)(EMA,1)(DOUG,1)(ALICE,1)(CENDI,1)but the result is :(EMA,1) if i set different parallelism ,the result is different.</description>
      <version>1.14.2</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.source.InputConversionOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.source.InputConversionOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25694" opendate="2022-1-18 00:00:00" fixdate="2022-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Presto to resolve GSON/Alluxio Vulnerability</summary>
      <description>GSON has a bug, which was fixed in 2.8.9, see https://github.com/google/gson/pull/1991. This results in the possibility for DOS attacks.GSON is included in the `flink-s3-fs-presto` plugin, because Alluxio includes it in their shaded client. I've opened an issue in Alluxio: https://github.com/Alluxio/alluxio/issues/14868. When that is fixed, the plugin also needs to be updated.</description>
      <version>1.14.2</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27544" opendate="2022-5-9 00:00:00" fixdate="2022-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="29347" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to restore from list state with empty protobuf object</summary>
      <description>I use protobuf generated class in an union list state.When my flink job restores from checkpoint, I get exception:Caused by: java.lang.RuntimeException: Could not create class com.MY_PROTOBUF_GENERATED_CLASS at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left. at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] Caused by: java.io.EOFException: No more bytes left. at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]  I find it is because when protobuf serializer serializes an object, which is built directly with builder without assign any value to field, the serializer will generate a zero length byte[] and then write it into state with content '\0'（indicates zero length data).When recovered from checkpoint, protobuf seralizer deserialize the data. It get length 0, and call InputStream#read(byte[] bytes, int offset, int count) with count = 0.The underlying Input implementation is NoFetchingInput. It will call Inputsteam#read(byte[] bytes, int offset, int count) with count = 0.The InputStream implementation is ByteStateHandleInputStream, It will return -1 as long as no data left in memory，even if count is 0.A simple fix is add check before return -1. If caller reads 0 bytes, it should always return 0 instead of -1.</description>
      <version>1.14.2,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandleTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="31993" opendate="2023-5-4 00:00:00" fixdate="2023-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize and pass down FailureEnrichers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingPartialDispatcherServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingJobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingDispatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.failure.FailureEnricherUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.PartialDispatcherServicesWithJobPersistenceComponents.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.PartialDispatcherServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobManagerRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
