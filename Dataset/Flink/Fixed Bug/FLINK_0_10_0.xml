<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="14691" opendate="2019-11-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support database related DDLs (don&amp;#39;t need return values) through sqlUpdate() method in TableEnvironment</summary>
      <description>According to FLIP-69, we should support such following DDLs related to the database in the TableEnvironment through `sqlUpdate()` method.1. createDatabaseStatement:CREATE DATABASE [ IF NOT EXISTS ] [ catalogName.] dataBaseName[ COMMENT database_comment ][WITH ( name=value &amp;#91;, name=value&amp;#93;*)]2. dropDatabaseStatement:DROP DATABASE [ IF EXISTS ] [ catalogName.] dataBaseName[ (RESTRICT|CASCADE)]3. alterDatabaseStatement:ALTER DATABASE [ catalogName.] dataBaseName SET( name=value &amp;#91;, name=value&amp;#93;*)4. useDatabaseStatement:USE [ catalogName.] dataBaseName</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlUseDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14692" opendate="2019-11-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alter/rename table through `sqlUpdate` in TableEnvironment</summary>
      <description>1. alterTableStatement:ALTER TABLE [&amp;#91;catalogName.&amp;#93; dataBasesName].tableNameRENAME TO newTableName2. ALTER TABLE [&amp;#91;catalogName.&amp;#93; dataBasesName].tableNameSET ( name=value &amp;#91;, name=value&amp;#93;*)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="14693" opendate="2019-11-10 00:00:00" fixdate="2019-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tox checks fails on travis</summary>
      <description>ImportError: cannot import name 'ensure_is_path' from 'importlib_metadata._compat' (/home/travis/build/apache/flink/flink-python/dev/.conda/lib/python3.7/site-packages/importlib_metadata/_compat.py)============tox checks... &amp;#91;FAILED&amp;#93;============see: https://api.travis-ci.org/v3/job/609614353/log.txt</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1496" opendate="2015-2-9 00:00:00" fixdate="2015-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Events at unitialized input channels are lost</summary>
      <description>If a program sends an event backwards to the producer task, it might happen that some of it input channels have not been initialized yet (UnknownInputChannel). In that case, the events are lost and will never be received at the producer.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="1505" opendate="2015-2-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Separate buffer reader and channel consumption logic</summary>
      <description>Currently, the hierarchy of readers (f.a.o.runtime.io.network.api) is bloated. There is no separation between consumption of the input channels and the buffer readers.This was not the case up until release-0.8 and has been introduced by me with intermediate results. I think this was a mistake and we should seperate this again. flink-streaming is currently the heaviest user of these lower level APIs and I have received feedback from gyfora to undo this as well.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NetworkStackThroughputITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.IndexedReaderIterator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.IndexedMutableReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.CoRecordReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.InputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.CoStreamVertex.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.Tasks.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.event.EventNotificationHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.queue.PipelinedPartitionQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockProducer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockNotificationListener.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockConsumer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.UnionBufferReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.MockIteratorBufferReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.MockBufferReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.event.EventNotificationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationSynchronizationSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BufferWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.UnionBufferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.RecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.ReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
    </fixedFiles>
  </bug>
  <bug id="1519" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web frontend task state mismatch when cancelling</summary>
      <description>When cancelling a task via the web interface, the web interface immediately switches the tasks to the CANCELED state although they actually transition from CANCELLING to CANCELED.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jobmanagerFrontend.js</file>
    </fixedFiles>
  </bug>
  <bug id="15191" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix can&amp;#39;t create table source for Kafka if watermark or computed column is defined</summary>
      <description>We should add schema.watermark.* into the supported properties of Kafka factory and add some tests.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.InMemoryTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.SchemaValidatorTest.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSourceValidation.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableFormatFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableSourceMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableSourceFactoryMock.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.CsvTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.descriptors.SchemaValidator.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectBatchTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15192" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split &amp;#39;SQL&amp;#39; page into multiple sub pages for better structure</summary>
      <description>with FLINK-15190, we are gonna add a bunch of ddl which makes the page too long and not really readable.I suggest split "SQL" page into sub pages of "SQL DDL", "SQL DML", "SQL DQL", and others if needed.assigned to Terry temporarily. cc Jark Wu Jingsong Lee As example, the SQL doc directory of Hive looks like below, which is a lot better that Flink's current oneCHILD PAGESPagesLanguageManualLanguageManual CliLanguageManual DDLLanguageManual DMLLanguageManual SelectLanguageManual JoinsLanguageManual LateralViewLanguageManual UnionLanguageManual SubQueriesLanguageManual SamplingLanguageManual ExplainLanguageManual VirtualColumnsConfiguration PropertiesLanguageManual ImportExportLanguageManual AuthorizationLanguageManual TypesLiteralsLanguageManual VariableSubstitutionLanguageManual ORCLanguageManual WindowingAndAnalyticsLanguageManual IndexingLanguageManual JoinOptimizationLanguageManual LZOLanguageManual CommandsParquetEnhanced Aggregation, Cube, Grouping and RollupFileFormatsHive HPL/SQL</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.zh.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.tuning.streaming.aggregation.optimization.zh.md</file>
      <file type="M">docs.dev.table.tuning.streaming.aggregation.optimization.md</file>
      <file type="M">docs.dev.table.tuning.index.zh.md</file>
      <file type="M">docs.dev.table.tuning.index.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.index.zh.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.modules.zh.md</file>
      <file type="M">docs.dev.table.modules.md</file>
      <file type="M">docs.dev.table.index.zh.md</file>
      <file type="M">docs.dev.table.index.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.hive.hive.catalog.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.catalog.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">docs.dev.table.functions.index.zh.md</file>
      <file type="M">docs.dev.table.functions.index.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="15700" opendate="2020-1-21 00:00:00" fixdate="2020-1-21 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve Python API Tutorial doc</summary>
      <description>Adds the content of preparing input data in the Python API Tutorial doc</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="15970" opendate="2020-2-10 00:00:00" fixdate="2020-2-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the Python UDF execution to only serialize the value</summary>
      <description>Currently, the window/timestamp/pane info are also serialized and sent between the Java operator and the Python worker. These informations are useless and after bumping beam to 2.19.0(BEAM-7951), optimization is possible to not serialize these fields.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15971" opendate="2020-2-10 00:00:00" fixdate="2020-4-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Adjust the default value of bundle size and bundle time</summary>
      <description>Currently the default value for "python.fn-execution.bundle.size" is 1000 and the default value for "python.fn-execution.bundle.time" is 1000ms. We should try to find out a meaningful default value which works best in most scenarios.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15972" opendate="2020-2-10 00:00:00" fixdate="2020-2-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python building blocks to make sure the basic functionality of Python TableFunction could work</summary>
      <description>We need to add a few Python building blocks such as TableFunctionOperation, TableFunctionRowCoder, etc for Python TableFunction execution. TableFunctionOperation is subclass of Operation in Beam and TableFunctionRowCoder, etc are subclasses of Coder in Beam. These classes will be registered into the Beam’s portability framework to make sure they take effects.This PR makes sure that a basic end to end Python UDTF could be executed.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.AbstractPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="15974" opendate="2020-2-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to use the Python UDF directly in the Python Table API</summary>
      <description>Currently, a Python UDF has been registered before using in Python Table API, e.g.t_env.register_function("inc", inc)table.select("inc(id)") \ .insert_into("sink")It would be great if we could support to use Python UDF directly in the Python Table API, e.g.table.select(inc("id")) \ .insert_into("sink")</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
    </fixedFiles>
  </bug>
  <bug id="1610" opendate="2015-2-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java docs do not build</summary>
      <description>Among a bunch of warnings, I get the following error which prevents the java doc generation from finishing:javadoc: error - com.sun.tools.doclets.internal.toolkit.util.DocletAbortException: com.sun.tools.javac.code.Symbol$CompletionFailure: class file for akka.testkit.TestKit not foundCommand line was: /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home/bin/javadoc -Xdoclint:none @options @packages at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.executeJavadocCommandLine(AbstractJavadocMojo.java:5074) at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.executeReport(AbstractJavadocMojo.java:1999) at org.apache.maven.plugin.javadoc.JavadocReport.generate(JavadocReport.java:130) at org.apache.maven.plugin.javadoc.JavadocReport.execute(JavadocReport.java:315) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216) at org.apache.maven.cli.MavenCli.main(MavenCli.java:160) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16109" opendate="2020-2-17 00:00:00" fixdate="2020-2-17 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Move the Python scalar operators and table operators to separate package</summary>
      <description>Currently both the Python scalar operators and table operators are under the same package org.apache.flink.table.runtime.operators.python. There are already many operators under this package. After introducing the aggregate function support and Vectorized Python function support in the future, there will be more and more operators under the same package. We could improve it by the following package structure: org.apache.flink.table.runtime.operators.python.scalar org.apache.flink.table.runtime.operators.python.tableorg.apache.flink.table.runtime.operators.python.aggregate (in the future)org.apache.flink.table.runtime.operators.python.scalar.arrow (in the future)As these classes are internal, it's safe to do so and there are no backwards compatibility issues.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.PassThroughPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.BaseRowPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.AbstractPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.AbstractPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.BaseRowPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="1709" opendate="2015-3-16 00:00:00" fixdate="2015-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for programs with slot count exceeding parallelism</summary>
      <description>Currently, we can't run programs with higher parallelism than available slots.For example, if we currently have a map-reduce program and 4 task slots configured (e.g. 2 task managers with 2 slots per task manager), the map and reduce tasks will be scheduled with pipelined results and the same parallelism in shared slots. Setting the parallelism to more than available slots will result in a NoResourcesAvailableException.As a first step to support these kinds of programs, we can add initial support for this when running in batch mode (after https://github.com/apache/flink/pull/471 is merged).This is easier than the original pipelined scenario, because the map tasks can be deployed after each other to produce the blocking result. The blocking result can then be consumed after all map tasks produced their result. The mechanism in #471 to deploy result receivers can be used for this and should not need any modifications.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17150" opendate="2020-4-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Canal format to support reading canal changelogs</summary>
      <description>Introduce CanalFormatFactory and CanalRowDeserializationSchema to read canal changelogs.CREATE TABLE my_table ( ...) WITH ( 'connector'='...', -- e.g. 'kafka' 'format'='canal-json', 'canal-json.ignore-parse-errors'='true' -- default false);</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug id="17244" opendate="2020-4-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Getting Started / Overview: training and python</summary>
      <description>The Getting Started page needs a bit of general editing, and should it also mention the Training section and the Python Table API walkthrough.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17340" opendate="2020-4-23 00:00:00" fixdate="2020-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs which related to default planner changes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.index.zh.md</file>
      <file type="M">docs.dev.table.index.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="1770" opendate="2015-3-23 00:00:00" fixdate="2015-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename the variable &amp;#39;contentAdressable&amp;#39; to &amp;#39;contentAddressable&amp;#39;</summary>
      <description>Rename the variable 'contentAdressable' to 'contentAddressable' in order to better understanding.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="1774" opendate="2015-3-23 00:00:00" fixdate="2015-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the redundant code in try{} block</summary>
      <description>Remove the redundant code of "fos.close(); fos = null;" in try block because the fos,close() code will always executes in finally block.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="1790" opendate="2015-3-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the redundant import code</summary>
      <description>Remove the redundant import code</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1806" opendate="2015-3-31 00:00:00" fixdate="2015-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve S3 file system error message when no access/secret key provided</summary>
      <description>If no access/secret key has been configured for S3, the error message only states that "Cannot determine access key to Amazon S3".This should give a hint about how to configure it correctly as in FLINK-1646.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.s3.S3FileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="18104" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Test pyflink on windows</summary>
      <description>Test pyflink on windows as follows: maven test maven build python build python test, python install python udf example with dependenciesFeel free to add any other check items.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.PythonDependencyInfoTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="1877" opendate="2015-4-13 00:00:00" fixdate="2015-1-13 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Stalled Flink on Tez build</summary>
      <description>https://travis-ci.org/uce/incubator-flink/jobs/57951373https://s3.amazonaws.com/archive.travis-ci.org/jobs/57951373/log.txt</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-tez.src.test.java.org.apache.flink.tez.test.TezProgramTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="1891" opendate="2015-4-15 00:00:00" fixdate="2015-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add isEmpty check when the input dir</summary>
      <description>Add the input storageDirectory empty check, if input of storageDirectory is empty, we should use tmp as the base dir</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18910" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create the new document structure for Python documentation according to FLIP-133</summary>
      <description>Create the following catalog structure under the "Application Development" catalog:Application Development  - Python API        - Getting Started       - User Guide          - Table API</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.types.zh.md</file>
      <file type="M">docs.dev.table.python.python.types.md</file>
      <file type="M">docs.dev.table.python.python.config.zh.md</file>
      <file type="M">docs.dev.table.python.python.config.md</file>
      <file type="M">docs.dev.table.python.metrics.zh.md</file>
      <file type="M">docs.dev.table.python.metrics.md</file>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
      <file type="M">docs.dev.table.python.dependency.management.zh.md</file>
      <file type="M">docs.dev.table.python.dependency.management.md</file>
      <file type="M">docs.dev.table.python.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.table.python.conversion.of.pandas.md</file>
      <file type="M">docs.dev.table.python.common.questions.zh.md</file>
      <file type="M">docs.dev.table.python.common.questions.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18913" opendate="2020-8-13 00:00:00" fixdate="2020-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "TableEnvironment" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="1957" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error handling of transport failures</summary>
      <description>Currently, transport failures fail the receiver silently w/o proper error attribution to the transport.We want the following behaviour: Attributed to the receiver Receiver fails with an Exception Subpartition on the sender side stays sane Netty channel needs to be closed (as result of transport error) and data transfer aborted</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="1959" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Pending Closed">
    <buginformation>
      <summary>Accumulators BROKEN after Partitioning</summary>
      <description>while running the Accumulator example in https://github.com/Elbehery/flink/blob/master/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java, I tried to alter the data flow with "PartitionByHash" function before applying "Filter", and the resulted accumulator was NULL. By Debugging, I could see the accumulator in the RunTime Map. However, by retrieving the accumulator from the JobExecutionResult object, it was NULL. The line caused the problem is "file.partitionByHash(1).filter(new EmptyFieldFilter())" instead of "file.filter(new EmptyFieldFilter())"</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AccumulatorHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2089" opendate="2015-5-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Buffer recycled" IllegalStateException during cancelling</summary>
      <description>rmetzger reported the following stack trace during cancelling of high parallelism jobs:Error: java.lang.IllegalStateException: Buffer has already been recycled.at org.apache.flink.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:173)at org.apache.flink.runtime.io.network.buffer.Buffer.ensureNotRecycled(Buffer.java:142)at org.apache.flink.runtime.io.network.buffer.Buffer.getMemorySegment(Buffer.java:78)at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.setNextBuffer(SpillingAdaptiveSpanningRecordDeserializer.java:72)at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:80)at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34)at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:73)at org.apache.flink.runtime.operators.MapDriver.run(MapDriver.java:96)at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496)at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)at java.lang.Thread.run(Thread.java:745)This looks like a concurrent buffer pool release/buffer usage error. I'm investing this today.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.RecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21090" opendate="2021-1-22 00:00:00" fixdate="2021-9-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add E2E/ITCase test for checkpoints after tasks finished</summary>
      <description>We should write an E2e test for the feature. The corner cases should include: must check for committing side effects (notifyCheckpointComplete() was called) BroadcastState both non keyed followed by keyed exchanges? aligned/unaligned checkpoints? (with unaligned checkpoint keyed operators/subtasks can be partially finished, which is very very rare in aligned checkpoints) - maybe it’s good enough to test only unaligned checkpoints.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.StopWithSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestJobBuilders.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.BoundedSourceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21095" opendate="2021-1-22 00:00:00" fixdate="2021-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy slotmanagement profile</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSlotSharingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSimpleITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="2123" opendate="2015-6-1 00:00:00" fixdate="2015-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix CLI client logging</summary>
      <description>The CLI client complains about missing log4j configuration and prints too much information.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21231" opendate="2021-2-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add "SHOW VIEWS" to SQL client</summary>
      <description>SQL client cannot run "SHOW VIEWS" statement now, We should add the "SHOW VIEWS" implement to it.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2134" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock in SuccessAfterNetworkBuffersFailureITCase</summary>
      <description>I ran into the issue in a Travis run for a PR: https://s3.amazonaws.com/archive.travis-ci.org/jobs/64994288/log.txtI can reproduce this locally by running SuccessAfterNetworkBuffersFailureITCase multiple times:cluster = new ForkableFlinkMiniCluster(config, false);for (int i = 0; i &lt; 100; i++) { // run test programs CC, KMeans, CC}The iteration tasks wait for superstep notifications like this:"Join (Join at runConnectedComponents(SuccessAfterNetworkBuffersFailureITCase.java:128)) (8/6)" daemon prio=5 tid=0x00007f95f374f800 nid=0x138a7 in Object.wait() [0x0000000123f2a000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000007f89e3440&gt; (a java.lang.Object) at org.apache.flink.runtime.iterative.concurrent.SuperstepKickoffLatch.awaitStartOfSuperstepOrTermination(SuperstepKickoffLatch.java:57) - locked &lt;0x00000007f89e3440&gt; (a java.lang.Object) at org.apache.flink.runtime.iterative.task.IterationTailPactTask.run(IterationTailPactTask.java:131) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559) at java.lang.Thread.run(Thread.java:745)I've asked rmetzger to reproduce this and it deadlocks for him as well. The system needs to be under some load for this to occur after multiple runs.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2156" opendate="2015-6-4 00:00:00" fixdate="2015-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala modules cannot create logging file</summary>
      <description>The Scala only modules flink-scala-shell and flink-ml use Maven's scalatest plugin to run their tests. The scalatest plugin has no forkNumber, though. Therefore, the logging fails to create the logging file as specified in the log4j-travis.properties file.We can fix this issue by giving these two modules different log4j.properties files which don't require a forkNumber. Or we fix the forkNumber to 1.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.pom.xml</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2195" opendate="2015-6-10 00:00:00" fixdate="2015-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set Configuration for Configurable Hadoop InputFormats</summary>
      <description>Configurable Hadoop InputFormats like HBase's mapreduce.TableInputFormat can require the Configuration to be set in order to work correctly.The mapreduce.TableInputFormat (HBase 1.0.1.1) for example fails with a NPE because of this.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2206" opendate="2015-6-11 00:00:00" fixdate="2015-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager webinterface shows 5 finished jobs at most</summary>
      <description>The webinterface of the JobManager shows at most 5 finished jobs. This is because only the last 5 JobGraphs are remembered in the JobManager.The same might apply to canceled and failed jobs.I think this is very confusing to users and could be easily fixed. We should add three simple counters in the JobManager to correctly track these numbers and give the right counts to the webinterface.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.ArchiveMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.MemoryArchivist.scala</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jobmanagerFrontend.js</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JobManagerInfoServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="2210" opendate="2015-6-14 00:00:00" fixdate="2015-10-14 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Table API aggregate by ignoring null values</summary>
      <description>Attempting to aggregate on columns which may have null values results in NullPointerException.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionAggregateFunction.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.aggregations.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2216" opendate="2015-6-15 00:00:00" fixdate="2015-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples directory contains `flink-java-examples-0.9.0-javadoc.jar`</summary>
      <description>Example directory of the packaged code contains `flink-java-examples-0.9.0-javadoc.jar`.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2226" opendate="2015-6-15 00:00:00" fixdate="2015-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail YARN application on failed single-job YARN cluster</summary>
      <description>Users find it confusing that jobs submitted in single-job YARN cluster mode leave the Flink YARN application in state SUCCEEDED after the job fails.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterActor.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobStatus.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.Client.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="22260" opendate="2021-4-13 00:00:00" fixdate="2021-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Source schema in CREATE TABLE LIKE statements is not inferred correctly</summary>
      <description>When using a LIKE statement such as in the following (assume some_sink and some_source to be two tables with the same schema)CREATE TEMPORARY TABLE b LIKE some_sinkINSERT INTO b SELECT * FROM some_sourcethe source schema for the INSERT operation is not actually inferred correctly, causing the entire query to fail:org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default.default.b' do not match.Cause: Different number of columns.Query schema: &amp;#91;name: STRING, ts: TIMESTAMP(3) *ROWTIME*&amp;#93;Sink schema:  []</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.OperationMatchers.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22320" opendate="2021-4-16 00:00:00" fixdate="2021-2-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation for new introduced ALTER TABLE statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.alter.md</file>
    </fixedFiles>
  </bug>
  <bug id="2246" opendate="2015-6-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add chained combine driver strategy for ReduceFunction</summary>
      <description>Running the WordCount example with a text file input/output results and a manual reduce function (instead of the sum(1)) results in a combiner, which is not chained.Replace sum(1) with the following to reproduce and use a text file as input:fileOutput = true;textPath = "...";outputPath = "...";.reduce(new ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() { @Override public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception { return new Tuple2&lt;String, Integer&gt;(value1.f0, value1.f1 + value2.f1); } });</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="22540" opendate="2021-4-30 00:00:00" fixdate="2021-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove YAML environment file support in SQL Client</summary>
      <description>As discussed in https://cwiki.apache.org/confluence/display/FLINK/FLIP-163%3A+SQL+Client+Improvements, YAML environment file is deprecated in 1.13 version and should be removed in 1.14 version. Users are recommended to use SQL script to initialize session.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-python-functions.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-streaming.yaml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.CatalogEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ConfigEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ConfigurationEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.DeploymentEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.FunctionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ModuleEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SinkTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SourceSinkTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SourceTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.TableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.TemporalTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ViewEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.YamlConfigUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.LegacyTableEnvironmentInitializer.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.assembly.test-table-factories.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.DummyTableSinkFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.DummyTableSourceFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.EnvironmentFileUtil.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.UserDefinedFunctions.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-factory-services-file</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-configuration.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-dialect.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-execution.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-modules.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="2264" opendate="2015-6-23 00:00:00" fixdate="2015-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate integration tests for Gelly</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConfigurationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnNeighborMethodsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnEdgesMethodsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.MapVerticesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.MapEdgesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.JoinWithVerticesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.JoinWithEdgesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphOperationsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphMutationsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationWithMapperITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.FromCollectionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.CompareResults.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.GatherSumApplyConfigurationITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2275" opendate="2015-6-24 00:00:00" fixdate="2015-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate test from package &amp;#39;org.apache.flink.test.javaApiOperators&amp;#39;</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.UnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.TypeHintITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.SumMinMaxITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.SortPartitionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ReplicatingDataSourceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ProjectITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.PartitionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupCombineITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.FlatMapITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.FirstNITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.FilterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.DistinctITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.DataSourceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.DataSinkITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CrossITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.AggregateITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2283" opendate="2015-6-27 00:00:00" fixdate="2015-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make grouped reduce/fold/aggregations stateful using Partitioned state</summary>
      <description>Currently the inner state of the grouped aggregations are not persisted as an operator state. These operators should be reimplemented to use the newly introduced partitioned state abstractions which will make them fault tolerant and scalable for the future.A suggested implementation would be to use a stateful mapper to implement the desired behaviour.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UdfStreamOperatorCheckpointingITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.StreamGroupedReduceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.StreamGroupedFoldTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateCheckpointer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFold.java</file>
    </fixedFiles>
  </bug>
  <bug id="22831" opendate="2021-6-1 00:00:00" fixdate="2021-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Scala shell</summary>
      <description>Remove references to flink-table-planner in the Scala shell.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22832" opendate="2021-6-1 00:00:00" fixdate="2021-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in SQL Client</summary>
      <description>Drop legacy planner support for SQL Client</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-python-functions.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-modules.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-execution.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-dialect.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-configuration.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.LegacyTableEnvironmentInitializer.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="22860" opendate="2021-6-3 00:00:00" fixdate="2021-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supplement &amp;#39;HELP&amp;#39; command prompt message for SQL-Cli.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help-command.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22914" opendate="2021-6-8 00:00:00" fixdate="2021-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Kafka new source in Table/SQL connector</summary>
      <description>Currently the Kafka Table/SQL connector is still using the legacy Kafka SourceFunction. In order to align DataStream and Table/SQL API, the new Kafka source should also be used in Table/SQL connector.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="2298" opendate="2015-6-30 00:00:00" fixdate="2015-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow setting custom YARN application names through the CLI</summary>
      <description>A Flink user asked for adding an option to the YARN CLI frontend to provide a custom application name when starting Flink on YARN.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.setup.yarn.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="23112" opendate="2021-6-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Linting should be enforced for Flink UI</summary>
      <description>There is some formatting infrastructure in place, however this currently doesn't seem to work or at least be enforced in the CI.We should make sure that formatting works "out of the box" as much as possible, and definitely ensure that the CI enforces consistent formatting.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23114" opendate="2021-6-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address vulnerabilities in Flink UI</summary>
      <description>We should at least run npm audit and address any current, open vulnerabilities.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2327" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Log the limit of open file handles at startup</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EnvironmentInformationTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="2331" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Whitelist some exceptions to be valid in YARN logs</summary>
      <description>The YARN logs may occasionally contain this exception that occurs on shutdown:14:55:18,819 ERROR akka.remote.RemoteActorRef - swallowing exception during message sendakka.remote.RemoteTransportExceptionNoStackTrace: Attempted to send remote message but Remoting is not running.Whitelisting the akka.remote.RemoteTransportExceptionNoStackTrace should make the tests more reliable.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23330" opendate="2021-7-9 00:00:00" fixdate="2021-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate toAppendStream and toRetractStream</summary>
      <description>The new toDataStream and toChangelogStream should be stable by now. They will not be marked experimental in 1.14. So we can deprecate the old methods and remove them in a couple of releases.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">docs.content.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.zh.docs.deployment.repls.scala.shell.md</file>
    </fixedFiles>
  </bug>
  <bug id="2339" opendate="2015-7-9 00:00:00" fixdate="2015-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent asynchronous checkpoint calls from overtaking each other</summary>
      <description>Currently, when checkpoint state materialization takes very long, and the checkpoint interval is low, the asynchronous calls to trigger checkpoints (on the sources) could overtake prior calls.We can fix that by making sure that all calls are dispatched in order by the same thread, rather than spawning a new thread for each call.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.MemoryLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.TaskEventHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23400" opendate="2021-7-15 00:00:00" fixdate="2021-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to decode a single record for the Python coder</summary>
      <description>Currently, the Python coder always output a Python generator. This makes it impossible to separate the timers and the data into different channels.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.AbstractStreamArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonTimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonFlatMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoFlatMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.OneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="2343" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default garbage collector in streaming environments</summary>
      <description>When starting Flink, we don't pass any particular GC related JVM flags to the system. That means, it uses the default garbage collectors, which are the bulk parallel GCs for both old gen and new gen.For streaming applications, this results in vastly fluctuating latencies. Latencies are much more constant with either the CMS or G1 GC.I propose to make the CMS the default GC for streaming setups.G1 may become the GC of choice in the future, but fro various articles I found, it is still somewhat in "beta" status (see for example here: http://jaxenter.com/kirk-pepperdine-on-the-g1-for-java-9-118190.html )</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2351" opendate="2015-7-12 00:00:00" fixdate="2015-11-12 01:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate config builders in InputFormats and Output formats</summary>
      <description>Old APIs used to pass functions as classes and parameters as configs. To support that, all input- and output formats had config builders.As the record API is deprecated and about to be removed, these builders are no longer needed and should be deprecated and removed.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.FileDataSourceBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="23541" opendate="2021-7-29 00:00:00" fixdate="2021-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call StreamOperator#finish on EndOfData</summary>
      <description>We want to be able to bring the entire topology with a single checkpoint/savepoint. In order to do that we must be able to call finish on all operators before such a checkpoint is triggered. The idea is to emit EndOfData, which once received, triggers finish()/endInput(). After such a sequence we can trigger a checkpoint/savepoint which meets the aforementioned criteria.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2356" opendate="2015-7-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resource leak in checkpoint coordinator</summary>
      <description>The shutdown method of the checkpoint coordinator is not called when a Flink cluster is shutdown via SIGINT. The issue is that the checkpoint coordinator shutdown/cleanup is only called after the job enters a final state. This does not happen for regular cluster shutdown (via kill). Because we don't have proper stopping of streaming jobs, this means that every program using checkpointing is suffering from this.I've tested this only locally for now with a custom WordCount checkpointing the current count. When stopping the process, the files still exist. Since this is the same mechanism as in a distributed setup with HDFS, this should mean that files in HDFS will be lingering around.The problem is that the postStop method of the JM actor is not called when shutting down. The task manager components, which need to do resource cleanup register custom shutdown hooks and don't rely on a shutdown call from the task manager.For 0.9.1 we need to make sure that the state is simply cleaned up with a shutdown hook (as in the blob manager). For 0.10 with HA we need to be more careful and not clean it up when other job manager instances need access. See FLINK-2354 for details.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23800" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose newly added RocksDB native metrics</summary>
      <description>Since Flink bumped RocksDB version, we could expose more newly added RocksDB native metrics.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBProperty.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.native.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2382" opendate="2015-7-20 00:00:00" fixdate="2015-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Live Metric Reporting Does Not Work for Two-Input StreamTasks</summary>
      <description>Also, there are no tests for the live metrics in streaming.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="2407" opendate="2015-7-26 00:00:00" fixdate="2015-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an API switch to select between "exactly once" and "at least once" fault tolerance</summary>
      <description>Based on the addition of the BarrierTracker, we can add a switch to choose between the two modes "exactly once" and "at least once".</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.IterateTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="2412" opendate="2015-7-27 00:00:00" fixdate="2015-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race leading to IndexOutOfBoundsException when querying for buffer while releasing SpillablePartition</summary>
      <description>When running a code as simple as: ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;Edge&lt;String, NullValue&gt;&gt; edges = getEdgesDataSet(env); Graph&lt;String, NullValue, NullValue&gt; graph = Graph.fromDataSet(edges, env); DataSet&lt;Tuple2&lt;String, Long&gt;&gt; degrees = graph.getDegrees();degrees.writeAsCsv(outputPath, "\n", " "); env.execute();on the Freindster data set: https://snap.stanford.edu/data/com-Friendster.html; on 30 Wally nodes I get the following exception:java.lang.Exception: The data preparation for task 'CoGroup (CoGroup at inDegrees(Graph.java:701))' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:471) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559) at java.lang.Thread.run(Thread.java:722)Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607) at org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1145) at org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98) at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:466) ... 3 moreCaused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:784)Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:227) at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:162) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at java.lang.Thread.run(Thread.java:722)Caused by: java.io.IOException: Index: 133, Size: 0Code works fine for the twitter data set, for instance, which is bigger in size, but contains less vertices.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="2420" opendate="2015-7-28 00:00:00" fixdate="2015-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OutputFlush thread in stream writers does not propagate exceptions</summary>
      <description>The output flush thread only throws exceptions that it encountered. This simply lets the thread die, when the exceptions reach the root of the stack.The exceptions never reach the actual writer code. That way, exceptions that only happen on flush (or the last flush) would never be detected.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpoinedITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.SpillingBufferOrEventTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="24271" opendate="2021-9-13 00:00:00" fixdate="2021-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for special char in JSON_VALUE</summary>
      <description>If user has a json string:{"fields": {"system.process": &amp;#91;0.998&amp;#93;}}It is hard to write a valid json path to get 0.998.The correct json path should be '$.fields.&amp;#91;&amp;#39;&amp;#39;system.process&amp;#39;&amp;#39;&amp;#93;&amp;#91;0&amp;#93;'</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2438" opendate="2015-7-30 00:00:00" fixdate="2015-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance of channel events</summary>
      <description>Until now, channel events were sufficiently rare to not matter in their performance. With frequent checkpointing, their performance becomes a factor.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.StreamingScalabilityAndLatency.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.TestEvent.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BufferSpillerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferMassiveRandomTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OutputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.CheckpointBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.event.task.AbstractEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.event.task.RuntimeEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.event.task.TaskEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.EndOfPartitionEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.EndOfSuperstepEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.ReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.TaskEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.TaskEventDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.concurrent.SuperstepBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.event.IterationEventWithAggregators.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.event.TerminationEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationSynchronizationSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.SyncEventHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.event.task.EventList.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.event.task.IntegerTaskEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.event.task.StringTaskEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.event.task.TaskEventTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.AbstractReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestConsumerCallback.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestSubpartitionConsumer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestTaskEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.iterative.concurrent.SuperstepBarrierTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.event.TaskEventHandlerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.FreeingBufferRecycler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="24441" opendate="2021-10-1 00:00:00" fixdate="2021-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block SourceReader when watermarks are out of alignment</summary>
      <description>SourceReader should become unavailable once it's latest watermark is too far into the future</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.TestingSourceOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.CollectingDataOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTestContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.WatermarkToDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.TimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.NoOpTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2445" opendate="2015-7-30 00:00:00" fixdate="2015-3-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add tests for HadoopOutputFormats</summary>
      <description>The HadoopOutputFormats and HadoopOutputFormatBase classes are not sufficiently covered by unit tests.We need tests that ensure that the methods of the wrapped Hadoop OutputFormats are correctly called.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>1.0.1,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormatTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24501" opendate="2021-10-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unexpected behavior of cumulate window aggregate for late event after recover from sp/cp</summary>
      <description>Problem descriptionAfter recover from savepoint or checkpoint, unexpected behavior of cumulate window aggregate for late event may happened.Bug analyzeCurrently, for cumulate window aggregate, late events belongs to the cleaned slice would be merged into the merged window state, and would be counted into the later slice.For example, for a CUMULATE window, step is 1 minute, size is 1 day.SELECT window_start, window_end, COUNT(USER_ID) FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '1' MINUTES, INTERVAL '1' DAY)) GROUP BY window_start, window_end;When the watermark already comes to 11:01, result of window [00:00, 11:01) would be emitted. Let's assume the result is INSERT (00:00, 11:01, 4)Then if a late record which event time is 11:00 comes, it would be merged into merged state, and would be counted into the later slice, for example, for window [00:00, 11:02), [00:00, 11:03)... But the emitted window result INSERT (00:00, 11:01, 4) would not be retracted and updated.The behavior would be different if the job recover from savepoint/checkpoint.Let's do a savepoint after watermark comes to 11:01 and emit (00:00, 11:01, 4).Then recover the job from savepoint. Watermarks are not checkpointed and they need to be repopulated again. So after recovered, the watermark may rollback to 11:00, then if a record which event time is 11:00 comes, it would not be processed as late event, after watermark comes to 11:01 again, a window result INSERT (00:00, 11:01, 5)  would be emitted to downstream.So the downstream operator would receive two INSERT record for WINDOW (00:00, 11:01) which may leads to wrong result. SolutionThere are two solutions for the problem: save watermark to state in slice shared operator. (Prefered) update the behavior for late event. For example, retract the emitted result and send the updated result. It needs to change the behavior of slice state clean mechanism because we clean the slice state after watermark exceeds the slice end currently. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.rank.window.processors.WindowRankProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.window.processors.RowTimeWindowDeduplicateProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2456" opendate="2015-8-1 00:00:00" fixdate="2015-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flink-hbase module dependencies hadoop-2 specifies a repository ID</summary>
      <description>In some special network environment, we can only use maven mirrors to download dependencies for mvn package.It cannot complete construction using one mirror.Error like:&amp;#91;ERROR&amp;#93; Failed to execute goal on project flink-hbase: Could not resolve dependencies for project org.apache.flink:flink-hbase:jar:0.10-SNAPSHOT: Failure to find org.apache.hbase:hbase-server:jar:0.98.11-hadoop2 in http://mirrors.ibiblio.org/pub/mirrors/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of ibiblio.org has elapsed or updates are forced -&gt; &amp;#91;Help 1&amp;#93;We need to specify a repository ID to hadoop-2 plugin so that it can be downloaded over another mirror.This will not affect the normal network environment.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2462" opendate="2015-8-2 00:00:00" fixdate="2015-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong exception reporting in streaming jobs</summary>
      <description>When streaming tasks are fail and are canceled, they report a plethora of followup exceptions.The batch operators have a clear model that makes sure that root causes are reported, and followup exceptions are not reported. That makes debugging much easier.A big part of that is to have a single consistent place that logs exceptions, and that has a view of whether the operation is still running, or whether it has been canceled.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamingRuntimeContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OutputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamingReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterFactory.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BlockingQueueBroker.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.Output.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamEdge.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointNotificationOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="24771" opendate="2021-11-4 00:00:00" fixdate="2021-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump test dependencies mariadb-java-client to 2.7.4 and mysql-connector-java to 8.0.27</summary>
      <description>The JDBC test dependencies are still a bit outdated: org.mariadb.jdbc:mariadb-java-client can be bumped from 2.5.4 to 2.7.4 mysql:mysql-connector-java can be bumped from 8.0.20 to 8.0.27</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24780" opendate="2021-11-4 00:00:00" fixdate="2021-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port time to time casting logic to CastRule</summary>
      <description>Port various timestamp related cast logic (timestamp to timestamp_ltz, timestamp_ltz to timestamp, timestamp to timestamp and timestamp_ltz to timestamp_ltz) to the new CastRule stack</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.IdentityCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRulePredicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BinaryToStringCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="2480" opendate="2015-8-4 00:00:00" fixdate="2015-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improving tests coverage for org.apache.flink.streaming.api</summary>
      <description>The streaming API is quite a bit newer than the other code so it is not that well covered with tests.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.functions.PrintSinkFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24902" opendate="2021-11-15 00:00:00" fixdate="2021-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port boolean &lt;-&gt; numeric casting logic to CastRule</summary>
      <description>More details on the parent task</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.IntegerNumericToBooleanCastRule.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="24911" opendate="2021-11-15 00:00:00" fixdate="2021-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable line numbers in SQL Client</summary>
      <description>Should be enabled/disabled via property sql-client.prompt.show-line-numbersAlso add widget to make it possible to toggle with a key-stroke</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SqlClientOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">docs.layouts.shortcodes.generated.sql.client.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2492" opendate="2015-8-6 00:00:00" fixdate="2015-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename remaining runtime classes from "match" to "join"</summary>
      <description>While working with the runtime join classes, I saw that many of them still refer to the "join" as "match".Since all other parts now consistently refer to "join", we should adjust the runtime classes as well. Makes it easier for new contributors.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatableDanglingPageRank.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.CustomCompensatableDanglingPageRankWithCombiner.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.CustomCompensatableDanglingPageRank.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.ConnectedComponentsNepheleITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CachedMatchTaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MatchDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BuildSecondCachedMatchDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BuildFirstCachedMatchDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideMatchDriver.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24922" opendate="2021-11-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spelling errors in the word "parallism"</summary>
      <description>Fix the spelling error of "parallism" in the document of SQL client.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="2499" opendate="2015-8-7 00:00:00" fixdate="2015-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>start-cluster.sh can start multiple TaskManager on the same node</summary>
      <description>11562 JobHistoryServer3251 Main10596 Jps17934 RunJar6879 Main8837 Main19215 RunJar28902 DataNode6627 TaskManager642 NodeManager10408 RunJar10210 TaskManager5067 TaskManager357 ApplicationHistoryServer3540 RunJar28501 ResourceManager28572 SecondaryNameNode17630 QuorumPeerMain9069 TaskManagerIf we keep execute the start-cluster.sh, it may generate infinite TaskManagers in a single system.And the "nohup" command in the start-cluster.sh can generate nohup.out file that disturb any other nohup processes in the system.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24990" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LookupJoinITCase fails on Java 17</summary>
      <description>The UserDefinedFunctionHelper validates that a given function is public.The InMemory&amp;#91;Async&amp;#93;LookupFunction classes are not public however. Probably some Java&lt;-&gt;Scala interplay that causes this to not be detected at this time.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.InMemoryLookupableTableSource.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24996" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CI image to contain Java 17</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2500" opendate="2015-8-8 00:00:00" fixdate="2015-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some reviews to improve code quality</summary>
      <description>I reviewed the Streaming module and there are some suggestions to improve the code quality(.i.e reduce the complexity of the loop, fix memory leak...).</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="25000" opendate="2021-11-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala 2.12.7 doesn&amp;#39;t compile on Java 17</summary>
      <description>Fails with "fails with /packages cannot be represented as URI" during compilation.2.12.15 was working fine.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.ScalaSerializersMigrationTest.scala</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25004" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable spotless on Java 17</summary>
      <description>The current spotless version doesn't on Java 17. An upgrade is at this time not possible because compatible versions no longer run on Java 8.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25005" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add java17-target profile</summary>
      <description>Add a new profile analogous to the java11-target profile.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25063" opendate="2021-11-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow calls to @VisibleForTesting from enclosing class</summary>
      <description>Pretty much FLINK-25042, just in reverse.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
    </fixedFiles>
  </bug>
  <bug id="2509" opendate="2015-8-11 00:00:00" fixdate="2015-8-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve error messages when user code classes are not found</summary>
      <description>When a job fails because the user code classes are not found, we should add some information about the class loader and class path into the exception message.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ClassLoaderUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25190" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The number of TaskManagers whose status is Pending should be reported</summary>
      <description>The number of TaskManagers whose status is Pending should be reported to allow the outside world to perceive the lack of resources.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="25192" opendate="2021-12-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement proper no-claim mode support</summary>
      <description>In the no-claim mode should not depend on any artefacts of the initial snapshot after the restore. In order to do that we should pass a flag along with the RPC and later on with a CheckpointBarrier to notify TaskManagers about that intention. Moreover state backends should take the flag into consideration and take "full snapshots".The flag should be passed until the first checkpoint succeeds RocksDB state backend should upload all files instead of reusing artefacts from the initial onehttps://cwiki.apache.org/confluence/x/bIyqCw#FLIP193:Snapshotsownership-No-claimmode(defaultmode)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtilTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersisterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.RestoredCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointTypeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointOptionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.hashmap.HashMapStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointConfigOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.RestoreMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.StubStateBackend.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.savepoint.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="25193" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document claim &amp; no-claim mode</summary>
      <description>We should describe how the different restore modes work. It is important to go through the FLIP and include all NOTES in the written documentation</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="25194" opendate="2021-12-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement an API for duplicating artefacts</summary>
      <description>We should implement methods that let us duplicate artefacts in a DFS. We can later on use it for cheaply duplicating shared snapshots artefacts instead of reuploading them.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointFailureHandlingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.NonCheckpointingStorageAccess.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockerCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockCheckpointStorage.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.TestCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.BackendForTestStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestingCheckpointStorageAccessCoordinatorView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestCheckpointStorageWorkerView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStorageWorkerView.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.EntropyInjector.java</file>
    </fixedFiles>
  </bug>
  <bug id="2523" opendate="2015-8-14 00:00:00" fixdate="2015-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make task canceling interrupt interval configurable</summary>
      <description>When a task is canceled, the cancellation calls periodically "interrupt()" on the task thread, if the task thread does not cancel with a certain time.Currently, this value is hard coded to 10 seconds. We should make that time configurable.Until then, I would like to increase the value to 30 seconds, as many tasks (here I am observing it for Kafka consumers) can take longer then 10 seconds for proper cleanup.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="25230" opendate="2021-12-9 00:00:00" fixdate="2021-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden type serialization in JSON plan</summary>
      <description>1. Introduce two representations for LogicalTypeCompact one (using asSerializableString):// compact oneoutputType: "ROW&lt;i INT, s VARCHAR(2147483647)&gt;"// full one for all kinds of logical types (time attributes, char(0), inline structured, etc.)outputType: { "root" : "ROW", "nullable" : true, "fields" : [ { "i" : "INT" }, { "s" : "VARCHAR(2147483647)" }]}2. Drop support of legacy types and symbol classes which should not be part of the plan3. Rework DataView support (shorten, remove concrete classes, support any external type in accumulators)4. Implement a DataTypeJsonDeSerializer5. Replace RelDataTypeJsonDeSerializer with LogicalTypeJsonDeSerializer</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.schema.StructuredRelDataType.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.groupwindow.WindowReference.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeSerdeCoverageTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.typeutils.DataViewUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SerdeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.UnionJsonPlanTest.jsonplan.testUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testOverwrite.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.jsonplan.testGetJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testChangelogSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
    </fixedFiles>
  </bug>
  <bug id="25231" opendate="2021-12-9 00:00:00" fixdate="2021-7-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update PyFlink to use the new type system</summary>
      <description>Currently, there are a lot of places in PyFlink Table API still using the legacy type system. We need to revisit this and migrate them to the new type system(DataType/LogicalType).</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonTableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonScalarFunction.java</file>
      <file type="M">flink-python.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestUpsertSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestRetractSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestCollectionTableFactory.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestAppendSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TableFunc1.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RowSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RowCollector.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RichFunc0.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.MaxAccumulator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.CustomExtractor.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.CustomAssigner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.ByteMaxAggFunction.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonInputFormatTableSource.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.sources.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="2529" opendate="2015-8-16 00:00:00" fixdate="2015-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix on some unused code for flink-runtime</summary>
      <description>In file BlobServer.java, I found the Thread.currentThread() will never return null in my learned knowledge.So I think "shutdownHook != null“ is not necessary in code 'if (shutdownHook != null &amp;&amp; shutdownHook != Thread.currentThread())';And I am not complete sure.Maybe I am wrong.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="25431" opendate="2021-12-23 00:00:00" fixdate="2021-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement file-based JobResultStore</summary>
      <description>The implementation should comply to what's described in FLIP-194</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedJobResultStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.AbstractZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedJobResultStore.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="2547" opendate="2015-8-19 00:00:00" fixdate="2015-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Cluster Status Info</summary>
      <description>Add a request to the web server that returns{ "taskmanagers" : numTaskmanagers, "slots-total" : numSlotsTotal, "slots-available" : nonSlotsAvailable, "jobs-running" : numJobsRunning, "jobs-finished" : numJobsFinished, "jobs-cancelled" : numJobsCancelled, "jobs-failed" : numJobsFailed}</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.vendor-local.d3-timeline.js</file>
      <file type="M">flink-runtime-web.web-dashboard.gulpfile.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.timeline.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.dir.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.jade</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.webmonitor.StatusWithJobIDsOverview.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.webmonitor.RequestStatusWithJobIDsOverview.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.legacy.JobManagerInfoHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.JsonFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="25520" opendate="2022-1-5 00:00:00" fixdate="2022-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement "ALTER TABLE ... COMPACT" SQL</summary>
      <description>FLINK-25176 Introduce "ALTER TABLE ... COMPACT" syntax. In this ticket, we should implement "ALTER TABLE ... COMPACT" SQL</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.TestManagedTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.TestManagedSource.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SourceQueryOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableCompactOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="2554" opendate="2015-8-20 00:00:00" fixdate="2015-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add exception reporting handler</summary>
      <description>The web dashboard should report all encountered exceptions: The root failure cause A list of exceptions encountered at individual tasks, together with the names of the tasks and the executing task managers</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.JsonFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25541" opendate="2022-1-6 00:00:00" fixdate="2022-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-test-utils</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.metric.testutils.MetricListenerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25543" opendate="2022-1-6 00:00:00" fixdate="2022-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-yarn</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTestUtils.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTaskExecutorRunnerTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnLocalResourceDescriptionTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterClientFactoryTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnApplicationFileUploaderTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.ResourceInformationReflectorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.RegisterApplicationMasterResponseReflectorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FallbackYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.entrypoint.YarnWorkerResourceSpecFactoryTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypointTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtilsTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.ContainerRequestReflectorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.configuration.YarnDeploymentTargetTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.AbstractYarnClusterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriverTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25545" opendate="2022-1-6 00:00:00" fixdate="2022-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-clients</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.InternalMiniClusterExtension.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.TestJob.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.ClasspathProvider.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.retry.ExponentialWaitStrategyTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientConfigurationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PackagedProgramUtilsTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PackagedProgramUtilsPipelineTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PackagedProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.OptimizerPlanEnvironmentTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanCreationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.DefaultPackagedProgramRetrieverTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.ExecutionEnvironmentTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.ClusterClientServiceLoaderTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.JobStatusPollingUtilsTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.JarManifestParserTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.FromJarEntryClassInformationProviderTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.FromClasspathEntryClassInformationProviderTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapITCase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.GenericCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.DefaultCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendTestUtils.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendTestBase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendPackageProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendListTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendITCase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendDynamicPropertiesTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendCancelTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.ClientOptionsTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.ClientUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25548" opendate="2022-1-6 00:00:00" fixdate="2022-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-sql-parser</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.TestRelDataTypeFactory.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.TableApiIdentifierParsingTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.ReservedKeywordTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlUnParserTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25549" opendate="2022-1-6 00:00:00" fixdate="2022-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-dstl</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.TestingStateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterSqnTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25550" opendate="2022-1-6 00:00:00" fixdate="2022-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-kuberbetes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResource.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.junit5.MiniClusterExtension.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.utils.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedKubernetesServer.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesWorkerNodeTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.cli.KubernetesSessionCliTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.configuration.KubernetesDeploymentTargetTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.entrypoint.KubernetesWorkerResourceSpecFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesCheckpointIDCounterTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesTestFixture.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.CmdJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.CmdTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.DecoratorWithPodTemplateTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.EnvSecretsDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorAccountTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorWithPodTemplateTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorAccountTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorWithPodTemplateTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.MountSecretsDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.PodTemplateMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesFactoryWithPodTemplateTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryWithPodTemplateTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryWithPodTemplateTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.FlinkPodTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElectorITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElectorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.services.ServiceTypeTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesPodTemplateTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2567" opendate="2015-8-24 00:00:00" fixdate="2015-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CsvParser: Quotes cannot be escaped inside quoted fields</summary>
      <description>We should allow users to escape the quote character inside a quoted field.Quoting could be realized through the \ character like in: "This is an \"escaped\" quotation."Mailing list thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/jira-Created-FLINK-2567-CsvParser-Quotes-cannot-be-escaped-inside-quoted-fields-td7654.html</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.QuotedStringValueParserTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.parser.QuotedStringParserTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.GenericCsvInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringValueParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.GenericCsvInputFormat.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="2570" opendate="2015-8-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Triangle Count Library Method</summary>
      <description>The Gather-Sum-Apply-Scatter version of this algorithm receives an undirected graph as input and outputs the total number of triangles formed by the graph's edges. The implementation consists of three phases:1). Select neighbours with id greater than the current vertex id.Gather: no-opSum: create a set out of these neighboursApply: attach the computed values to the vertices2). Propagate each received value to neighbours with higher id (again using GSA)3). Compute the number of Triangles by verifying if the final vertex contains the sender's id in its list.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.PageRankData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.MusicProfilesData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.LabelPropagationData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.CommunityDetectionData.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="2575" opendate="2015-8-26 00:00:00" fixdate="2015-9-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Don&amp;#39;t activate hash table bloom filter optimisation by default</summary>
      <description>I would like to set the default value of taskmanager.runtime.hashjoin-bloom-filters to false until FLINK-2545 is fixed. This issue is to keep track of this for the upcoming release. After FLINK-2545 is fixed, this issue becomes obsolete.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="25810" opendate="2022-1-25 00:00:00" fixdate="2022-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-171] Add SQL Client Uber jar for KinesisDataStreams Sink.</summary>
      <description>MotivationUser stories:As a Flink user, I’d like to have an uber-jar for kinesis-data-streams sink sql connector with all dependencies without the need for all source dependencies.Scope: Add a new module for flink-sql-connector-kinesis-data-streams which shades flink-connector-kinesis-data-streams with needed dependencies. Add end to end tests for the new connector.ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-kinesis-streams.src.test.resources.send-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-kinesis-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.test.KinesisDataStreamsTableApiIT.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-data-streams.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.table.util.KinesisProducerOptionsMapperTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDataStreamsConnectorOptionsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2584" opendate="2015-8-27 00:00:00" fixdate="2015-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ASM dependency is not shaded away</summary>
      <description>ASM is not correctly shaded away. If you build the quick start against the snapshot version, you will see the following dependencies. Robert is fixing this.[INFO] +- org.apache.flink:flink-java:jar:0.9.1:compile[INFO] | +- org.apache.flink:flink-core:jar:0.9.1:compile[INFO] | | \- commons-collections:commons-collections:jar:3.2.1:compile[INFO] | +- org.apache.flink:flink-shaded-include-yarn:jar:0.9.1:compile[INFO] | +- org.apache.avro:avro:jar:1.7.6:compile[INFO] | | +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile[INFO] | | +- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile[INFO] | | +- com.thoughtworks.paranamer:paranamer:jar:2.3:compile[INFO] | | +- org.xerial.snappy:snappy-java:jar:1.0.5:compile[INFO] | | \- org.apache.commons:commons-compress:jar:1.4.1:compile[INFO] | | \- org.tukaani:xz:jar:1.0:compile[INFO] | +- com.esotericsoftware.kryo:kryo:jar:2.24.0:compile[INFO] | | +- com.esotericsoftware.minlog:minlog:jar:1.2:compile[INFO] | | \- org.objenesis:objenesis:jar:2.1:compile[INFO] | +- com.twitter:chill_2.10:jar:0.5.2:compile[INFO] | | +- org.scala-lang:scala-library:jar:2.10.4:compile[INFO] | | \- com.twitter:chill-java:jar:0.5.2:compile[INFO] | +- com.twitter:chill-avro_2.10:jar:0.5.2:compile[INFO] | | +- com.twitter:chill-bijection_2.10:jar:0.5.2:compile[INFO] | | | \- com.twitter:bijection-core_2.10:jar:0.7.2:compile[INFO] | | \- com.twitter:bijection-avro_2.10:jar:0.7.2:compile[INFO] | +- de.javakaffee:kryo-serializers:jar:0.36:compile[INFO] | | +- com.esotericsoftware:kryo:jar:3.0.3:compile[INFO] | | | +- com.esotericsoftware:reflectasm:jar:1.10.1:compile[INFO] | | | | \- org.ow2.asm:asm:jar:5.0.3:compile[INFO] | | | \- com.esotericsoftware:minlog:jar:1.3.0:compile[INFO] | | \- com.google.protobuf:protobuf-java:jar:2.6.1:compile</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2586" opendate="2015-8-27 00:00:00" fixdate="2015-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable Storm Compatibility Tests</summary>
      <description>The Storm Compatibility tests frequently fail.The reason is that they kill the topologies after a certain time interval. That may fail on CI infrastructure when certain steps are delayed beyond usual. Trying to guarantee progress by time is inherently problematic: Waiting too short makes tests unstable Waiting too long makes tests slowThe right way to go is letting the program decide when to terminate, for example by throwing a special SuccessException.Have a look at the Kafka connector tests, they do this a lot and hence run exactly as short or as long as they need to.Here is an example of a failed run: https://s3.amazonaws.com/archive.travis-ci.org/jobs/77499577/log.txtFrom FLINK-2801The tests for the storm compatibiliy layer are all working with timeouts (running the program for 10 seconds) and then checking whether teh expected result has been written.That is inherently unstable and slow (long delays). They should be rewritten in a similar manner like for example the KafkaITCase tests, where the streaming jobs terminate themselves with a "SuccessException", which can be recognized as successful completion when thrown by the job client.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.StormTupleTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkTopologyTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.WordCountLocalITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormUnionITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormFieldsGroupingITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SpoutSplitITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamSpoutLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamBoltLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitSpoutTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBoltTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.BoltSplitITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocalByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.VerifyAndEnrichBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.print.PrintSampleStream.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.join.SingleJoinExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationLocal.java</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="25860" opendate="2022-1-28 00:00:00" fixdate="2022-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move read buffer allocation and output file creation to setup method for sort-shuffle result partition to avoid blocking main thread</summary>
      <description>Currently, the read buffer allocation and output file creation of sort-shuffle is performed by the main thread. These operations are a little heavy and can block the main thread for a while which may influence other RPC calls including follow-up task deployment. This ticket aims to solve the issue by moving read buffer allocation and output file creation to setup method. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="25867" opendate="2022-1-28 00:00:00" fixdate="2022-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ZH] Add ChangelogBackend documentation</summary>
      <description>In FLINK-25024, documentation for Changelog was added.Chinese version is a copy of English one and needs translation.</description>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="25868" opendate="2022-1-28 00:00:00" fixdate="2022-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable japicmp for all modules</summary>
      <description>While we have many modules with stability annotations, the japicmp plugin is only run on very few (essentially just dataset/-stream + metrics-core), essentially voiding the purpose of the annotations.We should look into enabling the plugin for all modules.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3,elasticsearch-3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader-bundle.pom.xml</file>
      <file type="M">flink-formats.flink-sql-protobuf.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-sql-orc.pom.xml</file>
      <file type="M">flink-formats.flink-sql-json.pom.xml</file>
      <file type="M">flink-formats.flink-sql-csv.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist-scala.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26011" opendate="2022-2-8 00:00:00" fixdate="2022-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop ArchUnit test (infra) for formats test code</summary>
      <description>ArchUnit test (infra) should be developed for formats submodules after the ArchUnit infra for test code has been built.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-orc-nohive.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-compress.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sequence-file.pom.xml</file>
      <file type="M">flink-formats.flink-hadoop-bulk.pom.xml</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26030" opendate="2022-2-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set FLINK_LIB_DIR to &amp;#39;lib&amp;#39; under working dir in YARN containers</summary>
      <description>Currently, we utilize org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils#tryFindUserLibDirectory to locate usrlib in both flink client and cluster side. This method relies on the value of environment variable FLINK_LIB_DIR to find the usrlib.It makes sense in client side since in bin/config.sh, FLINK_LIB_DIR will be set by default(i.e. FLINK_HOME/lib if not exists. But in YARN cluster's containers, when we want to reuse this method to find usrlib, as the YARN usually starts the process using commands like/bin/bash -c /usr/lib/jvm/java-1.8.0/bin/java -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b ...FLINK_LIB_DIR is not guaranteed to be set in such case. Current codes will use current working dir to locate the usrlib which is correct in most cases. But bad things can happen if the machine which the YARN container resides in has already set FLINK_LIB_DIR to a different folder. In that case, codes will try to find usrlib in a undesired place.One possible solution would be overriding the FLINK_LIB_DIR in YARN container env to the lib dir under YARN's working dir.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="2607" opendate="2015-9-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade plugin copies signature files from original jar into fat jar</summary>
      <description>The Quickstart project contains a Maven configuration that builds a fat jar using the Maven Shade plugin. It copies the META-INF folder of the original jar into the fat jar as well. That can lead to a SecurityException when submitting jobs to the cluster because the signature file contained in the original jar is not suitable for the fat jar. java.lang.SecurityException: Invalid signature file digest for Manifest main attributesThe solution is to change the configuration of the Shade plugin to ignore the signature files in the META-INF folder when copying the dependencies to the fat jar. See also:http://zhentao-li.blogspot.ch/2012/06/maven-shade-plugin-invalid-signature.htmlhttp://stackoverflow.com/questions/999489/invalid-signature-file-when-attempting-to-run-a-jar</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2608" opendate="2015-9-2 00:00:00" fixdate="2015-1-2 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Arrays.asList(..) does not work with CollectionInputFormat</summary>
      <description>When using Arrays.asList(..) as input for a CollectionInputFormat, the serialization/deserialization fails when deploying the task.See the following program:public class WordCountExample { public static void main(String[] args) throws Exception { final ExecutionEnvironment env =ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;String&gt; text = env.fromElements( "Who's there?", "I think I hear them. Stand, ho! Who's there?"); // DOES NOT WORK List&lt;Integer&gt; elements = Arrays.asList(0, 0, 0); // The following works: //List&lt;Integer&gt; elements = new ArrayList&lt;&gt;(new int[] {0,0,0}); DataSet&lt;TestClass&gt; set = env.fromElements(new TestClass(elements)); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new LineSplitter()) .withBroadcastSet(set, "set") .groupBy(0) .sum(1); wordCounts.print(); } public static class LineSplitter implements FlatMapFunction&lt;String,Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; out) { for (String word : line.split(" ")) { out.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); } } } public static class TestClass implements Serializable { private static final long serialVersionUID = -2932037991574118651L; List&lt;Integer&gt; integerList; public TestClass(List&lt;Integer&gt; integerList){ this.integerList=integerList; } }Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Cannot initialize task 'DataSource (at main(Test.java:32) (org.apache.flink.api.java.io.CollectionInputFormat))': Deserializing the InputFormat ([mytests.Test$TestClass@4d6025c5]) failed: unread block data at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$4.apply(JobManager.scala:523) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$4.apply(JobManager.scala:507) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:507) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:190) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:43) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:92) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: Deserializing the InputFormat ([mytests.Test$TestClass@4d6025c5]) failed: unread block data at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:60) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$4.apply(JobManager.scala:520) ... 25 moreCaused by: java.lang.IllegalStateException: unread block data at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2424) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1383) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:302) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:264) at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:282) at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:57) ... 26 more</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.kryo.KryoCollectionsSerializerTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26090" opendate="2022-2-11 00:00:00" fixdate="2022-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove pre FLIP-84 methods</summary>
      <description>See https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878 and https://issues.apache.org/jira/browse/FLINK-16364.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Table.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
    </fixedFiles>
  </bug>
  <bug id="2611" opendate="2015-9-3 00:00:00" fixdate="2015-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN reports failed final state for successful jobs</summary>
      <description>Flink sets the final app state to failed even though the application succeeded.http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-batch-runs-OK-but-Yarn-container-fails-in-batch-mode-with-m-yarn-cluster-td2652.html</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="2614" opendate="2015-9-3 00:00:00" fixdate="2015-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Shell&amp;#39;s default local execution mode is broken</summary>
      <description>With recent changes on the master, the FlinkMiniCluster does not start automatically anymore. That's why the Scala Shell doesn't startup anymore:./bin/start-scala-shell.shStarting Flink Shell:Creating new local server</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2624" opendate="2015-9-4 00:00:00" fixdate="2015-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ source / sink should participate in checkpointing</summary>
      <description>The RabbitMQ connector does not offer any fault tolerance guarantees right now, because it does not participate in the checkpointing.We should integrate it in a similar was as the FlinkKafkaConsumer is integrated.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledingSourceBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.cluster.execution.md</file>
    </fixedFiles>
  </bug>
  <bug id="26251" opendate="2022-2-18 00:00:00" fixdate="2022-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-rpc-akka</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.SupervisorActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RobustActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RemoteAkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.ContextClassLoadingSettingTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaUtilsTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSerializedValueTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorHandshakeTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaBootstrapToolsTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.ActorSystemResource.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.concurrent.akka.ClassLoadingUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2632" opendate="2015-9-7 00:00:00" fixdate="2015-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web Client does not respect the class loader of submitted jobs</summary>
      <description>The Web Client offers to submit a Flink jar first and then run it later. For submitted Jars, the class loader is not stored, only the JobGraph. When submitting the JobGraph for execution, the system class loader is used rather than the appropriate class loader.Looks like this is a regression of FLINK-2218.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobSubmissionServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="26320" opendate="2022-2-23 00:00:00" fixdate="2022-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hive doc for 1 m-&gt;1 min</summary>
      <description>Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds)‘1 m’ is misleading when we used, which is not correct. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="26361" opendate="2022-2-25 00:00:00" fixdate="2022-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Unexpected correlate variable $cor0" when using hive dialect to write a subquery</summary>
      <description>With hive dialect, can be reproduced using such code: tableEnv.executeSql("CREATE TABLE src (key string, value string)");tableEnv.executeSql("create view cv1 as \n" + "select * \n" + "from src b where exists\n" + " (select a.key \n" + " from src a \n" + " where b.value = a.value )");List&lt;Row&gt; results = CollectionUtil.iteratorToList(tableEnv.executeSql("select * from src where src" + ".key in (select key from cv1)").collect());The plan for such sql is :LogicalSink(table=[*anonymous_collect$1*], fields=[key, value]) LogicalProject(key=[$0], value=[$1]) LogicalFilter(condition=[IN($0, {LogicalProject(key=[$0]) LogicalProject(key=[$0], value=[$1]) LogicalFilter(condition=[EXISTS({LogicalProject(key=[$0]) LogicalFilter(condition=[=($cor0.value, $1)]) LogicalTableScan(table=[[test-catalog, default, src]])})]) LogicalTableScan(table=[[test-catalog, default, src]])})]) LogicalTableScan(table=[[test-catalog, default, src]])The node LogicalFilter(condition=&amp;#91;=($cor0.value, $1)&amp;#93;) contains `$cor0`, but miss variablesSet. To fix it, we should pass variablesSet when create LogicalFilter.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="26366" opendate="2022-2-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "insert directory"</summary>
      <description>In hive, it allow to write data into the filesystem from queries using the following sqlINSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ...See more detail in Writingdataintothefilesystemfromqueries.As it's quite often used in production environment. We also need to support such usage in hive dialect.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserConstants.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQB.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionLoader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="26371" opendate="2022-2-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support read hive variables using the syntax like select ${hiveconf:select_time}</summary>
      <description>In hive, it's allow to read hive variables using "${xxx}" in sql like:select ${hiveconf:select_time}But currently, such sql is not supported using hive dialect.We're intented to support it. To support it, following hive's implementation, we can just substitute "${xxx}" with actual variable value to get clean sql.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="2638" opendate="2015-9-8 00:00:00" fixdate="2015-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add @SafeVarargs to the environment.fromElements(...) method</summary>
      <description>This annotation suppresses the warnings when people use these methods with generic types.Since we check types and serialize them in the input format anyways, we should be actually safe.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="2639" opendate="2015-9-8 00:00:00" fixdate="2015-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building Flink for specific HDP versions fails</summary>
      <description>Building Flink for the latest Hadoop version in HDP 2.2 fails{{ mvn clean install -DskipTests -Dhadoop.version=2.6.0.2.2.6.0-2800 -Pvendor-repos}}fails with[ERROR] Failed to execute goal on project flink-shaded-include-yarn-tests: Could not resolve dependencies for project org.apache.flink:flink-shaded-include-yarn-tests:jar:0.10-SNAPSHOT: The following artifacts could not be resolved: org.mortbay.jetty:jetty:jar:6.1.26.hwx, org.mortbay.jetty:jetty-util:jar:6.1.26.hwx: Could not find artifact org.mortbay.jetty:jetty:jar:6.1.26.hwx in cloudera-releases (https://repository.cloudera.com/artifactory/cloudera-repos) -&gt; [Help 1]</description>
      <version>0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2640" opendate="2015-9-8 00:00:00" fixdate="2015-9-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Integrate the off-heap configurations with YARN runner</summary>
      <description>The YARN runner needs to adjust the -Xmx, -Xms, and -XX:MaxDirectMemorySize parameters according to the off-heap memory settings.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterActor.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="26410" opendate="2022-3-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support hive transform using syntax</summary>
      <description>Hive provide the synax like transform col using xxxto allow process data using scripts.It's also expected to provide such synax using hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="26412" opendate="2022-3-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect supports "CREATE FUNCTION USING xxx.jar"</summary>
      <description>In Hive, it's supported to use such sql likeCREATE FUNCTION USING xxx.jarto create udf.It's also need to be supported using Hive dialect in Flink</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="26413" opendate="2022-3-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support "LOAD DATA INPATH"</summary>
      <description>In Hive, it's supported to use such sql like LOAD DATA INPATH to import data to hive table.It's also need to support it using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV200.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26414" opendate="2022-3-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect supports "MACRO"</summary>
      <description>In Hive, it's support to using such sql like:CREATE TEMPORARY MACRO macro_name([col_name col_type, ...]) expression;which enable use to define function using sql statement.It's also need to be supported using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.planner.delegation.hive.HiveASTParserTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveFunctionWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2647" opendate="2015-9-9 00:00:00" fixdate="2015-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stream operators need to differentiate between close() and dispose()</summary>
      <description>Currently, operators make no distinction between closing (which needs to emit remaining buffered data) and disposing (releasing resources).In case of a failure, we want to only release the resources, and not attempt to send pending buffered data.Effectively, streaming operators need to implement these methods then: open() processRecord() processWatermark() close() dispose()</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedActiveDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.test.java.org.apache.flink.stormcompatibility.wrappers.StormBoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.main.java.org.apache.flink.stormcompatibility.wrappers.StormBoltWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2651" opendate="2015-9-10 00:00:00" fixdate="2015-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing NettyServerLowAndHighWatermarkTest</summary>
      <description>https://travis-ci.org/aljoscha/flink/jobs/79610050</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2655" opendate="2015-9-10 00:00:00" fixdate="2015-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimize intermediate merging of spilled buffers</summary>
      <description>If the number of spilled buffers exceeds taskmanager.runtime.max-fan then the number of files must reduced with an intermediate merge by reading, merging, and spilling into a single, larger file.The current implementation performs an intermediate merge on all files. An optimal implementation minimizes the amount of merged data by performing partial merges first.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="26551" opendate="2022-3-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the legacy behavior disabled by default</summary>
      <description>Followup of https://issues.apache.org/jira/browse/FLINK-25111For the discussion, see https://lists.apache.org/thread/r13y3plwwyg3sngh8cz47flogq621txv</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.CorrelateJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.functions.AdvancedFunctionsExampleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="2656" opendate="2015-9-10 00:00:00" fixdate="2015-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaConsumer is failing with OutOfRangeException</summary>
      <description>FlinkKafkaConsumer is failing with an OutOfRangeException. There is actually a configuration parameter for the high level kafka consumer how to handle these situations (the high level c) doesn't fail on that exception.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="2662" opendate="2015-9-12 00:00:00" fixdate="2015-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompilerException: "Bug: Plan generation for Unions picked a ship strategy between binary plan operators."</summary>
      <description>I have a Flink program which throws the exception in the jira title. Full text:Exception in thread "main" org.apache.flink.optimizer.CompilerException: Bug: Plan generation for Unions picked a ship strategy between binary plan operators. at org.apache.flink.optimizer.traversals.BinaryUnionReplacer.collect(BinaryUnionReplacer.java:113) at org.apache.flink.optimizer.traversals.BinaryUnionReplacer.postVisit(BinaryUnionReplacer.java:72) at org.apache.flink.optimizer.traversals.BinaryUnionReplacer.postVisit(BinaryUnionReplacer.java:41) at org.apache.flink.optimizer.plan.DualInputPlanNode.accept(DualInputPlanNode.java:170) at org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199) at org.apache.flink.optimizer.plan.DualInputPlanNode.accept(DualInputPlanNode.java:163) at org.apache.flink.optimizer.plan.DualInputPlanNode.accept(DualInputPlanNode.java:163) at org.apache.flink.optimizer.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:194) at org.apache.flink.optimizer.traversals.BinaryUnionReplacer.preVisit(BinaryUnionReplacer.java:49) at org.apache.flink.optimizer.traversals.BinaryUnionReplacer.preVisit(BinaryUnionReplacer.java:41) at org.apache.flink.optimizer.plan.DualInputPlanNode.accept(DualInputPlanNode.java:162) at org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199) at org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199) at org.apache.flink.optimizer.plan.OptimizedPlan.accept(OptimizedPlan.java:127) at org.apache.flink.optimizer.Optimizer.compile(Optimizer.java:520) at org.apache.flink.optimizer.Optimizer.compile(Optimizer.java:402) at org.apache.flink.client.LocalExecutor.getOptimizerPlanAsJSON(LocalExecutor.java:202) at org.apache.flink.api.java.LocalEnvironment.getExecutionPlan(LocalEnvironment.java:63) at malom.Solver.main(Solver.java:66) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)The execution plan:http://compalg.inf.elte.hu/~ggevay/flink/plan_3_4_0_0_without_verif.txt(I obtained this by commenting out the line that throws the exception)The code is here:https://github.com/ggevay/flink/tree/plan-generation-bugThe class to run is "Solver". It needs a command line argument, which is a directory where it would write output. (On first run, it generates some lookuptables for a few minutes, which are then placed to /tmp/movegen)</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>1.0.0,1.1.5,1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.BinaryUnionOpDescriptor.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.UnionReplacementTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.dataexchange.UnionClosedBranchingTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.BinaryUnionNode.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.OperatorTranslation.java</file>
    </fixedFiles>
  </bug>
  <bug id="26700" opendate="2022-3-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update chinese documentation regarding restore modes</summary>
      <description>Translate FLINK-25193</description>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug id="2674" opendate="2015-9-15 00:00:00" fixdate="2015-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework windowing logic</summary>
      <description>The windowing logic needs a major overhaul. This follows the design documents: https://cwiki.apache.org/confluence/display/FLINK/Time+and+Order+in+Streams https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=60624830Specifically, the following shortcomings need to be addressed: Global parallel windows should be dropped -&gt; for time, local windows are aligned and serve the same purpose -&gt; there is currently no known robust and efficient parallel implementation of custom strategies Event time and out of order arrival needs to be supported Eviction of not accessed keys does not work. Non-accessed keys linger infinitely Performance is currently bad for time windows, due to a overly general implementation Resources are leaking, threads are not shut down Elements are stored multiple times (discretizers, window buffers) Finally, many implementations are buggy, produce wrong results</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.NonKeyedWindowOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="2675" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add utilities for scheduled triggers</summary>
      <description>These utilities help schedule triggers for the future, ensure non-concurrent trigger execution, and proper trigger shutdown and release.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.DispatherThreadFactory.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.GroupedProcessingTimeWindowExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.operators.windows.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.operators.windows.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.operators.TriggerTimerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamingRuntimeContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.windows.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.TriggerTimer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.Triggerable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2677" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a general-purpose keyed-window operator</summary>
      <description>This operator should support: Customizable triggers Eviction on triggers: all / none / custom Discard by time (expiry of state) Event time time window assignmentThis set of requirements is effectively a mix between the current trigger/evict model and the Cloud Dataflow window definition model.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.PolicyToOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.KeyedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="26824" opendate="2022-3-23 00:00:00" fixdate="2022-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Flink&amp;#39;s supported Cassandra versions to match with the Cassandra community supported versions</summary>
      <description>Flink's Cassandra connector is currently only supporting com.datastax.cassandra:cassandra-driver-core version 3.0.0. The Cassandra community supports 3 versions. One GA (general availability, the latest version), one stable and one older supported release per https://cassandra.apache.org/_/download.html.These are currently:Cassandra 4.0 (GA)Cassandra 3.11 (Stable)Cassandra 3.0 (Older supported release).We should support (and follow) the supported versions by the Cassandra community</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.archunit-violations.dc1ba6f4-3d84-498c-a085-e02ba5936201</file>
    </fixedFiles>
  </bug>
  <bug id="2683" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add utilities for heap-backed keyed state in time panes</summary>
      <description>Time panes are commonly used for aligned time windows (tumbling and sliding). These utilities would represent the panes with per-pane keyed state.The implementation must support: Evicting panes efficiently iterating over the union of all panes' state per key for all keys (computing the window function for all keys) efficient append the state for a key in a random pane</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTaskException.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.Function.java</file>
    </fixedFiles>
  </bug>
  <bug id="26851" opendate="2022-3-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly model reporter options as ConfigOptions</summary>
      <description>The generic reporter options are currently not properly modeled as config options. Some don't have a config option at all, others are just for documentation purposes and not actually used in the code.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.influxdb.reporter.configuration.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.InMemoryReporter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.ReporterSetup.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.layouts.shortcodes.generated.metric.configuration.html</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug id="2686" opendate="2015-9-15 00:00:00" fixdate="2015-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend JSON plan dump to contain channel info</summary>
      <description>The execution plan JSON (via env.getExecutionPlan()) contains no information about the data exchange mode of the used channels.I think this would be super helpful while debugging problems like FLINK-2685. The channel types should also be considered in the plan rendering, but that's a separate issue.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2687" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Moniroting api / web dashboard: Create request handlers list subtask details and accumulators</summary>
      <description>As part of the new web dashboard and monitoring api, we need handlers that report details about subtasks and accumulators</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.runner.TestRunner.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.legacy.JsonFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.legacy.JobManagerInfoHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JsonFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractExecutionGraphRequestHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.JsonJobGraphGenerationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.WebMonitorMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonGeneratorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.MemoryArchivist.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AccumulatorHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2688" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about monitoring api</summary>
      <description>The documentation should list what requests are available and how to extend the API.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.GenericMessageTester.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JsonMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2689" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reusing null object for joins with SolutionSet</summary>
      <description>Joins and CoGroups with a solution set have outer join semantics because a certain key might not have been inserted into the solution set yet. When probing a non-existing key, the CompactingHashTable will return null.In object reuse mode, this null value is used as reuse object when the next key is probed.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetFirstDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2702" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an implementation of distributed copying utility using Flink</summary>
      <description>Add the DistCP example proposed in this pull request: https://github.com/apache/flink/pull/1090</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTaskInputSplit.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTaskInputFormat.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTask.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.DistCp.java</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2703" opendate="2015-9-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove log4j classes from fat jar / document how to use Flink with logback</summary>
      <description>Flink is using slf4j as the logging interface and log4j as the logging backend.I got requests from users which want to use a different logging backend (logback) with Flink. Currently, its quite hard for them, because they have to do a custom Flink build with log4j excluded.The purpose of this JIRA is to ship a Flink build that is prepared for users to use a different logging backend.I'm also going to document how to use logback with Flink.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.apis.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="2704" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up naming of State/Checkpoint Interfaces</summary>
      <description>Add the name cleanups proposed in https://github.com/apache/flink/pull/671They refer to an internal interface that is implemented by stateful tasks that are checkpointed. It consolidates three interfaces into one, because all stateful tasks need to implement all three of them together anyways.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.OperatorStateCarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointNotificationOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointedOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27063" opendate="2022-4-5 00:00:00" fixdate="2022-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive 2.3 connector to version 2.3.6</summary>
      <description>As discussed on the Dev mailing list we should update our Hive 2.3 connector to Hive 2.3.6 (the latest supported version of the Hive 2.3.* release)Discussion can be found in https://lists.apache.org/thread/2w046dwl46tf2wy750gzmt0qrcz17z8t</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-defining-excess-dependencies.modulelist</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.slf4j-api</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.reflectasm</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.minlog</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.kryo</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.jodd</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.javolution</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="27064" opendate="2022-4-5 00:00:00" fixdate="2022-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Centralize ArchUnit rules for production code</summary>
      <description>It is better to centralize ArchUnit rules so that external repos are able to use them. </description>
      <version>None</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.test.java.org.apache.flink.architecture.rules.TableApiRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.test.java.org.apache.flink.architecture.ArchitectureTest.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2714" opendate="2015-9-20 00:00:00" fixdate="2015-10-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Port the Flink DataSet Triangle Enumeration example to the Gelly library</summary>
      <description>Currently, the Gelly library contains one method for counting the number of triangles in a graph: a gather-apply-scatter version. This issue proposes the addition of a library method based on this Flink example:https://github.com/apache/flink/blob/master/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/graph/EnumTrianglesOpt.java</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.library.TriangleEnumeratorITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.TriangleEnumerator.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.TriangleCountData.java</file>
    </fixedFiles>
  </bug>
  <bug id="2716" opendate="2015-9-20 00:00:00" fixdate="2015-1-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Checksum method for DataSet and Graph</summary>
      <description>DataSet.count(), Graph.numberOfVertices(), and Graph.numberOfEdges() provide measures of the number of distributed data elements. New DataSet.checksum() and Graph.checksum() methods will summarize the content of data elements and support algorithm validation, integration testing, and benchmarking.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.util.DataSetUtilsITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.DataSetUtilsITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.utils.package.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27185" opendate="2022-4-11 00:00:00" fixdate="2022-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run the assertj conversion script to convert assertions in connectors and formats</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.ElasticsearchResource.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestClientBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.WatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.UniformShardAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.RecordEmitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2UtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2FactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardHandleTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.StartingPositionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.SentinelSequenceNumberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.DynamoDBStreamsShardHandleTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.metrics.ShardConsumerMetricsReporterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.metrics.PollingRecordPublisherMetricsReporterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerFanOutTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordBatchTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.StreamConsumerRegistrarTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.metrics.testutils.MetricMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.utils.SerdeUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueueTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceMetricsITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManagerTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.CoordinatedSourceRescaleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.XidImplTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkNoInsertionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkH2Test.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkDerbyTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaFacadeTestHelper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaFacadeImplTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.utils.JdbcTypeUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImplTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.split.NumericBetweenParametersProviderTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcRowOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcFullTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProviderTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProviderDriverClassConcurrentLoadingITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.dialect.oracle.OracleTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.dialect.oracle.OraclePreparedStatementTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresTablePathTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.factory.JdbcCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleWriteAheadSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.planner.delegation.hive.HiveASTParserTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.util.HiveTableUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.util.HiveConfUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveTableFileInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.PartitionMonitorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveOutputFormatFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDeserializeExceptionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.TestPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptionsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.JSONKeyValueDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicsDescriptorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueueTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscovererTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherWatermarksTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressionFactoryITCase.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.RowCsvInputFormatSplitTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.RowCsvInputFormatTest.java</file>
      <file type="M">flink-formats.flink-hadoop-bulk.src.test.java.org.apache.flink.formats.hadoop.bulk.AbstractFileCommitterTest.java</file>
      <file type="M">flink-formats.flink-hadoop-bulk.src.test.java.org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.java</file>
      <file type="M">flink-formats.flink-hadoop-bulk.src.test.java.org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPendingFileRecoverableSerializerMigrationTest.java</file>
      <file type="M">flink-formats.flink-hadoop-bulk.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.HadoopPathBasedBulkFormatBuilderTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemFilterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcSplitReaderUtilTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.util.OrcBulkWriterTestUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.AvroParquetFileReadITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.AvroParquetStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.protobuf.ParquetProtoStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SequenceStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SerializableHadoopConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommittableSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.TransactionIdFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.TransactionToAbortCheckerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetricsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.testutils.KafkaSourceTestEnv.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkFixedPartitionerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2722" opendate="2015-9-21 00:00:00" fixdate="2015-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use InetAddress.getLocalHost() first when detecting TaskManager IP address</summary>
      <description>A user reported a connection issue with Netty being unable to connect to a TaskManager to subscribe to an intermediate result.The problem occurred when the TaskManager and JobManager were running on the same host (something that can easily happen on YARN).In that case, the TaskManager was reporting a host-local ip address to the JobManager when connecting.To avoid the issue in the future, the TaskManager first tries to use the hostname returned by InetAddress.getLocalHost(). In a properly set-up environment, this will return a connection which is accessible by all machines in a cluster.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.NetUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27240" opendate="2022-4-14 00:00:00" fixdate="2022-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ADD PARTITION statement for partitioned table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAddPartitions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="27241" opendate="2022-4-14 00:00:00" fixdate="2022-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DROP PARTITION statement for partitioned table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropPartitions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="27243" opendate="2022-4-14 00:00:00" fixdate="2022-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SHOW PARTITIONS statement for partitioned table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlOtherOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ShowPartitionsOperation.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlShowPartitions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="27244" opendate="2022-4-14 00:00:00" fixdate="2022-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support subdirectories with Hive tables</summary>
      <description>Hive support to read recursive directory by setting the property 'set mapred.input.dir.recursive=true', and Spark also support [such behavior|https://stackoverflow.com/questions/42026043/how-to-recursively-read-hadoop-files-from-directory-using-spark].For normal case, it won't happed for reading recursive directory. But it may happen in the following case:I have a paritioned table `fact_tz` with partition day/hourCREATE TABLE fact_tz(x int) PARTITIONED BY (ds STRING, hr STRING) Then I want to create an external table `fact_daily` refering to  `fact_tz`, but with a coarse-grained partition day. create external table fact_daily(x int) PARTITIONED BY (ds STRING) location 'fact_tz_localtion' ;ALTER TABLE fact_daily ADD PARTITION (ds='1') location 'fact_tz_localtion/ds=1'But it wll throw exception "Not a file: fact_tz_localtion/ds=1" when try to query this table `fact_daily` for it's the first level of the origin partition and is actually a directory .  </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="2725" opendate="2015-9-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Max/Min/Sum Aggregation of mutable types</summary>
      <description>Support mutable value types in min, max, and sum aggregations.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.AggregateITCase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ValueTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.SumAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MinAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MaxAggregationFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="27250" opendate="2022-4-14 00:00:00" fixdate="2022-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from sql-parser[-hive]</summary>
      <description>These modules disable fork reuse and set the fork count to 1. After a quick test I see now reason why that is required, so we get rid of this special case.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2727" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a base class for MessageQueue-with-acknowledgement sources</summary>
      <description>Several message queues (RabbitMQ, Amazon SQS) have the pattern that you retrieve messages and acknowledge them back by ID.We can create a simple base non-parallel source that provides tooling for: Collecting the IDs of elements emitted between two checkpoints Persisting them with the checkpoint, respecting proper serialization Acknowledging them when a checkpoint is notified of completion.This assumes that the Message Queues retain unacknowledged messages and re-emit them after the acknowledgement period expired.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.checkpoint.Checkpointed.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DataOutputSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2729" opendate="2015-9-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add TaskManager overview</summary>
      <description>The dashboard needs a task manager overview, similar to the old web frontend.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2730" opendate="2015-9-22 00:00:00" fixdate="2015-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CPU/Network utilization graphs to new web dashboard</summary>
      <description>The charts rendered in the previous dashboard should be added to the new web dashboard.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanager.taskmanager.dir.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanagers.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanagers.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanagers.taskmanagers.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanagers.taskmanagers.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanagers.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanagers.index.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2731" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add JobManager log file access</summary>
      <description>Add access to the JobManager log file and out file in the web dashboard</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.web.WebFrontendITCase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobmanager.stdout.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobmanager.logfile.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobmanager.jobmanager.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobmanager.jobmanager.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobmanager.stdout.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobmanager.logfile.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobmanager.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.MimeTypes.java</file>
    </fixedFiles>
  </bug>
  <bug id="2734" opendate="2015-9-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayKeySelector returns wrong positions (or fails)</summary>
      <description>The ArrayKeySelector is broken and returns wrong values in all cases except for &amp;#91;0&amp;#93; as a single only key position.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.keys.KeySelectorUtil.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.KeyedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="27340" opendate="2022-4-21 00:00:00" fixdate="2022-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-python</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.AbstractStreamArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RichFunc0.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.ProtoUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonDependencyUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonConfigUtilTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.CompressionUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.metric.FlinkMetricContainerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.PythonDependencyInfoTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.process.ProcessPythonEnvironmentManagerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonShellParserTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverOptionsParserFactoryTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.cli.PythonProgramOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.cli.PythonProgramOptionsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27390" opendate="2022-4-25 00:00:00" fixdate="2022-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused flink-tests dependencies</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2744" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of concurrent test forks to 1 for the Kafka connector project</summary>
      <description>Since the Kafka connector tests are heavyweight with many Mini Clusters, their stability would benefit from not having multiple builds competing for resources.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27470" opendate="2022-5-2 00:00:00" fixdate="2022-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-statebackend-heap-spillable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.TestAllocator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListSerializerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListKeyComparatorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.OnHeapLevelIndexHeaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteSkipListStateMapTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteSkipListStateMapComplexOpTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteSkipListStateMapBasicOpTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27471" opendate="2022-5-2 00:00:00" fixdate="2022-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ARRAY_DISTINCT supported in SQL &amp; Table API</summary>
      <description>Removes duplicate values from the array.Syntax:array_distinct(array) Arguments: array: An ARRAY to be handled.Returns:An ARRAY. If value is NULL, the result is NULL. Keeps order of elements.Examples:SELECT array_distinct(ARRAY[1, 2, 3, 2, 1]);-- [1, 2, 3]SELECT array_distinct(ARRAY[1, NULL, 1]);-- [1, NULL]See also https://spark.apache.org/docs/latest/api/sql/index.html#array_distinct</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-python.docs.reference.pyflink.table.expressions.rst</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27476" opendate="2022-5-2 00:00:00" fixdate="2022-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build new import option that only focus on maven main classes</summary>
      <description>ImportOption.DoNotIncludeTests.class used currently has some issue when running test with testContainer. It would be good to define the target class path precisely.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.main.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-base.src.main.java.org.apache.flink.architecture.common.SourcePredicates.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-base.src.main.java.org.apache.flink.architecture.common.ImportOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="27477" opendate="2022-5-2 00:00:00" fixdate="2022-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop flink-yarn test-jar</summary>
      <description>We could do just fine without this test-jar.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnPrioritySchedulingITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2015-9-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulator fetch failure leads to duplicate job result response</summary>
      <description>On JobStatusChanged message and a failure to catch the accumulator result the client will receive a JobResultFailure and JobResultSuccess responsenewJobStatus match { case JobStatus.FINISHED =&gt; val accumulatorResults: java.util.Map[String, SerializedValue[AnyRef]] = try { executionGraph.getAccumulatorsSerialized() } catch { case e: Exception =&gt; log.error(s"Cannot fetch final accumulators for job $jobID", e) val exception = new JobExecutionException(jobID, "Failed to retrieve accumulator results.", e) jobInfo.client ! decorateMessage(JobResultFailure( new SerializedThrowable(exception))) Collections.emptyMap() &lt;&lt;&lt; HERE } val result = new SerializedJobExecutionResult( jobID, jobInfo.duration, accumulatorResults) jobInfo.client ! decorateMessage(JobResultSuccess(result)) &lt;&lt;&lt; HEREFurthermore the indentation is off.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
    </fixedFiles>
  </bug>
  <bug id="27490" opendate="2022-5-4 00:00:00" fixdate="2022-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-table-code-splitter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.ReturnValueRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.MemberFieldRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaParserTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaCodeSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.IfStatementRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.FunctionSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.DeclarationRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.CodeRewriterTestBase.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.AddBooleanBeforeReturnRewriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2751" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart is in documentation but only linked through the Flink homepage</summary>
      <description>The Quickstart docs contained in docs/quickstart should also be included in the documentation menu. Basically, we could copy over the Quickstart menu from the Flink homepage.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug id="2752" opendate="2015-9-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Documentation is not easily differentiable from the Flink homepage</summary>
      <description>When users go to the documentation, either via the homepage's quickstart menu or the documentation menu, the transition to the documentation is not easily noticeable; the layout of both pages are pretty much the same. There should be a hint in the documentation page "Flink documentation version X. Click here to go back to the homepage" or something similar.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.how.to.contribute.md</file>
      <file type="M">docs.internals.coding.guidelines.md</file>
    </fixedFiles>
  </bug>
  <bug id="27532" opendate="2022-5-6 00:00:00" fixdate="2022-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop flink-clients test-jar</summary>
      <description>The test-jar is actually unused and could be removed entirely.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-clients.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2761" opendate="2015-9-24 00:00:00" fixdate="2015-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent instantiation of new ExecutionEnvironments in the Scala Shell</summary>
      <description>When someone mistakenly creates a new ExecutionEnvironment in the Scala Shell, the programs don't work. The Scala Shell should prevent new ExecutionEnvironment instantiations.That can be done by setting a context environment factory that throws an error when attempting to create a new environment.See here for a user with that problem:http://stackoverflow.com/questions/32763052/flink-datasources-outputs-caused-an-error-could-not-read-the-user-code-wrappe/32765236#32765236</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITSuite.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="27620" opendate="2022-5-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support percent_rank</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2767" opendate="2015-9-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support Scala 2.11 to Scala shell</summary>
      <description>Since FLINK-2200 is resolved, the Flink community provides JARs for Scala 2.11. But currently, there is no Scala shell with Scala 2.11. If we add support Scala 2.11 to Scala shell, the user with Scala 2.11 could use Flink easily.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITSuite.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-staging.flink-scala-shell.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27672" opendate="2022-5-17 00:00:00" fixdate="2022-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-table-common</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeMergingTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TypeStringUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TypeMappingUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.print.TableauStyleTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PartitionPathUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.EncodingUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ValueDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.constraints.UniqueConstraintTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.columnar.vector.ColumnVectorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.columnar.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.JoinedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.utils.ProjectedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.descriptors.DescriptorPropertiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.descriptors.DescriptorTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.expressions.ExpressionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.module.CoreModuleFactoryTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TableSinkFactoryServiceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.functions.UserDefinedFunctionHelperTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.module.CoreModuleTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.plan.stats.TableStatsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.sources.TableSourceTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ClassDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.TypeInferenceExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.ComparableInputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.MappingTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.ArrayTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.CurrentWatermarkInputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.CurrentWatermarkTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.DecimalTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.GetTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.MapTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.RepeatingSequenceInputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.RowTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.StringConcatTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.SymbolArgumentTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.TypeLiteralArgumentTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.SubsequenceInputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalCommonTypeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeDuplicatorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecksTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27673" opendate="2022-5-17 00:00:00" fixdate="2022-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-table-api-scala</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.TypeInferenceExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.DataTypeExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.expressions.ObjectToExpressionScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ImplicitConversionsTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ExpressionsConsistencyCheckTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2768" opendate="2015-9-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong Java version requirements in "Quickstart: Scala API" page</summary>
      <description>Since Flink 0.10, we dropped Java 6 support. But "Quickstart: Scala API" page says that Java 6 is one of minimum requirement.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.local.setup.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="2776" opendate="2015-9-28 00:00:00" fixdate="2015-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print job id to standard out on CLI job submission</summary>
      <description>When executing Flink jobs, the job id is printed as part of the JobClient and JobManager log messages. This information is available in the client log file but not printed to standard out by default when using the CLI client.Some users have requested that the job identifier should be printed to standard out such that is is available in detached execution mode. In detached execution mode, the job id is important for querying the status of a submitted job.We could ask users to change the log4-cli.properties settings. IMHO a better solution would be to always print the job id to standard out for submitted jobs.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="2796" opendate="2015-10-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI -q flag to supress the output does not work</summary>
      <description>The log output is shown regardless of whether -q is specified:/bin/flink run -q examples/WordCount.jar</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="2797" opendate="2015-10-1 00:00:00" fixdate="2015-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI: Missing option to submit jobs in detached mode</summary>
      <description>Jobs can only be submitted in detached mode using YARN but not on a standalone installation. This has been requested by users who want to submit a job, get the job id, and later query its status.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="27971" opendate="2022-6-9 00:00:00" fixdate="2022-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-orc and flink-orc-nohive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkWriterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkWriterITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.writer.OrcBulkRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.util.OrcBulkWriterTestUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcSplitReaderUtilTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemFilterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.test.java.org.apache.flink.orc.nohive.OrcColumnarRowSplitReaderNoHiveTest.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27985" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkRecomputeStatisticsProgram to compute statistics after filter push and partition pruning</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.PartitionPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.FilterPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.Date.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="27989" opendate="2022-6-10 00:00:00" fixdate="2022-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CSV format supports reporting statistics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28011" opendate="2022-6-12 00:00:00" fixdate="2022-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize getAllPartitions in HiveSource</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.HivePartitionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2811" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add page with configuration overview</summary>
      <description>The old web interface contained a page to view the configuration of the JobManager.This issue is about adding the page again.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28121" opendate="2022-6-20 00:00:00" fixdate="2022-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Extension Points" and "Full Stack Example" in "User-defined Sources &amp; Sinks" page</summary>
      <description>The links are https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#extension-pointsand https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#full-stack-example</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="28122" opendate="2022-6-20 00:00:00" fixdate="2022-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Overview " and "Project Configuration" in "User-defined Sources &amp; Sinks" page</summary>
      <description>The links arehttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#overviewand https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#project-configuration</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="2817" opendate="2015-10-4 00:00:00" fixdate="2015-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileMonitoring function throws NPE when location is empty</summary>
      <description>StreamExecutionEnvironment.readFileStream() does not handle a missing location properly. I would suggest to log that the location is empty and continue running the job.A test covering the correct behavior is also needed.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="28201" opendate="2022-6-22 00:00:00" fixdate="2022-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize utils around dependency-plugin</summary>
      <description>We'll be adding another safeguard against developer mistakes which also parses the output of the dependency plugin, like the scala suffix checker.We should generalize this parsing such that both checks can use the same code.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28202" opendate="2022-6-22 00:00:00" fixdate="2022-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize utils around shade-plugin</summary>
      <description>We'll be adding another safeguard against developer mistakes which also parses the output of the shade plugin, like the license checker.We should generalize this parsing such that both checks can use the same code.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.NoticeFileCheckerTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28203" opendate="2022-6-22 00:00:00" fixdate="2022-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark all bundled dependencies as optional</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.shared.DependencyTree.java</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader-bundle.pom.xml</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java-uber.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-formats.flink-sql-protobuf.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-sql-orc.pom.xml</file>
      <file type="M">flink-formats.flink-sql-json.pom.xml</file>
      <file type="M">flink-formats.flink-sql-csv.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist-scala.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28310" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce aggregating task metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexTaskManagersInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.metrics.IOMetricsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexTaskManagersInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="28311" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce REST APIs for the environmental information</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="28312" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce REST APIs for log URL retrieval</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.layouts.shortcodes.generated.history.server.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2833" opendate="2015-10-8 00:00:00" fixdate="2015-10-8 01:00:00" resolution="Done">
    <buginformation>
      <summary>Unstage Gelly and Module refactoring</summary>
      <description>This is for moving Gelly out of flink-staging and adding it into a new module, flink-libraries.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.SingleSourceShortestPathsData.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConfigurationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.TestGraphUtils.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnNeighborsWithExceptionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnNeighborMethodsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnEdgesWithExceptionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnEdgesMethodsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.MapVerticesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.MapEdgesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.JoinWithVerticesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.JoinWithEdgesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphOperationsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphMutationsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationWithMapperITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationWithCsvITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphCreationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.FromCollectionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesWithExceptionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.library.TriangleCountITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.library.PageRankITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.library.LabelPropagationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.library.ConnectedComponentsWithRandomisedEdgesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.library.CommunityDetectionITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.GatherSumApplyITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.GatherSumApplyConfigurationITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.SingleSourceShortestPathsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.MusicProfilesITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.JaccardSimilarityMeasureITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.IncrementalSSSPITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.EuclideanGraphWeighingITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.example.ConnectedComponentsITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.CollectionModeSuperstepITCase.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelTranslationTest.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelCompilerTest.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.gsa.GSATranslationTest.java</file>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.gsa.GSACompilerTest.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Vertex.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.validation.InvalidVertexIdsValidator.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.validation.GraphValidator.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.utils.VertexToTuple2Map.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.utils.Tuple3ToEdgeMap.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.utils.Tuple2ToVertexMap.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.utils.NullValueEdgeMapper.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.utils.EdgeToTuple3Map.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Triplet.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexUpdateFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricIteration.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricConfiguration.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessagingFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessageIterator.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.ReduceNeighborsFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.ReduceEdgesFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.NeighborsFunctionWithVertexValue.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.NeighborsFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSATriangleCount.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAPageRank.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAConnectedComponents.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.library.CommunityDetection.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.IterationConfiguration.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.SumFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.Neighbor.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.GSAConfiguration.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.GatherSumApplyIteration.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.GatherFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.gsa.ApplyFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.GraphCsvReader.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.GraphAlgorithm.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.TriangleCountData.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">flink-staging.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.EdgesFunction.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.EdgesFunctionWithVertexValue.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.ConnectedComponents.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.GraphMetrics.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.GSASingleSourceShortestPaths.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.SingleSourceShortestPaths.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.Graph.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.NeighborsFunction.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.NeighborsFunctionWithVertexValue.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.package.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.EdgeToTuple3Map.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.Tuple2ToVertexMap.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.Tuple3ToEdgeMap.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.utils.VertexToTuple2Map.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.GellyScalaAPICompletenessTest.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.DegreesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphCreationWithCsvITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphMutationsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphOperationsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.JoinWithEdgesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.JoinWithVerticesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.MapEdgesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.MapVerticesITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.ReduceOnEdgesMethodsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.ReduceOnNeighborMethodsITCase.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.TestGraphUtils.scala</file>
      <file type="M">flink-staging.flink-gelly.pom.xml</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.Edge.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.EdgeDirection.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.EdgesFunction.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.EdgesFunctionWithVertexValue.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.ConnectedComponents.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.EuclideanGraphWeighing.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GraphMetrics.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.JaccardSimilarityMeasure.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.MusicProfiles.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.CommunityDetectionData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.ConnectedComponentsDefaultData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.EuclideanGraphData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.ExampleUtils.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.IncrementalSSSPData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.JaccardSimilarityMeasureData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.LabelPropagationData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.MusicProfilesData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.PageRankData.java</file>
    </fixedFiles>
  </bug>
  <bug id="28380" opendate="2022-7-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Produce one intermediate dataset for multiple consumers consuming the same data</summary>
      <description>Currently, if one output of an upstream job vertex is consumed by multiple downstream job vertices, the upstream vertex will produce multiple dataset. For blocking shuffle, it means serialize and persist the same data multiple times. This ticket aims to optimize this behavior and make the upstream job vertex produce one dataset which will be read by multiple downstream vertex.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.partitioner.BinaryHashPartitioner.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamConfig.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForUnspecifiedPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForConsecutiveHashPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
    </fixedFiles>
  </bug>
  <bug id="28382" opendate="2022-7-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce new compression algorithms of higher compression ratio</summary>
      <description>Currently, we use lz4 for shuffle data compression which is a good balance between IO optimization and CPU consumption. But for some scenarios, the IO becomes bottleneck and the storage space is limited (especially for k8s environment). For these cases, we need compression algorithms of higher compression ratio to further reduce IO.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageClientSideSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.compression.BlockCompressionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferDecompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferCompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.Lz4BlockDecompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.Lz4BlockCompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.Lz4BlockCompressionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.InsufficientBufferException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.DataCorruptionException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.BlockDecompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.BlockCompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.compression.BlockCompressionFactory.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="28420" opendate="2022-7-6 00:00:00" fixdate="2022-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partial lookup caching in lookup join runners</summary>
      <description>Support partial lookup caching in lookup join runners, including LookupJoinRunner, LookupJoinWithCalcRunner, AsyncLookupJoinRunner and AsyncLookupJoinWithCalcRunner</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.LookupFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AsyncLookupFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="2843" opendate="2015-10-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DataSet outer joins</summary>
      <description>Outer joins are not included in the documentation yet.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="2844" opendate="2015-10-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old web interface and default to the new one</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.spinning.less</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.web.WebFrontendITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.taskmanagers.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.timeline.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.taskmanager.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.rickshaw.min.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jquery-2.1.0.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jobmanagerFrontend.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jcanvas.min.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.helpers.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.d3.min.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.d3.layout.min.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.configuration.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.bootstrap.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.analyzer.js</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.index.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.img.flink-logo.png</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.history.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..variables.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..stacked.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..spinning.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..rotated-flipped.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..path.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..mixins.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..list.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..larger.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..icons.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..fixed-width.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..core.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss..bordered-pulled.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.scss.font-awesome.scss</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.variables.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.stacked.less</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.services.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobmanager.jobmanager.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.overview.overview.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanager.taskmanager.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JobManagerInfoServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JsonFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.LogfileInfoServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.MenuServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.SetupInfoServlet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.WebInfoServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.analyze.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.blank-page.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.configuration.html</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.bootstrap.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.bootstrap.css.map</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.bootstrap.min.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.nephelefrontend.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.rickshaw.min.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.sb-admin.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.css.timeline.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.css.font-awesome.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.css.font-awesome.min.css</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.fonts.fontawesome-webfont.eot</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.fonts.fontawesome-webfont.svg</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.fonts.fontawesome-webfont.ttf</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.fonts.fontawesome-webfont.woff</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.fonts.FontAwesome.otf</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.bordered-pulled.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.core.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.fixed-width.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.font-awesome.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.icons.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.larger.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.list.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.mixins.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.path.less</file>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.font-awesome.less.rotated-flipped.less</file>
    </fixedFiles>
  </bug>
  <bug id="2846" opendate="2015-10-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit checkpoint barriers earlier, before drawing the state snapshot</summary>
      <description>State snapshot drawing and downstream barrier emitting occur in an atomic scope. Currently, the barriers are emitted after the state snapshot has been drawn.There is no reason why the barriers could not be emitted at the beginning of the atomic scope, and it would reduce the short stalls in the streaming pipeline when drawing the state snapshot of an operator checkpoint.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="2855" opendate="2015-10-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a documentation section for Gelly library methods</summary>
      <description>We should add a separate documentation section for the Gelly library methods. For each method, we should have an overview of the used algorithm, implementation details and usage information.You can find an example of what these should look like here.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAPageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.CommunityDetection.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="28551" opendate="2022-7-14 00:00:00" fixdate="2022-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store the number of bytes instead of the number of buffers in index entry for sort-shuffle</summary>
      <description>Currently, in each index entry of sort-shuffle index file, one filed is the number of buffers in the current data region. The problem is that it is hard to know the data boundary before reading the file, to solve the problem, we can store the number of bytes instead of the number of buffers in index entry. Based on this change, we can do some optimization, for example, read larger size of data than a buffer for better sequential IO like what's mentioned in FLINK-28373.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="2861" opendate="2015-10-16 00:00:00" fixdate="2015-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fields grouping on split streams fails</summary>
      <description>Using split streams works for shuffle grouping, but not for Fields grouping.The reason is that the KeySelector expects an array, and the given type is the SplitStreamType.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopologyBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="28610" opendate="2022-7-19 00:00:00" fixdate="2022-7-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution of sources</summary>
      <description>Currently speculative execution of sources is disabled. It can be enabled with the improvement done to support InputFormat sources and new sources to work correctly with speculative execution.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="28621" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register Java 8 modules in all internal object mappers</summary>
      <description>In FLINK-25588 we extended flink-shaded-jackson to also bundle the jackson extensions for handling Java 8 time / optional classes, but barely any of the internal object mappers were adjusted to register said module.We can improve the user experience by always registering this module (in cases where users can provide a mapper), and solve some incompatibilities in others (like the JsonNodeDeserializationSchema). </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.JsonJobGraphGenerationTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonTestUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SortSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankRangeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankProcessStrategySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.PartitionSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.CompactPartitions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.jackson.JacksonMapperFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.jackson.JacksonMapperFactoryTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.util.serialization.JSONKeyValueDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.testutils.KinesisFirehoseTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.examples.SinkIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.JSONKeyValueDeserializationSchemaTest.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.test.KinesisFirehoseTableITTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.test.KinesisStreamsTableApiIT.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.model.Order.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvBulkWriter.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvReaderFormat.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowCsvInputFormat.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.DataStreamCsvITCase.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowSchemaConverter.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonNodeDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.metric.FlinkMetricContainer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.FileSystemJobResultStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.history.FsJobArchivist.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.util.RestMapperUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.FileSystemJobResultStoreTestInternal.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonGeneratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.JobResultDeserializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.SerializedValueSerializerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2863" opendate="2015-10-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka producer does not fail in case of write failure</summary>
      <description>The async producer used in the Kafka connector only logs errors, but does not fail the program in case of an error.I will change it such that it fails by default on error and add a flag for the "lenient" mode that only logs failures.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="28630" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetSchemas in the HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.StringRowDataUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28631" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetFunctions in the HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionIdentifier.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28632" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetColumns/GetTableTypes/GetPrimaryKeys in the HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ResolvedCatalogBaseTable.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28633" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetTables in the HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28634" opendate="2022-7-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a simple Json (De) SerializationSchema</summary>
      <description>Add a basic schema to read/write JSON.This is so common that users shouldn't have to implement that themselves.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonNodeDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonNodeDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2871" opendate="2015-10-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OuterJoin strategy with HashTable on outer side</summary>
      <description>Outer joins are currently supported with two local execution strategies: sort-merge join hash join where the hash table is built on the inner side. Hence, this strategy is only supported for left and right outer joins.In order to support hash-tables on the outer side, we need a special hash table implementation that gives access to all records which have not been accessed during the probe phase.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.HashTableRecordWidthCombinations.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.OuterJoinITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RightOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.LeftOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondReOpenableHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstReOpenableHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondReOpenableHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstReOpenableHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.FullOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.OuterJoinNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.costs.CostEstimator.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.RightOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.LeftOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.FullOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="28710" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports dynamic filtering execution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputPriorityConflictResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalDynamicFilteringTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalDynamicFilteringDataCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputPriorityConflictResolver.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="28711" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector implements SupportsDynamicFiltering interface</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.DynamicFilteringData.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="28713" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused curator-test dependency from flink-test-utils</summary>
      <description>Remove an unused dependency that also pulls in log4j1 into user projects.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2874" opendate="2015-10-20 00:00:00" fixdate="2015-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Certain Avro generated getters/setters not recognized</summary>
      <description>For Avro schemas where value null is not allowed, the field is unboxed e.g. int but the getter/setter methods provide the boxed Integer as interface:{ "fields": [ { "type": "double", "name": "time" }, }This results in Java private double time; public java.lang.Double getTime() { return time; } public void setTime(java.lang.Double value) { this.time = value; }There is also a problem when there is an underscore in the Avro schema, e.g.: { "default": null, "type": [ "null", "long" ], "name": "conn_id" }, This results in Java:private java.lang.Long conn_id; public java.lang.Long getConnId() { return conn_id; } public void setConnId(java.lang.Long value) { this.conn_id = value; }</description>
      <version>0.9.1,0.9.0,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.PojoTypeInfoTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2876" opendate="2015-10-20 00:00:00" fixdate="2015-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minutiae</summary>
      <description>A collection of small documentation and grammar updates.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.PartialOrderPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationIntermediateTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClient.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.Tuple3UnwrappingIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.EmptyFieldsCountAccumulator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AverageAccumulator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.Accumulator.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="2883" opendate="2015-10-20 00:00:00" fixdate="2015-2-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation to forbid key-modifying ReduceFunction</summary>
      <description>If one uses a combinable reduce operation which also changes the key value of the underlying data element, then the results of the reduce operation can become wrong. The reason is that after the combine phase, another reduce operator is executed which will then reduce the elements based on the new key values. This might be not so surprising if one explicitly defined ones GroupReduceOperation as combinable. However, the ReduceFunction conceals the fact that a combiner is used implicitly. Furthermore, the API does not prevent the user from changing the key fields which could solve the problem.The following example program illustrates the problemval env = ExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(1)val input = env.fromElements((1,2), (1,3), (2,3), (3,3), (3,4))val result = input.groupBy(0).reduce{ (left, right) =&gt; (left._1 + right._1, left._2 + right._2)}result.output(new PrintingOutputFormat[Int]())env.execute()The expected output is (2, 5)(2, 3)(6, 7)However, the actual output is(4, 8)(6, 7)I think that the underlying problem is that associativity and commutativity is not sufficient for a combinable reduce operation. Additionally we also need to make sure that the key stays the same.</description>
      <version>0.10.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="28851" opendate="2022-8-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetTypeInfo in the HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.OperationInfo.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="2887" opendate="2015-10-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sendMessageToAllNeighbors ignores the EdgeDirection</summary>
      <description>In vertex-centric iterations, while getEdges() correctly gathers all edges when EdgeDirection is set to ALL, sendMessageToAllNeighbors only sends messages to out-neighbors, no matter what edge direction has been configured.</description>
      <version>0.9.0,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConfigurationITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessagingFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="2891" opendate="2015-10-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Key for Keyed State is not set upon Window Evaluation</summary>
      <description>In both the aligned and the general-purpose windows the key for the keyed operator state is not set when evaluating the windows. This silently leads to incorrect results.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.NonKeyedWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2895" opendate="2015-10-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate immutable object creation</summary>
      <description>Inverse of FLINK-2724. When object reuse is disabled a few operators are creating and passing objects locally. In the case of immutable objects these will be discarded by the TypeSerializer when deserializing.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllReduceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="28951" opendate="2022-8-14 00:00:00" fixdate="2022-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Header in janino generated java files can merge with line numbers</summary>
      <description>Since Line numbers are generated only for debug output it should not be a big issue.From the other side currently this behavior leads to not compiled code.The suggestion is usage of one-line comments for header to prevent this</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2898" opendate="2015-10-22 00:00:00" fixdate="2015-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invert Travis CI build order</summary>
      <description>The Travis CI builds generally perform fastest to slowest. When running additional, concurrent Travis CI builds it would be preferable to have the slowest tasks begin first.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2900" opendate="2015-10-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove Record-API dependencies from Hadoop Compat module</summary>
      <description>The Hadoop Compat module includes wrappers for Hadoop Input/OutputFormat for the Record API classes and a corresponding test.These need to be removed before removing the Record API.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.record.HadoopRecordInputOutputITCase.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSource.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSink.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCountWithOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCount.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapperConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapper.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableComparableWrapper.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.FlinkTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultHadoopTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultFlinkTypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2902" opendate="2015-10-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface sort tasks newest first</summary>
      <description>Sort completed jobs in reverse order so the most recently finished are at the top of the list. With a long list of completed jobs the user must scroll down to view recently completed jobs.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.running-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.completed-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.overview.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.running-jobs.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.completed-jobs.jade</file>
    </fixedFiles>
  </bug>
  <bug id="2903" opendate="2015-10-22 00:00:00" fixdate="2015-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface numeric localization</summary>
      <description>It would be nice to localize numbers in the web interface as 10+ digits is difficult to parse without separators.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug id="29030" opendate="2022-8-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print a log message if a Pojo/Tuple contains a generic type</summary>
      <description>Users are encouraged to use POJO types, that will be serialized by the PojoSerializer which supports schema evolution.If a user does not use a POJO we print an info message, linking to the docs and citing potential performance issues.However, no such message is printed if a POJO contains a generic type.As a result there may be users out there believing to have optimal performance and support for schema evolution since, after all, they are able to use the POJO serializer, when this may not be the case.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2904" opendate="2015-10-22 00:00:00" fixdate="2015-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface truncated task counts</summary>
      <description>Task counts have only three digits visible as the color square needs to dynamically expand.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
    </fixedFiles>
  </bug>
  <bug id="29260" opendate="2022-9-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto-wipe exclusion list after updating reference version</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.japicmp.configuration.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29262" opendate="2022-9-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.upgrading.md</file>
      <file type="M">docs.content.zh.docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug id="29263" opendate="2022-9-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Elasticsearch connector from apache/flink repo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.NoOpFailureHandler.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.cache.docker.images.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.test.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExternalContextFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.test.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExternalContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.test.java.org.apache.flink.streaming.tests.Elasticsearch7SinkE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.main.java.org.apache.flink.streaming.tests.UpdateRequest7Factory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.main.java.org.apache.flink.streaming.tests.Elasticsearch7Client.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.test.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExternalContextFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.test.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExternalContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.test.java.org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.main.java.org.apache.flink.streaming.tests.UpdateRequest6Factory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6Client.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.UpdateRequestFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.QueryParams.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.KeyValue.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchTestEmitter.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchSinkExternalContextFactoryBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchSinkExternalContextBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchDataReader.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.src.main.java.org.apache.flink.streaming.tests.ElasticsearchClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7TestClient.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.archunit-violations.e1f30f33-c61c-4707-8c78-a3a80479564e</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.archunit-violations.1af7baaa-05dc-452a-9de7-653c8b3b324f</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6TestClient.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6ConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.archunit-violations.db3972e4-f3a3-45b2-9643-27cba0cef09d</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.archunit-violations.25e52d29-fa7e-42fa-a571-b5c76235df52</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.ElasticsearchResource.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.TestRequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.TestContext.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.TestContext.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestClientBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.ElasticsearchUtil.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler.java</file>
      <file type="M">docs.data.sql.connectors.yml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.archunit-violations.dd583797-83e1-414c-a38d-330773978813</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.archunit-violations.de342dd1-c974-42c9-8f64-ef182ba8c56d</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.BulkProcessorBuilderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.BulkProcessorConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.BulkRequestConsumerFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.FlushBackoffType.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.MapElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.NetworkClientConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.AbstractTimeIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchSinkBuilderSupplier.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchValidationUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.IndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.KeyExtractor.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.LogicalTypeWithIndex.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.StaticIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.BufferingNoOpRequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.AbstractTimeIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchValidationUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractor.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RequestFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.StaticIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.IgnoringFailureHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2936" opendate="2015-10-28 00:00:00" fixdate="2015-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when using EventTimeSourceFunction in non-EventTime program</summary>
      <description>Using an EventTimeSourceFunction in a DataStream programs that does not operate with TimeCharacteristic.EventTime leads to a ClassCastException when the first Watermark is emitted:Caused by: java.lang.ClassCastException: org.apache.flink.streaming.api.watermark.Watermark cannot be cast to org.apache.flink.streaming.runtime.streamrecord.StreamRecordThis exception is not very helpful for users that simply for got to set the correct TimeCharacteristic and should be improved.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.timestamp.TimestampITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="2938" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming docs not in sync with latest state changes</summary>
      <description>The section about "Working with State" does not reflect the recent changes to access the key/value in the RuntimeContext. The code examples need to be updated.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="29400" opendate="2022-9-22 00:00:00" fixdate="2022-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default Value of env.log.max in documentation is incorrect</summary>
      <description>The default value of env.log.max is 10 as per the code in master (https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/bin/config.sh#L137).However the Flink Documentation says the default value is 5 (https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#env-log-max) which is incorrect </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.environment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2942" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dangling operators in web UI&amp;#39;s program visualization (non-deterministic)</summary>
      <description>When visualizing a program with three MapPartition operators that branch off from an OuterJoin operator, two of the three MapPartition operators are not connected to the OuterJoin operator and appear to have no input.The problem is present in FireFox as well as in Chrome. I'll attach a screenshot.The problem and be reproduced by executing the "Cascading for the impatient" TFIDF example program using the Cascading Flink Connector.Update: It appears that the problem is non-deterministic. I ran the same job again (same setup) and the previously missing connections were visualized. However, the UI showed only one input for a binary operator (OuterJoin). Running the job a third time resulted in a graph layout which was again different from both runs before. However, two of the MapPartition operators had not inputs just as in the first run.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.dir.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="29420" opendate="2022-9-26 00:00:00" fixdate="2022-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Zookeeper to 3.7</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29421" opendate="2022-9-26 00:00:00" fixdate="2022-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support python 3.10</summary>
      <description>The apache-flink package fails to install on Python 3.10 due to inability to compile numpy numpy/core/src/multiarray/scalartypes.c.src:3242:12: error: too few arguments to function ‘_Py_HashDouble’ 3242 | return _Py_HashDouble(npy_half_to_double(((PyHalfScalarObject *)obj)-&gt;obval)); | ^~~~~~~~~~~~~~ In file included from /home/sirianni/.asdf/installs/python/3.10.6/include/python3.10/Python.h:77, from numpy/core/src/multiarray/scalartypes.c.src:3: /home/sirianni/.asdf/installs/python/3.10.6/include/python3.10/pyhash.h:10:23: note: declared here 10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);Numpy issue https://github.com/numpy/numpy/issues/19033Mailing list thread</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.state.BeamMapStateHandler.java</file>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">tools.releasing.create.binary.release.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.state.BeamStateRequestHandler.java</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.tutorial.md</file>
      <file type="M">docs.content.zh.docs.dev.python.installation.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.flinkDev.building.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.dev.python.datastream.tutorial.md</file>
      <file type="M">docs.content.docs.dev.python.installation.md</file>
      <file type="M">docs.content.docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.flinkDev.building.md</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-python.dev.build-wheels.sh</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
      <file type="M">flink-python.lib.cloudpickle-2.1.0-src.zip</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.pyflink.datastream.formats.tests.test.csv.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.worker.pool.service.py</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyproject.toml</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.runners.fnexecution.state.GrpcStateService.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.sdk.fn.server.ServerFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p43p2.io.grpc.internal.SharedResourceHolder.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.embedded.EmbeddedPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.ProtoUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.state.BeamBagStateHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2943" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Confusing Bytes/Records "read" and "write" labels in WebUI job view</summary>
      <description>The job detail view of the WebUI shows the amount of data and records received from and sent out by each individual operator. This information is very valuable, if correctly interpreted. However, the table headings for the corresponding columns ("Bytes read", "Records read", "Bytes written", and "Records written") are confusing in my opinion because they do not indicate that only incoming and outgoing data is tracked and disk IO is not considered at all. For example, the UI shows no "Bytes/Records read" and only "Bytes/Records written" for DataSources which I find counter-intuitive.I propose to rename these labels to either "X received" and "X sent" or "X incoming" and "X outgoing" to make clear that this information only affects incoming and outgoing data.Opinions?</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug id="2954" opendate="2015-11-2 00:00:00" fixdate="2015-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not able to pass custom environment variables in cluster to processes that spawning TaskManager</summary>
      <description>There are programs that rely on custom environment variables. In hadoop mapreduce job we can use -Dmapreduce.map.env and - Dmapreduce.reduce.env to do pass them. Similarly in Sparkwe can use --conf 'spark.executor.XXX=value for XXX'. There is no such feature yet in Flink.This has given Flink a serious disadvantage when customers need such feature.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="2957" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make cancel button look less like a label</summary>
      <description>The cancel button looks like a label. Its visual appearance could be improved.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.jade</file>
    </fixedFiles>
  </bug>
  <bug id="29570" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump org.jsoup:jsoup to v1.15.3</summary>
      <description>Bump JSoup to avoid getting flagged for CVE-2022-36033 (which doesn't affect Flink directly)</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29640" opendate="2022-10-14 00:00:00" fixdate="2022-1-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enhance the function configured by execution.shutdown-on-attached-exit by heartbeat between client and dispatcher</summary>
      <description>If the param execution.shutdown-on-attached-exit is set to true, perform a best-effort cluster shutdown when the CLI is terminated abruptly.But when the client is killed by 'kill -9', the cluster will not shutdown for that the signal can not be sent. We can enhance the behavior by building the heart beat between the client and the job. Once the job can not received any heartbeat from the client, the job cancel itself. </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StreamContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.MiniClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.PipelineExecutorUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterClientJobClientAdapter.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ClientOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.client.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2967" opendate="2015-11-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TM address detection might not always detect the right interface on slow networks / overloaded JMs</summary>
      <description>I'm talking to a user which is facing the following issue:Some of the TaskManagers select the wrong IP address out of the available network interfaces.The first address we try to connect to is the one returned by InetAddress.getLocalHost(). This address is the right IP address to use, but the JobManager is not able to respond within the timeout (50ms) to that connection request.So the TM tries the next address, which is not publicly reachable. However, the TM can connect to the JM from there. Netty will later fail to connect to the TM from the other TMs.There are two solutions for this issue: Allow users to configure a higher timeout for the first address detection strategy. In most cases, the address returned by InetAddress.getLocalHost() is correct. By setting a high timeout, users with slow networks / overloaded JMs can make sure the TM picks this address add an Akka message which we send from the TM to the JM, and the JM tries to connect to the TM. If that succeeds, we know that the TM is reachable from the outside.The problem is that we have to start a separate actor system on the TaskManager first. We have to do this because might use a wrong ip address for the TM (so we might end up starting actor systems until we found an externally reachable ip)I'm first going to implement the first approach. If that solution works well for my user, I'll contribute this to 0.10 / 1.0.If not, I'll implement the second approach.</description>
      <version>0.9,0.10.0,1.0.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="29862" opendate="2022-11-3 00:00:00" fixdate="2022-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to flink-shaded 16.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="2987" opendate="2015-11-9 00:00:00" fixdate="2015-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink 0.10 fails to start on YARN 2.6.0</summary>
      <description>While testing Flink for the release, I noticed that it does not start on YARN due to classloading issues.The error message is the followingrobert@hn0-apache:~/flink-0.10.0$ ./bin/yarn-session.sh -n 2Exception in thread "main" java.lang.RuntimeException: Could not instantiate type 'org.apache.flink.yarn.FlinkYarnClient' Most likely the constructor (or a member variable initialization) threw an exception: com/sun/jersey/api/client/config/ClientConfig at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:152) at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:118) at org.apache.flink.client.FlinkYarnSessionCli.getFlinkYarnClient(FlinkYarnSessionCli.java:262) at org.apache.flink.client.FlinkYarnSessionCli.createFlinkYarnClient(FlinkYarnSessionCli.java:107) at org.apache.flink.client.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:400) at org.apache.flink.client.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:351)Caused by: java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:45) at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:163) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) at org.apache.flink.yarn.FlinkYarnClientBase.&lt;init&gt;(FlinkYarnClientBase.java:157) at org.apache.flink.yarn.FlinkYarnClient.&lt;init&gt;(FlinkYarnClient.java:23) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:379) at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:139) ... 5 moreCaused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 16 moreThe issue occurs with the following versions: flink-0.10.0-bin-hadoop26-scala_2.11.tgz flink-0.10.0-bin-hadoop26-scala_2.10.tgz flink-0.10.0-bin-hadoop27-scala_2.10.tgzIt works for: flink-0.10.0-bin-hadoop24-scala_2.10.tgz flink-0.9.1-bin-hadoop26.tgzInterestingly, the issue only occurs, when HADOOP_CONF_DIR is set. Otherwise, the YARN client is starting until its unable to connect to the RM.The missing class is in the jersey-client dependency, which we exclude from Hadoop, in flink-shaded-hadoop2.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2991" opendate="2015-11-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Window Operators to Allow Efficient Fold Operation</summary>
      <description>Right now, a window fold is implemented as a WindowFunction that gets all the elements as input. No pre-aggregation is performed. The window operator should be extended to also allow the fold to also be pre-aggregated.This requires changing the signature of the WindowBuffer so that it can emit a type other than the input type.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.state.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.FoldWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceWindowFunctionWithWindow.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateBackend.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
    </fixedFiles>
  </bug>
  <bug id="2994" opendate="2015-11-10 00:00:00" fixdate="2015-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client sysout logging does not report exceptions</summary>
      <description>For failures on the JobManager, the client simply prints that the job switched to RESTARTING and does not print the exception.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="3000" opendate="2015-11-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ShutdownHook to YARN CLI to prevent lingering sessions</summary>
      <description>Submitting a job viabin/flink run -m yarn-cluster ...and terminating the client can lead to lingering YARN sessions allocating cluster resources.This was reported by a user.1) One starts a flink job in the yarn mode2) He sees that containers are not allocated since cluster is busy3) Presses Ctrl+C4) An “empty” flink session remains in the cluster although the Flink didn’t print that “you can track your application on the X URL”</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="3002" opendate="2015-11-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an EitherType to the Java API</summary>
      <description>Either types are recurring patterns and should be serialized efficiently, so it makes sense to add them to the core Java API.Since Java does not have such a type as of Java 8, we would need to add our own version.The Scala API handles the Scala Either Type already efficiently. I would not use the Scala Either Type in the Java API, since we are trying to get the flink-java project "Scala free" for people that don't use Scala and o not want to worry about Scala version matches and mismatches.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="3011" opendate="2015-11-13 00:00:00" fixdate="2015-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot cancel failing/restarting streaming job from the command line</summary>
      <description>I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:The exception I get:13:58:11,240 INFO org.apache.flink.runtime.jobmanager.JobManager - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING.13:58:25,234 INFO org.apache.flink.runtime.jobmanager.JobManager - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5.13:58:25,561 WARN akka.remote.ReliableDeliverySupervisor - Association with remote system &amp;#91;akka.tcp://flink@127.0.0.1:42012&amp;#93; has failed, address is now gated for &amp;#91;5000&amp;#93; ms. Reason is: &amp;#91;Disassociated&amp;#93;.</description>
      <version>0.10.0,1.0.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="30170" opendate="2022-11-23 00:00:00" fixdate="2022-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Parquet version to 1.12.3</summary>
      <description>Flink currently uses version 1.12.2, which should be upgraded to 1.12.3</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="30171" opendate="2022-11-23 00:00:00" fixdate="2022-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump socket.io-parser from 4.0.4 to 4.0.5</summary>
      <description>Dependabot created PR: https://github.com/apache/flink/pull/21279</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="30172" opendate="2022-11-23 00:00:00" fixdate="2022-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump loader-utils from 2.0.2 to 2.0.4</summary>
      <description>Dependabot created PR: https://github.com/apache/flink/pull/21327</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="30174" opendate="2022-11-23 00:00:00" fixdate="2022-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump engine.io from 6.2.0 to 6.2.1</summary>
      <description>Bump engine.io from 6.2.0 to 6.2.1 to avoid false flag for CVE-2022-41940</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="30175" opendate="2022-11-23 00:00:00" fixdate="2022-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump snakeyaml from 1.31 to 1.33</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0,pulsar-4.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="30177" opendate="2022-11-23 00:00:00" fixdate="2022-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update pgjdbc to fix CVE-2022-41946</summary>
      <description>There is CVE-2022-41946 fixed in 42.5.1, 42.4.3 42.3.8, 42.2.27.jre7.Also mentioned in released noteshttps://jdbc.postgresql.org/changelogs/2022-11-23-42.5.1-release/</description>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3019" opendate="2015-11-16 00:00:00" fixdate="2015-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI does not list running/restarting jobs</summary>
      <description>While reproducing FLINK-3011 locally, I run bin/flink list and get no running/scheduled jobs although the job is displayed on the web interface (with job status RESTARTING).A job that fails repeatedly to be re-deployed (like FLINK-3011), is not shown and can not be easily cancelled if there is no log access/running web interface to gather the job ID.Furthermore, the log shows this output for me:Successfully retrieved list of jobsNULNULNULNULNULNUL...NUL (NUL denotes the NUL character) I would display all jobs which have been returned to the client with their status. This will then have the same behaviour as the web interface.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="30191" opendate="2022-11-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update py4j from 0.10.9.3 to 0.10.9.7</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.lib.py4j-0.10.9.3-src.zip</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3020" opendate="2015-11-16 00:00:00" fixdate="2015-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local streaming execution: set number of task manager slots to the maximum parallelism</summary>
      <description>Quite an inconvenience is the local execution configuration behavior. It sets the number of task slots of the mini cluster to the default parallelism. This causes problem if you use setParallelism(parallelism) on an operator and set a parallelism larger than the default parallelism.Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Task to schedule: &lt; Attempt #0 (Flat Map (9/100)) @ (unassigned) - [SCHEDULED] &gt; with groupID &lt; fa7240ee1fed08bd7e6278899db3e838 &gt; in sharing group &lt; SlotSharingGroup [f3d578e9819be9c39ceee86cf5eb8c08, 8fa330746efa1d034558146e4604d0b4, fa7240ee1fed08bd7e6278899db3e838] &gt;. Resources available to scheduler: Number of instances=1, total number of slots=8, available slots=0 at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:256) at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:131) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:298) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:458) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:322) at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:686) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply$mcV$sp(JobManager.scala:982) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply(JobManager.scala:962) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply(JobManager.scala:962) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ... 2 moreI propose to change this behavior to setting the number of task slots to the maximum parallelism present in the user program.What do you think?</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="3021" opendate="2015-11-17 00:00:00" fixdate="2015-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job submission times out due to classloading issue on JobManager</summary>
      <description>A user reported the following issue when submitting a very simple job using the DataStream API:Caused by: org.apache.flink.runtime.client.JobExecutionException: Communication with JobManager failed: Job submission to the JobManager timed out. at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:141) at org.apache.flink.client.program.Client.runBlocking(Client.java:368) ... 13 moreCaused by: org.apache.flink.runtime.client.JobClientActorSubmissionTimeoutException: Job submission to the JobManager timed out. at org.apache.flink.runtime.client.JobClientActor.handleMessage(JobClientActor.java:255) at org.apache.flink.runtime.akka.FlinkUntypedActor.handleLeaderSessionID(FlinkUntypedActor.java:88) at org.apache.flink.runtime.akka.FlinkUntypedActor.onReceive(FlinkUntypedActor.java:68) at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:167) at akka.actor.Actor$class.aroundReceive(Actor.scala:465)The problem is that akka can not deserialize the job submit message on the JobManager. From the logs, the issue becomes apparent:22:14:12,964 DEBUG akka.serialization.Serialization(akka://flink) - Using serializer[akka.serialization.JavaSerializer] for message [akka.actor.ActorIdentity]22:14:12,995 DEBUG akka.serialization.Serialization(akka://flink) - Using serializer[akka.serialization.JavaSerializer] for message [java.lang.Integer]22:14:13,007 DEBUG org.apache.flink.runtime.blob.BlobServerConnection - Received PUT request for content addressable BLOB22:14:13,134 ERROR akka.remote.EndpointWriter - AssociationError [akka.tcp://flink@127.0.0.1:6123] &lt;- [akka.tcp://flink@127.0.0.1:58424]: Error [com.dataartisans.SimpleEntity] [java.lang.ClassNotFoundException: com.dataartisans.SimpleEntity at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:625) at akka.util.ClassLoaderObjectInputStream.resolveClass(ClassLoaderObjectInputStream.scala:19) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517) at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1483) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1333) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at java.util.HashMap.readObject(HashMap.java:1180) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) at akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136) at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104) at scala.util.Try$.apply(Try.scala:161) at akka.serialization.Serialization.deserialize(Serialization.scala:98) at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23) at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58) at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58) at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76) at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)]22:14:13,137 WARN akka.remote.ReliableDeliverySupervisor - Association with remote system [akka.tcp://flink@127.0.0.1:58424] has failed, address is now gated for [5000] ms. Reason is: [com.dataartisans.SimpleEntity].22:14:13,142 DEBUG akka.remote.EndpointWriter - Disassociated [akka.tcp://flink@127.0.0.1:6123] &lt;- [akka.tcp://flink@127.0.0.1:58424]I suspect the issue is that the job is using an AvroInputFormat which holds a reference to the POJO.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3025" opendate="2015-11-17 00:00:00" fixdate="2015-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Kafka consumer may get stuck due to Kafka/Zookeeper client bug</summary>
      <description>In some cases the Flink kafka consumer might fail due to https://issues.apache.org/jira/browse/KAFKA-824.Subsequently it can happen that the sources gets stuck in a Zookeeper client call (zookeeper bug).A proposed fix would be bumping the zookeeper dependency to a version that includes the fix for this bug.</description>
      <version>0.10.0,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3032" opendate="2015-11-17 00:00:00" fixdate="2015-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink does not start on Hadoop 2.7.1 (HDP), due to class conflict</summary>
      <description>Steps to reproduce: Build flink mvn clean install -DskipTests -Dhadoop.version=2.7.1.2.3.2.0-2950 -Pvendor-repos Start it on a HDP 2.3.2 Hadoop as a yarn-session Watch it fail with6:18:56,459 INFO org.apache.flink.runtime.filecache.FileCache - User file cache uses directory /hadoop/yarn/local/usercache/flink/appcache/application_1447687546708_0005/flink-dist-cache-f4710796-598c-4778-992c-5df000faffae16:18:56,561 ERROR akka.actor.OneForOneStrategy - exception during creationakka.actor.ActorInitializationException: exception during creation at akka.actor.ActorInitializationException$.apply(Actor.scala:164) at akka.actor.ActorCell.create(ActorCell.scala:596) at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:456) at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478) at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:279) at akka.dispatch.Mailbox.run(Mailbox.scala:220) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at akka.util.Reflect$.instantiate(Reflect.scala:66) at akka.actor.ArgsReflectConstructor.produce(Props.scala:352) at akka.actor.Props.newActor(Props.scala:252) at akka.actor.ActorCell.newActor(ActorCell.scala:552) at akka.actor.ActorCell.create(ActorCell.scala:578) ... 10 moreCaused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z at com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:458) at com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:379) at org.apache.flink.runtime.taskmanager.TaskManager.&lt;init&gt;(TaskManager.scala:153) at org.apache.flink.yarn.YarnTaskManager.&lt;init&gt;(YarnTaskManager.scala:32) ... 19 more16:18:56,564 ERROR org.apache.flink.runtime.taskmanager.TaskManager - Actor akka://flink/user/taskmanager#-1189186354 terminated, stopping process...</description>
      <version>0.10.0,1.0.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="3040" opendate="2015-11-18 00:00:00" fixdate="2015-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs describing how to configure State Backends</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug id="30421" opendate="2022-12-14 00:00:00" fixdate="2022-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TGT renewal to hadoop module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="30422" opendate="2022-12-14 00:00:00" fixdate="2022-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize token framework provider API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.resources.META-INF.services.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenProvider</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.TestHadoopDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdaterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenConverterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.ExceptionThrowingHadoopDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.resources.META-INF.services.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenProvider</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.NoOpDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3050" opendate="2015-11-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add custom Exception type to suppress job restarts</summary>
      <description>In case of failures and configured execution retries, the job will be be restarted even in cases when the failure is not recoverable.We can add a custom Exception type like UnrecoverableFailure in order to suppress restarts in certain cases. The execution graph restart logic can check the failure type on recovery and skip the restarting.This Exception can be used both by the system and the user.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="3051" opendate="2015-11-19 00:00:00" fixdate="2015-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define a maximum number of concurrent inflight checkpoints</summary>
      <description>The checkpoint coordinator should define an option to limit the maximum number of current inflight checkpoints, as well as the checkpoint timeouts.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3052" opendate="2015-11-20 00:00:00" fixdate="2015-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer does not push properties out of bulk iterations</summary>
      <description>Flink's optimizer should be able to reuse interesting properties from outside the loop. In order to do that it is sometimes necessary to append a NoOp node to the step function which recomputes the required properties.This is currently not working for BulkIterations, because the plans with the appended NoOp nodes are not added to the overall list of candidates.This not only leads to sub-optimal plan selection but sometimes to the rejection of valid jobs. The following job, for example, will be falsely rejected by flink.ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;Tuple1&lt;Long&gt;&gt; input1 = env.generateSequence(1, 10).map(new MapFunction&lt;Long, Tuple1&lt;Long&gt;&gt;() { @Override public Tuple1&lt;Long&gt; map(Long value) throws Exception { return new Tuple1&lt;&gt;(value); } }); DataSet&lt;Tuple1&lt;Long&gt;&gt; input2 = env.generateSequence(1, 10).map(new MapFunction&lt;Long, Tuple1&lt;Long&gt;&gt;() { @Override public Tuple1&lt;Long&gt; map(Long value) throws Exception { return new Tuple1&lt;&gt;(value); } }); DataSet&lt;Tuple1&lt;Long&gt;&gt; distinctInput = input1.distinct(); IterativeDataSet&lt;Tuple1&lt;Long&gt;&gt; iteration = distinctInput.iterate(10); DataSet&lt;Tuple1&lt;Long&gt;&gt; iterationStep = iteration .coGroup(input2) .where(0) .equalTo(0) .with(new CoGroupFunction&lt;Tuple1&lt;Long&gt;, Tuple1&lt;Long&gt;, Tuple1&lt;Long&gt;&gt;() { @Override public void coGroup( Iterable&lt;Tuple1&lt;Long&gt;&gt; first, Iterable&lt;Tuple1&lt;Long&gt;&gt; second, Collector&lt;Tuple1&lt;Long&gt;&gt; out) throws Exception { Iterator&lt;Tuple1&lt;Long&gt;&gt; it = first.iterator(); if (it.hasNext()) { out.collect(it.next()); } } }); DataSet&lt;Tuple1&lt;Long&gt;&gt; iterationResult = iteration.closeWith(iterationStep); iterationResult.output(new DiscardingOutputFormat&lt;Tuple1&lt;Long&gt;&gt;());</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.IterationsCompilerTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.util.NoOpUnaryUdfOp.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.WorksetIterationNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.UnaryOperatorNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.BulkIterationNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="3057" opendate="2015-11-22 00:00:00" fixdate="2015-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Provide a way to pass information back to the plan process</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Iterator.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Connection.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.StreamPrinter.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Sender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Receiver.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonMapPartition.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCombineIdentity.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCoGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="3063" opendate="2015-11-23 00:00:00" fixdate="2015-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Remove combiner</summary>
      <description>The current combiner implementation in the PythonAPI is quite a mess. It adds a lot of unreadable clutter, is inefficient at times, and can straight up break in some edge cases.I will revisit this feature after FLINK-2501 is resolved. Several changes for that issue will make the reimplementation easier.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.OperationInfo.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.ReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.GroupReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCombineIdentity.java</file>
    </fixedFiles>
  </bug>
  <bug id="30670" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Done">
    <buginformation>
      <summary>Ignore broadcast bytes when computing parallelism and input infos for adaptive batch scheduler</summary>
      <description>Currently, we include the broadcast bytes in the "jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task" when calculating the parallelism (see PR17952 for details), and set a cap ratio(0.5) for the broadcast bytes. Considering that the broadcast bytes are generally relatively small, we can ignore the broadcast bytes to simplify the logic.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="3071" opendate="2015-11-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add asynchronous materialization thread</summary>
      <description>Add a thread to the stream task that handles background materialization and acknowledges the checkpoint ones the materialization is complete.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimerException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AsynchronousStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="3083" opendate="2015-11-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add docs how to configure streaming fault tolerance</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="3087" opendate="2015-11-27 00:00:00" fixdate="2015-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API do not support multi count in aggregation.</summary>
      <description>Multi count in aggregation is not supported, for example:table.select("a.count", "b.count")It's valid in grammar, besides, a.count and b.count may have different values actually if NULL value handling is enabled.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.ExpandAggregations.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3115" opendate="2015-12-4 00:00:00" fixdate="2015-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch connector to 2.X</summary>
      <description>The Elasticsearch connector is not up to date anymore. In version 2.X the API changed. The code needs to be adapted. Probably it makes sense to have a new class ElasticsearchSink2.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="3181" opendate="2015-12-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The vertex-centric SSSP example and library method send unnecessary messages during the first superstep</summary>
      <description>The result is correct, but the current implementation has unnecessary message overhead in the first superstep. The Messaging function should only produce messages from the source during the first superstep. This can be fixed with a simple check of the vertex value.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.SingleSourceShortestPaths.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3303" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Move all non-batch specific classes in flink-java to flink-core</summary>
      <description>Currently, flink-java has a lot of classes that are also needed by the streaming api and that are useful to the classes in flink-core.In particular, certain improvements to the state API are blocked by the fact that certain classes are not in flink-core.I suggest to move classes from flink-java to flink-core after the following pattern: flink-core will contain all classes that are common across the batch and streaming API. flink-java will contain all batch API specific classes (we may eventually even think about renaming it to flink-batch-java.Because flink-java references flink-core, this will not be a breaking change.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.DataOutputEncoder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple9.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple8.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple7.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple6.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple5.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple4.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple3.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple25.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple24.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple23.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple22.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple21.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple20.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple2.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple19.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple18.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple17.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple16.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple15.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple14.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple13.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple12.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple11.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple10.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple1.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple0.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple9Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple8Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple7Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple6Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple5Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple4Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple3Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple2Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple25Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple24Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple23Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple22Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple21Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple20Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple1Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple19Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple18Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple17Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple16Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple15Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple14Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple13Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple12Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple11Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple10Builder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.builder.Tuple0Builder.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.JoinOperatorTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupCombineITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.CoGroupOperatorTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.keys.KeySelectorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.unfinishedKeyPairOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.UnfinishedCoGroupOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.CoGroupDataSet.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaAggregateOperator.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorInputFormatsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeInformationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.WritableTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.ValueTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.TypeInfoParserTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.TupleTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializerUUIDTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableID.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableComparatorUUIDTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueSerializerUUIDTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueID.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueComparatorUUIDTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.tuple.base.TuplePairComparatorTestBase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.tuple.base.TupleComparatorTestBase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerTestInstance.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT1Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorISD3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorISD2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorISD1Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDXC2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDX1Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDC3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TestDataOutputSerializer.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.SubclassFromInterfaceSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.StringArrayWritable.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSubclassSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSubclassComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoContainingTuple.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.MultidimensionalArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.SerializersTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoWithCustomSerializersTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericTypeComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoClearedBufferTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericPairComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.EitherSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AvroSerializerEmptyArrayTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AvroGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AvroGenericTypeComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AvroGenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.PojoTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.MissingTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.GenericTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.EnumTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.EitherTypeInfoTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.CompositeTypeTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.tuple.Tuple2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.SelectorFunctionKeysTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.NamesTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.ExpressionKeysTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.WritableTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ValueTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeInfoParser.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.WritableComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.ValueSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.ValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.Tuple0Serializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimeSerializerFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimePairComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimeComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.KryoUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericTypeComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.EitherSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.KeySelector.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SemanticPropUtil.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.SplitDataProperties.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.AggregateOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupRawOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIteration.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIterationResultSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Grouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.join.JoinOperatorSetsBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingGroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingSortedGroupCombineOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingSortedReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UdfOperatorUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.AvroTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.Either.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.EitherTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.EnumTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.InputTypeConfigurable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.MissingTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoField.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ResultTypeQueryable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.AvroSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.DataInputDecoder.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.DataInputViewStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="3478" opendate="2016-2-23 00:00:00" fixdate="2016-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink serves arbitary files through the web interface</summary>
      <description>Flink serves arbitrary files through the web server of the 8081 port, e.g. ../../../../../../../../../../etc/passwd.The requested path needs to be validated before it is served.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.StaticFileServerHandler.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
