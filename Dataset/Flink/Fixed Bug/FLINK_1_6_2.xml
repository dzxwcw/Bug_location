<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10465" opendate="2018-9-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: runit supervised sshd is stopped on tear down</summary>
      <description>When tearing down the DB, we tear down all services supervised by runit. However when running the tests in Docker, sshd is under supervision by runit. When sshd is stopped, the tests cannot be continued because the control node cannot interact with the DB nodes anymore.How to reproduceRun command below in control-node container:./docker/run-tests.sh 1 [...]/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgzExpected behaviorsshd should never be stopped</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  <bug id="1072" opendate="2014-8-27 00:00:00" fixdate="2014-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Travis testing to more Hadoop versions.</summary>
      <description>http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Extend-Travis-CI-build-matrix-td1516.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="10748" opendate="2018-11-1 00:00:00" fixdate="2018-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jobmanager in HA setup, redirects (307) don&amp;#39;t work when behind a load balancer</summary>
      <description>In a HA Jobmanager setup, connecting to a follower results in a redirect (HTTP/1.1 307 Temporary Redirect) to the leader. However, it redirects to an ip address (why is it not the hostname?) which is in another network and not reachable. I have configured hostnames, not ip addresses.Wouldn't it be better to proxy the request to another jobmanager instead of using a redirect?  </description>
      <version>1.3.3,1.4.2,1.5.5,1.6.2</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobTerminationHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.HandlerRedirectUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingDispatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RedirectHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractAggregatingMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.RedirectHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.akka.AkkaJobManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.SessionDispatcherFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.StandaloneDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractJobVertexHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskAttemptHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobIdsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10763" opendate="2018-11-2 00:00:00" fixdate="2018-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interval join produces wrong result type in Scala API</summary>
      <description>When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.In the union method of DataStream, the type is first checked for equality.Here is an example:object Test { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val orderA: DataStream[Order] = env.fromCollection(Seq( Order(1L, "beer", 3), Order(1L, "diaper", 4), Order(3L, "rubber", 2))) val orderB: DataStream[Order] = env.fromCollection(Seq( new Order(2L, "pen", 3), new Order(2L, "rubber", 3), new Order(4L, "beer", 1))) val orderC: DataStream[Order] = orderA.keyBy(_.user) .intervalJoin(orderB.keyBy(_.user)) .between(Time.seconds(0), Time.seconds(0)) .process(new ProcessJoinFunction[Order, Order, Order] { override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = { out.collect(left) }}) println("C: " + orderC.dataType.toString) println("B: " + orderB.dataType.toString) orderC.union(orderB).print() env.execute() } case class Order(user: Long, product: String, amount: Int)}Here is the Exception:Exception in thread "main" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType&lt;com.manbuyun.awesome.flink.Test.Order&gt; and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer) at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219) at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357) at com.manbuyun.awesome.flink.Test$.main(Test.scala:38) at com.manbuyun.awesome.flink.Test.main(Test.scala) </description>
      <version>1.6.2</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.IntervalJoinITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10764" opendate="2018-11-2 00:00:00" fixdate="2018-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add S3 entropy injection end-to-end/IT test</summary>
      <description>It would be good to add an IT/end-to-end test which verifies the entropy injection for the S3 filesystems introduce by FLINK-9061.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.7.0,1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-core.src.test.resources.META-INF.services.org.apache.flink.core.fs.FileSystemFactory</file>
    </fixedFiles>
  </bug>
  <bug id="10774" opendate="2018-11-4 00:00:00" fixdate="2018-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>connection leak when partition discovery is disabled and open throws exception</summary>
      <description>Here is the scenario to reproduce the issue partition discovery is disabled open method throws an exception (e.g. when broker SSL authorization denies request)In this scenario, run method won't be executed. As a result, partitionDiscoverer.close() won't be called. that caused the connection leak, because KafkaConsumer is initialized but not closed. That has caused outage that brought down our Kafka cluster, when a high-parallelism job got into a restart/failure loop.</description>
      <version>1.4.2,1.5.5,1.6.2</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10799" opendate="2018-11-6 00:00:00" fixdate="2018-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set -Xms when starting JobManager in YARN mode</summary>
      <description>When start JobManager on Yarn mode, only set -Xmx parameter , add -Xms also to avoid high frequency full gc  at start up phase.</description>
      <version>1.3.3,1.4.2,1.5.5,1.6.2</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="10809" opendate="2018-11-7 00:00:00" fixdate="2018-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using DataStreamUtils.reinterpretAsKeyedStream produces corrupted keyed state after restore</summary>
      <description>I've tried using DataStreamUtils.reinterpretAsKeyedStream for results of windowed aggregation: DataStream&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;&gt; eventStream4 = eventStream2.keyBy(Event::getKey) .window(SlidingEventTimeWindows.of(Time.milliseconds(150 * 3), Time.milliseconds(150))) .apply(new WindowFunction&lt;Event, Tuple2&lt;Integer, List&lt;Event&gt;&gt;, Integer, TimeWindow&gt;() { private static final long serialVersionUID = 3166250579972849440L; @Override public void apply( Integer key, TimeWindow window, Iterable&lt;Event&gt; input, Collector&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;&gt; out) throws Exception { out.collect(Tuple2.of(key, StreamSupport.stream(input.spliterator(), false).collect(Collectors.toList()))); } }); DataStreamUtils.reinterpretAsKeyedStream(eventStream4, events-&gt; events.f0) .flatMap(createSlidingWindowCheckMapper(pt)) .addSink(new PrintSinkFunction&lt;&gt;());and then in the createSlidingWindowCheckMapper I verify that each event belongs to 3 consecutive windows, for which I keep contents of last window in ValueState. In a non-failure setup this check runs fine, but it misses few windows after restore at the beginning.public class SlidingWindowCheckMapper extends RichFlatMapFunction&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;, String&gt; { private static final long serialVersionUID = -744070793650644485L; /** This value state tracks previously seen events with the number of windows they appeared in. */ private transient ValueState&lt;List&lt;Tuple2&lt;Event, Integer&gt;&gt;&gt; previousWindow; private final int slideFactor; SlidingWindowCheckMapper(int slideFactor) { this.slideFactor = slideFactor; } @Override public void open(Configuration parameters) throws Exception { ValueStateDescriptor&lt;List&lt;Tuple2&lt;Event, Integer&gt;&gt;&gt; previousWindowDescriptor = new ValueStateDescriptor&lt;&gt;("previousWindow", new ListTypeInfo&lt;&gt;(new TupleTypeInfo&lt;&gt;(TypeInformation.of(Event.class), BasicTypeInfo.INT_TYPE_INFO))); previousWindow = getRuntimeContext().getState(previousWindowDescriptor); } @Override public void flatMap(Tuple2&lt;Integer, List&lt;Event&gt;&gt; value, Collector&lt;String&gt; out) throws Exception { List&lt;Tuple2&lt;Event, Integer&gt;&gt; previousWindowValues = Optional.ofNullable(previousWindow.value()).orElseGet( Collections::emptyList); List&lt;Event&gt; newValues = value.f1; newValues.stream().reduce(new BinaryOperator&lt;Event&gt;() { @Override public Event apply(Event event, Event event2) { if (event2.getSequenceNumber() - 1 != event.getSequenceNumber()) { out.collect("Alert: events in window out ouf order!"); } return event2; } }); List&lt;Tuple2&lt;Event, Integer&gt;&gt; newWindow = new ArrayList&lt;&gt;(); for (Tuple2&lt;Event, Integer&gt; windowValue : previousWindowValues) { if (!newValues.contains(windowValue.f0)) { out.collect(String.format("Alert: event %s did not belong to %d consecutive windows. Event seen so far %d times.Current window: %s", windowValue.f0, slideFactor, windowValue.f1, value.f1)); } else { newValues.remove(windowValue.f0); if (windowValue.f1 + 1 != slideFactor) { newWindow.add(Tuple2.of(windowValue.f0, windowValue.f1 + 1)); } } } newValues.forEach(e -&gt; newWindow.add(Tuple2.of(e, 1))); previousWindow.update(newWindow); }}</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="10863" opendate="2018-11-13 00:00:00" fixdate="2018-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assign uids to all operators</summary>
      <description>We should assign uids to operators in the test so that we can also properly test removing operators.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="10865" opendate="2018-11-13 00:00:00" fixdate="2018-12-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement Flink&amp;#39;s own Aliyun OSS filesystem</summary>
      <description>Aliyun OSS is widely used among China’s cloud users, and Hadoop supports Aliyun OSS since 2.9.1. Open this jira to wrap AliyunOSSFileSystem in flink(similar to s3 support), so that user can read from &amp; write to OSS more easily in flink.  </description>
      <version>1.6.2</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10866" opendate="2018-11-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queryable state can prevent cluster from starting</summary>
      <description>The KvStateServerImpl can currently prevent the TaskExecutor from starting. Currently, the QS server starts per default on port 9067. If this port is not free, then it fails and stops the whole initialization of the TaskExecutor. I think the QS server should not stop the TaskExecutor from starting.We should at least change the default port to 0 to avoid port conflicts. However, this will break all setups which don't explicitly set the QS port because now it either needs to be setup or extracted from the logs.Additionally, we should think about whether a QS server startup failure should lead to a TaskExecutor failure or simply be logged. Both approaches have pros and cons. Currently, a failing QS server will also affect users which don't want to use QS. If we tolerate failures in the QS server, then a user who wants to use QS might run into problems with state not being reachable.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">docs..includes.generated.queryable.state.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10869" opendate="2018-11-13 00:00:00" fixdate="2018-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update S3 testing settings</summary>
      <description>Currently S3 tests go against a bucket hosted by 'data Artisans'.As part of reworking the AWS permission setup, we need to adapt the credentials and buckets for these tests.Future tests should refer to the following environment variables for S3 tests: `IT_CASE_S3_BUCKET` `IT_CASE_S3_ACCESS_KEY` `IT_CASE_S3_SECRET_KEY`</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.s3.S3Credentials.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.presto.s3.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.hadoop.s3a.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10872" opendate="2018-11-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend SQL client end-to-end to test KafkaTableSink for kafka connector 0.11</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10880" opendate="2018-11-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failover strategies should not be applied to Batch Execution</summary>
      <description>When configuring a failover strategy other than "full", DataSet/Batch execution is currently not correct.This is expected, the failover region strategy is an experimental WIP feature for streaming that has not been extended to the DataSet API.We need to document this and prevent execution of DataSet features with other failover strategies than "full".</description>
      <version>1.6.2</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs.release-notes.flink-1.7.md</file>
      <file type="M">docs.release-notes.flink-1.6.md</file>
      <file type="M">docs.release-notes.flink-1.5.md</file>
    </fixedFiles>
  </bug>
  <bug id="10917" opendate="2018-11-18 00:00:00" fixdate="2018-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump io.dropwizard:metrics-core to 3.2.6</summary>
      <description>I've experienced back pressure once in a while from a streaming pipeline of mine &amp;#91;1&amp;#93;.I strongly suspect SlidingTimeWindowReservoir from io.dropwizard.metrics:metrics-core:3.1.5.It is known to cause long GCs &amp;#91;2&amp;#93; so a new implementation called SlidingTimeWindowArrayReservoir is introduced in v3.2.3.So I suggest to bump up dropwizard's metrics-core to v3.2.3 or higher to use the new implementation to prevent back pressure which actually has nothing to do with Flink itself.I just tested compatibility very simply by importing io.dropwizard.metrics:metrics-core:4.0.3 in my own project in order to shadow v3.1.5 which is introduced by flink-metrics-dropwizard.It works without any incompatibility issues for me; there was no NoSuchMethodError or something.However, I'm not sure whether bumping up to 3.2.x or 4.x is okay for other users.&amp;#91;1&amp;#93; https://www.slideshare.net/ssuser6bb12d/realtime-driving-score-service-using-flink/30&amp;#91;2&amp;#93; https://github.com/dropwizard/metrics/pull/1139</description>
      <version>1.6.2</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-metrics.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="10918" opendate="2018-11-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incremental Keyed state with RocksDB throws cannot create directory error in windows</summary>
      <description>Facing error while enabling keyed state with RocksDBBackend with checkpointing to a local windows directory Caused by: org.rocksdb.RocksDBException: Failed to create dir: /c:/tmp/data/job_dbe01128760d4d5cb90809cd94c2a936_op_StreamMap_b5c8d46f3e7b141acf271f12622e752b__3_8__uuid_45c1f62b-a198-44f5-add5-7683079b03f8/chk-1.tmp: Invalid argument                at org.rocksdb.Checkpoint.createCheckpoint(Native Method)                at org.rocksdb.Checkpoint.createCheckpoint(Checkpoint.java:51)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$RocksDBIncrementalSnapshotOperation.takeSnapshot(RocksDBKeyedStateBackend.java:2549)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$IncrementalSnapshotStrategy.performSnapshot(RocksDBKeyedStateBackend.java:2008)                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:498)                at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:406)                ... 13 more  </description>
      <version>1.6.2,1.9.2,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotTransformerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SnapshotDirectoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SnapshotDirectory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DirectoryStateHandle.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10941" opendate="2018-11-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slots prematurely released which still contain unconsumed data</summary>
      <description>Our case is: Flink 1.5 batch mode, 32 parallelism to read data source and 4 parallelism to write data sink. The read task worked perfectly with 32 TMs. However when the job was executing the write task, since only 4 TMs were needed, other 28 TMs were released. This caused RemoteTransportException in the write task: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager ’the_previous_TM_used_by_read_task'. This might indicate that the remote task manager was lost. at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelInactive(PartitionRequestClientHandler.java:133) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) ... After skimming YarnFlinkResourceManager related code, it seems to me that Flink is releasing TMs when they’re idle, regardless of whether working TMs need them. Put in another way, Flink seems to prematurely release slots which contain unconsumed data and, thus, eventually release a TM which then fails a consuming task.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="10980" opendate="2018-11-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong statement in CEP Scala part</summary>
      <description>The CEP 'Selecting from patterns' part of Doc gives wrong code.{{ val startEvent = pattern.get("start").get.next }}val endEvent = pattern.get("end").get.nextshould be .get.head</description>
      <version>1.6.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="10992" opendate="2018-11-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10997" opendate="2018-11-23 00:00:00" fixdate="2018-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro-confluent-registry does not bundle any dependency</summary>
      <description>The flink-avro-confluent-registry is not bundling any dependencies, yet defines a relocation for the transitive jackson dependency pulled in by kafka-schema-registry-client.It is like that the registry-client should be included in the jar.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11003" opendate="2018-11-26 00:00:00" fixdate="2018-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document of Java Lambda Expressions has a mistake</summary>
      <description>Documentation of Java Lambda Expressions has a mistake which may cause confusion.In the last code block, it presents some way to solve type missing problem.In 15 line:public static class MyTuple2Mapper extends MapFunction&lt;Integer, Integer&gt; {    @Override    public Tuple2&lt;Integer, Integer&gt; map(Integer i){         return Tuple2.of(i, i);     }}The second generic type in MapFunction should be Tuple2&lt;Integer, Integer&gt;</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug id="11017" opendate="2018-11-28 00:00:00" fixdate="2018-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution</summary>
      <description>If a time interval was specified with YEAR TO MONTH resolution like e.g.:SELECT * FROM MytableGROUP BY TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11023" opendate="2018-11-28 00:00:00" fixdate="2018-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update LICENSE and NOTICE files for flink-connectors</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE files for flink-connectors.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1104" opendate="2014-9-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate Tuple1 from flink-streaming-connectors</summary>
      <description>The streaming connectors contains unnecessary usage of Tuple1. E.g. in KafkaSource:@Overridepublic Tuple1&lt;String&gt; deserialize(byte[] msg) { String s = new String(msg); if(s.equals("q")){ closeWithoutSend(); } return new Tuple1&lt;String&gt;(s);}Please adjust the underlying interfaces and update the documentation accordingly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.GenSequenceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTopology.java</file>
      <file type="M">docs.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="11051" opendate="2018-12-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Bounded(Group Window) FlatAggregate operator to streaming Table API</summary>
      <description>Add FlatAggregate operator to streaming group window Table API as described in FLIP-29.The usage:tab.window(Tumble/Session/Slide... as 'w)   .groupBy('w, 'k1, 'k2)   .flatAggregate(tableAggregate('a))   .select('w.rowtime, 'k1, 'k2, 'col1, 'col2)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupTableAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamTableAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.tableImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.WindowGroupedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.FlatAggregateTable.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="11079" opendate="2018-12-5 00:00:00" fixdate="2018-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip deployment for flnk-storm-examples</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE for flink-storm-examples. This project creates several fat example jars that are deployed to maven central.Alternatively we could about dropping these examples.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializerSnapshot.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CollectionSerializerConfigSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug id="11088" opendate="2018-12-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow pre-install Kerberos authentication keytab discovery on YARN</summary>
      <description>Currently flink-yarn assumes keytab is shipped as application master environment local resource on client side and will be distributed to all the TMs. This does not work for YARN proxy user mode &amp;#91;1&amp;#93; since proxy user or super user might not have access to actual users' keytab, but can request delegation tokens on users' behalf. Based on the type of security options for long-living YARN service&amp;#91;2&amp;#93;, we propose to have the keytab file path discovery configurable depending on the launch mode of the YARN client. Reference: &amp;#91;1&amp;#93; https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html&amp;#91;2&amp;#93; https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html#Securing_Long-lived_YARN_Services</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityConfiguration.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtilsTest.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTaskExecutorRunnerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="1111" opendate="2014-9-22 00:00:00" fixdate="2014-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Basic and Array Type Information into "flink-core" Project</summary>
      <description>They are logically part of the common ground under the Java/Scala API.Tests against the common API are currently impossible, because no type information is in the common project.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.IterationWithChainingNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatableDanglingPageRank.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.ConnectedComponentsNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.broadcastvars.KMeansIterativeNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.broadcastvars.BroadcastVarsNepheleITCase.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.translation.ReduceTranslationTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.io.CollectionInputFormatTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.functions.SemanticPropertiesTranslationTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.unfinishedKeyPairOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.TypeUtils.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.crossDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvOutputFormat.java</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvInputFormat.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.KeyGroupedIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.RecordOutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeCoGroupIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MergeIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllGroupReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CachedMatchTaskTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorInputFormatsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeInformationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.TypeInfoParserTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.AggregateOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.ReduceTranslationTests.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CollectionInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SelectByFunctionsTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.WritableTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ValueTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeInfoParser.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordSerializerFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordPairComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordPairComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericArraySerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ResultTypeQueryable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.RecordTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PrimitiveArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoField.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.InputTypeConfigurable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.CompositeType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.BasicTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.BasicArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.AtomicType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.GenericDataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.FileDataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanFilterOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Operator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.IterativeDataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FlatMapOperator.java</file>
      <file type="M">flink-java8-tests.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.KeySelectorTest.java</file>
      <file type="M">flink-java8-tests.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.ReduceITCase.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.java.io.AvroInputFormat.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.java.io.AvroInputFormatTypeExtractionTest.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.example.JDBCExample.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexCentricIteration.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.BatchedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SplitDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.CoStreamTask.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.InputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.OutputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.TypeSerializerWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockInvokable.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.SinkJoiner.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.JavaApiPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.RecordModelPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpBinaryUdfOp.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpUnaryUdfOp.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.FeedbackPropertiesMatchTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.plan.ChannelTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RichGroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.BulkIterationBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.BinaryOperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Operator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.OperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.UnaryOperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.NothingTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.TypeInformation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.InvalidTypesException.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SemanticPropUtil.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.LocalCollectionOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.BulkIterationResultSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIteration.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIterationResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="1112" opendate="2014-9-22 00:00:00" fixdate="2014-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GroupSorting with KeySelectors</summary>
      <description>Group sorting is currently only supported for field-index keys and not for KeySelectors.This feature was requested.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.CustomPartitioningGroupingKeySelectorTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.GroupingKeySelectorTranslationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11120" opendate="2018-12-10 00:00:00" fixdate="2018-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIMESTAMPADD function handles TIME incorrectly</summary>
      <description>The error occur when timestampadd(MINUTE, 1, time '01:00:00') is executed:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Longat org.apache.calcite.rex.RexBuilder.clean(RexBuilder.java:1520)at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1318)at org.apache.flink.table.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:135)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:620)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:540)at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:288)I think it should meet the following conditions:expressionExpect the resulttimestampadd(MINUTE, -1, time '00:00:00')23:59:00timestampadd(MINUTE, 1, time '00:00:00')00:01:00timestampadd(MINUTE, 1, time '23:59:59')00:00:59timestampadd(SECOND, 1, time '23:59:59')00:00:00timestampadd(HOUR, 1, time '23:59:59')00:59:59This problem seems to be a bug in calcite. I have submitted isuse to calcite. The following is the link.CALCITE-2699</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11126" opendate="2018-12-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter out AMRMToken in the TaskManager credentials</summary>
      <description>Currently, Flink JobManager propagates its storage tokens to TaskManager to meet the requirement of YARN log aggregation (see FLINK-6376). But in this way the AMRMToken is also included in the TaskManager credentials, which could be potentially insecure. We should filter out AMRMToken before setting the tokens to TaskManager's container launch context.</description>
      <version>1.6.2,1.6.4,1.7.0,1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11136" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11142" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undefined behavior in the conversion from DataStream/DataSet to Table</summary>
      <description>When we try to convert DataStream/DataSet to Table. There are two ways of adding schema information, ByName or ByPosition.This feature first proposed in this pr.In ByPosition mode, the current code does not check if the number of fields less than its in  DataStream/DataSet. This may cause undefined behavior, e.g. make a projection in ByPosition mode.We can either fix it by adding some checking or regard this as a feature and just improve the doc to clarify it. In my opinion, the latter way seems better.twalthr Could you take a look at it when you free?</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="11159" opendate="2018-12-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow configuration whether to fall back to savepoints for restore</summary>
      <description>Ever since FLINK-3397, upon failure, Flink would restart from the latest checkpoint/savepoint which ever is more recent. With the introduction of local recovery and the knowledge that a RocksDB checkpoint restore would just copy the files, it may be time to re-consider / making this configurable:In certain situations, it may be faster to restore from the latest checkpoint only (even if there is a more recent savepoint) and reprocess the data between. On the downside, though, that may not be correct because that might break side effects if the savepoint was the latest one, e.g. consider this chain: chk1 -&gt; chk2 -&gt; sp … restore chk2 -&gt; …. Then all side effects between chk2 -&gt; sp would be reproduced.Making this configurable will allow the user to set whatever he needs / can to get the lowest recovery time in Flink.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="11207" opendate="2018-12-20 00:00:00" fixdate="2018-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="11232" opendate="2018-12-29 00:00:00" fixdate="2018-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Start Time of sub-task on web dashboard</summary>
      <description></description>
      <version>1.5.5,1.6.2,1.6.3,1.7.0,1.7.1</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="11336" opendate="2019-1-15 00:00:00" fixdate="2019-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink HA didn&amp;#39;t remove ZK metadata</summary>
      <description>Flink HA didn't remove ZK metadatasuch as go to zk cli  : ls /flinkone i suggest we should delete this metadata when the application  cancel or throw exception</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="11469" opendate="2019-1-30 00:00:00" fixdate="2019-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="1157" opendate="2014-10-13 00:00:00" fixdate="2014-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document TaskManager Slots</summary>
      <description>Slots are not explained in the documentation.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.setup.quickstart.md</file>
      <file type="M">docs.config.md</file>
      <file type="M">docs.cluster.setup.md</file>
      <file type="M">docs.cluster.execution.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
