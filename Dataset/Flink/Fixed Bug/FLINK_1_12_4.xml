<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="22198" opendate="2021-4-11 00:00:00" fixdate="2021-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaTableITCase hang.</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16287&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6625There is no any artifacts.</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22683" opendate="2021-5-17 00:00:00" fixdate="2021-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The total Flink/process memory of memoryConfiguration in /taskmanagers can be null or incorrect value</summary>
      <description>In FLINK-14435, we add the `memoryConfiguration` to /taskmanagers REST API. However, that field can be `null` if it is not configured by the user or the incorrect value in fine-grained resource management. We need to calculate them proactively.</description>
      <version>1.13.0,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="22747" opendate="2021-5-21 00:00:00" fixdate="2021-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-io to 2.8</summary>
      <description>commons-io 2.7 has known vulnerabilities that are detected in Flink by some tools. Even though it is unlikely that we use the mentioned class. We should upgrade it to make the tools happy. Context:VULNDB-239195"Vendor Specific News/Changelog Entryhttps://commons.apache.org/proper/commons-io/changes-report.html#a2.8.0Vendor Specific Solution URLhttps://github.com/apache/commons-io/commit/0de91c048fb575b9e7906e966a4428574fd03695Vendor Specific Solution URLhttps://github.com/apache/commons-io/commit/97ae01c95837f50a2e9be34c370b271c4d8fc88bBug Trackerhttps://issues.apache.org/jira/browse/IO-675"</description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="22802" opendate="2021-5-28 00:00:00" fixdate="2021-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Fabric8 Kubernetes Client to &gt;= 5.X</summary>
      <description>Hi All, Currently, Flink is using version 4.9.2 of the Fabric8 Kubernetes Client which does not support new versions of the Kubernetes API such as 1.19 or 1.20 as pointed out by their Compatibility Matrix which is support in 5.4.0 or above.As far as I have seen in the Flink documentation, Flink supports `Kubernetes &gt;= 1.9.` but due to this dependency, it might not be the case. Is there a plan to update this dependency?What is the plan moving forwards when new versions of Kubernetes are released?I am raising this because I have been testing Flink HA Session Cluster on Kubernetes 1.19 and 1.20 and I have encountered some frequent errors that force the JM pods to restart or even result in an unrecoverable state.Thanks!</description>
      <version>1.13.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedDispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubernetesSharedWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-session.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22814" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New sources are not defining/exposing checkpointStartDelayNanos metric</summary>
      <description>checkpointStartDelayNanos metric for new (FLIP-27) sources is always 0ms in the WebUI, regardless how long it took to finally start triggering the checkpoint.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="22815" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable unaligned checkpoints for broadcast partitioning</summary>
      <description>Broadcast partitioning can not work with unaligned checkpointing. Thereare no guarantees that records are consumed at the same rate in allchannels. This can result in some tasks applying state changescorresponding to a certain broadcasted event while others don't. In turnupon restore, it may lead to an inconsistent state.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.11.4,1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapperTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="22833" opendate="2021-6-1 00:00:00" fixdate="2021-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Source tasks (both old and new) are not reporting checkpoint start delay via CheckpointMetrics</summary>
      <description>checkpointStartDelay is still not reported in the WebUI. Previous bugs FLINK-22814 and FLINK-18656 have fixed this issue only for the Task's metric, not for checkpoint metrics.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTestBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="22886" opendate="2021-6-6 00:00:00" fixdate="2021-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread leak in RocksDBStateUploader</summary>
      <description>ExecutorService in RocksDBStateUploader is not shut down, which may leak thread when tasks fail.BTW, we should name the thread group in ExecutorService, otherwise what we see in the stack, is a lot of threads named with pool-m-thread-n like this: </description>
      <version>1.11.3,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDataTransfer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="22939" opendate="2021-6-9 00:00:00" fixdate="2021-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize JDK switch in azure setup</summary>
      <description>Our current azure setup makes it a bit difficult to switch to a different JDK because the "jdk" parameter is only evaluated if it is set to "jdk11".Instead, we could generalize this a bit so that it is always evaluated, such that if the image contains the JDK (under some expected location) one can just specify jdk: 14.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="22952" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>docs_404_check fail on azure due to ruby version not available</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18852&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5Starting: UseRubyVersion==============================================================================Task : Use Ruby versionDescription : Use the specified version of Ruby from the tool cache, optionally adding it to the PATHVersion : 0.186.0Author : Microsoft CorporationHelp : https://docs.microsoft.com/azure/devops/pipelines/tasks/tool/use-ruby-version==============================================================================##[error]Version spec = 2.4 for architecture %25s did not match any version in Agent.ToolsDirectory.Available versions: /opt/hostedtoolcache2.5.9,2.6.7,2.7.3,3.0.1If this is a Microsoft-hosted agent, check that this image supports side-by-side versions of Ruby at https://aka.ms/hosted-agent-software.If this is a self-hosted agent, see how to configure side-by-side Ruby versions at https://go.microsoft.com/fwlink/?linkid=2005989.Finishing: UseRubyVersion</description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug id="22957" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rank TTL should use enableTimeToLive of state instead of timer</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyFirstNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug id="22963" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of taskmanager.memory.task.heap.size in the official document is incorrect</summary>
      <description>When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22964" opendate="2021-6-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector-base exposes dependency to flink-core.</summary>
      <description>Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.Fix is to make `flink-core` provided in `connector-base`.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22994" opendate="2021-6-15 00:00:00" fixdate="2021-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the performance of nesting udf invoking</summary>
      <description>BackGroundIn some nesting udf invoking cases, Flink convert the udf result to external object and then convert to internalOrNull object as params for next udf invoking. The performance of some converter is poor.   Test ParamsSource = Kafka, Schema = PB with snappy; Flink Slot = 1; taskmanager.memory.process.size=4g; Linux Core = Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz UDF Introduction: ipip:  input: int ip, output: map ip_info, map size = 14. ip_2_country: input map ip_info, output: string country. ip_2_region: input  map ip_info, output: string region. ip_2_isp_domain: input  map ip_info, output: string isp. ip_2_timezone: input map ip_info, output: string timezone.Performance Compare with MapMapConverterThe throughput of nesting udf invoking with MapMapConverter(5 times): 41.42 k/sGoalFor some cases, skip toInternalOrNull &amp; toExternal, Use the udf result directly.Performance Compare without MapMapConverterThe throughput of nesting udf invoking without MapMapConverter: 174.41 k/s </description>
      <version>1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23004" opendate="2021-6-16 00:00:00" fixdate="2021-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misleading log</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23074" opendate="2021-6-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>There is a class conflict between flink-connector-hive and flink-parquet</summary>
      <description>flink-connector-hive 2.3.6 include parquet-hadoop 1.8.1 version but flink-parquet include 1.11.1.org.apache.parquet.hadoop.example.GroupWriteSupport is different.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23093" opendate="2021-6-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit number of I/O pool and Future threads in Mini Cluster</summary>
      <description>When running tests on CI via the minicluster, the mini cluster typically spawns 100s of I/O threads, both in the MiniCluster I/O pool and in the TM I/O pool.The standard rule for the maximum pool size is 4*num-cores, but the number of cores can be fairly large these days. Various Java versions also mess up core counting when running in containers (JVM container might have been given 2 cores as resource limits, but the JVM counts the system as a whole, like 64/128 cores).This is both a nuisance for debugging, and a big waste of memory (each thread takes by default around 1MB when spawned, so the test JVM wastes 100s of MBs for nothing).I would suggest to set a default of 8 I/O threads for the Mini Cluster. The scaling-with-cores is important for proper TM/JM deployments, but not for the Mini Cluster.</description>
      <version>1.13.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23097" opendate="2021-6-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Queryable state (rocksdb) with TM restart end-to-end test&amp;#39; fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19310&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529&amp;l=12333SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.Jun 22 14:04:55 MapState has 22 entriesJun 22 14:04:56 TaskManager 422719 killed.Jun 22 14:04:56 Number of running task managers 1 is not yet 0.Jun 22 14:05:00 Number of running task managers 1 is not yet 0.Jun 22 14:05:04 Number of running task managers 1 is not yet 0.Jun 22 14:05:08 Number of running task managers has reached 0.Jun 22 14:05:08 Latest snapshot count was 42Jun 22 14:05:09 Starting taskexecutor daemon on host fv-az68-17.Jun 22 14:05:09 Number of running task managers 0 is not yet 1.Jun 22 14:05:13 Number of running task managers has reached 1.Jun 22 14:05:15 Job (5b515e0f9168e338d1645bf2e9f92820) is running.Jun 22 14:05:15 Starting to wait for completion of 18 checkpointsJun 22 14:05:15 13/18 completed checkpointsJun 22 14:05:17 13/18 completed checkpointsJun 22 14:05:19 17/18 completed checkpointsJun 22 14:05:21 17/18 completed checkpointsSLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.Jun 22 14:05:24 after: 40Jun 22 14:05:24 An error occurredJun 22 14:05:24 [FAIL] Test script contains errors.Jun 22 14:05:24 Checking of logs skipped.Jun 22 14:05:24 Jun 22 14:05:24 [FAIL] 'Queryable state (rocksdb) with TM restart end-to-end test' failed after 0 minutes and 50 seconds! Test exited with exit code 1Jun 22 14:05:24 14:05:24 ##[group]Environment Information</description>
      <version>1.14.0,1.12.4</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-queryable-state-test.src.main.java.org.apache.flink.streaming.tests.queryablestate.QsStateProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23119" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that the exception that General Python UDAF is unsupported is not thrown in Compile Stage.</summary>
      <description></description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonOverWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonGroupWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalPythonWindowAggregateRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="23202" opendate="2021-7-1 00:00:00" fixdate="2021-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RpcService should fail result futures if messages could not be sent</summary>
      <description>The RpcService should fail result futures if messages could not be sent. This would speed up the failure detection mechanism because it would not rely on the timeout. One way to achieve this could be to listen to the dead letters and then sending a Failure message back to the sender.</description>
      <version>1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RemoteAkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.UnfencedMessage.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RunAsync.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RpcInvocation.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteHandshakeMessage.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.HandshakeSuccessMessage.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.FencedMessage.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.CallAsync.java</file>
    </fixedFiles>
  </bug>
  <bug id="23208" opendate="2021-7-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Late processing timers need to wait 1ms at least to be fired</summary>
      <description>The problem is from the codes below:public static long getProcessingTimeDelay(long processingTimestamp, long currentTimestamp) { // delay the firing of the timer by 1 ms to align the semantics with watermark. A watermark // T says we won't see elements in the future with a timestamp smaller or equal to T. // With processing time, we therefore need to delay firing the timer by one ms. return Math.max(processingTimestamp - currentTimestamp, 0) + 1;}Assuming a Flink job creates 1 timer per millionseconds, and is able to consume 1 timer/ms. Here is what will happen: Timestmap1(1st ms): timer1 is registered and will be triggered on Timestamp2. Timestamp2(2nd ms): timer2 is registered and timer1 is triggered Timestamp3(3rd ms): timer3 is registered and timer1 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer2, and timer2 will be triggered on Timestamp4(wait 1ms at least) Timestamp4(4th ms): timer4 is registered and timer2 is triggered Timestamp5(5th ms): timer5 is registered and timer2 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer3, and timer3 will be triggered on Timestamp6(wait 1ms at least)As we can see here, the ability of the Flink job is consuming 1 timer/ms, but it's actually able to consume 0.5 timer/ms. And another problem is that we cannot observe the delay from the lag metrics of the source(Kafka). Instead, what we can tell is that the moment of output is much later than expected. I've added a metrics in our inner version, we can see the lag of the timer triggering keeps increasing: In another word, we should never let the late processing timer wait 1ms, I think a simple change would be as below:return Math.max(processingTimestamp - currentTimestamp, -1) + 1;</description>
      <version>1.11.0,1.11.3,1.13.0,1.14.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23249" opendate="2021-7-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce ShuffleMasterContext to ShuffleMaster</summary>
      <description>Introduce ShuffleMasterContext to ShuffleMaster. Just like the ShuffleEnvironmentContext at the TaskManager side, the ShuffleMasterContext can act as a proxy of ShuffleMaster and other components of Flink like the ResourceManagerPartitionTracker.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.ShuffleServiceLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23267" opendate="2021-7-6 00:00:00" fixdate="2021-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate Java code splitter for all generated classes</summary>
      <description>In the second step we activate Java code splitter introduced in the first step.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedWatermarkGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedTableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedResultFuture.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedProjection.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedNormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedNamespaceTableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedJoinCondition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedInput.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedHashFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedCollector.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedClass.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.over.RangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.over.MultiFieldRangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23297" opendate="2021-7-7 00:00:00" fixdate="2021-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Protobuf to 3.17.3</summary>
      <description>In order to support compilation with ARM (e.g. Apple M1 chip), we need to bump our Protobuf dependency to version 3.17.3.</description>
      <version>1.14.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23493" opendate="2021-7-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug id="2429" opendate="2015-7-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the "enableCheckpointing()" without interval variant</summary>
      <description>I think it is not very obvious what the default checkpointing interval is.Also, when somebody activates checkpointing, shouldn't they think about what they want in terms of frequency and recovery latency tradeoffs?</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="24290" opendate="2021-9-15 00:00:00" fixdate="2021-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support STRUCTURED_TYPE for JSON_OBJECT / JSON_ARRAY</summary>
      <description>In FLINK-16203 we excluded this because we run into a limitation with fromValues when testing it. I will apply the patch that should implement this, but we need to find a way to make the test work.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificInputTypeStrategies.java</file>
    </fixedFiles>
  </bug>
  <bug id="24474" opendate="2021-10-7 00:00:00" fixdate="2021-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone clusters should bind to localhost by default</summary>
      <description>By default the REST endpoints bind to 0.0.0.0.This is fine for docker use-cases as it simplifies the setup and the API isn't reachable unless the user explicitly enables that via docker.However, for standalone clusters this is a different story, and it is currently too easy for users to accidentally expose their clusters to the outside world.We should set the bind address by default to localhost, and change the docker-scripts to set this to 0.0.0.0 .</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="26270" opendate="2022-2-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL write data to kafka by CSV format , decimal type was converted to scientific notation</summary>
      <description>Source：Oraclefield type：decimal Sink:：kafkafield type：decimalformat：CSV Cannot set not to convert to scientific notation </description>
      <version>1.12.4</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatOptions.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvCommons.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.csv.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.csv.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
