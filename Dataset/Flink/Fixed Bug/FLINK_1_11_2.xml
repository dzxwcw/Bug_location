<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="12678" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractCatalog to manage the common catalog name and default database name for catalogs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="12679" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;default-database&amp;#39; config for catalog entries in SQL CLI yaml file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12683" opendate="2019-5-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Provide task manager&amp;#39;s location information for checkpoint coordinator specific log messages</summary>
      <description>Currently, the AcknowledgeCheckpoint does not contain the task manager's location information. When a task's snapshot task sends an ack message to the coordinator, we can only log this message:Received late message for now expired checkpoint attempt 6035 from ccd88d08bf82245f3466c9480fb5687a of job 775ef8ff0159b071da7804925bbd362f.Sometimes we need to get this sub task's location information to do the further debug work, e.g. stack trace dump. But, without the location information, It will not help to quickly locate the problem. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12685" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports UNNEST query in blink planner</summary>
      <description>this issue aim to support queries with UNNEST keyword, which relate to nested fields.for example: table name: MyTableschema: a: int, b int, c array&amp;#91;int&amp;#93;sql:SELECT a, b, s FROM MyTable, UNNEST(MyTable.c) AS A (s)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkDecorrelateProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16945" opendate="2020-4-2 00:00:00" fixdate="2020-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="16946" opendate="2020-4-2 00:00:00" fixdate="2020-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="16947" opendate="2020-4-2 00:00:00" fixdate="2020-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1695" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17342" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedule savepoint if max-inflight-checkpoints limit is reached instead of forcing (in UC mode)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointPropertiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18902" opendate="2020-8-12 00:00:00" fixdate="2020-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot serve results of asynchronous REST operations in per-job mode</summary>
      <description>Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.InFlightRequestTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19151" opendate="2020-9-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink does not normalize container resource with correct configurations when Yarn FairScheduler is used</summary>
      <description>ProblemIt's a Yarn protocol that the requested container resource will be normalized for allocation. That means, the allocated container may have different resource (larger than or equal to) compared to what is requested.Currently, Flink matches the allocated containers to the original requests by reading the Yarn configurations and calculate how the requested resources should be normalized.What has been overlooked is that, Yarn FairScheduler (and its subclass SLSFairScheduler) has overridden the normalization behavior. To be specific, By default, Yarn normalize container resources to integer multiple of "yarn.scheduler.minimum-allocation-&amp;#91;mb|vcores&amp;#93;" FairScheduler normalize container resources to integer multiple of "yarn.resource-types.&amp;#91;memory-mb|vcores&amp;#93;.increment-allocation" (or the deprecated keys "yarn.scheduler.increment-allocation-&amp;#91;mb|vcores&amp;#93;"), while making sure the resource is no less than "yarn.scheduler.minimum-allocation-&amp;#91;mb|vcores&amp;#93;"Proposal for short term solutionTo fix this problem, a quick and easy way is to also read Yarn configuration and learn which scheduler is used, and perform normalization calculations accordingly. This should be good enough to cover behaviors of all the schedulers that Yarn currently provides. The limitation is that, Flink will not be able to deal with custom Yarn schedulers which override the normalization behaviors.Proposal for long term solutionFor long term, it would be good to use Yarn ContainerRequest#allocationRequestId to match the allocated containers with the original requests, so that Flink no longer needs to understand how Yarn normalize container resources. Yarn ContainerRequest#allocationRequestId is introduced in Hadoop 2.9, while ATM Flink claims to be compatible with Hadoop 2.4+. Therefore, this solution would not work at the moment.Another idea is to support various Hadoop versions with different container matching logics. We can abstract the container matching logics into a dedicating component, and provide different implementations for it. This will allow Flink to take advantages of the new versions (e.g., work well with custom schedulers), while stay compatible with the old versions without those advantages.Given that we need the resource based matching anyway for the old Hadoop versions, and the cost for maintaining two sets of matching logics, I tend to think this approach as a back-up option to be worked on when we indeed see a need for it.</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.WorkerSpecContainerResourceAdapterTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.WorkerSpecContainerResourceAdapter.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19161" opendate="2020-9-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port File Sources to FLIP-27 API</summary>
      <description>Porting the File sources to the FLIP-27 API means combining the FileInputFormat from the DataSet Batch API The Monitoring File Source from the DataStream API.The two currently share the same reader code already and partial enumeration code.StructureThe new File Source will have three components: File enumerators that discover the files. File split assigners that decide which reader gets what split File Reader Formats, which deal with the decoding.The main difference between the Bounded (Batch) version and the unbounded (Streaming) version is that the streaming version repeatedly invokes the file enumerator to search for new files.Checkpointing EnumeratorsThe enumerators need to checkpoint the not-yet-assigned splits, plus, if they are in continuous discovery mode (streaming) the paths / timestamps already processed.Checkpointing ReadersThe new File Source needs to ensure that every reader can be checkpointed.Some readers may be able to expose the position in the input file that corresponds to the latest emitted record, but many will not be able to do that due to storing compresses record batches using buffered decoders where exact position information is not accessibleWe therefore suggest to expose a mechanism that combines seekable file offsets and records to read and skip after that offset. In the extreme cases, files can work only with seekable positions or only with records-to-skip. Some sources, like Avro, can have periodic seek points (sync markers) and count records-to-skip after these markers.Efficient and Convenient ReadersTo balance efficiency (batch vectorized reading of ORC / Parquet for vectorized query processing) and convenience (plug in 3-rd party CSV decoder over stream) we offer three abstraction for record readers Bulk Formats that run over a file Path and return a iterable batch at a time (most efficient) File Record formats which read files record-by-record. The source framework hands over a pre-defined-size batch from Split Reader to Record Emitter. Stream Formats that decode an input stream and rely on the source framework to decide how to batch record handover (most convenient)</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.TestingReaderContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19162" opendate="2020-9-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Split Reader based sources to reuse record batches</summary>
      <description>The Split Readers hand over a batch of records at a time from the I/O thread (fetching and decoding) to the main operator processing thread. These structures can memory intensive and expensive and performance greatly benefits from reusing them. This is especially true for high-performance format readers like ORC and Parquet.While previous sources (where I/O was in the main thread) could reuse objects in a trivial manner, the new Split Reader API (with multiple threads) needs an explicit recycle() hook to allow returning/reusing these objects.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SplitsRecordIterator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsWithSplitIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="19163" opendate="2020-9-8 00:00:00" fixdate="2020-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add building py38 wheel package of PyFlink in Azure CI</summary>
      <description>Add building py38 wheel package of PyFlink in Azure CI</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.build-wheels.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19174" opendate="2020-9-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>idleTimeMsPerSecond can report incorrect values (0 or values more than one second)</summary>
      <description>If task is blocked for more than 60 seconds (org.apache.flink.metrics.MeterView#DEFAULT_TIME_SPAN_IN_SECONDS), idleTimeMsPerSecond can be reported as zero, despite task being completely idle. Once the task is unblocked and the idleTimeMsPerSecond metric is updated, it can for the next 60 seconds exceed 1000ms/second.Average value over the longer periods of time will be correct and accurate.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.Slf4jReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19178" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the memory weights configuration option and interfaces</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.OperatorValidationUtils.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19179" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement the managed memory fraction calculation logic</summary>
      <description>This also means migrating the batch operator use cases.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19180" opendate="2020-9-10 00:00:00" fixdate="2020-10-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Make RocksDB respect the calculated managed memory fraction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ManagedMemoryUseCase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.ManagedMemoryUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.KeyedMultipleInputTransformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19181" opendate="2020-9-10 00:00:00" fixdate="2020-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make python processes respect the calculated managed memory fraction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.AbstractStreamArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessTwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonReduceOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="19182" opendate="2020-9-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update document for intra-slot managed memory sharing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
    </fixedFiles>
  </bug>
  <bug id="19245" opendate="2020-9-15 00:00:00" fixdate="2020-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default queue capacity for FLIP-27 source handover queue to 2</summary>
      <description>Based on initial investigation by jqin a capacity value of two results in good queue throughput while still keeping the number of elements small.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueueTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="19259" opendate="2020-9-16 00:00:00" fixdate="2020-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use classloader release hooks with Kinesis producer to avoid metaspace leak</summary>
      <description>FLINK-17554 introduced hooks for clearing references before unloading a classloader.The Kinesis Producer library is currently preventing the usercode classloader from being unloaded because it keeps references around.This ticket is to use the hooks with the Kinesis producer.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19280" opendate="2020-9-17 00:00:00" fixdate="2020-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The option "sink.buffer-flush.max-rows" for JDBC can&amp;#39;t be disabled by set to zero</summary>
      <description>According to the documentation, when set "sink.buffer-flush.max-rows=0", it should be disabled to flush buffered data by number of rows, however, it still flushes when received each row.</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19282" opendate="2020-9-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support watermark push down with WatermarkStrategy</summary>
      <description>Push the WatermarkStrategy into TableSourceScan in the interface SupportsWatermarkPushDown. Sometimes users define watermark on computed column. Therefore, we will push the computed column of rowtime into WatermarkStrategy first. For more info, please take a look at discussion.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.catalog.JavaCatalogTableTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.NestedProjectionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sources.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="19339" opendate="2020-9-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Avro&amp;#39;s unions with logical types</summary>
      <description>Avro 1.9.x introduced yet another mechanism for registering/looking up conversions for logical types. See https://issues.apache.org/jira/browse/AVRO-1891If a logical type is part of a union a static field MODEL$ of type SpecificData will be added to the generated Avro class with registered conversions for a logical type. We should use that SpecificData in AvroSerializer and Avro(De)SerializationSchema whenever available.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="19340" opendate="2020-9-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AggregateITCase.testListAggWithDistinct failed with "expected:&lt;List(1,A, 2,B, 3,C#A, 4,EF)&gt; but was:&lt;List(1,A, 2,B, 3,C#A, 4,EF#EF)&gt;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6734&amp;view=logs&amp;j=e25d5e7e-2a9c-5589-4940-0b638d75a414&amp;t=a6e0f756-5bb9-5ea8-a468-5f60db442a292020-09-22T04:59:30.1229430Z [ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase) Time elapsed: 0.346 s &lt;&lt;&lt; FAILURE!2020-09-22T04:59:30.1230120Z java.lang.AssertionError: expected:&lt;List(1,A, 2,B, 3,C#A, 4,EF)&gt; but was:&lt;List(1,A, 2,B, 3,C#A, 4,EF#EF)&gt;2020-09-22T04:59:30.1232835Z at org.junit.Assert.fail(Assert.java:88)2020-09-22T04:59:30.1233314Z at org.junit.Assert.failNotEquals(Assert.java:834)2020-09-22T04:59:30.1233688Z at org.junit.Assert.assertEquals(Assert.java:118)2020-09-22T04:59:30.1234034Z at org.junit.Assert.assertEquals(Assert.java:144)2020-09-22T04:59:30.1234528Z at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testListAggWithDistinct(AggregateITCase.scala:667)</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.RowDataSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.RowDataSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19401" opendate="2020-9-24 00:00:00" fixdate="2020-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job stuck in restart loop due to excessive checkpoint recoveries which block the JobMaster</summary>
      <description>Flink job sometimes got into a restart loop for many hours and can't recover until redeployed. We had some issue with Kafka that initially caused the job to restart. Below is the first of the many exceptions for "ResourceManagerException: Could not find registered job manager" error.2020-09-19 00:03:31,614 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [flink-akka.actor.default-dispatcher-35973] - Requesting new slot [SlotRequestId{171f1df017dab3a42c032abd07908b9b}] and profile ResourceProfile{UNKNOWN} from resource manager.2020-09-19 00:03:31,615 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [flink-akka.actor.default-dispatcher-35973] - Requesting new slot [SlotRequestId{cc7d136c4ce1f32285edd4928e3ab2e2}] and profile ResourceProfile{UNKNOWN} from resource manager.2020-09-19 00:03:31,615 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [flink-akka.actor.default-dispatcher-35973] - Requesting new slot [SlotRequestId{024c8a48dafaf8f07c49dd4320d5cc94}] and profile ResourceProfile{UNKNOWN} from resource manager.2020-09-19 00:03:31,615 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [flink-akka.actor.default-dispatcher-35973] - Requesting new slot [SlotRequestId{a591eda805b3081ad2767f5641d0db06}] and profile ResourceProfile{UNKNOWN} from resource manager.2020-09-19 00:03:31,620 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [flink-akka.actor.default-dispatcher-35973] - Source: k2-csevpc -&gt; k2-csevpcRaw -&gt; (vhsPlaybackEvents -&gt; Flat Map, merchImpressionsClientLog -&gt; Flat Map) (56/640) (1b0d3dd1f19890886ff373a3f08809e8) switched from SCHEDULED to FAILED.java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:433) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forward$21(FutureUtils.java:1065) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792) at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153) at org.apache.flink.runtime.concurrent.FutureUtils.forward(FutureUtils.java:1063) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.createRootSlot(SlotSharingManager.java:155) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateMultiTaskSlot(SchedulerImpl.java:511) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSharedSlot(SchedulerImpl.java:311) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.internalAllocateSlot(SchedulerImpl.java:160) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSlotInternal(SchedulerImpl.java:143) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSlot(SchedulerImpl.java:113) at org.apache.flink.runtime.executiongraph.SlotProviderStrategy$NormalSlotProviderStrategy.allocateSlot(SlotProviderStrategy.java:115) at org.apache.flink.runtime.scheduler.DefaultExecutionSlotAllocator.lambda$allocateSlotsFor$0(DefaultExecutionSlotAllocator.java:104) at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) at org.apache.flink.runtime.scheduler.DefaultExecutionSlotAllocator.allocateSlotsFor(DefaultExecutionSlotAllocator.java:102) at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlots(DefaultScheduler.java:342) at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:311) at org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy.allocateSlotsAndDeploy(EagerSchedulingStrategy.java:76) at org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy.restartTasks(EagerSchedulingStrategy.java:57) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$restartTasks$1(DefaultScheduler.java:268) at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628) at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlot(SlotPoolImpl.java:438) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.requestNewAllocatedSlot(SchedulerImpl.java:236) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateMultiTaskSlot(SchedulerImpl.java:506) ... 39 moreCaused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.slotRequestToResourceManagerFailed(SlotPoolImpl.java:360) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.lambda$requestSlotFromResourceManager$1(SlotPoolImpl.java:348) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792) at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153) at org.apache.flink.runtime.concurrent.FutureUtils.whenCompleteAsyncIfNotDone(FutureUtils.java:941) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestSlotFromResourceManager(SlotPoolImpl.java:342) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlotInternal(SlotPoolImpl.java:309) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlot(SlotPoolImpl.java:437) ... 41 moreCaused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not find registered job manager for job 70216adbeed914b35d77717c4b7b13ea. at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628) at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:214) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:129) at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.invoke(FencedAkkaInvocationHandler.java:78) at com.sun.proxy.$Proxy94.requestSlot(Unknown Source) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestSlotFromResourceManager(SlotPoolImpl.java:337) ... 43 moreCaused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not find registered job manager for job 70216adbeed914b35d77717c4b7b13ea. at org.apache.flink.runtime.resourcemanager.ResourceManager.requestSlot(ResourceManager.java:443) at sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ... 20 moreGrepped through our log. job manager registration happened once when the job was deployed a few days ago.2020-09-16 17:23:28,081 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-60] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.Then there were a flurry of 9 registrations in the same milli-seconds that happened ~30 mins after the first error of " Could not find registered job manager". The issue persisted many hours after this. Because this is a pre-prod job, so we didn't have alert on it.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,827 INFO com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963] - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.2020-09-19 00:33:07,828 INFO org.apache.flink.runtime.jobmaster.JobMaster [flink-akka.actor.default-dispatcher-35968] - JobManager successfully registered at ResourceManager, leader id: bf239dac186bc8ba901a8702f4bb42e3.I have the job manager logs for the hour with INFO level that I can share offline if needed. </description>
      <version>1.10.1,1.11.2</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="19403" opendate="2020-9-25 00:00:00" fixdate="2020-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Stream Group Window Aggregation</summary>
      <description>We will add Stream Physical Pandas Group Window RelNode and StreamArrowPythonGroupWindowAggregateFunctionOperator to support Pandas Stream Group Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="19404" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Stream Over Window Aggregation</summary>
      <description>We will add Stream Physical Pandas Over Window RelNode and StreamArrowPythonOverWindowAggregateFunctionOperator to support Pandas Stream Over Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="19406" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting row time to timestamp loses nullability info</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19410" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RestAPIStabilityTest does not assert on enum changes</summary>
      <description>org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest does not assert on enum changes as happened in FLINK-16866.The consequence is that the test is not failing even though there was a REST API change.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
    </fixedFiles>
  </bug>
  <bug id="19416" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python datetime object in from_collection of Python DataStream</summary>
      <description>Support Python datetime object in from_collection of Python DataStream</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug id="19421" opendate="2020-9-26 00:00:00" fixdate="2020-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDAF in streaming mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonAggregateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunctionInfo.java</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="19449" opendate="2020-9-29 00:00:00" fixdate="2020-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LEAD/LAG cannot work correctly in streaming mode</summary>
      <description></description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLocalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalGlobalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1945" opendate="2015-4-27 00:00:00" fixdate="2015-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make python tests less verbose</summary>
      <description>Currently, the python tests print a lot of log messages to stdout. Furthermore there seems to be some println statements which clutter the console output. I think that these log messages are not required for the tests and thus should be suppressed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.types.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.text.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.csv.py</file>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.PythonPlanBinderTest.java</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19450" opendate="2020-9-29 00:00:00" fixdate="2020-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the Python CI Test</summary>
      <description>Currently, the CI test of PyFlink will run 4 versions of Python, which takes a lot of time. We will optimize CI test to run only one version of Python. And then nightly test will run all versions of Python.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19492" opendate="2020-10-2 00:00:00" fixdate="2020-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate Source Events between Source API and Split Reader API</summary>
      <description>The Source API (flink-core) and the SplitReader API (flink-connector-base) have currently separate copies of events to request splits and to signal that no splits are available.We should consolidate those in flink-core.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.SplitRequestEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.NoSplitAvailableEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.FileSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.RequestSplitEvent.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.NoMoreSplitsEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="19498" opendate="2020-10-3 00:00:00" fixdate="2020-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port LocatableInputSplitAssigner to new File Source API</summary>
      <description>The new File Source API needs a locality aware input split assigner.To preserve the experience, I suggest to port the existing LocatableInputSplitAssigner from the InputFormat API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.FileSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="19523" opendate="2020-10-7 00:00:00" fixdate="2020-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide sensitive command-line configurations</summary>
      <description>When jobmanager starts up, it logs configurations from jvm options and application arguments. This includes secrets (eg. "s3.secret-key").Ideally, secrets should be masked as it is when loading from configuration file.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19527" opendate="2020-10-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Update SQL Pages</summary>
      <description>SQLGoal: Show users the main features early and link to concepts if necessary.How to use SQL? Intended for users with SQL knowledge. Overview Getting started with link to more detailed execution section. Full Reference Available operations in SQL as a table. This location allows to further split the page in the future if we think an operation needs more space without affecting the top-level structure. Data Definition Explain special SQL syntax around DDL. Pattern Matching Make pattern matching more visible. ... more features in the future</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.table.api.zh.md</file>
      <file type="M">docs.try-flink.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="19535" opendate="2020-10-8 00:00:00" fixdate="2020-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SourceCoordinator should avoid fail job multiple times.</summary>
      <description>Currently the SourceCoordinator may invoke SourceCoordinatorContext#failJob() multiple times from the same instance. This may cause the job to failover multiple times unnecessarily. The SourceCoordinator should instead just fail the job once.</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime context not initialized in RichWindowMapFunction</summary>
      <description>Trying to access the runtime context in a rich window map function results in an exception. The following snippet demonstrates the bug: env.generateSequence(0, 1000) .window(Count.of(10)) .mapWindow(new RichWindowMapFunction&lt;Long, Tuple2&lt;Long, Long&gt;&gt;() { @Override public void mapWindow(Iterable&lt;Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { long self = getRuntimeContext().getIndexOfThisSubtask(); for (long value : input) { out.collect(new Tuple2&lt;&gt;(self, value)); } } }).flatten().print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19569" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade ICU4J to 67.1+</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2020-10531</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19581" opendate="2020-10-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce OrcColumnarRowInputFormat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.ColumnarRowIterator.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileCompactionITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemFilterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFilters.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19586" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement StreamCommitterOperator for new Sink API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="19594" opendate="2020-10-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SubTasks start index don&amp;#39;t unified and may confuse users</summary>
      <description>In flink web ui page, subTasks index start from 0 in SubTasks tab while in BackPressure tab start from 1, at the same time the subTasks index start from 1 in Checkpoints page.I think this may confuse users and does there have some design purpose ?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19622" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Flinksql version 1.11 is for the NullPointerException of the Map type value value in the avro format</summary>
      <description>Hello, when I use flinksql version 1.11 to analyze data in avro format in Kafka, I used the Map type. For this type, my definition is MAP&lt;VARCHAR,VARCHAR&gt;, but the analysis is because the value in the map is empty. NullPointerExceptionThe source code is attached below (AvroRowDataDeserializationSchema)private static DeserializationRuntimeConverter createMapConverter(LogicalType type) { final DeserializationRuntimeConverter keyConverter = createConverter( DataTypes.STRING().getLogicalType()); final DeserializationRuntimeConverter valueConverter = createConverter( extractValueTypeToAvroMap(type)); return avroObject -&gt; { final Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) avroObject; Map&lt;Object, Object&gt; result = new HashMap&lt;&gt;(); for (Map.Entry&lt;?, ?&gt; entry : map.entrySet()) { Object key = keyConverter.convert(entry.getKey()); Object value = valueConverter.convert(entry.getValue()); result.put(key, value); } return new GenericMapData(result); };}  </description>
      <version>1.11.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="19624" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Update deadlock break-up algorithm to cover more cases</summary>
      <description>Current deadlock breakup algorithm fails to cover the following case: We're going to introduce a new deadlock breakup algorithm to cover this.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeVisitorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="19625" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce multi-input exec node</summary>
      <description>For multi-input to work in Blink planner, we should first introduce multi-input exec node in the planner.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19626" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce multi-input operator construction algorithm</summary>
      <description>We should introduce an algorithm to organize exec nodes into multi-input exec nodes.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.reuse.InputPriorityConflictResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.reuse.InputPriorityConflictResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs..includes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="19627" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce multi-input operator for batch</summary>
      <description>After the planner is ready for multi-input, we should introduce multi-input operator for batch.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TestingTwoInputStreamOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TestingOneInputStreamOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.output.OutputTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandlerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.UnionStreamOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.ExceptionInMultipleInputOperatorException.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSpec.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandler.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19629" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NullPointException when deserializing map field with null value for Avro format</summary>
      <description>create table tableA ( name STRING, hobly MAP&lt;STRING, STRING&gt;, phone STRING) with ( 'connector' = 'kafka-0.11', 'topic' = 'ShizcTest', 'properties.bootstrap.servers' = 'localhost:9092', 'properties.group.id' = 'ShizcTest', 'scan.startup.mode' = 'earliest-offset', 'format' = 'avro');if hobly have an null value like this:{"name": "shizc", "hobly": {"key1":null}, "phone": "1104564"}cause an NullPointException:java.io.IOException: Failed to deserialize Avro record. at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:150) at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:75) at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:81) at org.apache.flink.streaming.connectors.kafka.internals.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) at org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.runFetchLoop(Kafka010Fetcher.java:147) at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)Caused by: java.lang.NullPointerException: null at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createConverter$57e941b$5(AvroRowDataDeserializationSchema.java:252) at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createMapConverter$7941d275$1(AvroRowDataDeserializationSchema.java:315) at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createNullableConverter$c3bac5d8$1(AvroRowDataDeserializationSchema.java:221) at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createRowConverter$80d8b6bd$1(AvroRowDataDeserializationSchema.java:206) at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:148) ... 8 common frames omitted</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDataDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroToRowDataConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="19630" opendate="2020-10-14 00:00:00" fixdate="2020-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sink data in [ORC] format into Hive By using Legacy Table API caused unexpected Exception</summary>
      <description>ENV:Flink version 1.11.2Hive exec version: 2.0.1Hive file storing type ：ORCSQL or Datastream: SQL APIKafka Connector :  custom Kafka connector which is based on Legacy API (TableSource/`org.apache.flink.types.Row`)Hive Connector : totally follows the Flink-Hive-connector (we only made some encapsulation upon it)Using StreamingFileCommitter：YES  Description:   try to execute the following SQL:    """      insert into hive_table (select * from kafka_table)    """   HIVE Table SQL seems like:    """CREATE TABLE `hive_table`( // some fieldsPARTITIONED BY ( `dt` string, `hour` string)STORED AS orcTBLPROPERTIES ( 'orc.compress'='SNAPPY','type'='HIVE','sink.partition-commit.trigger'='process-time','sink.partition-commit.delay' = '1 h','sink.partition-commit.policy.kind' = 'metastore,success-file',)      """When this job starts to process snapshot, here comes the weird exception:As we can see from the message：Owner thread shall be the &amp;#91;Legacy Source Thread&amp;#93;, but actually the streamTaskThread which represents the whole first stage is found. So I checked the Thread dump at once.                                                                     The legacy Source Thread                                                                The StreamTask Thread    According to the thread dump info and the Exception Message, I searched and read certain source code and then DID A TEST    Since the Kafka connector is customed, I tried to make the KafkaSource a serpated stage by changing the TaskChainStrategy to Never. The task topology as follows: Fortunately, it did work! No Exception is throwed and Checkpoint could be snapshot successfully!  So, from my perspective, there shall be something wrong when HiveWritingTask and  LegacySourceTask chained together. the Legacy source task is a seperated thread, which may be the cause of the exception mentioned above.                                                                  </description>
      <version>1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="19639" opendate="2020-10-14 00:00:00" fixdate="2020-11-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support nested push down for SupportsProjectionPushDown interface in planner</summary>
      <description>Support to push nested projection into TableSourceScan. But it may cause name conflicts in some situaion. For example, we create the table with ddlCREATE TABLE NestedTable ( nest1 ROW&lt;a INT&gt;, nest2 ROW&lt;a INT&gt;)and with querySELECT nest1.a, nest2.a from NestedTableand we will get 2 `a` in the new schema when pushing projection. Here we use '_' to concatenate the names of all levels as the name of the nested fields. In this example, we will get fields nest1_a, nest2_a in the new schema.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeRewriterTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19650" opendate="2020-10-15 00:00:00" fixdate="2020-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the limit push down for the Jdbc connector</summary>
      <description>Currently the blink planner has already supported rule PushLimitIntoLegacyTableSourceScanRule. It's ready to add this feature for the jdbc connector.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.resources.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTablePlanTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.PostgresDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.MySQLDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.DerbyDialect.java</file>
    </fixedFiles>
  </bug>
  <bug id="19667" opendate="2020-10-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="19739" opendate="2020-10-20 00:00:00" fixdate="2020-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException when windowing in batch mode: A method named "replace" is not declared in any enclosing class nor any supertype</summary>
      <description>Example script:from pyflink.table import EnvironmentSettings, BatchTableEnvironmentfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_env = BatchTableEnvironment.create(environment_settings=env_settings)table_env.execute_sql( """ CREATE TABLE table1 ( amount INT, ts TIMESTAMP(3), WATERMARK FOR ts AS ts - INTERVAL '5' SECOND ) WITH ( 'connector.type' = 'filesystem', 'format.type' = 'csv', 'connector.path' = '/home/alex/work/test-flink/data1.csv' )""")table1 = table_env.from_path("table1")table = ( table1 .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())Output:WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil (file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release/* 1 *//* 2 */ public class LocalHashWinAggWithoutKeys$59 extends org.apache.flink.table.runtime.operators.TableStreamOperator/* 3 */ implements org.apache.flink.streaming.api.operators.OneInputStreamOperator, org.apache.flink.streaming.api.operators.BoundedOneInput {/* 4 *//* 5 */ private final Object[] references;/* 6 */ /* 7 */ private static final org.slf4j.Logger LOG$2 =/* 8 */ org.slf4j.LoggerFactory.getLogger("LocalHashWinAgg");/* 9 */ /* 10 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggMapKeyTypes$5;/* 11 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggBufferTypes$6;/* 12 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap aggregateMap$7;/* 13 */ org.apache.flink.table.data.binary.BinaryRowData emptyAggBuffer$9 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 14 */ org.apache.flink.table.data.writer.BinaryRowWriter emptyAggBufferWriterTerm$10 = new org.apache.flink.table.data.writer.BinaryRowWriter(emptyAggBuffer$9);/* 15 */ org.apache.flink.table.data.GenericRowData hashAggOutput = new org.apache.flink.table.data.GenericRowData(2);/* 16 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggMapKey$17 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 17 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggBuffer$18 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 18 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry reuseAggMapEntry$19 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry(reuseAggMapKey$17, reuseAggBuffer$18);/* 19 */ org.apache.flink.table.data.binary.BinaryRowData aggMapKey$3 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 20 */ org.apache.flink.table.data.writer.BinaryRowWriter aggMapKeyWriter$4 = new org.apache.flink.table.data.writer.BinaryRowWriter(aggMapKey$3);/* 21 */ private boolean hasInput = false;/* 22 */ org.apache.flink.streaming.runtime.streamrecord.StreamRecord element = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord((Object)null);/* 23 */ private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);/* 24 *//* 25 */ public LocalHashWinAggWithoutKeys$59(/* 26 */ Object[] references,/* 27 */ org.apache.flink.streaming.runtime.tasks.StreamTask task,/* 28 */ org.apache.flink.streaming.api.graph.StreamConfig config,/* 29 */ org.apache.flink.streaming.api.operators.Output output,/* 30 */ org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {/* 31 */ this.references = references;/* 32 */ aggMapKeyTypes$5 = (((org.apache.flink.table.types.logical.LogicalType[]) references[0]));/* 33 */ aggBufferTypes$6 = (((org.apache.flink.table.types.logical.LogicalType[]) references[1]));/* 34 */ this.setup(task, config, output);/* 35 */ if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {/* 36 */ ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)/* 37 */ .setProcessingTimeService(processingTimeService);/* 38 */ }/* 39 */ }/* 40 *//* 41 */ @Override/* 42 */ public void open() throws Exception {/* 43 */ super.open();/* 44 */ aggregateMap$7 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap(this.getContainingTask(),this.getContainingTask().getEnvironment().getMemoryManager(),computeMemorySize(), aggMapKeyTypes$5, aggBufferTypes$6);/* 45 */ /* 46 */ /* 47 */ emptyAggBufferWriterTerm$10.reset();/* 48 */ /* 49 */ /* 50 */ if (true) {/* 51 */ emptyAggBufferWriterTerm$10.setNullAt(0);/* 52 */ } else {/* 53 */ emptyAggBufferWriterTerm$10.writeInt(0, ((int) -1));/* 54 */ }/* 55 */ /* 56 */ emptyAggBufferWriterTerm$10.complete();/* 57 */ /* 58 */ }/* 59 *//* 60 */ @Override/* 61 */ public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {/* 62 */ org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();/* 63 */ /* 64 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 65 */ int field$11;/* 66 */ boolean isNull$11;/* 67 */ int field$12;/* 68 */ boolean isNull$12;/* 69 */ boolean isNull$13;/* 70 */ int result$14;/* 71 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 72 */ org.apache.flink.table.data.TimestampData field$21;/* 73 */ boolean isNull$21;/* 74 */ boolean isNull$22;/* 75 */ long result$23;/* 76 */ boolean isNull$24;/* 77 */ long result$25;/* 78 */ boolean isNull$26;/* 79 */ long result$27;/* 80 */ boolean isNull$28;/* 81 */ long result$29;/* 82 */ boolean isNull$30;/* 83 */ long result$31;/* 84 */ boolean isNull$32;/* 85 */ long result$33;/* 86 */ boolean isNull$34;/* 87 */ boolean result$35;/* 88 */ boolean isNull$36;/* 89 */ long result$37;/* 90 */ boolean isNull$38;/* 91 */ long result$39;/* 92 */ boolean isNull$40;/* 93 */ long result$41;/* 94 */ boolean isNull$42;/* 95 */ long result$43;/* 96 */ boolean isNull$44;/* 97 */ long result$45;/* 98 */ boolean isNull$46;/* 99 */ long result$47;/* 100 */ boolean isNull$48;/* 101 */ long result$49;/* 102 */ boolean isNull$50;/* 103 */ long result$51;/* 104 */ boolean isNull$52;/* 105 */ long result$53;/* 106 */ boolean isNull$55;/* 107 */ long result$56;/* 108 */ boolean isNull$57;/* 109 */ long result$58;/* 110 */ /* 111 */ /* 112 */ if (!in1.isNullAt(1)) {/* 113 */ hasInput = true;/* 114 */ // input field access for group key projection, window/pane assign/* 115 */ // and aggregate map update/* 116 */ isNull$11 = in1.isNullAt(0);/* 117 */ field$11 = -1;/* 118 */ if (!isNull$11) {/* 119 */ field$11 = in1.getInt(0);/* 120 */ }/* 121 */ isNull$21 = in1.isNullAt(1);/* 122 */ field$21 = null;/* 123 */ if (!isNull$21) {/* 124 */ field$21 = in1.getTimestamp(1, 3);/* 125 */ }/* 126 */ // assign timestamp(window or pane)/* 127 */ /* 128 */ /* 129 */ /* 130 */ /* 131 */ /* 132 */ isNull$22 = isNull$21;/* 133 */ result$23 = -1L;/* 134 */ if (!isNull$22) {/* 135 */ /* 136 */ result$23 = field$21.getMillisecond();/* 137 */ /* 138 */ }/* 139 */ /* 140 */ /* 141 */ isNull$24 = isNull$22 || false;/* 142 */ result$25 = -1L;/* 143 */ if (!isNull$24) {/* 144 */ /* 145 */ result$25 = (long) (result$23 * ((long) 1L));/* 146 */ /* 147 */ }/* 148 */ /* 149 */ isNull$26 = isNull$21;/* 150 */ result$27 = -1L;/* 151 */ if (!isNull$26) {/* 152 */ /* 153 */ result$27 = field$21.getMillisecond();/* 154 */ /* 155 */ }/* 156 */ /* 157 */ /* 158 */ isNull$28 = isNull$26 || false;/* 159 */ result$29 = -1L;/* 160 */ if (!isNull$28) {/* 161 */ /* 162 */ result$29 = (long) (result$27 * ((long) 1L));/* 163 */ /* 164 */ }/* 165 */ /* 166 */ /* 167 */ isNull$30 = isNull$28 || false;/* 168 */ result$31 = -1L;/* 169 */ if (!isNull$30) {/* 170 */ /* 171 */ result$31 = (long) (result$29 - ((long) 0L));/* 172 */ /* 173 */ }/* 174 */ /* 175 */ /* 176 */ isNull$32 = isNull$30 || false;/* 177 */ result$33 = -1L;/* 178 */ if (!isNull$32) {/* 179 */ /* 180 */ result$33 = (long) (result$31 % ((long) 432000000L));/* 181 */ /* 182 */ }/* 183 */ /* 184 */ /* 185 */ isNull$34 = isNull$32 || false;/* 186 */ result$35 = false;/* 187 */ if (!isNull$34) {/* 188 */ /* 189 */ result$35 = result$33 &lt; ((int) 0);/* 190 */ /* 191 */ }/* 192 */ /* 193 */ long result$54 = -1L;/* 194 */ boolean isNull$54;/* 195 */ if (result$35) {/* 196 */ /* 197 */ /* 198 */ /* 199 */ /* 200 */ /* 201 */ /* 202 */ isNull$36 = isNull$21;/* 203 */ result$37 = -1L;/* 204 */ if (!isNull$36) {/* 205 */ /* 206 */ result$37 = field$21.getMillisecond();/* 207 */ /* 208 */ }/* 209 */ /* 210 */ /* 211 */ isNull$38 = isNull$36 || false;/* 212 */ result$39 = -1L;/* 213 */ if (!isNull$38) {/* 214 */ /* 215 */ result$39 = (long) (result$37 * ((long) 1L));/* 216 */ /* 217 */ }/* 218 */ /* 219 */ /* 220 */ isNull$40 = isNull$38 || false;/* 221 */ result$41 = -1L;/* 222 */ if (!isNull$40) {/* 223 */ /* 224 */ result$41 = (long) (result$39 - ((long) 0L));/* 225 */ /* 226 */ }/* 227 */ /* 228 */ /* 229 */ isNull$42 = isNull$40 || false;/* 230 */ result$43 = -1L;/* 231 */ if (!isNull$42) {/* 232 */ /* 233 */ result$43 = (long) (result$41 % ((long) 432000000L));/* 234 */ /* 235 */ }/* 236 */ /* 237 */ /* 238 */ isNull$44 = isNull$42 || false;/* 239 */ result$45 = -1L;/* 240 */ if (!isNull$44) {/* 241 */ /* 242 */ result$45 = (long) (result$43 + ((long) 432000000L));/* 243 */ /* 244 */ }/* 245 */ /* 246 */ isNull$54 = isNull$44;/* 247 */ if (!isNull$54) {/* 248 */ result$54 = result$45;/* 249 */ }/* 250 */ }/* 251 */ else {/* 252 */ /* 253 */ /* 254 */ /* 255 */ /* 256 */ /* 257 */ isNull$46 = isNull$21;/* 258 */ result$47 = -1L;/* 259 */ if (!isNull$46) {/* 260 */ /* 261 */ result$47 = field$21.getMillisecond();/* 262 */ /* 263 */ }/* 264 */ /* 265 */ /* 266 */ isNull$48 = isNull$46 || false;/* 267 */ result$49 = -1L;/* 268 */ if (!isNull$48) {/* 269 */ /* 270 */ result$49 = (long) (result$47 * ((long) 1L));/* 271 */ /* 272 */ }/* 273 */ /* 274 */ /* 275 */ isNull$50 = isNull$48 || false;/* 276 */ result$51 = -1L;/* 277 */ if (!isNull$50) {/* 278 */ /* 279 */ result$51 = (long) (result$49 - ((long) 0L));/* 280 */ /* 281 */ }/* 282 */ /* 283 */ /* 284 */ isNull$52 = isNull$50 || false;/* 285 */ result$53 = -1L;/* 286 */ if (!isNull$52) {/* 287 */ /* 288 */ result$53 = (long) (result$51 % ((long) 432000000L));/* 289 */ /* 290 */ }/* 291 */ /* 292 */ isNull$54 = isNull$52;/* 293 */ if (!isNull$54) {/* 294 */ result$54 = result$53;/* 295 */ }/* 296 */ }/* 297 */ isNull$55 = isNull$24 || isNull$54;/* 298 */ result$56 = -1L;/* 299 */ if (!isNull$55) {/* 300 */ /* 301 */ result$56 = (long) (result$25 - result$54);/* 302 */ /* 303 */ }/* 304 */ /* 305 */ /* 306 */ isNull$57 = isNull$55 || false;/* 307 */ result$58 = -1L;/* 308 */ if (!isNull$57) {/* 309 */ /* 310 */ result$58 = (long) (result$56 - ((long) 0L));/* 311 */ /* 312 */ }/* 313 */ /* 314 */ // process each input/* 315 */ /* 316 */ // build aggregate map key/* 317 */ /* 318 */ /* 319 */ aggMapKeyWriter$4.reset();/* 320 */ /* 321 */ /* 322 */ if (false) {/* 323 */ aggMapKeyWriter$4.setNullAt(0);/* 324 */ } else {/* 325 */ aggMapKeyWriter$4.writeLong(0, result$58);/* 326 */ }/* 327 */ /* 328 */ aggMapKeyWriter$4.complete();/* 329 */ /* 330 */ // aggregate by each input with assigned timestamp/* 331 */ // look up output buffer using current key (grouping keys ..., assigned timestamp)/* 332 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 333 */ currentAggBuffer$8 = lookupInfo$20.getValue();/* 334 */ if (!lookupInfo$20.isFound()) {/* 335 */ /* 336 */ // append empty agg buffer into aggregate map for current group key/* 337 */ try {/* 338 */ currentAggBuffer$8 =/* 339 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 340 */ } catch (java.io.EOFException exp) {/* 341 */ /* 342 */ LOG$2.info("BytesHashMap out of memory with {} entries, output directly.", aggregateMap$7.getNumElements());/* 343 */ // hash map out of memory, output directly/* 344 */ /* 345 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 346 */ aggregateMap$7.getEntryIterator();/* 347 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 348 */ /* 349 */ /* 350 */ /* 351 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 352 */ /* 353 */ output.collect(outElement.replace(hashAggOutput));/* 354 */ }/* 355 */ /* 356 */ // retry append/* 357 */ /* 358 */ // reset aggregate map retry append/* 359 */ aggregateMap$7.reset();/* 360 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 361 */ try {/* 362 */ currentAggBuffer$8 =/* 363 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 364 */ } catch (java.io.EOFException e) {/* 365 */ throw new OutOfMemoryError("BytesHashMap Out of Memory.");/* 366 */ }/* 367 */ /* 368 */ /* 369 */ }/* 370 */ }/* 371 */ // aggregate buffer fields access/* 372 */ isNull$12 = currentAggBuffer$8.isNullAt(0);/* 373 */ field$12 = -1;/* 374 */ if (!isNull$12) {/* 375 */ field$12 = currentAggBuffer$8.getInt(0);/* 376 */ }/* 377 */ // do aggregate and update agg buffer/* 378 */ int result$16 = -1;/* 379 */ boolean isNull$16;/* 380 */ if (isNull$11) {/* 381 */ /* 382 */ isNull$16 = isNull$12;/* 383 */ if (!isNull$16) {/* 384 */ result$16 = field$12;/* 385 */ }/* 386 */ }/* 387 */ else {/* 388 */ int result$15 = -1;/* 389 */ boolean isNull$15;/* 390 */ if (isNull$12) {/* 391 */ /* 392 */ isNull$15 = isNull$11;/* 393 */ if (!isNull$15) {/* 394 */ result$15 = field$11;/* 395 */ }/* 396 */ }/* 397 */ else {/* 398 */ /* 399 */ /* 400 */ /* 401 */ isNull$13 = isNull$12 || isNull$11;/* 402 */ result$14 = -1;/* 403 */ if (!isNull$13) {/* 404 */ /* 405 */ result$14 = (int) (field$12 + field$11);/* 406 */ /* 407 */ }/* 408 */ /* 409 */ isNull$15 = isNull$13;/* 410 */ if (!isNull$15) {/* 411 */ result$15 = result$14;/* 412 */ }/* 413 */ }/* 414 */ isNull$16 = isNull$15;/* 415 */ if (!isNull$16) {/* 416 */ result$16 = result$15;/* 417 */ }/* 418 */ }/* 419 */ if (isNull$16) {/* 420 */ currentAggBuffer$8.setNullAt(0);/* 421 */ } else {/* 422 */ currentAggBuffer$8.setInt(0, result$16);/* 423 */ }/* 424 */ /* 425 */ }/* 426 */ }/* 427 *//* 428 */ /* 429 */ @Override/* 430 */ public void endInput() throws Exception {/* 431 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 432 */ int field$11;/* 433 */ boolean isNull$11;/* 434 */ int field$12;/* 435 */ boolean isNull$12;/* 436 */ boolean isNull$13;/* 437 */ int result$14;/* 438 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 439 */ org.apache.flink.table.data.TimestampData field$21;/* 440 */ boolean isNull$21;/* 441 */ boolean isNull$22;/* 442 */ long result$23;/* 443 */ boolean isNull$24;/* 444 */ long result$25;/* 445 */ boolean isNull$26;/* 446 */ long result$27;/* 447 */ boolean isNull$28;/* 448 */ long result$29;/* 449 */ boolean isNull$30;/* 450 */ long result$31;/* 451 */ boolean isNull$32;/* 452 */ long result$33;/* 453 */ boolean isNull$34;/* 454 */ boolean result$35;/* 455 */ boolean isNull$36;/* 456 */ long result$37;/* 457 */ boolean isNull$38;/* 458 */ long result$39;/* 459 */ boolean isNull$40;/* 460 */ long result$41;/* 461 */ boolean isNull$42;/* 462 */ long result$43;/* 463 */ boolean isNull$44;/* 464 */ long result$45;/* 465 */ boolean isNull$46;/* 466 */ long result$47;/* 467 */ boolean isNull$48;/* 468 */ long result$49;/* 469 */ boolean isNull$50;/* 470 */ long result$51;/* 471 */ boolean isNull$52;/* 472 */ long result$53;/* 473 */ boolean isNull$55;/* 474 */ long result$56;/* 475 */ boolean isNull$57;/* 476 */ long result$58;/* 477 */ /* 478 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 479 */ aggregateMap$7.getEntryIterator();/* 480 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 481 */ /* 482 */ /* 483 */ /* 484 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 485 */ /* 486 */ output.collect(outElement.replace(hashAggOutput));/* 487 */ }/* 488 */ /* 489 */ }/* 490 */ /* 491 *//* 492 */ @Override/* 493 */ public void close() throws Exception {/* 494 */ super.close();/* 495 */ aggregateMap$7.free();/* 496 */ /* 497 */ }/* 498 *//* 499 */ /* 500 */ }/* 501 */ Traceback (most recent call last): File "/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py", line 32, in &lt;module&gt; print(table.to_pandas()) File "/home/alex/work/flink/flink-python/pyflink/table/table.py", line 829, in to_pandas if batches.hasNext(): File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py", line 1285, in __call__ return_value = get_return_value( File "/home/alex/work/flink/flink-python/pyflink/util/exceptions.py", line 147, in deco return f(*a, **kw) File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py", line 326, in get_return_value raise Py4JJavaError(py4j.protocol.Py4JJavaError: An error occurred while calling o51.hasNext.: java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) at org.apache.flink.table.runtime.arrow.ArrowUtils$1.hasNext(ArrowUtils.java:644) at org.apache.flink.table.runtime.arrow.ArrowUtils$2.hasNext(ArrowUtils.java:666) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) ... 16 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172) ... 18 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680) at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658) at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117) ... 19 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:413) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'LocalHashWinAggWithoutKeys$59' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:613) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:583) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:574) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:164) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65) ... 13 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 15 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 18 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 351, Column 33: A method named "replace" is not declared in any enclosing class nor any supertype, nor through a static import at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1842) at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1498) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3052) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3136) at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2966) at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2770) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2742) at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1499) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$TryStatement.accept(Java.java:3238) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 24 moreHowever it works fine in streaming mode:env_settings = ( EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build())table_env = StreamTableEnvironment.create(environment_settings=env_settings)How the table is created seems irrelevant - this raises the same error:from datetime import datetimefrom pyflink.table import DataTypes, BatchTableEnvironment, EnvironmentSettingsfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_environment = BatchTableEnvironment.create(environment_settings=env_settings)transactions = table_environment.from_elements( [ (1, datetime(2000, 1, 1, 0, 0, 0)), (-2, datetime(2000, 1, 2, 0, 0, 0)), (3, datetime(2000, 1, 3, 0, 0, 0)), (-4, datetime(2000, 1, 4, 0, 0, 0)), ], DataTypes.ROW( [ DataTypes.FIELD("amount", DataTypes.BIGINT()), DataTypes.FIELD("ts", DataTypes.TIMESTAMP(3)), ] ),)table = ( transactions .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19750" opendate="2020-10-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19762" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Selecting Job-ID and TaskManager-ID in web UI covers more than the ID</summary>
      <description>Not only the ID is selected when trying to copy the Job ID from the web UI by double-clicking it. See the attached screenshot: The same thing happens for the TaskManager ID in the corresponding TaskManager Overview page.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19764" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add More Metrics to TaskManager in Web UI</summary>
      <description>update the UI since https://issues.apache.org/jira/browse/FLINK-14422 and https://issues.apache.org/jira/browse/FLINK-14406 has been fixed</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="19767" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractSlotPoolFactory</summary>
      <description>FLINK-19314 introduces another slot pool implementation, with the factory for it being a carbon copy of the factory for the existing slot pool factory.We can introduce an abstract factory class to reduce code duplication.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="1977" opendate="2015-5-6 00:00:00" fixdate="2015-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework Stream Operators to always be push based</summary>
      <description>This is a result of the discussion on the mailing list. This is an excerpt from the mailing list that gives the basic idea of the change:I propose to change all streaming operators to be push based, with aslightly improved interface: In addition to collect(), which I wouldcall receiveElement() I would add receivePunctuation() andreceiveBarrier(). The first operator in the chain would also get datafrom the outside invokable that reads from the input iterator andcalls receiveElement() for the first operator in a chain.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamProject.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureStreamingRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointingITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.test.java.org.apache.flink.streaming.examples.test.windowing.TopSpeedWindowingExampleITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.StockPrices.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowingExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.StockPrices.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.SessionWindowing.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.ml.IncrementalLearningSkeleton.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.join.WindowJoin.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.iteration.IterateExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCollector.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.TypeFillTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamtask.StreamVertexTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.SourceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowPartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowMergerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowMapperTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowFolderTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowFlattenerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.GroupedReduceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.complex.ComplexIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.StreamCollectorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTaskContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OutputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.InputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.CoStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowPartitioner.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMerger.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFlattener.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedStreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedActiveDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamReduce.java</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeTopology.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.KafkaSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.persistent.PersistentKafkaSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.CollectorWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.StreamOutput.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DiscretizedStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.GroupedDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.IterativeDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.sink.FileSinkFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileSourceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.GenSequenceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.SourceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.WindowingOptimizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.ChainableStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamWindow.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamCounter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFilter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19773" opendate="2020-10-22 00:00:00" fixdate="2020-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exponential backoff restart strategy</summary>
      <description>There are situations where the current restart strategies (fixed-delay and failure-rate) seem to be suboptimal.  For example, in HDFS sinks, a delay between restarts shorter than the lease expiration time in HDFS is going to result in many restart attempts which fail, putting somewhat pointless stress on a cluster.  On the other hand, setting a delay of close to the lease expiration time will mean far more downtime than necessary when the cause of failure is something that works itself out quickly. An exponential backoff restart strategy would address this.  For example a backoff strategy where the jobs are contending for a lease on a shared resource that terminates after 1200 seconds of inactivity might have successive delays of 1, 2, 4, 8, 16... 1024 seconds (after which a cumulative delay of more than 1200 seconds has passed).While not intrinsically tied to exponential backoff (it's more of an example of variable delay), in the case of many jobs failing due to an infrastructure failure, a thundering herd scenario can be mitigated by adding jitter to the delays, e.g. 0 -&gt; 1 -&gt; 2 -&gt; 3/4/5 -&gt; 5/6/7/8/9/10/11 seconds.  With this jitter, eventually a set of jobs competing to restart will spread out.(logging the ticket more to start a discussion and perhaps get context around if this had been considered and rejected, etc.)</description>
      <version>1.11.2</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategyFactoryLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategyFactoryLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.restartstrategy.RestartStrategies.java</file>
      <file type="M">docs..includes.generated.restart.strategy.configuration.html</file>
      <file type="M">docs.dev.task.failure.recovery.md</file>
    </fixedFiles>
  </bug>
  <bug id="19781" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons_codec to 1.13 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons_codec &amp;#91;1&amp;#93;. We should try to upgrade this version to 1.13 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="19782" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr to 4.7.1 or newer</summary>
      <description>A user reported dependency vulnerabilities which affect antlr &amp;#91;1&amp;#93;. We should upgrade this dependency to 4.7.1 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19783" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade mesos to 1.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects mesos &amp;#91;1&amp;#93;. We should upgrade mesos to 1.7.0 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19785" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons-io &amp;#91;1&amp;#93;. We should try to upgrade this dependency to 2.7 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="19789" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Hive connector to new table source sink interface</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19790" opendate="2020-10-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Writing MAP&lt;STRING, STRING&gt; to Kafka with JSON format produces incorrect data.</summary>
      <description>Running the following SQL script writes incorrect data to Kafka:CREATE TEMPORARY TABLE tmp_1 (m MAP&lt;String, String&gt;) WITH ( 'connector' = 'kafka', 'format' = 'json', 'properties.bootstrap.servers' = '...', 'properties.group.id' = '...', 'topic' = 'tmp-1');CREATE TEMPORARY TABLE gen (k STRING, v STRING) WITH ( 'connector' = 'datagen');CREATE TEMPORARY VIEW gen_short ASSELECT SUBSTR(k, 0, 4) AS k, SUBSTR(v, 0, 4) AS v FROM gen;INSERT INTO tmp_1SELECT MAP[k, v] FROM gen_short; Printing the content of the tmp-1 topics results in the following output:$ kafka-console-consumer --bootstrap-server ... --from-beginning --topic tmp-1 | head -n 5{"m":{"8a93":"6102"}}{"m":{"8a93":"6102","7922":"f737"}}{"m":{"8a93":"6102","7922":"f737","9b63":"15b0"}}{"m":{"8a93":"6102","7922":"f737","9b63":"15b0","c38b":"b55c"}}{"m":{"8a93":"6102","7922":"f737","9b63":"15b0","c38b":"b55c","222c":"f3e2"}}As you can see, the map is not correctly encoded as JSON and written to Kafka.I've run the query with the Blink planner with object reuse and operator pipelining disabled.Writing with Avro works as expected.Hence I assume that the JSON encoder/serializer reuses the Map object when encoding the JSON.   </description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.RowDataToJsonConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="19796" opendate="2020-10-25 00:00:00" fixdate="2020-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error results when use constant decimal array</summary>
      <description>The result is wrong when use constant decimal array with different precisions. Here is an example:create table print_sink( data array&lt;decimal(3,2)&gt;) with ( 'connector' = 'print');insert into print_sinkselect array[0.12, 0.5, 0.99];The result will be:+I([0.12, 0.05, 0.99])</description>
      <version>1.11.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="1980" opendate="2015-5-6 00:00:00" fixdate="2015-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allowing users to decorate input streams</summary>
      <description>Users may have to do unforeseeable operations on file input streams before they can be used by the actual input format logic, e.g., exotic compression formats or preambles such as byte order marks. Therefore, it would be useful to provide the user with a hook to decorate input streams in order to handle such issues.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.FileInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.InflaterInputStreamFSInputWrapper.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1982" opendate="2015-5-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependencies on Record for Flink runtime and core</summary>
      <description>Seemed like there are several uses of Record API in core and runtime module that need to be updated before Record API could be removed.</description>
      <version>None</version>
      <fixedVersion>0.10.0,1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MapTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19823" opendate="2020-10-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Filesystem and Hive connector with changelog format (e.g. debezium-json)</summary>
      <description>Once Filesystem and Hive have been migrated to FLIP-95 interfaces, it should be possible to integrate with changelog format (e.g. debezium-json and canal-json) in source and sink. We should add tests for that.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowDataPartitionComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.AbstractFileSystemTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.FsStreamingSinkTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.FileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19878" opendate="2020-10-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix WatermarkAssigner shouldn&amp;#39;t be after ChangelogNormalize</summary>
      <description>Cutrrently, for a upsertSource like upsert-kafka, the WatermarkAssigner is followed after ChangelogNormalize Node,  it may returns Long.MaxValue as watermark if some parallelism doesn't have data.   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UA,D]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime], changelogMode=[I,UA,D]) +- ChangelogNormalize(key=[currency], changelogMode=[I,UA,D]) +- Exchange(distribution=[hash[currency]], changelogMode=[UA,D]) +- TableSourceScan(table=[[default_catalog, default_database, rates_history]], fields=[currency, rate, rowtime], changelogMode=[UA,D]) As an improvement, we can move the WatermarkAssigner to be after the SourceCan Node and thus the watermark will produce like general Source. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19885" opendate="2020-10-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Extend the JobManager&amp;#39;s web UI in accordance to FLIP-104</summary>
      <description>See FLIP-104 for more details on the UI.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug id="19886" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate file compaction to Hive connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactBulkReader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FileCompactionITCaseBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19888" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Hive source to FLIP-27 source interface for streaming</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionFetcher.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.ContinuousPartitionFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousPartitionFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousMonitoringFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.PendingSplitsCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.ContinuousEnumerationSettings.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.AbstractFileSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="19908" opendate="2020-10-31 00:00:00" fixdate="2020-11-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan should respect source reuse config option</summary>
      <description>Currently we have the table.optimizer.reuse-sub-plan-enabled config option to configure the reuse of sources. However FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan do not respect this option and are always reused.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19949" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unescape CSV format line and field delimiter character</summary>
      <description>We should unescape the line delimiter characters first because the DDL can be read from a file. So that the new line "\n" in the DDL options was recognized as 2 characters.While what user want is actually the invisible new line character.create table t1( ...) with ( 'format' = 'csv', 'csv.line-delimiter' = '\n' ...)Same as the field delimiter.</description>
      <version>1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19959" opendate="2020-11-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple input creation algorithm will deduce an incorrect input order if the inputs are related under PIPELINED shuffle mode</summary>
      <description>Consider the following SQLWITH T1 AS (SELECT x.a AS a, y.d AS b FROM y LEFT JOIN x ON y.d = x.a), T2 AS (SELECT a, b FROM (SELECT a, b FROM T1) UNION ALL (SELECT x.a AS a, x.b AS b FROM x))SELECT * FROM T2 LEFT JOIN t ON T2.a = t.aThe multiple input creation algorithm will currently deduce the following plan under the PIPELINED shuffle mode:MultipleInputNode(readOrder=[0,1,1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, b, a0, b0, c], build=[right])\n:- Union(all=[true], union=[a, b])\n: :- Calc(select=[a, CAST(d) AS b])\n: : +- NestedLoopJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, a], build=[right])\n: : :- [#3] Calc(select=[d])\n: : +- [#4] Exchange(distribution=[broadcast])\n: +- [#2] Calc(select=[a, b])\n+- [#1] Exchange(distribution=[broadcast])\n]):- Exchange(distribution=[broadcast]): +- BoundedStreamScan(table=[[default_catalog, default_database, t]], fields=[a, b, c]):- Calc(select=[a, b]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx], reuse_id=[1]):- Calc(select=[d]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny])+- Exchange(distribution=[broadcast]) +- Calc(select=[a]) +- Reused(reference_id=[1])It's obvious that the 2nd and the 4th input for the multiple input node should have the same input priority, otherwise a deadlock will occur.This is because the current algorithm fails to consider the case when the inputs are related out of the multiple input node.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.TopologyGraphTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.TopologyGraph.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19971" opendate="2020-11-4 00:00:00" fixdate="2020-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-hbase-base depends on hbase-server, instead of hbase-client</summary>
      <description>flink-connector-hbase-base depends on hbase-server, instead of hbase-client and hbase-common.I believe hbase-server is only needed for starting a Minicluster and therefore should be in the test scope.This was introduced by FLINK-1928.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19972" opendate="2020-11-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide more details when type serializers are not compatible</summary>
      <description>Currently, when the type serializer is incompatible, you get exceptions like these:StateMigrationException("For heap backends, the new namespace serializer must be compatible.");StateMigrationException("The new namespace serializer must be compatible.");StateMigrationException("For heap backends, the new state serializer must not be incompatible.");StateMigrationException("The new state serializer cannot be incompatible.")StateMigrationException("The new key serializer must be compatible.")which are not really helpful to the user in debugging serializers.Since we already have the old serializer (snapshot) and the new one available, we should add this detail to the exceptions for improved usability.</description>
      <version>1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.AbstractRocksDBRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="19998" opendate="2020-11-5 00:00:00" fixdate="2020-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Links in docs</summary>
      <description>Multiple broken links matching pattern: site.baseurl }}{% linkAll these need to be replaced. Eg:In the docs/concepts/stateful-stream-processing.md file, under the first section (What is State), the following two links are broken: Checkpoints: &amp;#91;checkpoints&amp;#93;({{ site.baseurl}}{% link dev/stream/state/checkpointing.md %}) Savepoints: &amp;#91;savepoints&amp;#93;({{ site.baseurl }}{%link ops/state/savepoints.md %})This results in the target link as follows: For Checkpoints: https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/checkpointing.html https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/ops/state/savepoints.html</description>
      <version>1.11.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.parallel.zh.md</file>
      <file type="M">docs.dev.java.lambdas.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="2000" opendate="2015-5-11 00:00:00" fixdate="2015-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL-style aggregations for Table API</summary>
      <description>Right now, the syntax for aggregations is "a.count, a.min" and so on. We could in addition offer "COUNT(a), MIN(a)" and so on.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20018" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pipeline.cached-files option cannot escape &amp;#39;:&amp;#39; in path</summary>
      <description>pipeline.cached-files option cannot escape ':' in pathe.g. When setting it to `name:file1,path:oss://bucket/file1` The path of file1 is parsed as oss. And there are no way to escape the ':' in path.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.StructuredOptionsSplitterTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.StructuredOptionsSplitter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.cache.DistributedCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="2002" opendate="2015-5-12 00:00:00" fixdate="2015-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Iterative test fails when ran with other tests in the same environment</summary>
      <description>I run tests in the same StreamExecutionEnvironment with MultipleProgramsTestBase. One of the tests uses an iterative data stream. It fails as well as all tests after that. (When I put the iterative test in a separate environment, all tests passes.) For me it seems that it is a state-related issue but there is also some problem with the broker slots.The iterative test throws:java.lang.Exception: TaskManager sent illegal state update: CANCELING at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:618) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply$mcV$sp(JobManager.scala:222) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Task to schedule: &lt; Attempt #0 (GroupedActiveDiscretizer (2/4)) @ (unassigned) - &amp;#91;SCHEDULED&amp;#93; &gt; with groupID &lt; e8f7c9c85e64403962648bc7e2aead8b &gt; in sharing group &lt; SlotSharingGroup &amp;#91;5e62f1cc5cae2c088430ef935470a8d5, 5bc227941969d1daa1ebb1ba070b55ce, d999ee6c10730775a8fef1c6f1af1dbd, 45b73caa75424d84adbb7bb92671591d, 5c94c54d9316b827c6eba6c721329549, 794d6c56bee347dcdd62ffdf189de267, 4c3b72e17a4acecde4241fe6e63355b8, f6a6028c224a7b81e4802eeaf9c8487e, 989c68790fc7c5e2f8b8c150a33fef89, db93daa1f9e5194f0079df2629b08efb, bf7dbb1fd756ce322249eb973844b375, 9ddf3bd146c21c574077c58a1f64aeaa, e888ff4653070b9c4adcbb22a8121292, 9c620fd6d784bc4f5d7e100ad1dcb442, e8f7c9c85e64403962648bc7e2aead8b, 4fa798b9eab295876fdd21aeb6c7cfec, 32851c5f48ac128f71df0ec76f5b5ccd, c3f65a51704444b676cd392fbda91872&amp;#93; &gt;. Resources available to scheduler: Number of instances=1, total number of slots=1, available slots=0 at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:212) at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:110) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:263) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:437) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:306) at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:447) at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:580) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:194) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ... 2 morewhile the following tests throw:java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:192) at org.apache.flink.runtime.taskmanager.TaskManager.org$apache$flink$runtime$taskmanager$TaskManager$$initializeTask(TaskManager.scala:855) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply$mcV$sp(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.registerInputOutput(StreamTask.java:86) at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:189) ... 12 moreCaused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39) at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62) ... 14 more</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.complex.ComplexIntegrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20054" opendate="2020-11-9 00:00:00" fixdate="2020-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>3 Level List is not supported in ParquetInputFormat</summary>
      <description>In flink-parquet module doesn't supportreading a 3-level list type in parquet though it is able to process a2-level list type.3-leveloptional group my_list (LIST) { repeated group element { required binary str (UTF8); };} 2-leveloptional group my_list (LIST) { repeated int32 element;}</description>
      <version>1.11.2</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.TestUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.ParquetRecordReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetTableSourceTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetTableSourceITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetPojoInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetMapInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.RowConverter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20059" opendate="2020-11-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Outdated SQL docs on aggregate functions&amp;#39; merge</summary>
      <description>In the java docs as well as the user docs, the merge method of an aggregation UDF is described as optional, e.g.Merges a group of accumulator instances into one accumulator instance. This function must be implemented for data stream session window grouping aggregates and data set grouping aggregates.However, it seems that nowadays this method is required in more cases (I stumbled on this for a HOP window in streaming):StreamExecGlobalGroupAggregate.scala .needMerge(mergedAccOffset, mergedAccOnHeap, mergedAccExternalTypes)StreamExecGroupWindowAggregateBase.scala generator.needMerge(mergedAccOffset = 0, mergedAccOnHeap = false)StreamExecIncrementalGroupAggregate.scala .needMerge(mergedAccOffset, mergedAccOnHeap = true, mergedAccExternalTypes)StreamExecLocalGroupAggregate.scala .needMerge(mergedAccOffset = 0, mergedAccOnHeap = true)</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AggregateFunction.java</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="20102" opendate="2020-11-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase connector documentation for HBase 2.x supporting</summary>
      <description>Currently, the HBase connector page says it only supports HBase 1.4.x.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
      <file type="M">docs..data.sql-connectors.yml</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="20130" opendate="2020-11-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ZStandard format to inputs</summary>
      <description>Allow Flink to read files compressed in ZStandard (.zst)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.GenericCsvInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">docs.content.zh.docs.dev.dataset.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="20144" opendate="2020-11-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change link format to "{% link ... %}" in docs/ops dir</summary>
      <description>Some documents' link format in docs/ops is "{{site.baseurl}/... }". But it is preferred to use "{% link ... %}" format.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20183" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the default PYTHONPATH is overwritten in client side</summary>
      <description></description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20184" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="20195" opendate="2020-11-17 00:00:00" fixdate="2020-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jobs endpoint returns duplicated jobs</summary>
      <description>The GET /jobs endpoint can, for a split second, return a duplicated job after it has been cancelled. This occurred in Ververica Platform after canceling a job (using PATCH /jobs/{jobId}) and calling GET /jobs.I've reproduced this and queried the endpoint in a relatively tight loop (~ every 0.5s) to log the responses of GET /jobs and got this:  …{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"RUNNING"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELLING"}]}{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"RUNNING"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELLING"}]}{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"}]}{"jobs":[{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"}]}{"jobs":[{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"}]}… You can see in in between that for just a moment, the endpoint returned the same Job ID twice. </description>
      <version>1.11.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="20200" opendate="2020-11-18 00:00:00" fixdate="2020-1-18 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>SQL Hints are not supported in "Create View" syntax</summary>
      <description>I have aready set the config option `table.dynamic-table-options.enabled` to be true, but "SQL Hints" are not supported in View syntax. I got an error:Exception in thread "main" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.SqlSyntax$6: SPECIAL at org.apache.calcite.util.Util.needToImplement(Util.java:967) at org.apache.calcite.sql.SqlSyntax$6.unparse(SqlSyntax.java:116) at org.apache.calcite.sql.SqlOperator.unparse(SqlOperator.java:333) at org.apache.calcite.sql.SqlDialect.unparseCall(SqlDialect.java:470) at org.apache.calcite.sql.SqlCall.unparse(SqlCall.java:101) at org.apache.calcite.sql.SqlSelectOperator.unparse(SqlSelectOperator.java:176) at org.apache.calcite.sql.SqlDialect.unparseCall(SqlDialect.java:470) at org.apache.calcite.sql.SqlSelect.unparse(SqlSelect.java:246) at org.apache.calcite.sql.SqlNode.toSqlString(SqlNode.java:151) at org.apache.calcite.sql.SqlNode.toSqlString(SqlNode.java:173) at org.apache.calcite.sql.SqlNode.toSqlString(SqlNode.java:182) at org.apache.flink.table.planner.operations.SqlToOperationConverter.getQuotedSqlString(SqlToOperationConverter.java:784) at org.apache.flink.table.planner.utils.Expander$Expanded.substitute(Expander.java:169) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertViewQuery(SqlToOperationConverter.java:694) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateView(SqlToOperationConverter.java:665) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:228) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)The sql code is as follows：drop table if exists SourceA;create table SourceA ( id string, name string) with ( 'connector' = 'kafka-0.11', 'topic' = 'MyTopic', 'properties.bootstrap.servers' = 'localhost:9092', 'properties.group.id' = 'Test', 'scan.startup.mode' = 'group-offsets', 'format' = 'csv');drop table if exists print;create table print ( id string, name string) with ( 'connector' = 'print');drop view if exists test_view;create view test_view asselect *from SourceA /*+ OPTIONS('properties.group.id'='NewGroup') */;insert into printselect * from test_view;</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20202" opendate="2020-11-18 00:00:00" fixdate="2020-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add check of unsupported result type in Pandas UDAF</summary>
      <description>Currently the return type of Pandas UDAF should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64. Any should ideally be a specific scalar type accordingly. We will add related DataType check and throw a more readable exception for unsupported DataTypes. What's more, we will add related notes in docs.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="20209" opendate="2020-11-18 00:00:00" fixdate="2020-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing checkpoint configuration to Flink UI</summary>
      <description>Some of the checkpointing configurations are not shown in the Flink UI，Can we consider adding these configurations to the checkpoints configuration tab to make it easier for users to view checkpoint configurations.These configurations need to be added：execution.checkpointing.prefer-checkpoint-for-recoveryexecution.checkpointing.tolerable-failed-checkpointsexecution.checkpointing.unaligned </description>
      <version>1.11.2</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="20213" opendate="2020-11-18 00:00:00" fixdate="2020-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition commit is delayed when records keep coming</summary>
      <description>When set partition-commit.delay=0, Users expect partitions to be committed immediately.However, if the record of this partition continues to flow in, the bucket for the partition will be activated, and no inactive bucket will appear.We need to consider listening to bucket created.</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.3,1.12.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.stream.StreamingFileWriterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactFileWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.AbstractStreamingWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20245" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document how to create a Hive catalog from DDL</summary>
      <description>I'd appreciate if the documentation contained a description how to create the hive catalog from DDL. What I am missing especially are the options that HiveCatalog expects (type, conf-dir). We should have a table somewhere with a description possible values etc. the same way as we have such tables for other connectors and formats. See e.g. https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20258" opendate="2020-11-20 00:00:00" fixdate="2020-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configured memory sizes on the JM metrics page should be displayed with proper units.</summary>
      <description>Configured memory sizes for JM are displayed in bytes. It would be better to use proper units here, same as what we do for TM. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.pipe.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="20259" opendate="2020-11-20 00:00:00" fixdate="2020-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explanation that "configured values" for JM/TM memory sizes include automatically derived values</summary>
      <description>The column title `Configured Values` might be a bit confusing. Not all of the values are explicitly configured by users. There could be values that are automatically derived from users' configuration. I would suggest to add a bit explanation (e.g., a  with some hidden texts) for both JM and TM.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="2026" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error message in count() only jobs</summary>
      <description>If I run a job that only calls count() on a dataset (which is a valid data flow IMHO), Flink executes the job but complains that no sinks are defined.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="20275" opendate="2020-11-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment should be ;</summary>
      <description>Currently, the path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment is ",", this would cause the rest client fail to upload the specified jars and stuck forever without errors. It should be ";" instead.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="20359" opendate="2020-11-25 00:00:00" fixdate="2020-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support adding Owner Reference to Job Manager in native kubernetes setup</summary>
      <description>Flink implementation is often a part of the larger application. As a result a synchronized management - clean up of Flink resources, when a main application is deleted is important. In Kubernetes, a common approach for such clean up is usage of the owner's reference (https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/)Adding owner reference support to Flink Job manager would be a nice addition to Flink kubernetes native support to accommodate such use cases </description>
      <version>1.11.2</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20365" opendate="2020-11-26 00:00:00" fixdate="2020-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The native k8s cluster could not be unregistered when executing Python DataStream application attachedly.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
      <file type="M">flink-python.pyflink.util.exceptions.py</file>
    </fixedFiles>
  </bug>
  <bug id="20387" opendate="2020-11-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support defining event time attribute on TIMESTAMP_LTZ column</summary>
      <description>Currently, only TIMESTAMP type can be used as rowtime attribute. Would be better to support TIMESTAMP WITH LOCAL ZONE TIME as well.As a workaround, users can cast the TIMESTAMP WITH LOCAL ZONE TIME into TIMESTAMP, CAST(ts AS TIMESTAMP).</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.TimeWindowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.Trigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.ProcessingTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.EventTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.combines.WindowCombineFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.processors.WindowRankProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.combines.TopNRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.GlobalAggAccCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.AggRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.WindowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.RecordsWindowBuffer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowRankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateUseDaylightTimeHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Schema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerRowtimeAttribute.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanAcrossCalcRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.GroupWindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TemporalSortJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.WindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
    </fixedFiles>
  </bug>
  <bug id="20413" opendate="2020-11-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sources should add splits back in "resetSubtask()", rather than in "subtaskFailed()".</summary>
      <description>Because "subtaskFailed()" has no strong order guarantees with checkpoint completion, we need to return failed splits in "resetSubtask()" instead.See FLINK-20396 for a detailed explanation of the race condition.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SplitAssignmentTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SplitAssignmentTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20417" opendate="2020-11-30 00:00:00" fixdate="2020-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle "Too old resource version" exception in Kubernetes watch more gracefully</summary>
      <description>Currently, when the watcher(pods watcher, configmap watcher) is closed with exception, we will call WatchCallbackHandler#handleFatalError. And this could cause JobManager terminating and then failover.For most cases, this is correct. But not for "too old resource version" exception. See more information here&amp;#91;1&amp;#93;. Usually this exception could happen when the APIServer is restarted. And we just need to create a new watch and continue to do the pods/configmap watching. This could help the Flink cluster reducing the impact of K8s cluster restarting. The issue is inspired by this technical article&amp;#91;2&amp;#93;. Thanks the guys from tencent for the debugging. Note this is a Chinese documentation. &amp;#91;1&amp;#93;. https://stackoverflow.com/questions/61409596/kubernetes-too-old-resource-version&amp;#91;2&amp;#93;. https://cloud.tencent.com/developer/article/1731416</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.NoOpWatchCallbackHandler.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20496" opendate="2020-12-5 00:00:00" fixdate="2020-3-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>RocksDB partitioned index filter option</summary>
      <description>When using RocksDBStateBackend and enabling state.backend.rocksdb.memory.managed and state.backend.rocksdb.memory.fixed-per-slot, flink will strictly limited rocksdb memory usage which contains "write buffer" and "block cache". With these options rocksdb stores index and filters in block cache, because in default options index/filters can grows unlimited. But it's lead another issue, if high-priority cache(configure by state.backend.rocksdb.memory.high-prio-pool-ratio) can't fit all index/filters blocks, it will load all metadata from disk when cache missed, and program went extremely slow. According to Partitioned Index Filters[1], we can enable two-level index having acceptable performance when index/filters cache missed. Enable these options can get over 10x faster in my case&amp;#91;2&amp;#93;, I think we can add an option state.backend.rocksdb.partitioned-index-filters and default value is false, so we can use this feature easily.&amp;#91;1&amp;#93; Partitioned Index Filters: https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters&amp;#91;2&amp;#93; Deduplicate scenario, state.backend.rocksdb.memory.fixed-per-slot=256M, SSD, elapsed time 4.91ms -&gt; 0.33ms.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBSharedResources.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.rocksdb.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configuration.html</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20508" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PythonStreamGroupTableAggregateOperator</summary>
      <description>Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20521" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null result values are being swallowed by RPC system</summary>
      <description>If an RPC method returns a null value, then it seems that the request future won't get completed as reported in FLINK-17921.We should either not allow to return null values as responses or make sure that a null value is properly transmitted to the caller.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20522" opendate="2020-12-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make implementing a built-in function straightforward</summary>
      <description>FLIP-65 has introduced new abstractions for functions. They make implementing user-defined functions straightforward. However, currently, these abstraction are not available for built-in functions. Adding a built-in function requires many changes at different locations as shown in FLINK-6810. This is error-prone, limits contributions, and is also not well supported by the current module system.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.OperatorBindingCallContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.MiscFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.InternalAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="20536" opendate="2020-12-8 00:00:00" fixdate="2020-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update migration tests in master to cover migration from release-1.12</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorMigrationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20557" opendate="2020-12-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support STATEMENT SET in SQL CLI</summary>
      <description>Support to submit multiple insert into in a single job on SQL CLI, this can be done by support statement set syntax in SQL CLI. The syntax had been discussed and reached an consensus on the mailing list: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-SQL-Syntax-for-Table-API-StatementSet-td42515.html</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2056" opendate="2015-5-20 00:00:00" fixdate="2015-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide to create a chainable predictor in docs</summary>
      <description>The upcoming API for pipelines should have good documentation to guide and encourage the implementation of more algorithms.For this task we will create a guide that shows how the pipeline mechanism works through Scala implicits, and a full guide to implementing a chainable predictor, using Generalized Linear Models as an example.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20626" opendate="2020-12-16 00:00:00" fixdate="2020-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Canceling a job when it is failing will result in job hanging in CANCELING state</summary>
      <description>If user manually cancels a job when the job is failing(here failing means the job encounters unrecoverable failure and is about to fail), the job will hang in CANCELING state and cannot terminate. The cause is that DefaultScheduler currently will always try to transition from `FAILING` to `FAILED` to terminate the job. However, job canceling will change job status to `CANCELING` so that the transition to `FAILED` will not success.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="20666" opendate="2020-12-18 00:00:00" fixdate="2020-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deserialized Row losing the field_name information in PyFlink</summary>
      <description>Now, the deserialized Row loses the field_name information.@udf(result_type=DataTypes.STRING())def get_string_element(my_list): my_string = 'xxx' for element in my_list: if element.integer_element == 2: # element lost the field_name information my_string = element.string_element return my_stringt = t_env.from_elements( [("1", [Row(3, "flink")]), ("3", [Row(2, "pyflink")]), ("2", [Row(2, "python")])], DataTypes.ROW( [DataTypes.FIELD("Key", DataTypes.STRING()), DataTypes.FIELD("List_element", DataTypes.ARRAY(DataTypes.ROW( [DataTypes.FIELD("integer_element", DataTypes.INT()), DataTypes.FIELD("string_element", DataTypes.STRING())])))]))print(t.select(get_string_element(t.List_element)).to_pandas())element lost the field_name information</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="21028" opendate="2021-1-19 00:00:00" fixdate="2021-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming application didn&amp;#39;t stop properly</summary>
      <description>I have a Flink job running on YARN with a disjoint graph, i.e. a single job contains two independent and isolated pipelines.From time to time, I stop the job with a savepoint like so:flink stop -p ${SAVEPOINT_BASEDIR}/${FLINK_JOB_NAME}/SAVEPOINTS --yarnapplicationId=${FLINK_YARN_APPID} ${ID}A few days ago, this job suddenly didn't stop properly as usual but ran into a possible race condition.On the CLI with stop, I received a simple timeout:org.apache.flink.util.FlinkException: Could not stop with a savepoint job "f23290bf5fb0ecd49a4455e4a65f2eb6". at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:495) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:864) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:487) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:931) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:493) ... 9 more The root of the problem however is that on a taskmanager, I received an exception in shutdown, which lead to restarting (a part) of the pipeline and put it back to running state, thus the console command for stopping timed out (as the job was (partially) back in running state). the exception which looks like a race condition for me in the logs is:2021-01-12T06:15:15.827877+01:00 WARN org.apache.flink.runtime.taskmanager.Task Source: rawdata_source1 -&gt; validation_source1 -&gt; enrich_source1 -&gt; map_json_source1 -&gt; Sink: write_to_kafka_source1) (3/18) (bc68320cf69dd877782417a3298499d6) switched from RUNNING to FAILED.java.util.concurrent.ExecutionException: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302) at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:576) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:544) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:642) at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41) at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator$WatermarkEmitter.emitWatermark(TimestampsAndWatermarksOperator.java:165) at org.apache.flink.streaming.runtime.operators.util.AssignerWithPeriodicWatermarksAdapter.onPeriodicEmit(AssignerWithPeriodicWatermarksAdapter.java:54) at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.close(TimestampsAndWatermarksOperator.java:125) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:205) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:203) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:177) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:90) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155) ... 13 moreCaused by: java.lang.RuntimeException at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:123) at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:570) at org.apache.flink.streaming.api.operators.ProcessOperator.processWatermark(ProcessOperator.java:72) at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:638) ... 25 moreCaused by: java.lang.IllegalStateException at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179) at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:83) at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.copyToBufferBuilder(SpanningRecordSerializer.java:90) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:136) at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:80) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:121) ... 29 moreI already raised a question regarding this bug on the user mailing list with the conclusion to just open a ticket here. Original on user mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Bugs-in-Streaming-job-stopping-Weird-graceful-stop-restart-for-disjoint-job-graph-td40610.htmledit:In Flink 1.12.x this bug can probably lead to corrupted data stream and all kinds of deserialisation errors on the downstream task.Also the same bug can leave any code (regardless if it's Flink's network stack, user code, or some 3rd party library) that sleeps interruptible in an invalid state.</description>
      <version>1.11.2,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21030" opendate="2021-1-19 00:00:00" fixdate="2021-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken job restart for job with disjoint graph</summary>
      <description>Building on top of bugs:https://issues.apache.org/jira/browse/FLINK-21028 and https://issues.apache.org/jira/browse/FLINK-21029 : I tried to stop a Flink application on YARN via savepoint which didn't succeed due to a possible bug/racecondition in shutdown (Bug 21028). Due to some reason, Flink attempted to restart the pipeline after the failure in shutdown (21029). The bug here:As I mentioned: My jobgraph is disjoint and the pipelines are fully isolated. Lets say the original error occured in a single task of pipeline1. Flink then restarted the entire pipeline1, but pipeline2 was shutdown successfully and switched the state to FINISHED.My job thus was in kind of an invalid state after the attempt to stopping: One of two pipelines was running, the other was FINISHED. I guess this is kind of a bug in the restarting behavior that only all connected components of a graph are restarted, but the others aren't...</description>
      <version>1.11.2</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21140" opendate="2021-1-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract zip file dependencies before adding to PYTHONPATH</summary>
      <description>Not all zip files are importable in Python and so we should expand zip file dependencies and add the root directory to PYTHONPATH.</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManagerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21289" opendate="2021-2-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Application mode ignores the pipeline.classpaths configuration</summary>
      <description>我尝试将flink作业以application mode方式提交到kubernetes上运行。但程序的依赖包并不完全存在于local:///opt/flink/usrlib/xxxx.jar中。导致找不到类。在yarn上可以工作，是因为我们用 -C http://xxxx 的方式，让依赖可以被URLClassloader加载。但我发现，当实验提交到kubernetes时，-C只会在 configmap/flink-conf.yaml 中生成一个pipeline.classpaths 配置条目。我们的main函数可以执行，但是在加载外部依赖类的时候提示找不到类。通过阅读源码，我发现运行用户代码的类加载器实际并没有把 pipeline.classpaths 中的条目加入候选URL，这导致了无法加载类的情况。从源码中，我也发现，通过将依赖包放在usrlib目录下（默认的userClassPath）可以解决问题。但我们的依赖可能是动态的，不合适一次性打到镜像里面。我提议可以改进这个过程，将pipeline.classpaths也加入到对应的类加载器。这个改动很小，我自己经过测试，可以完美解决问题。  English translation:I'm trying to submit flink job to kubernetes cluster with application mode, but throw ClassNotFoundException when some dependency class is not shipped in kind of local:///opt/flink/usrlib/xxxx.jar.This works on yarn, since we use -C http://xxxx command line style that let dependency class  can be load by URLClassloader.But i figure out that not works on kubernetes. When submit to kubernetes cluster, -C is only shipped as item "pipeline.classpaths" in configmap/flink-conf.yaml。After read the source code, i find out that the Classloader launching the "main" entry of user code miss consider adding pipeline.classpaths into candidates URLs. from source code, i also learn that we can ship the dependency jar in the usrlib dir to solve the problem. But that failed for us, we are not preferred to ship dependencies in image at compile time, since dependencies are known dynamically in runtimeI proposed to improve the process, let the Classloader consider usrlib as well as pipeline.classpaths, this is a quite little change. I test the solution and it works quite well  </description>
      <version>1.11.2,1.12.1</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.DefaultPackagedProgramRetrieverTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.DefaultPackagedProgramRetriever.java</file>
    </fixedFiles>
  </bug>
  <bug id="21290" opendate="2021-2-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Projection push down for Window TVF</summary>
      <description>@Test def testTumble_ProjectionPushDown(): Unit = { // TODO: [b, c, e, proctime] are never used, should be pruned val sql = """ |SELECT | a, | window_start, | window_end, | count(*), | sum(d) |FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) |GROUP BY a, window_start, window_end """.stripMargin util.verifyRelPlan(sql) }For the above test, currently we get the following plan:Calc(select=[a, window_start, window_end, EXPR$3, EXPR$4])+- WindowAggregate(groupBy=[a], window=[TUMBLE(time_col=[rowtime], size=[15 min])], select=[a, COUNT(*) AS EXPR$3, SUM(d) AS EXPR$4, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- Calc(select=[a, d, rowtime]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 1000:INTERVAL SECOND)]) +- Calc(select=[a, b, c, d, e, rowtime, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, d, e, rowtime])It should be able to prune fields and get the following plan:Calc(select=[a, window_start, window_end, EXPR$3, EXPR$4])+- WindowAggregate(groupBy=[a], window=[TUMBLE(time_col=[rowtime], size=[15 min])], select=[a, COUNT(*) AS EXPR$3, SUM(d) AS EXPR$4, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 1000:INTERVAL SECOND)]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, d, rowtime])The reason is we didn't transpose Project and WindowTableFunction in logical phase. LogicalAggregate(group=[{0, 1, 2}], EXPR$3=[COUNT()], EXPR$4=[SUM($3)])+- LogicalProject(a=[$0], window_start=[$7], window_end=[$8], d=[$3]) +- LogicalTableFunctionScan(invocation=[TUMBLE($6, DESCRIPTOR($5), 900000:INTERVAL MINUTE)], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, DECIMAL(10, 3) d, BIGINT e, TIME ATTRIBUTE(ROWTIME) rowtime, TIME ATTRIBUTE(PROCTIME) proctime, TIMESTAMP(3) window_start, TIMESTAMP(3) window_end, TIME ATTRIBUTE(ROWTIME) window_time)]) +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4], rowtime=[$5], proctime=[$6]) +- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($5, 1000:INTERVAL SECOND)]) +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4], rowtime=[$5], proctime=[PROCTIME()]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24228" opendate="2021-9-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-171] Firehose implementation of Async Sink</summary>
      <description>MotivationUser stories:As a Flink user, I’d like to use Kinesis Firehose as sink for my data pipeline.Scope: Implement an asynchronous sink for Kinesis Firehose by inheriting the AsyncSinkBase class. The implementation can for now reside in its own module in flink-connectors. The module and package name can be anything reasonable e.g. flink-connector-aws-kinesis for the module name and org.apache.flink.connector.aws.kinesis for the package name. The implementation must use the Kinesis Java Client. The implementation must allow users to configure the Kinesis Client, with reasonable default settings. Implement an asynchornous sink writer for Firehose by extending the AsyncSinkWriter. The implementation must deal with failed requests and retry them using the requeueFailedRequestEntry method. If possible, the implementation should batch multiple requests (PutRecordsRequestEntry objects) to Firehose for increased throughput. The implemented Sink Writer will be used by the Sink class that will be created as part of this story. Unit/Integration testing. Use Kinesalite (in-memory Kinesis simulation). We already use this in KinesisTableApiITCase. Java / code-level docs. End to end testing: add tests that hits a real AWS instance. (How to best donate resources to the Flink project to allow this to happen?)ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.examples.SinkIntoFirehose.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.java</file>
      <file type="M">docs.content.docs.connectors.datastream.firehose.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.firehose.md</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2UtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2Util.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Factory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.config.AWSKinesisDataStreamsConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2423" opendate="2015-7-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Properly test checkpoint notifications</summary>
      <description>Checkpoint notifications (via the CheckpointNotifier interface) are currently not properly tested. A test should be included to verify that checkpoint notifications are eventually called on successful checkpoints, and that they are only called once per checkpointID.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24825" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update nifi-site-to-site-client to v1.14.0</summary>
      <description>We should update org.apache.nifi:nifi-site-to-site-client from 1.6.0 to 1.14.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25516" opendate="2022-1-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add catalog object compile/restore options</summary>
      <description>A prerequisite for serialization/deserialization of various entities.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25518" opendate="2022-1-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden JSON Serialization utilities</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.groupwindow.WindowReference.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LookupKeySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JoinSpecJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.IntervalJoinSpecJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.InputPropertySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DurationJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ChangelogModeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SerdeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexLiteralJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.FlinkDeserializationContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DurationJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
