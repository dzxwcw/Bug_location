<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="14896" opendate="2019-11-21 00:00:00" fixdate="2019-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector doesn&amp;#39;t shade jackson dependency</summary>
      <description>flink-kinesis-connector depends on aws java sdk which is shaded to org.apache.flink.kinesis.shaded.com.amazonaws. However, the aws sdk has a transitive dependency to jackson wich is not shaded in the artifact. This creates problem when running flink on YARN: The aws sdk requires jackson-core v2.6 but hadoop pulls in 2.3. See here. If YARN uses the loads wrong jackson version from classpath. Jod fails with2019-11-20 17:23:11,563 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler - Unhandled exception.org.apache.flink.client.program.ProgramInvocationException: The program caused an error:     at org.apache.flink.client.program.OptimizerPlanEnvironment.getOptimizedPlan(OptimizerPlanEnvironment.java:93)    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:80)    at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:126)    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$6(JarRunHandler.java:142)    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.enable([Lcom/fasterxml/jackson/core/JsonParser$Feature;)Lcom/fasterxml/jackson/databind/ObjectMapper;    at com.amazonaws.partitions.PartitionsLoader.&lt;clinit&gt;(PartitionsLoader.java:54)    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:65)    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:53)    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)    at com.amazonaws.client.builder.AwsClientBuilder.getRegionObject(AwsClientBuilder.java:256)    at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:460)    at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)    at com.amazonaws.client.builder.AwsAsyncClientBuilder.build(AwsAsyncClientBuilder.java:80)...The flink-kinesis-connector should do as other connectors: shade jackson or use the flink-shaded-jackson core dependency</description>
      <version>1.9.0,1.16.0,1.15.2</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16583" opendate="2020-3-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid classloader during pipeline creation</summary>
      <description>The end-to-end test SQLClientKafkaITCase.testKafka failed with18:13:02.425 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 32.246 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:11:46 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)18:13:02.425 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 34.539 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:12:21 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)https://api.travis-ci.org/v3/job/661535183/log.txt</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="20761" opendate="2020-12-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot read hive table/partition whose location path contains comma</summary>
      <description>We probably need to call hadoop StringUtils::escapeString to escape the input path string.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceFileEnumerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24960" opendate="2021-11-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>Nov 18 22:37:08 ================================================================================Nov 18 22:37:08 Test testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase) is running.Nov 18 22:37:08 --------------------------------------------------------------------------------Nov 18 22:37:25 22:37:25,470 [ main] INFO org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase [] - Extracted hostname:port: 5718b812c7ab:38622Nov 18 22:52:36 ==============================================================================Nov 18 22:52:36 Process produced no output for 900 seconds.Nov 18 22:52:36 ============================================================================== https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&amp;view=logs&amp;j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&amp;t=cc452273-9efa-565d-9db8-ef62a38a0c10&amp;l=36395</description>
      <version>1.14.3,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="25712" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Merge flink-connector-testing into flink-connector-test-utils</summary>
      <description>Both "flink-connector-testing" and "flink-connector-test-utils" modules are designed for providing connector testing infrastructures and helper classes, so merging these two modules could simplify dependencies of connectors and reduce the complexity of Flink project. The plan is to move classes in flink-connector-testing to flink-connector-test-utils, since the latter one have been existing for a longer time in the projects, and flink-connector-testing is in experimental status now.Discussion in mailing list: https://lists.apache.org/thread/r30tjwt3vdbzd5c18q2y3o0toqfmk79o</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.TestDataMatchersTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.utils.TestDataMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.TestResource.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.TestLoggerExtension.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.TestCaseInvocationContextProvider.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.ConnectorTestingExtension.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.TestEnv.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.ExternalSystem.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.ExternalContextFactory.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.SourceSplitDataWriter.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.ExternalContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.DefaultContainerizedExternalSystem.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.TestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.RemoteClusterTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.ClusterControllable.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.README.md</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.UniformShardAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25715" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Application Mode: Add option to submit a failed job on application error</summary>
      <description>Currently in application mode, any exception happens in the application driver (before submitting an actual job) leads to a fail-over. These errors are usually not retryable and we don't have a good way of reporting them to the user.We'll introduce a new config option `execution.submit-failed-job-on-application-error` that submits a failed job with the `$internal.pipeline.job-id` instead.This is intended to be used in combination with `execution.shutdown-on-application-finish = false` to allow user to retrieve the information about the failed submission.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DeploymentOptions.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.BlockingJob.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.java</file>
      <file type="M">docs.layouts.shortcodes.generated.deployment.configuration.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.TestingDispatcherGatewayService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtensionTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25720" opendate="2022-1-20 00:00:00" fixdate="2022-7-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support Python UDTF in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="25725" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Agile configurable modules in web ui</summary>
      <description>For modules like task-manager and job-manager, use an injection token to allow a configuration object customizing/overriding individual component/variable (e.g. rewrite a new chart node component and inject it through the token), increasing these modules' reusability (also a better DI practice).</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.CompactManagedTableITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSinkCommitter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="26082" opendate="2022-2-11 00:00:00" fixdate="2022-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CancelPartitionRequestTest.testDuplicateCancel failed on azure due to bind failed</summary>
      <description>Feb 10 01:56:01 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.273 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTestFeb 10 01:56:01 [ERROR] testDuplicateCancel(org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest) Time elapsed: 1.877 s &lt;&lt;&lt; ERROR!Feb 10 01:56:01 org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in useFeb 10 01:56:01 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31070&amp;view=logs&amp;j=f0ac5c25-1168-55a5-07ff-0e88223afed9&amp;t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&amp;l=6768</description>
      <version>1.13.5,1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="26125" opendate="2022-2-14 00:00:00" fixdate="2022-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doc overhaul for the CAST behaviour</summary>
      <description>This includes: Proper documentation of the new TRY_CAST Add a CAST matrix to document which CAST tuples are supported</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="26167" opendate="2022-2-15 00:00:00" fixdate="2022-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explicitly set the partitioner for the sql operators whose shuffle and sort are removed</summary>
      <description>After FLINK-25995 is finished, we have add an exchange (which will be converted to ForwardForConsecutiveHashPartitioner) for the nodes which do not need explicitly hash shuffle (which input has already hashed)e.g.WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%') SELECT sum(b1) FROM r group by a1the plan after FLINK-25995 is finished:Calc(select=[EXPR$0])+- HashAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0]) +- Exchange(distribution=[keep_input_as_is[hash[a1]]) +- Calc(select=[a1, b1]) +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left]) :- Exchange(distribution=[hash[a1]]) : +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')]) : +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1]) +- Exchange(distribution=[hash[a2]]) +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])but data between Calc and HashJoin may be out of order once their parallelism is different, so an Exchange(distribution=[keep_input_as_is[hash&amp;#91;a1&amp;#93;]) should be added between them.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ForwardHashExchangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.ForwardHashExchangeITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.ForwardHashExchangeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ForwardHashExchangeProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.InputProperty.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange.java</file>
    </fixedFiles>
  </bug>
  <bug id="26168" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Done">
    <buginformation>
      <summary>The judgment of chainable ignores StreamExchangeMode when partitioner is ForwardForConsecutiveHashPartitioner</summary>
      <description>After FLINK-26167, if the exchange mode is set as ALL_EDGES_BLOCKING through table.exec.shuffle-mode, the shuffle mode of  ForwardForConsecutiveHashPartitioner will be set as BATCH (becase it may be converted to hash shuffle). But it should not affect the chain creation, so we need chang the chain logic and let the judgment of chainable ignores StreamExchangeMode when partitioner is ForwardForConsecutiveHashPartitioner.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.StreamPartitionerTestUtils.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForConsecutiveHashPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="26189" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bundling of grizzled-slf4j from rpc-akka</summary>
      <description>This dependency is unnecessary nowadays.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2619" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some Scala Tests not being executed by Maven</summary>
      <description>Some Scala Tests are not executed by Maven. Originally this issue are reported by StephanEwen. I also executed mvn clean verify and found the same circumstance.Original post is here</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.misc.MassiveCaseClassSortingITCase.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITSuite.scala</file>
      <file type="M">flink-staging.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphOperationsITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.executiongraph.TaskManagerLossFailsTasksTest.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerRegistrationTest.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="26557" opendate="2022-3-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend CsvFormat documentation based on release-testing feedback</summary>
      <description>Incorporate feedback about the documentation from CsvFormat release testing:https://issues.apache.org/jira/browse/FLINK-26311 </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.formats.csv.md</file>
    </fixedFiles>
  </bug>
  <bug id="26638" opendate="2022-3-14 00:00:00" fixdate="2022-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reintroduce ActionFailureHandler for Elasticsearch sink connectors</summary>
      <description>In FLINK-26281 we found out that users depend on the ActionFailureHandler that was not ported over to the new unified Sink. We not want to add the failure handler back.</description>
      <version>1.16.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="2664" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove partitioned states when set to null</summary>
      <description>Currently there is no way to remove a specific key from the partitioned state stored at streaming operators, which can cause unnecessary state accumulation.I suggest to remove the partitioned state for the current input when the state is set to null, and upon next retrieval simply return the default value. This allows the implementation of streaming programs that can garbage collect their own unwanted state.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.PartitionedStreamOperatorState.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.PartitionedStateStore.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.EagerStateStore.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.OperatorState.java</file>
    </fixedFiles>
  </bug>
  <bug id="26641" opendate="2022-3-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the time of fetching job status in the job submission of session cluster</summary>
      <description>Currently, in the session cluster, the client will wait until the job initialization is finished in the job submission period. However, it will first get the job details from the `ExecutionGraphCache`. For short queries in OLAP scenarios, the job might have been finished before the cache refreshed, which increase the e2e time of fetching the result.We proposed to introduce a REST API for the job status to replace it.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="26687" opendate="2022-3-16 00:00:00" fixdate="2022-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove NiFi connector</summary>
      <description>The community voted to deprecate NiFi in 1.15 and we should remove it from 1.16 onwards.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.test.resources.NiFi.Flink.xml</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.test.java.org.apache.flink.streaming.connectors.nifi.examples.NiFiSourceTopologyExample.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.test.java.org.apache.flink.streaming.connectors.nifi.examples.NiFiSinkTopologyExample.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.StandardNiFiDataPacket.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSource.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSink.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiDataPacketBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiDataPacket.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.nifi.md</file>
    </fixedFiles>
  </bug>
  <bug id="26689" opendate="2022-3-16 00:00:00" fixdate="2022-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace TableConfig with ReadableConfig where possible</summary>
      <description>Following the removal of `nullCheck` in the code generation, we can replace TableConfig with ReadableConfig in various places, and take advantage of the new `TableConfig implments ReadableConfig` approach.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedWatermarkGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedTableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedResultFuture.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedProjection.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedNamespaceTableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedJoinCondition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedInput.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedHashFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedCollector.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.PartitionPrunerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.CodeSplitTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.PartitionPruner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ValuesCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.over.RangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.over.MultiFieldRangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.TableConfigUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.KeySelectorUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecBoundedStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExpand.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecValues.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDataStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="26712" opendate="2022-3-17 00:00:00" fixdate="2022-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metadata keys should not conflict with physical columns</summary>
      <description>If you have an field called timestamp and in addition want to read the timestamp from the metadata:CREATE TABLE animal_sightings_with_metadata ( `timestamp` TIMESTAMP(3), `name` STRING, `country` STRING, `number` INT, `append_time` TIMESTAMP(3) METADATA FROM 'timestamp', `partition` BIGINT METADATA VIRTUAL, `offset` BIGINT METADATA VIRTUAL, `headers` MAP&lt;STRING, BYTES&gt; METADATA, `timestamp-type` STRING METADATA, `leader-epoch` INT METADATA, `topic` STRING METADATA)This gives:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [timestamp]</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsReadingMetadata.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsWritingMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="26747" opendate="2022-3-20 00:00:00" fixdate="2022-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-external-resources</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-external-resources.flink-external-resource-gpu.src.test.java.org.apache.flink.externalresource.gpu.GPUDriverTest.java</file>
      <file type="M">flink-external-resources.flink-external-resource-gpu.src.test.java.org.apache.flink.externalresource.gpu.GPUDiscoveryScriptTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26757" opendate="2022-3-21 00:00:00" fixdate="2022-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change the default value of state.backend.rocksdb.restore-overlap-fraction-threshold</summary>
      <description>`state.backend.rocksdb.restore-overlap-fraction-threshold` is used to control how to restore a state handle, different thresholds can affect the performance of restoring. The behavior of deletion in restoring has been changed after FLINK-21321.In theory, setting the default value to 0 is most suitable, since `deleteRange()` takes less time than creating a new RocksDB instance and then scan-and-put the records. In fact, we also have some experimental data that the default value of 0 is more suitable. Here is a comparison of initialization times for different thresholds, we can see that the default value to 0 takes less time. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="26758" opendate="2022-3-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-container</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.PrintSinkFactoryTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneApplicationClusterConfigurationParserFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26798" opendate="2022-3-22 00:00:00" fixdate="2022-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobMaster.testJobFailureWhenTaskExecutorHeartbeatTimeout failed due to missing Execution</summary>
      <description>This build failed due to an ExecutionGraphException indicating that an expected Execution wasn't around:[...]Caused by: org.apache.flink.util.FlinkException: Execution 48dbc880c8225256b8bc112ea36e9082 is unexpectedly no longer running on task executor bbad15fcb93d4b2b4f80fe2c35e03e6d. at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:250) ~[classes/:?] ... 35 more</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26810" opendate="2022-3-22 00:00:00" fixdate="2022-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The local time zone does not take effect when the dynamic index uses a field of type timestamp_ltz</summary>
      <description>When using  TIMESTAMP_WITH_LOCAL_TIMEZONE field to generate a dynamic index,  it will alway use UTC timezone.     </description>
      <version>None</version>
      <fixedVersion>1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26813" opendate="2022-3-23 00:00:00" fixdate="2022-7-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports ADD/MODIFY column/watermark/constraint syntax parse for ALTER TABLE</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlLoadModule.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateView.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateCatalog.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterViewProperties.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableReset.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableOptions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveView.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveTable.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveViewProperties.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveTableSerDe.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveTableProps.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveTableAddReplaceColumn.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabaseProps.java</file>
    </fixedFiles>
  </bug>
  <bug id="26853" opendate="2022-3-24 00:00:00" fixdate="2022-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HeapStateBackend ignores metadata updates in certain cases</summary>
      <description>On recovery, HeapRestoreOperation reads state handles one by one; each handle contains metadata at the beginning; the metadata is always read, but not actually used if a state with the corresponding name was already registeredIn a rare case of downscaling + multiple checkpoints with different metadata; this might lead to data being deserialized incorrectly (always using the initial metadata).It also prevents incremental checkpoints with schema evolution.On first access, however, the backend itself will update (merge) metadata; so that it doesn't affect new state updates.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="26855" opendate="2022-3-25 00:00:00" fixdate="2022-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ImportError: cannot import name &amp;#39;environmentfilter&amp;#39; from &amp;#39;jinja2&amp;#39;</summary>
      <description>ar 24 17:38:39 ===========mypy checks... [SUCCESS]===========Mar 24 17:38:39 rm -rf _build/*Mar 24 17:38:39 /__w/2/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlMar 24 17:38:40 Traceback (most recent call last):Mar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/bin/sphinx-build", line 6, in &lt;module&gt;Mar 24 17:38:40 from sphinx.cmd.build import mainMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/cmd/build.py", line 23, in &lt;module&gt;Mar 24 17:38:40 from sphinx.application import SphinxMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/application.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.highlighting import lexer_classes, lexersMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/highlighting.py", line 30, in &lt;module&gt;Mar 24 17:38:40 from sphinx.ext import doctestMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/ext/doctest.py", line 28, in &lt;module&gt;Mar 24 17:38:40 from sphinx.builders import BuilderMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/builders/__init__.py", line 24, in &lt;module&gt;Mar 24 17:38:40 from sphinx.io import read_docMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/io.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.util.rst import append_epilog, docinfo_re, prepend_prologMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/util/rst.py", line 22, in &lt;module&gt;Mar 24 17:38:40 from jinja2 import environmentfilterMar 24 17:38:40 ImportError: cannot import name 'environmentfilter' from 'jinja2' (/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/jinja2/__init__.py)Mar 24 17:38:40 Makefile:76: recipe for target 'html' failedMar 24 17:38:40 make: *** [html] Error 1Mar 24 17:38:40 ==========sphinx checks... [FAILED]===========https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33717&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23450</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="26864" opendate="2022-3-25 00:00:00" fixdate="2022-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 25.03.2022</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=arrayKeyBy&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=remoteFilePartition&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=remoteSortPartition&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=tupleKeyBy&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="26865" opendate="2022-3-25 00:00:00" fixdate="2022-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the potential failure of loading library in Thread Mode</summary>
      <description>The failure occurs in session mode.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="26886" opendate="2022-3-28 00:00:00" fixdate="2022-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document reporter behavior w.r.t. scopes &amp; push/pull</summary>
      <description>The docs are lacking information for whether a reporter uses a metric identifier or the logical scope + tags, and whether they are push or pull based.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug id="26890" opendate="2022-3-28 00:00:00" fixdate="2022-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamoDB consumer error consuming partitions close to retention</summary>
      <description>BackgroundThe Amazon Kinesis Data Streams consumer supports consuming from Amazon DynamoDB via the DynamoDB Streams Kinesis Adapter. ProblemWe have seen instances of consumer throwing ResouceNotFoundException when attempting to invoke GetShardIterator.com.amazonaws.services.kinesis.model.ResourceNotFoundException: Requested resource not found: Shard does not exist According to the DynamoDB team, the DescribeStream call may return shard IDs that are no longer valid, and this exception needs to be handled by the client. SolutionModify the DynamoDB consumer to treat ResourceNotFoundException as a shard closed signal.</description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="26928" opendate="2022-3-30 00:00:00" fixdate="2022-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary Docker network creation in Kafka connector tests</summary>
      <description>Currently each Kafka test class will create a Docker network, which could flush the network usage on Docker host, and test would fail if all IP address in the pool of Docker are occupied. </description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="26929" opendate="2022-3-30 00:00:00" fixdate="2022-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce adaptive hash join for batch sql optimization</summary>
      <description>We propose an optimization method adaptive hash join for the batch join scenario, hoping to integrate the advantages of sorted-merge join and hash join according to the characteristics of runtime data. The adaptive hash join will try to use hash join strategy firstly, if it failed, will fall back to sort merge join.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.UniformBinaryRowGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.io.BinaryRowChannelInputViewIterator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeConfig.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="26931" opendate="2022-3-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar sink&amp;#39;s producer name should be unique</summary>
      <description>Pulsar's new sink interface didn't make the producer name unique. Which would make the pulsar fail to consume messages.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26986" opendate="2022-4-1 00:00:00" fixdate="2022-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated string expressions in Python Table API</summary>
      <description>In FLINK-26704, it has removed the string expressions in Table API. However, there are still some APIs still using string expressions in Python Table API, however, they should not work any more as the string expressions have already been removed in the Java Table API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.window.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.schema.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.tumble.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.sliding.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.session.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.over.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.pandas.pandas.udaf.py</file>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.python.table.api.connectors.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.python.table.api.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="27015" opendate="2022-4-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception for casting timestamp to decimal in Hive dialect</summary>
      <description>In hive, it's support to cast timestamp to decimal implictly with following sql:create table t1 (c1 DECIMAL(38,6));create table t2 (c2 TIMESTAMP);insert into t1 select * from t2;-- orselect cast(cast('2012-12-19 11:12:19.1234567' as timestamp) as decimal(30,8))But it'll throw exception "The cast conversion from TIMESTAMP type to NUMERIC type is not allowed" with Hive dialect in Flink. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="27016" opendate="2022-4-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve supporting for complex data type for Hive dialect</summary>
      <description>      There are some issue exist while involving complex data type using Hive dialect in Flink. It will throw exception when access array type in a struct. For example, such sql will fail:create table t (s6 map&lt;string, struct&lt;f20:array&lt;string&gt;&gt;&gt;);SELECT s6['key1'].f20[0] FROM nested_tbl_1;      2. Hive supports to access the field of a struct list by '.' , but it'll throw exception using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserTypeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="27017" opendate="2022-4-2 00:00:00" fixdate="2022-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix divide by zero exception in Hive dialect</summary>
      <description>It's support to divide by zero in Hive, but it'll throw exception while using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="27031" opendate="2022-4-3 00:00:00" fixdate="2022-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogRescalingITCase.test failed due to IllegalStateException</summary>
      <description>This build failed in ChangelogRescalingITCase.test:Apr 01 20:26:53 Caused by: java.lang.IllegalArgumentException: Key group 94 is not in KeyGroupRange{startKeyGroup=96, endKeyGroup=127}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).Apr 01 20:26:53 at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.getMapForKeyGroup(StateTable.java:305)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:261)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:143)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:94)Apr 01 20:26:53 at org.apache.flink.state.changelog.ChangelogListState.add(ChangelogListState.java:78)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:404)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:531)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:227)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:841)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:767)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)Apr 01 20:26:53 at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="27034" opendate="2022-4-3 00:00:00" fixdate="2022-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use testcontainers for google cloud pubsub e2e test</summary>
      <description>We can use testcontainers to simplify the test, which will even make it easier to run in different environments.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudUnitTestBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.java</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27044" opendate="2022-4-4 00:00:00" fixdate="2022-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop support for Hive versions 1.*, 2.1.* and 2.2.*</summary>
      <description>We should remove the connectors/support for the following Hive versions: 1.* 2.1.* 2.2.*These versions are no longer supported by the Hive community. This was discussed in https://lists.apache.org/thread/2w046dwl46tf2wy750gzmt0qrcz17z8t</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-defining-excess-dependencies.modulelist</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.minlog</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.kryo</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.jodd</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.javolution</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.reflectasm</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.minlog</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.kryo</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.jodd</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.javolution</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.HiveVersionTestUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimV3.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="27046" opendate="2022-4-4 00:00:00" fixdate="2022-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-*-glue-schema-registry</summary>
      <description>Migrate the two modules flink-avro-glue-schema-registry and flink-json-glue-schema registry.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchemaTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27048" opendate="2022-4-4 00:00:00" fixdate="2022-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ArchUnit tests for Elasticsearch to only depend on public API</summary>
      <description>We want to ensure that the Elasticsearch connectors class level dependencies are part of the public API. To do this we want to extend the existing ArchUnit tests.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.main.java.org.apache.flink.architecture.ProductionCodeArchitectureBase.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.stored.rules</file>
    </fixedFiles>
  </bug>
  <bug id="2705" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn fails with NoSuchMethodError when log level is set to DEBUG</summary>
      <description>The Yarn tests fail with a NoSuchMethodError when the log level is set to DEBUG. Apparently, we have a wrong dependency version of org.apache.commons.codec in our scope when the Yarn is executed. Exception in thread "LocalizerRunner for container_1442415732615_0009_01_000001" java.lang.NoSuchMethodError: org.apache.commons.codec.binary.Base64.&lt;init&gt;(I[BZ)V at org.apache.hadoop.security.token.Token.encodeWritable(Token.java:236) at org.apache.hadoop.security.token.Token.encodeToUrlString(Token.java:263) at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.writeCredentials(ResourceLocalizationService.java:1016) at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:972)Error while deploying YARN cluster: The YARN application unexpectedly switched to state FAILED during deployment. https://s3.amazonaws.com/archive.travis-ci.org/jobs/80641990/log.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerProcessReapingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.fs.s3.S3FileSystemTest.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.LICENSE</file>
    </fixedFiles>
  </bug>
  <bug id="27066" opendate="2022-4-5 00:00:00" fixdate="2022-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reintroduce e2e tests in ES as Java tests.</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27119" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup JobMasters</summary>
      <description>Several tests create JobMasters but don't make sure that it is shut down. We should change the tests to use a try-with-resource statement.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterQueryableStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27140" opendate="2022-4-8 00:00:00" fixdate="2022-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JobResultStore dirty entry creation into ioExecutor</summary>
      <description>The FileSystemJobResultStore is thread-safe and, therefore, we can move the dirty entry creation into the ioExecutor</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="27143" opendate="2022-4-8 00:00:00" fixdate="2022-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-docs</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.RestAPIDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27148" opendate="2022-4-8 00:00:00" fixdate="2022-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointITCase fails on AZP</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34394&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=5812https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34394&amp;view=logs&amp;j=baf26b34-3c6a-54e8-f93f-cf269b32f802&amp;t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&amp;l=6018https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34448&amp;view=logs&amp;j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&amp;t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&amp;l=41655Relevant error message: Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 12, already marked for checkpoint 11 at org.apache.flink.runtime.operators.coordination.OperatorEventValve.markForCheckpoint(OperatorEventValve.java:113)  [ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 174.732 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointITCase[ERROR] UnalignedCheckpointITCase.execute Time elapsed: 6.408 s &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:184) at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:287) at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.Verifier$1.evaluate(Verifier.java:35) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42) at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80) at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 12, already marked for checkpoint 11 at org.apache.flink.runtime.operators.coordination.OperatorEventValve.markForCheckpoint(OperatorEventValve.java:113) at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:302) at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:230) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27167" opendate="2022-4-11 00:00:00" fixdate="2022-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploying master snapshot failed due to generating javadoc failed</summary>
      <description>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:jar (attach-javadocs) on project flink-architecture-tests-production: MavenReportException: Error while creating archive:[ERROR] Exit code: 1 - javadoc: error - class file for org.junit.platform.commons.annotation.Testable not found[ERROR] [ERROR] Command line was: /usr/lib/jvm/java-8-openjdk-amd64/jre/../bin/javadoc -Xdoclint:none @options @packages[ERROR] [ERROR] Refer to the generated Javadoc files in '/__w/1/s/flink-architecture-tests/flink-architecture-tests-production/target/apidocs' dir.[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-architecture-tests-production##[error]Bash exited with code '1'.Finishing: Deploy maven snapshothttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34493&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&amp;l=13852</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27168" opendate="2022-4-11 00:00:00" fixdate="2022-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ContinuousProcessingTimeTrigger and ContinuousEventTimeTrigger</summary>
      <description>Add continuousprocessingtimetrigger and continuouseventtimetrigger triggers</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.window.py</file>
    </fixedFiles>
  </bug>
  <bug id="27174" opendate="2022-4-11 00:00:00" fixdate="2022-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-null check for bootstrapServers field is incorrect in KafkaSink</summary>
      <description>If the user-supplied kafkaProducerConfig contains bootstrapServers information, there is no need to define the value of this field separately through the setBootstrapServers method. Obviously, the current code doesn't notice this. Perhaps we can check bootstrapServers as follows: Or check bootstrapServers like KafkaSourceBuilder.  </description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="27199" opendate="2022-4-12 00:00:00" fixdate="2022-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.0 for fixing the unstable Pulsar test environment.</summary>
      <description>Pulsar's transaction is not stable. The standalone cluster often hangs the test, then we will meet a timeout for the tests at last.The latest Pulsar 2.10.0 drops the zookeeper and fixes a lot of issues in the Pulsar transaction. Bump to this version would resolve the current test issues.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockZooKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.producer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.MessageKeyHash.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27209" opendate="2022-4-12 00:00:00" fixdate="2022-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Half the Xmx and double the forkCount for unit tests</summary>
      <description>As per recent "Speeding up the test builds" discussion on the dev mailing.I'm proposing to half the memory allocated for running unit tests (as defined per &lt;test.unit.pattern&gt;**/Test.&lt;/test.unit.pattern&gt; property) but at the same time double the forkCounts for those tests. The premise is that they shouldn't need as much memory as ITCases to be stable, while increasing number of forks, should provide us with a couple of minutes improved build times.CC chesnay</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2721" opendate="2015-9-21 00:00:00" fixdate="2015-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tuple meta information</summary>
      <description>In Bolt.execute(Tuple input) the given input tuple contains meta information about its origin (like source component name, stream id, source task ID).This meta information in currently not provided by Flink and the corresponding methods throw an UnsupportedOperationException.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.StormTupleTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkTopologyTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.StormTuple.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.AbstractStormCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.TwoFlinkStreamsMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.StormFlinkStreamMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopology.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-contrib.flink-storm.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="27228" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redistributed modules across CI profiles</summary>
      <description>With the recent improvements around testing times it is time to redistribute the modules again to achieve a more even distribution.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27231" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL pulsar connector lists dependencies under wrong license</summary>
      <description>Pulsar sql connector lists following dependencies under ASL2 license while they are licensed with Bouncy Castle license (variant of MIT?).- org.bouncycastle:bcpkix-jdk15on:1.69- org.bouncycastle:bcprov-ext-jdk15on:1.69- org.bouncycastle:bcprov-jdk15on:1.69- org.bouncycastle:bcutil-jdk15on:1.69</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27260" opendate="2022-4-15 00:00:00" fixdate="2022-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose changelog configuration in RestAPI and WebUI</summary>
      <description>Currently, we can know whether changelog is enabled by state_backend in rest api&amp;#91;1&amp;#93;, if state_backend is ChangelogStateBackend, it means changelog is enabled.But the detailed configurations of changelog still cannot be known through rest api/web ui, this issue wants to add two fields:  1. changelog_storage  2. changelog_periodic_materialization_intervalinto the checkpoints-config&amp;#91;1&amp;#93; response, and show these fields on web UI when changelog is enabled.&amp;#91;1&amp;#93; https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/ops/rest_api/#jobs-jobid-checkpoints-config</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionConfigBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionGraph.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ArchivedExecutionConfig.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="27263" opendate="2022-4-15 00:00:00" fixdate="2022-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename the metadata column to the user specified name in DDL</summary>
      <description>Currently the source output the metadata column with the "$metadata"+ metadata key as the column name. It's better that keep the name align with the user name specified in the DDL,e.g. use event_time as the name if the user specifies`event_time` TIMESTAMP(3) FROM 'timestamp'.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsReadingMetadata.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsWritingMetadata.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="27267" opendate="2022-4-16 00:00:00" fixdate="2022-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-contrib</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.test.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27317" opendate="2022-4-20 00:00:00" fixdate="2022-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot deployment fails due to .scalafmt.conf not being found</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34852&amp;view=logs&amp;j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&amp;t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7Cause by the maven-source-plugin jar goal forking the build and apparently messing up the working directory.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-scala.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27341" opendate="2022-4-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager running together with JobManager are bind to 127.0.0.1</summary>
      <description>If some TaskManagers running with JobManager on the same machine while some other TaskManager not, the TaskManagers running together with JobManager would bind to localhost or 127.0.01, which makes the Netty connections across the TaskManagers fail.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27376" opendate="2022-4-25 00:00:00" fixdate="2022-7-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Hive dialect supports "select current_database()"</summary>
      <description>Hive supports select current database using the following sql:select current_database() It'll involve Hive's 'GenericUDFCurrentDatabase'in run time, in this udf, it will try to call 'SessionState.get().getCurrentDatabase()', but we has closed the SessionState before excuting.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.InternalConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27399" opendate="2022-4-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector didn&amp;#39;t set start consuming position correctly</summary>
      <description>The Pulsar connector didn't use the consuming position from the checkpoint. They just commit the position to Pulsar after the checkpoint is complete. And the connector starts to consume messages from Pulsar directly by the offset stored on the Pulsar subscription.This causes the test could be failed in some situations. The start cursor (position on Pulsar) would be reset to the wrong position, which caused the results didn't match the desired records.How to fix this issueChange the start position seeking mechanism from Pulsar consumer API to Pulsar admin API. Don't reset the start position when the topic has a subscription.This issue fixes FLINK-23944 FLINK-24872 FLINK-25815 FLINK-25884 FLINK-26177 FLINK-26721 FLINK-27833</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.SplitsAssignmentStateTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.SplitsAssignmentState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.PublishTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.NeverStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.MessageIdStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.LatestMessageStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.EventTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.TimestampStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.CursorPosition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.consumer.configuration.html</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="2740" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create data consumer for Apache NiFi</summary>
      <description>Create a connector to Apache NiFi to create Flink DataStreams from NiFi flows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27400" opendate="2022-4-25 00:00:00" fixdate="2022-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector subscribed the system topic when using the regex</summary>
      <description>Pulsar has a lot of internal topics which is used for metadata. It couldn't be consumed directly by the user. We accidentally exposed these topics to end-users when using the regex topics.</description>
      <version>1.14.4,1.16.0,1.15.2</version>
      <fixedVersion>1.16.0,1.15.3,1.14.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListenerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.TopicPatternSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="2743" opendate="2015-9-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new RNG based on XORShift algorithm</summary>
      <description>XORShift algorithm is an optimized algorithm for random number generator, implement a RNG based on it would help to improve the performance of operations where RNG is heavily used.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sampling.ReservoirSamplerWithReplacement.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sampling.ReservoirSamplerWithoutReplacement.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sampling.PoissonSampler.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sampling.BernoulliSampler.java</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27431" opendate="2022-4-27 00:00:00" fixdate="2022-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Duration for RpcTimeouts</summary>
      <description>To support the gradual migration of components to Duration we should allow the RpcTimeout annotation to also be used for Duration timeouts.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="27433" opendate="2022-4-27 00:00:00" fixdate="2022-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E log artifacts contain several thousand rocksdb logs</summary>
      <description>It would be great if we could minimize them somehow because as is they kinda break the azure UI and are just a pain (beyond taking up a lot of space). Even zipping them before the upload would be fine in my book.I also just don't get why so few e2e tests manage to create 5k files.yunta Is this caused by FLINK-23791?</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="27460" opendate="2022-4-30 00:00:00" fixdate="2022-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused notifyPartitionDataAvailable process</summary>
      <description>The `notifyPartitionDataAvailable` process was used to trigger downstream task scheduling once a pipelined partition has data produced at TM side. It is no longer used. Therefore I propose to remove it to cleanup the code base.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskSystemExitTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestTaskBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.DefaultJobTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.NettyShuffleUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DeploymentHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DeploymentOption.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionVertexDeploymentOption.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingStrategyUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.DefaultJobTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentWithSmallBlobCacheSizeLimitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphPartitionReleaseTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphVariousFailuesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RemoveCachedShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.AbstractPartitionTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.NoOpResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.deploying.DeployingTasksBenchmarkBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.failover.RegionToRestartInBatchJobBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.partitionrelease.PartitionReleaseInBatchJobBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DeploymentHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.StrategyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulerOperations.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestSchedulingStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="27461" opendate="2022-4-30 00:00:00" fixdate="2022-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a convenient way to set userAgent for the kubeclient</summary>
      <description>Currently, we construct the kubeclient from the kubeconfig file or from the context. However, If we have to set the user agent for the okttp client it will be a little hard. We use the kubernetes cluster with different team, we need to distinguish the request for k8s guys to monitor the apiserver requests. We usually set the user agent for different groups. The fabric client expose a way to extract config from the property or enviroments. But the key for the user agent is bundled with the client version, it's not so convenient. Can we support to set the user agent by a dedicated flink option.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="27485" opendate="2022-5-4 00:00:00" fixdate="2022-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation build pipeline is broken</summary>
      <description>The current documentation build pipeline is broken due to two failures: It uses git command git branch --show-current which isn't supported by the installed Git version on the Docker image. We can switch to git rev-parse --abbrev-ref HEAD as an alternative The manual Hugo download and installation is outdated and doesn't add Hugo to the PATH</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  <bug id="27486" opendate="2022-5-4 00:00:00" fixdate="2022-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce ArchUnit violations in connector base module</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.TextElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.ListElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.LinkElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.LineBreakElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.InlineElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.Formatter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.DescriptionElement.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.Description.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.BlockElement.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSource.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.CoordinatedSourceRescaleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.AsyncSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.archunit-violations.8ab2328f-b38b-4e34-b768-0deb6b6171fb</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.archunit-violations.84abeb9c-8355-4165-96aa-dda65b04e5e7</file>
    </fixedFiles>
  </bug>
  <bug id="27518" opendate="2022-5-6 00:00:00" fixdate="2022-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor migration tests to support version update automatically</summary>
      <description>Currently on releasing each version, we need to manually generate the snapshots for every migration tests and update the current versions. With more and more migration tests are added, this has been more and more intractable. It is better if we could make it happen automatically on cutting new branches. </description>
      <version>1.16.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializerUpgradeTest.scala</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainUnionTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainOrderTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthStatelessDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthIncreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainBreakTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.KeyedComplexChainTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StatefulJobSnapshotMigrationITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializerUpgradeTest.java</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorMigrationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TimerSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.BufferEntrySerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkStateSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.UnionSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TraversableSerializerUpgradeTest.scala</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.FlinkVersion.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaMigrationTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaSerializerUpgradeTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializerUpgradeTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.array.PrimitiveArraySerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BasicTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.EnumSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ListSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.MapSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.CompositeTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.CopyableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.NullableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.RowSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerUpgradeTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-fs-tests.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.NFASerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.LockableTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ArrayListSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.JavaSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.VoidNamespaceSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.OptionSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaEitherSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaTrySerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.EnumValueSerializerUpgradeTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="27523" opendate="2022-5-6 00:00:00" fixdate="2022-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime supports producing and consuming cached intermediate result</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingExecutionVertex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultExecutionVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.TestingJobMasterPartitionTracker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.NoOpResourceManagerPartitionTracker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.NoOpJobMasterPartitionTracker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.ConsumedPartitionGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.TaskExecutorPartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.DataSetMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ClusterPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.InternalExecutionGraphAccessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.partitionrelease.RegionPartitionGroupReleaseStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="27527" opendate="2022-5-6 00:00:00" fixdate="2022-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a file-based Upsert sink for testing internal components</summary>
      <description>There are a bunch of tests that in order to ensure correctness of their tested component rely on a Sink providing upserts. These tests (e.g. test-sql-client.sh) mostly use the ElasticsearchSink which is a lot of overhead. We want to provide a simple file-based upsert sink for Flink developers to test their components against. The sink should be very simple and is not supposed to be used in production scenarios but rather just to facilitate easier testing.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.completeness.TypeSerializerTestCoverageTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.completeness.TypeInfoTestCoverageTest.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27556" opendate="2022-5-9 00:00:00" fixdate="2022-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression in checkpointSingleInput.UNALIGNED on 29.04.2022</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=checkpointSingleInput.UNALIGNED&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="27570" opendate="2022-5-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint path error does not cause the job to stop</summary>
      <description>I configured the wrong checkpoint path when starting the job, and set：conf.set (executioncheckpointingoptions. Tolerable_failure_number, 0);env setRestartStrategy(RestartStrategies.noRestart());The job is expected to stop due to a checkpoint error, but the job is still running.Here is my job configuration and environment：</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FinishedTaskStateProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlan.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27579" opendate="2022-5-12 00:00:00" fixdate="2022-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The param client.timeout can not be set by dynamic properties when stopping the job</summary>
      <description>The default client.timeout value is one minute which may be too short when stop-with-savepoint for big state jobs.When we stop the job by dynamic properties(-D or -yD for yarn), the client.timeout is not effective.From the code, we can see that the dynamic properties are only effect for run command. We should support it for stop command.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendDynamicPropertiesTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="27607" opendate="2022-5-13 00:00:00" fixdate="2022-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.LegacyRowResource.java</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.BatchCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.BatchExecutionFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.committer.FileCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.AbstractCompactTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactCoordinatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactorOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileCommittableSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkCommittableSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkCompactionSwitchITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkITBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.StreamingCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.StreamingExecutionFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.utils.IntegerFileSinkTestDataUtils.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.enumerate.BlockSplittingRecursiveEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.enumerate.NonSplittingRecursiveEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitStateTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.AdapterTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileRecordFormatAdapterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileRecordsTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.StreamFormatAdapterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.ArrayResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.IteratorResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.SingletonResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.BinPackingTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.EnrichedRowDataTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.LimitableBulkFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.PartitionWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.RowPartitionComputerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.AbstractCompactTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactCoordinatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactFileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.StreamingFileWriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27630" opendate="2022-5-16 00:00:00" fixdate="2022-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven-source-plugin for table planner values connector for debug</summary>
      <description>add test source jar to reponsitory  when user use this values connector in flink-table-planner just like kafka/pulsar connector. So user can find the values source code.and we just need upload the */factories/* because it will be too large to upload all the flink-table-planner test source code. // code placeholderjust like kafka/pulsar&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-test-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;test-jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;archive&gt; &lt;!-- Globally exclude maven metadata, because it may accidentally bundle files we don't intend to --&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;includes&gt; &lt;include&gt;**/factories/**&lt;/include&gt; &lt;include&gt;META-INF/LICENSE&lt;/include&gt; &lt;include&gt;META-INF/NOTICE&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27651" opendate="2022-5-16 00:00:00" fixdate="2022-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CREATE FUNCTION USING JAR syntax</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.functions.UserDefinedFunctionHelperTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogFunction.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateTempSystemFunctionOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogFunctionImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateFunction.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.completeness.py</file>
    </fixedFiles>
  </bug>
  <bug id="27659" opendate="2022-5-17 00:00:00" fixdate="2022-7-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Planner support to use jar which is registered by "USING JAR" syntax</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.PlannerMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.ForwardHashExchangeITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.CompactManagedTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.agg.LocalAggregatePushDownITCase.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.resource.ResourceManagerTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.resource.ResourceManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-bridge-base.src.main.java.org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.UserDefinedFunctions.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.UserClassLoaderJarTestUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoaders.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27660" opendate="2022-5-17 00:00:00" fixdate="2022-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API support create function using customed jar</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="27669" opendate="2022-5-17 00:00:00" fixdate="2022-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-file-sink-common</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-file-sink-common.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileRecoverableMigrationTest.java</file>
      <file type="M">flink-connectors.flink-file-sink-common.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssignerTest.java</file>
      <file type="M">flink-connectors.flink-file-sink-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27692" opendate="2022-5-19 00:00:00" fixdate="2022-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support local recovery for materialized part(write, restore, discard)</summary>
      <description>Support local recovery for materialized part(write, restore, discard)</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="27693" opendate="2022-5-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support local recovery for non-materialized part(write, restore, discard)</summary>
      <description>Support local recovery for non-materialized part(write, restore, discard)</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.StateChangeLoggerTestBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateDiscardTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogTruncateHelper.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChangelogTaskLocalStateStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ChangelogTaskLocalStateStore.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterSqnTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.UploadResult.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploadScheduler.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFsUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFormat.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.OutputStreamWithPos.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogWriter.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageFactory.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="27732" opendate="2022-5-22 00:00:00" fixdate="2022-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-examples-table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.utils.ExampleOutputTestBase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.WordCountSQLExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.StreamSQLExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.GettingStartedExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.functions.AdvancedFunctionsExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.basics.WordCountSQLExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.basics.UpdatingTopCityExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.basics.StreamSQLExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.basics.GettingStartedExampleITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27767" opendate="2022-5-25 00:00:00" fixdate="2022-7-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Endpoint API and utils</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27769" opendate="2022-5-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the REST endpoint framework</summary>
      <description>Introduce the REST Endpoint Factory and its implementation. Make sure the SQL Gateway is avaliable to load the REST endpoint with config.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobManagerJobConfigurationHeaders.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.versioning.RestAPIVersionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestMessageHeaders.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestExternalHandlersITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.versioning.RestAPIVersion.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.YarnStopJobTerminationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.YarnCancelJobTerminationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.UntypedResponseMessageHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerThreadDumpHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerStdoutFileHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagersHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerLogsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerLogFileHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerCustomLogHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.TaskManagerLogUrlHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.SubtasksTimesHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.SubtasksAllAccumulatorsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptAccumulatorsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskCurrentAttemptDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexWatermarksHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractAggregatedMetricsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobStatusInfoHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobManagerJobEnvironmentHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactoryUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.data.TestEmptyMessageHeaders.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.data.TestExcludeMessageHeaders.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.RestAPIDocGeneratorTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractJarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHeaders.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationStatusMessageHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationTriggerMessageHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.WebContentHandlerSpecification.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerSpecification.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatisticDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ClusterConfigurationInfoHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ClusterOverviewHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.JobManagerCustomLogHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.JobManagerLogFileHeader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.JobManagerLogListHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.JobManagerStdoutFileHeader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.JobManagerThreadDumpHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.ShutdownHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.dataset.ClusterDataSetListHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobAccumulatorsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobConfigHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobIdsWithStatusesOverviewHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobManagerEnvironmentHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobManagerLogUrlHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobPlanHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobsOverviewHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexAccumulatorsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexTaskManagersHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.coordination.ClientCoordinationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobExecutionResultHeaders.java</file>
    </fixedFiles>
  </bug>
  <bug id="27770" opendate="2022-5-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the script to start/stop/stop-all gateway</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SQLJobSubmission.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.SqlGatewayTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.SqlGateway.java</file>
      <file type="M">flink-table.flink-sql-gateway.bin.sql-gateway.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.SQLGatewayITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.src.test.java.org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.src.test.java.org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.TestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.ClusterController.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="27773" opendate="2022-5-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce the E2E tests for SQL Gateway</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SQLJobSubmission.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.util.SqlGatewayRestOptions.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactoryUtilsTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.resources.gateway.e2e.sql</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.SqlGatewayE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug id="27779" opendate="2022-5-25 00:00:00" fixdate="2022-1-25 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Connectors should not depend on `flink-table-planner`</summary>
      <description>Connector modules currently rely heavily on `flink-table-planner` as a test dependency for testing the ITCases with 'DynamicTableX' using the TableFactory to load the respective connector. There is now a better way that only requires to have `flink-table-test-utils` as a test dependency. Therefore all connectors should be migrated to using the new way.</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2778" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add API for non-parallel non-keyed Windows</summary>
      <description>This addresses the NonParallelWindowStream section in the design doc: https://cwiki.apache.org/confluence/display/FLINK/Streams+and+Operations+on+Streams</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.TimeWindowTranslationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceWindowFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="27790" opendate="2022-5-26 00:00:00" fixdate="2022-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port ADD JAR /SHOW JARS syntax implementation from SqlClient to TableEnvironment side</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SQLJobSubmission.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.resource.ResourceManagerTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.resource.ResourceManager.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.resource.ClientResourceManager.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.src.test.java.org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.command.AddJarOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.src.test.java.org.apache.flink.table.sql.codegen.SqlITCaseBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27810" opendate="2022-5-27 00:00:00" fixdate="2022-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch e2e jars bundle way more than they should</summary>
      <description>The jars bundle flink-end-to-end-tests-common-elasticsearch and all of it's transitive dependencies, like junit or flink-rpc-core.All of these are unnecessary for the test to work and really shouldn't be bundled.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.src.test.java.org.apache.flink.streaming.tests.Elasticsearch7SinkE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch7.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.src.test.java.org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27811" opendate="2022-5-27 00:00:00" fixdate="2022-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove netty dependency in flink-test-utils</summary>
      <description>For some reason we bundle a relocated version of netty in flink-test-utils. AFAICT this should be unnecessary because nothing makes use of the relocated version.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.resources.META-INF.licenses.LICENSE.webbit</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.resources.META-INF.licenses.LICENSE.jzlib</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.resources.META-INF.licenses.LICENSE.jsr166y</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.resources.META-INF.licenses.LICENSE.base64</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27822" opendate="2022-5-28 00:00:00" fixdate="2022-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the doc of checkpoint/savepoint guarantees</summary>
      <description>Translate the change of FLINK-26134 </description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.vs.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.concepts.stateful-stream-processing.md</file>
    </fixedFiles>
  </bug>
  <bug id="27829" opendate="2022-5-29 00:00:00" fixdate="2022-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calcite usages in flink-python</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27856" opendate="2022-5-31 00:00:00" fixdate="2022-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding pod template without spec crashes job manager</summary>
      <description>While trying to add Pod annotation through pod template in FlinkDeployment, taskmanager was keep crashing.Pod template that I used: taskManager: podTemplate: apiVersion: v1 kind: Pod metadata: annotations: iam.amazonaws.com/role: fake-role-arnIt created below ConfigMap and mounted to the deployment:apiVersion: v1data: taskmanager-pod-template.yaml: | --- apiVersion: "v1" kind: "Pod" metadata: annotations: iam.amazonaws.com/role: "fake-role-arn"kind: ConfigMapLooks like missing "spec" stanza in pod template resulted in the crash and I couldn't find any documentation that "spec" is required for pod template even for just adding metadata annotations.Adding below worked fine taskManager: podTemplate: apiVersion: v1 kind: Pod metadata: annotations: iam.amazonaws.com/role: fake-role-arn spec: {}Corresponding ConfigMapapiVersion: v1data: taskmanager-pod-template.yaml: | --- apiVersion: "v1" kind: "Pod" metadata: annotations: iam.amazonaws.com/role: "fake-role-arn" spec: containers: []</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.utils.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesPodTemplateTestUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2786" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove Spargel from source code and update documentation in favor of Gelly</summary>
      <description>With Gelly getting more mature and ready to be top level project for Flink, we need to remove deprecated Spargel library from source and documentation.Gelly copies the library needed from Spargel so there should not be hard dependency between the 2 modules.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-spargel.src.test.resources.logback-test.xml</file>
      <file type="M">flink-staging.flink-spargel.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-staging.flink-spargel.src.test.java.org.apache.flink.test.spargel.SpargelConnectedComponentsITCase.java</file>
      <file type="M">flink-staging.flink-spargel.src.test.java.org.apache.flink.spargel.java.SpargelTranslationTest.java</file>
      <file type="M">flink-staging.flink-spargel.src.test.java.org.apache.flink.spargel.java.SpargelCompilerTest.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexUpdateFunction.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexCentricIteration.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.VertexUpdateFunction.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.SpargelIteration.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.MessagingFunction.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.MessageIterator.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.Edge.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.OutgoingEdge.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.MessagingFunction.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.MessageIterator.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelPageRankCountingVertices.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelPageRank.java</file>
      <file type="M">flink-staging.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelConnectedComponents.java</file>
      <file type="M">flink-staging.flink-spargel.pom.xml</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.libs.spargel.guide.md</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">docs.libs.fig.spargel.example.input.png</file>
      <file type="M">docs.libs.fig.spargel.example.png</file>
    </fixedFiles>
  </bug>
  <bug id="27861" opendate="2022-6-1 00:00:00" fixdate="2022-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce UserResourceManager to manage user defined resource</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.UserClassLoaderJarTestUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="27865" opendate="2022-6-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide and example for configuring SASL and SSL in Kafka SQL connector document</summary>
      <description>Using SASL and SSL in Kafka connector is a common case and usually quite complex for new users that not quite familiar with the design of Kafka connector, so it would be helpful to add a guidance of how to enable these security options in Kafka connector.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="27878" opendate="2022-6-2 00:00:00" fixdate="2022-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-232] Add Retry Support For Async I/O In DataStream API</summary>
      <description>FLIP-232: Add Retry Support For Async I/O In DataStream APIhttps://cwiki.apache.org/confluence/pages/viewpage.action?pageId=211883963</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AsyncDataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AsyncDataStream.java</file>
      <file type="M">docs.content.docs.dev.datastream.operators.asyncio.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.asyncio.md</file>
    </fixedFiles>
  </bug>
  <bug id="27895" opendate="2022-6-4 00:00:00" fixdate="2022-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable the CI test for Hive&amp;#39;s 3.x</summary>
      <description>We only enable Ci test for Hive's 2.3.9,  we also need enable the test for Hive's 3.x to guarantee the compatibility for Hive 3.x .</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2790" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add high availability support for Yarn</summary>
      <description>Add master high availability support for Yarn. The idea is to let Yarn restart a failed application master in a new container. For that, we set the number of application retries to something greater than 1. From version 2.4.0 onwards, it is possible to reuse already started containers for the TaskManagers, thus, avoiding unnecessary restart delays.From version 2.6.0 onwards, it is possible to specify an interval in which the number of application attempts have to be exceeded in order to fail the job. This will prevent long running jobs from eventually depleting all available application attempts.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTests.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.Messages.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterActor.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.appMaster.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn-tests.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractProcessFailureRecoveryTest.java</file>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.ForkableFlinkMiniCluster.scala</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingMemoryArchivist.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingCluster.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerRegistrationTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerProcessReapingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.ForwardingActorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionRetrievalTestingCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManagerConfiguration.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.LeaderSessionMessageFilter.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.LeaderRetrievalUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.RecoveryMode.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="2792" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set log level of actor messages to TRACE</summary>
      <description>Logging of received job manager actor messages happens at log level DEBUG right now. The used logger is that of the JobManager/TaskManager respectively. This means that as soon as you debug something related to the JobManager/TaskManager you are always flooded with a lot of debug messages.Therefore, I would like to set the log level to TRACE for these messages.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.LogMessages.scala</file>
    </fixedFiles>
  </bug>
  <bug id="27920" opendate="2022-6-6 00:00:00" fixdate="2022-6-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Documented enums constant support ExcludeFromDocumentation annotation</summary>
      <description>if a config option has @ExcludeFromDocumentation annotation, it will not appear in the document. But for an enumeration type, sometimes we only want some of it's constant values not to appear in the document, this ticket solves this problem.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryPlanTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectAggITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSumAggFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveDeclarativeAggregateFunction.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27931" opendate="2022-6-7 00:00:00" fixdate="2022-7-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the SqlGateway to assemble all components</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.SqlGatewayServiceExtension.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayEndpointFactory.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactoryUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27936" opendate="2022-6-7 00:00:00" fixdate="2022-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-cassandra</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.WriteAheadSinkTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleWriteAheadSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27937" opendate="2022-6-7 00:00:00" fixdate="2022-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-gcp-pubsub</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubConsumingTest.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.DeserializationSchemaWrapperTest.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.java.org.apache.flink.streaming.connectors.gcp.pubsub.common.AcknowledgeOnCheckpointTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27938" opendate="2022-6-7 00:00:00" fixdate="2022-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-hbase-base</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.test.java.org.apache.flink.connector.hbase.util.HBaseSerdeTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.test.java.org.apache.flink.connector.hbase.util.HBaseConfigLoadingTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27942" opendate="2022-6-7 00:00:00" fixdate="2022-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-rabbitmq</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27990" opendate="2022-6-10 00:00:00" fixdate="2022-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet format supports reporting statistics</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.java</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27991" opendate="2022-6-10 00:00:00" fixdate="2022-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC format supports reporting statistics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.StatisticsReportTestBase.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27995" opendate="2022-6-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Janino version</summary>
      <description>Currently, the Janino version doesn't support JDK11 well.  https://lists.apache.org/thread/q052xdn1mnhjm9k4ojjjz22dk4r1xwfz</description>
      <version>1.16.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.util.javac.JaninoCompiler.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.interpreter.JaninoRexCompiler.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.adapter.enumerable.EnumerableInterpretable.java</file>
      <file type="M">flink-formats.flink-protobuf.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27996" opendate="2022-6-10 00:00:00" fixdate="2022-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support INTERVAL type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserTypeCheckProcFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="28000" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>security.kerberos.login.principal can be defined wihtout keytab which is wrong configuration</summary>
      <description>Please see the parameter description here.Kerberos principal name associated with the keytab.</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="28002" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_connectors.py failed with "object is not an instance of declaring class" in JDK11</summary>
      <description>2022-06-10T02:43:20.7206790Z Jun 10 02:43:20 E : java.lang.IllegalArgumentException: object is not an instance of declaring class2022-06-10T02:43:20.7207481Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-06-10T02:43:20.7208200Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-06-10T02:43:20.7209003Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-06-10T02:43:20.7209720Z Jun 10 02:43:20 E at java.base/java.lang.reflect.Method.invoke(Method.java:566)2022-06-10T02:43:20.7210572Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-06-10T02:43:20.7211291Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-06-10T02:43:20.7212101Z Jun 10 02:43:20 E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-06-10T02:43:20.7212796Z Jun 10 02:43:20 E at java.base/java.lang.reflect.Method.invoke(Method.java:566)2022-06-10T02:43:20.7213500Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2022-06-10T02:43:20.7214327Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)2022-06-10T02:43:20.7215097Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2022-06-10T02:43:20.7215885Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2022-06-10T02:43:20.7216700Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2022-06-10T02:43:20.7217558Z Jun 10 02:43:20 E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2022-06-10T02:43:20.7218225Z Jun 10 02:43:20 E at java.base/java.lang.Thread.run(Thread.java:829)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36508&amp;view=logs&amp;j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&amp;t=40fc352e-9b4c-5fd8-363f-628f24b01ec2</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.environment.settings.py</file>
    </fixedFiles>
  </bug>
  <bug id="28080" opendate="2022-6-15 00:00:00" fixdate="2022-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce MutableURLClassLoader as parent class of FlinkUserClassLoader and SafetyNetWrapperClassLoader</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.resource.ResourceManagerTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.resource.ResourceManager.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FlinkUserCodeClassLoadersTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoaders.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="28092" opendate="2022-6-16 00:00:00" fixdate="2022-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ASCII and CHR built-in function in the Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2015-10-3 00:00:00" fixdate="2015-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[REFACTOR] Remove Pact from class and file names since it is no longer valid reference</summary>
      <description>Remove Pact word from class and file names in Apache Flink.Pact was the name used in Stratosphere time to refer to concept of distributed datasets (similar to Flink Dataset).It was used when Pact and Nephele still separate concept.As part of 0.10 cleanup effort, let's remove the Pact names to avoid confusion.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceDriver.java</file>
      <file type="M">flink-tests.src.test.resources.logback-test.xml</file>
      <file type="M">flink-staging.flink-tez.src.test.resources.logback-test.xml</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.runtime.TezTask.java</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.runtime.RegularProcessor.java</file>
      <file type="M">flink-staging.flink-ml.src.test.resources.logback-test.xml</file>
      <file type="M">flink-runtime.src.test.resources.logback-test.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.TestTaskContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.TaskConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.UnionWithTempOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ResettablePactDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.PactJobJSONServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.WebInterfaceServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.TypeInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.MapValue.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.CollectionsDataTypeTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-runtime-web.src.test.resources.logback-test.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.broadcast.BroadcastVariableManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.broadcast.BroadcastVariableMaterialization.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.concurrent.SolutionSetUpdateBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.concurrent.SolutionSetUpdateBarrierBroker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationHeadPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationSynchronizationSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationTailPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllGroupCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllGroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedCollectorMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.GroupCombineChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.SynchronousChainedCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupRawDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CollectorMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CrossDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.FlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapPartitionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceCombineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="28150" opendate="2022-6-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the hiveserver2 endpoint and factory</summary>
      <description>Introduce the HiveServer2Endpoint and Factory and allow the gateway to load them using SPI mechanism.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java-uber.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.ThreadUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.SessionManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.session.SessionEnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.session.SessionEnvironment.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ResultSet.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.HandleIdentifier.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactoryUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactory.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileLock.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-base.src.main.java.org.apache.flink.architecture.common.JavaFieldPredicates.java</file>
    </fixedFiles>
  </bug>
  <bug id="28151" opendate="2022-6-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to cancel the Operation for the HiveServer2 Endpoint</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.HiveServer2EndpointExtension.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointStatementITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28152" opendate="2022-6-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce the statement-related API for HiveServer2 Endpoint</summary>
      <description>Allow HiveServer2 Endpoint to submit sql and fetch results. It's better we can test under the YARN envrionment.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.OperationInfo.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.StaticResultProvider.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.TestSqlStatement.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.SqlScriptReader.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.result.ResultFetcherTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.SessionManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.Session.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.result.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.result.ResultFetcher.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.session.SessionEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.HiveServer2EndpointExtension.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointConfigOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.session.SessionHandle.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationHandle.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.HandleIdentifier.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
    </fixedFiles>
  </bug>
  <bug id="28163" opendate="2022-6-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the statement related API for REST endpoint</summary>
      <description>It includes executeStatement, fetchResults API in the FLIP-91.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.OperationCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ResultSet.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.main.java.org.apache.flink.architecture.rules.TableApiRules.java</file>
    </fixedFiles>
  </bug>
  <bug id="28164" opendate="2022-6-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce utilities API for REST endpint</summary>
      <description>It includes heartbeat, get_info, api_versions API in the REST endpoint.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28165" opendate="2022-6-21 00:00:00" fixdate="2022-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove include_hadoop_aws profile</summary>
      <description>The profile should be merged into the default configurations, because it was specific for Hadoop 2.6+ but we upgrade Hadoop to 2.8.5.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28173" opendate="2022-6-21 00:00:00" fixdate="2022-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple Parquet format tests are failing with NoSuchMethodError</summary>
      <description>Jun 21 02:44:38 java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)VJun 21 02:44:38 at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)Jun 21 02:44:38 at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)Jun 21 02:44:38 at org.apache.hadoop.conf.Configuration.readFields(Configuration.java:3798)Jun 21 02:44:38 at org.apache.flink.formats.parquet.utils.SerializableConfiguration.readObject(SerializableConfiguration.java:50)Jun 21 02:44:38 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jun 21 02:44:38 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jun 21 02:44:42 [ERROR] Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [ERROR] Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [INFO] Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testProjectJun 21 02:44:42 [ERROR] Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [ERROR] Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [INFO] Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testReachEndJun 21 02:44:42 [ERROR] Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [ERROR] Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)VJun 21 02:44:42 [INFO] Jun 21 02:44:42 [ERROR] AvroParquetRecordFormatTest.testCreateGenericReader:161-&gt;createReader:269 » NoSuchMethodJun 21 02:44:42 [ERROR] AvroParquetRecordFormatTest.testCreateReflectReader:133-&gt;createReader:269 » NoSuchMethodJun 21 02:44:42 [ERROR] AvroParquetRecordFormatTest.testCreateSpecificReader:118-&gt;createReader:269 » NoSuchMethodJun 21 02:44:42 [ERROR] AvroParquetRecordFormatTest.testReadWithRestoreGenericReader:203-&gt;restoreReader:293 » NoSuchMethodJun 21 02:44:42 [ERROR] AvroParquetRecordFormatTest.testReflectReadFromGenericRecords:147-&gt;createReader:269 » NoSuchMethodJun 21 02:44:42 [ERROR] ParquetRowDataWriterTest.testCompression:126 » NoSuchMethod com.google.common....Jun 21 02:44:42 [ERROR] ParquetRowDataWriterTest.testTypes:117-&gt;innerTest:168 » NoSuchMethod com.googl...Jun 21 02:44:42 [ERROR] SerializableConfigurationTest.testResource:45 » NoSuchMethod com.google.common...Jun 21 02:44:42 [INFO] Jun 21 02:44:42 [ERROR] Tests run: 31, Failures: 0, Errors: 24, Skipped: 0Jun 21 02:44:42 [INFO] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&amp;view=logs&amp;j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&amp;t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&amp;l=16375</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28175" opendate="2022-6-21 00:00:00" fixdate="2022-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve LicenseChecker output</summary>
      <description>The license checker output is difficult to parse for people who aren't too familiar with it. They just get bombarded with 200 log lines, that are too long, with too much redundant information, with no quick way to identify whether they are relevant for a particular change (e.g., module) without any guidance on whether something is critical or not.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28182" opendate="2022-6-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Avro generic record decoder in PyFlink</summary>
      <description>Avro generic record decoder is useful for format like parquet-avro, which enables PyFlink users read parquet files into python native objects within a given avro schema.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.utils.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28195" opendate="2022-6-22 00:00:00" fixdate="2022-6-22 01:00:00" resolution="Done">
    <buginformation>
      <summary>Annotate Python3.6 as deprecated in PyFlink 1.16</summary>
      <description>Python 3.6 extended support end on 23 December 2021. We plan that PyFlink 1.16 will be the last version support Python3.6.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28217" opendate="2022-6-23 00:00:00" fixdate="2022-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump mysql-connector-java from 8.0.27 to 8.0.28</summary>
      <description>We should bump our test dependency for mysql-connector-java to make sure that we support the latest version of MySQLThis will also address CVE-2022-21363 and CVE-2021-22569</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28238" opendate="2022-6-24 00:00:00" fixdate="2022-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix unstable testCancelOperationAndFetchResultInParallel</summary>
      <description>The failed test in https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2711&amp;view=logs&amp;j=43a[…]cc-244368da36b4&amp;t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&amp;l=11611It's possible the fetcher fetches the results from the closed operation.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.result.ResultFetcherTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="28259" opendate="2022-6-27 00:00:00" fixdate="2022-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-parquet doesn&amp;#39;t compile on M1 mac without rosetta</summary>
      <description>Compiling Flink 1.16-SNAPSHOT fails on an M1 Mac (apple silicon) without the rosetta translation layer, because the automatically downloaded "protoc-3.17.3-osx-aarch_64.exe" file is actually just a copy of "protoc-3.17.3-osx-x86_64.exe". (as you can read here: https://github.com/os72/protoc-jar/issues/93)This is the error:[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet: An error occurred while invoking protoc. Error while executing process. Cannot run program "/Users/rmetzger/Projects/flink/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.17.3-osx-aarch_64.exe": error=86, Bad CPU type in executable -&gt; [Help 1]</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="28274" opendate="2022-6-28 00:00:00" fixdate="2022-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ContinuousFileMonitoringFunction doesn&amp;#39;t work with reactive mode</summary>
      <description>This issue was first reported in the Flink Slack: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1656257678477659It seems that reactive mode is changing the parallelism of the `ContinuousFileMonitoringFunction`, which is supposed to always run with a parallelism of 1.This is the errorINITIALIZING to FAILED with failure cause: java.lang.IllegalArgumentException: ContinuousFileMonitoringFunction retrieved invalid state. at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)You can see from the logs that the parallelism is changing on a rescale event:2022-06-27 13:38:54,979 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from RUNNING to CANCELING.2022-06-27 13:38:55,254 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from CANCELING to CANCELED.2022-06-27 13:38:55,657 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from DEPLOYING to INITIALIZING.2022-06-27 13:38:55,722 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from INITIALIZING to RUNNING.2022-06-27 13:44:54,058 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (1/2) (665b12194741744d6bba4408a252fa45) switched from RUNNING to CANCELING.2022-06-27 13:45:00,825 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from DEPLOYING to INITIALIZING.2022-06-27 13:45:00,826 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from INITIALIZING to RUNNING.2022-06-27 13:45:01,434 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from DEPLOYING to INITIALIZING.2022-06-27 13:45:02,427 | INFO | .executiongraph.ExecutionGraph | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from INITIALIZING to RUNNING.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="28286" opendate="2022-6-28 00:00:00" fixdate="2022-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move “enablechangelog” constant out of flink-streaming-java module</summary>
      <description>Some methods in the flink-runtime module want to use StreamConfig.ENABLE_CHANGE_LOG_STATE_BACKEND constant(in flink-streaming-java module), but flink-runtime should not depend on flink-streaming-java. We should move ENABLE_CHANGE_LOG_STATE_BACKEND to a right place.See this discussion for more details. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.StateBootstrapTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="28314" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>[UI] Introduce "Cluster Environment" tab under history server</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.navigation.navigation.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.overview.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-detail.job-manager-log-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-detail.job-manager-log-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-detail.job-manager-log-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.thread-dump.job-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.thread-dump.job-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job-local.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="28315" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>[UI] Introduce aggregate stats in tables of the subtasks and taskmanagers</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-bytes.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-local.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.table-action.taskmanagers-table-action.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.table-action.taskmanagers-table-action.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.table-action.taskmanagers-table-action.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.table-action.subtasks-table-action.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.config.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.modules.completed-job.completed-job.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-subtask.ts</file>
    </fixedFiles>
  </bug>
  <bug id="28316" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>[UI] add external JM and TM log links under history server</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.rewrite.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.task-badge.task-badge.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.status.job-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.modules.completed-job.completed-job.module.ts</file>
    </fixedFiles>
  </bug>
  <bug id="28329" opendate="2022-6-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>List top 15 biggest directories in terms of used disk space</summary>
      <description>We are having the situation where a lot of disk space gets used by both Bash and Java E2E tests. In order to identify which tests aren't properly cleaning up, it would be good if we output the top 15 directories which are the biggest in used disk space</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.controller.utils.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28330" opendate="2022-6-30 00:00:00" fixdate="2022-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old delegation token framework code when new is working fine</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="28355" opendate="2022-7-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python Bash e2e tests don&amp;#39;t clean-up after they&amp;#39;ve ran, causing disk space issues</summary>
      <description>The Bash based E2E tests that are used in Python aren't cleaned-up after they've ran. These cause disk space issues further downstream.See the CI run from https://github.com/apache/flink/pull/20114 for results, for example:&amp;#8211; When starting with the Bash e2e tests08:47:10 ##[group]Top 15 biggest directories in terms of used disk spaceJul 01 08:47:12 3983560 .Jul 01 08:47:12 1266692 ./flink-end-to-end-testsJul 01 08:47:12 624568 ./flink-distJul 01 08:47:12 624180 ./flink-dist/targetJul 01 08:47:12 500076 ./flink-dist/target/flink-1.16-SNAPSHOT-binJul 01 08:47:12 500072 ./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOTJul 01 08:47:12 460812 ./flink-connectorsJul 01 08:47:12 392588 ./.gitJul 01 08:47:12 366396 ./.git/objectsJul 01 08:47:12 366388 ./.git/objects/packJul 01 08:47:12 349272 ./flink-tableJul 01 08:47:12 335592 ./.git/objects/pack/pack-38d46915823ebec2bc660fd160e5cfca5bc3e567.packJul 01 08:47:12 293044 ./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/optJul 01 08:47:12 251272 ./flink-filesystemsJul 01 08:47:12 246596 ./flink-end-to-end-tests/flink-streaming-kinesis-testhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&amp;view=logs&amp;j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&amp;t=860bfb5d-81b0-5968-f128-2a8b5362110d&amp;l=664&amp;#8211; After completing all Bash bashed e2e tests:2022-07-01T10:20:17.3594718Z Jul 01 10:20:17 ##[group]Top 15 biggest directories in terms of used disk space2022-07-01T10:20:18.7520631Z Jul 01 10:20:18 5425892 .2022-07-01T10:20:18.7521823Z Jul 01 10:20:18 1521472 ./flink-end-to-end-tests2022-07-01T10:20:18.7522566Z Jul 01 10:20:18 1242528 ./flink-python2022-07-01T10:20:18.7523244Z Jul 01 10:20:18 952336 ./flink-python/dev2022-07-01T10:20:18.7524159Z Jul 01 10:20:18 878764 ./flink-python/dev/.conda2022-07-01T10:20:18.7524870Z Jul 01 10:20:18 834200 ./flink-python/dev/.conda/lib2022-07-01T10:20:18.7525619Z Jul 01 10:20:18 726528 ./flink-python/dev/.conda/lib/python3.72022-07-01T10:20:18.7526397Z Jul 01 10:20:18 683256 ./flink-python/dev/.conda/lib/python3.7/site-packages2022-07-01T10:20:18.7527101Z Jul 01 10:20:18 624568 ./flink-dist2022-07-01T10:20:18.7527768Z Jul 01 10:20:18 624180 ./flink-dist/target2022-07-01T10:20:18.7528494Z Jul 01 10:20:18 500076 ./flink-dist/target/flink-1.16-SNAPSHOT-bin2022-07-01T10:20:18.7529298Z Jul 01 10:20:18 500072 ./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT2022-07-01T10:20:18.7530046Z Jul 01 10:20:18 460812 ./flink-connectors2022-07-01T10:20:18.7530546Z Jul 01 10:20:18 392588 ./.git2022-07-01T10:20:18.7531014Z Jul 01 10:20:18 366396 ./.git/objectshttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&amp;view=logs&amp;j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&amp;t=860bfb5d-81b0-5968-f128-2a8b5362110d&amp;l=9631</description>
      <version>1.16.0,1.15.2,1.14.6</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.yarn.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.pyflink.application.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28373" opendate="2022-7-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read a full buffer of data per file IO read request for sort-shuffle</summary>
      <description>Currently, for sort blocking shuffle, the corresponding data readers read shuffle data in buffer granularity. Before compression, each buffer is 32K by default, after compression the size will become smaller (may less than 10K). For file IO, this is pretty smaller. To achieve better performance and reduce IOPS, we can read more data per IO read request and parse buffer header and data in memory.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.CompositeBuffer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="28376" opendate="2022-7-4 00:00:00" fixdate="2022-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restrict the number of threads for sort-shuffle data read</summary>
      <description>Currently, the number of IO threads for shuffle data reading is relevant to the size of reading memory and the number of CPU cores. We should also consider the number of slots and the number of disks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SharedPoolNettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleEnvironmentContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28377" opendate="2022-7-4 00:00:00" fixdate="2022-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decrease the memory size per request for sort-shuffle data read from 8M to 4M</summary>
      <description>Currently, for sort blocking shuffle, the corresponding data reader always allocate a fixed size of 8M buffers for shuffle data reading. FLINK-28373 can increase buffer utilization, after which, we can reduce the buffer size per request to reduce the time waiting for buffers. (This change is guarded by TPC-DS test that there is no performance regression)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="28388" opendate="2022-7-5 00:00:00" fixdate="2022-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python doc build breaking nightly docs</summary>
      <description>For the past 5 days the nightly doc builds via GHA are broken:https://github.com/apache/flink/actions/workflows/docs.ymlException occurred: File "/root/flink/flink-python/pyflink/java_gateway.py", line 86, in launch_gateway raise Exception("It's launching the PythonGatewayServer during Python UDF execution "Exception: It's launching the PythonGatewayServer during Python UDF execution which is unexpected. It usually happens when the job codes are in the top level of the Python script file and are not enclosed in a `if name == 'main'` statement.The full traceback has been saved in /tmp/sphinx-err-3thh_wi2.log, if you want to report the issue to the developers.Please also report this if it was a user error, so that a better error message can be provided next time.A bug report can be filed in the tracker at &lt;https://github.com/sphinx-doc/sphinx/issues&gt;. Thanks!Makefile:76: recipe for target 'html' failedmake: *** [html] Error 2==========sphinx checks... [FAILED]===========Error: Process completed with exit code 1.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28392" opendate="2022-7-5 00:00:00" fixdate="2022-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RemoveCachedShuffleDescriptorTest#testRemoveOffloadedCacheForPointwiseEdgeAfterFailover causes fatal error on CI</summary>
      <description>Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in logJul 05 03:30:03 [ERROR] Process Exit Code: 239Jul 05 03:30:03 [ERROR] Crashed tests:Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTestJul 05 03:30:03 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Jul 05 03:30:03 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4932865857415988980.jar /__w/1/s/flink-runtime/target/surefire 2022-07-05T03-23-25_404-jvmRun1 surefire8916732512419442726tmp surefire_2130262314165063415tmpJul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in logJul 05 03:30:03 [ERROR] Process Exit Code: 239Jul 05 03:30:03 [ERROR] Crashed tests:Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTestJul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798&amp;l=8147</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultExecutionDeployerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionDeployer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.java</file>
    </fixedFiles>
  </bug>
  <bug id="28416" opendate="2022-7-6 00:00:00" fixdate="2022-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add the new LookupFunction interface for lookup table source</summary>
      <description>Add the new LookupFunction, AsyncLookupFunction and their related providers in place of the top-level TableFunction as the API for lookup table</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.TableFunctionProvider.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.LookupTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.AsyncTableFunctionProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="28417" opendate="2022-7-6 00:00:00" fixdate="2022-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add interface and default implementation for cache in lookup table</summary>
      <description>Add interfaces for cache in lookup table, including LookupCache, a default implementation DefaultLookupCache, and cache related metric group</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="28419" opendate="2022-7-6 00:00:00" fixdate="2022-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add runtime provider interface for full caching lookup</summary>
      <description>Add runtime provider interface for full caching lookup</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.ScheduledTask.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.lookup.LookupOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="28426" opendate="2022-7-6 00:00:00" fixdate="2022-7-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>PyFlink provides M1 wheel package</summary>
      <description>In FLINK-25188, pyflink has provided the support of M1 on MacOS. We also need to provide M1 wheel package.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-python-wheels.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28449" opendate="2022-7-7 00:00:00" fixdate="2022-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-parquet</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.SerializableConfigurationTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.protobuf.ParquetProtoStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFormatStatisticsReportTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.AvroParquetStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.AvroParquetFileReadITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressionFactoryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28453" opendate="2022-7-8 00:00:00" fixdate="2022-7-8 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>KafkaSourceLegacyITCase.testBrokerFailure hang on azure</summary>
      <description>2022-07-07T17:55:16.4876240Z "main" #1 prio=5 os_prio=0 tid=0x00007fd0b000b800 nid=0x258c waiting on condition [0x00007fd0b77f2000]2022-07-07T17:55:16.4876830Z java.lang.Thread.State: WAITING (parking)2022-07-07T17:55:16.4877232Z at sun.misc.Unsafe.park(Native Method)2022-07-07T17:55:16.4877916Z - parking to wait for &lt;0x00000000a46ffae8&gt; (a java.util.concurrent.CompletableFuture$Signaller)2022-07-07T17:55:16.4878495Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2022-07-07T17:55:16.4879100Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)2022-07-07T17:55:16.4879714Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)2022-07-07T17:55:16.4880318Z at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)2022-07-07T17:55:16.4880921Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2022-07-07T17:55:16.4881505Z at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:67)2022-07-07T17:55:16.4882182Z at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBrokerFailureTest(KafkaConsumerTestBase.java:1506)2022-07-07T17:55:16.4883003Z at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testBrokerFailure(KafkaSourceLegacyITCase.java:94)2022-07-07T17:55:16.4883648Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-07-07T17:55:16.4884261Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-07-07T17:55:16.4884907Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-07-07T17:55:16.4885487Z at java.lang.reflect.Method.invoke(Method.java:498)2022-07-07T17:55:16.4886039Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-07-07T17:55:16.4886697Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-07-07T17:55:16.4887406Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-07-07T17:55:16.4888051Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2022-07-07T17:55:16.4888678Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-07-07T17:55:16.4889330Z at org.apache.flink.testutils.junit.RetryRule$RetryOnFailureStatement.evaluate(RetryRule.java:135)2022-07-07T17:55:16.4889974Z at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-07-07T17:55:16.4890554Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-07-07T17:55:16.4891101Z at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-07-07T17:55:16.4891705Z at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-07-07T17:55:16.4892306Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-07-07T17:55:16.4892901Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)2022-07-07T17:55:16.4893525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)2022-07-07T17:55:16.4894166Z at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-07-07T17:55:16.4894712Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-07-07T17:55:16.4895267Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-07-07T17:55:16.4895913Z at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-07-07T17:55:16.4896468Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-07-07T17:55:16.4897105Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-07-07T17:55:16.4897722Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-07-07T17:55:16.4898385Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-07-07T17:55:16.4898967Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-07-07T17:55:16.4899507Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2022-07-07T17:55:16.4900014Z at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-07-07T17:55:16.4900557Z at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-07-07T17:55:16.4901061Z at org.junit.runner.JUnitCore.run(JUnitCore.java:137)2022-07-07T17:55:16.4901555Z at org.junit.runner.JUnitCore.run(JUnitCore.java:115)2022-07-07T17:55:16.4902127Z at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)2022-07-07T17:55:16.4902780Z at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)2022-07-07T17:55:16.4903409Z at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)2022-07-07T17:55:16.4904156Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-07-07T17:55:16.4904915Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-07-07T17:55:16.4905695Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-07-07T17:55:16.4906427Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator$$Lambda$200/1902430796.accept(Unknown Source)2022-07-07T17:55:16.4907238Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-07-07T17:55:16.4908039Z at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-07-07T17:55:16.4908719Z at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-07-07T17:55:16.4909362Z at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-07-07T17:55:16.4910081Z at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-07-07T17:55:16.4910837Z at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-07-07T17:55:16.4911572Z at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-07-07T17:55:16.4912342Z at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-07-07T17:55:16.4913098Z at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-07-07T17:55:16.4913845Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-07-07T17:55:16.4914491Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-07-07T17:55:16.4915098Z at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-07-07T17:55:16.4915692Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37848&amp;view=logs&amp;j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&amp;t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="28490" opendate="2022-7-11 00:00:00" fixdate="2022-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce "ANALYZE TABLE" Syntax in sql parser</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="28491" opendate="2022-7-11 00:00:00" fixdate="2022-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce APPROX_COUNT_DISTINCT aggregate function for batch sql</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstLastValueAggFunctionWithOrderTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28492" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "ANALYZE TABLE" execution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesCatalog.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.Date.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataString.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataLong.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDouble.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBoolean.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBinary.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatistics.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="28493" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document to describe "ANALYZE TABLE" syntax</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="28495" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typos or mistakes of Flink CEP Document in the official website</summary>
      <description>1. "how you can migrate your job from an older Flink version to Flink-1.3." -&gt; "how you can migrate your job from an older Flink version to Flink-1.13."2. "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: { C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}" -&gt; "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}, {C A2 B}, {C A2 A3 B}, {C A2 A4 B}, {C A2 A3 A4 B}, {C A3 B}, {C A3 A4 B}, {C A4 B}"3. "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no elements mapped to the specified variable." -&gt; "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no events mapped to the PatternName."</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.zh.docs.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="28532" opendate="2022-7-13 00:00:00" fixdate="2022-8-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support InputFormat as full caching provider in lookup table source</summary>
      <description>Support InputFormat as full caching provider in lookup table source</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.KeySelectorUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="28544" opendate="2022-7-14 00:00:00" fixdate="2022-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch6SinkE2ECase failed with no space left on device</summary>
      <description>2022-07-13T02:49:13.5455800Z Jul 13 02:49:13 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.38 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase2022-07-13T02:49:13.5465965Z Jul 13 02:49:13 [ERROR] org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase Time elapsed: 49.38 s &lt;&lt;&lt; ERROR!2022-07-13T02:49:13.5466765Z Jul 13 02:49:13 java.lang.RuntimeException: Failed to build JobManager image2022-07-13T02:49:13.5467621Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:67)2022-07-13T02:49:13.5468645Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configure(FlinkTestcontainersConfigurator.java:147)2022-07-13T02:49:13.5469564Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkContainers$Builder.build(FlinkContainers.java:197)2022-07-13T02:49:13.5470467Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.&lt;init&gt;(FlinkContainerTestEnvironment.java:88)2022-07-13T02:49:13.5471424Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.&lt;init&gt;(FlinkContainerTestEnvironment.java:51)2022-07-13T02:49:13.5472504Z Jul 13 02:49:13 at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.&lt;init&gt;(ElasticsearchSinkE2ECaseBase.java:58)2022-07-13T02:49:13.5473388Z Jul 13 02:49:13 at org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.&lt;init&gt;(Elasticsearch6SinkE2ECase.java:36)2022-07-13T02:49:13.5474161Z Jul 13 02:49:13 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)2022-07-13T02:49:13.5474905Z Jul 13 02:49:13 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)2022-07-13T02:49:13.5475756Z Jul 13 02:49:13 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)2022-07-13T02:49:13.5476734Z Jul 13 02:49:13 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)2022-07-13T02:49:13.5477495Z Jul 13 02:49:13 at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:550)2022-07-13T02:49:13.5478313Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)2022-07-13T02:49:13.5479220Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2022-07-13T02:49:13.5480165Z Jul 13 02:49:13 at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)2022-07-13T02:49:13.5481038Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2022-07-13T02:49:13.5481944Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2022-07-13T02:49:13.5482875Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2022-07-13T02:49:13.5483764Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2022-07-13T02:49:13.5484642Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2022-07-13T02:49:13.5486123Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2022-07-13T02:49:13.5488185Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:77)2022-07-13T02:49:13.5488883Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:355)2022-07-13T02:49:13.5490237Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:302)2022-07-13T02:49:13.5491099Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)2022-07-13T02:49:13.5491840Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:280)2022-07-13T02:49:13.5492618Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272)2022-07-13T02:49:13.5493228Z Jul 13 02:49:13 at java.util.Optional.orElseGet(Optional.java:267)2022-07-13T02:49:13.5493835Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271)2022-07-13T02:49:13.5494551Z Jul 13 02:49:13 at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)2022-07-13T02:49:13.5495253Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197)2022-07-13T02:49:13.5495940Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-13T02:49:13.5496616Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196)2022-07-13T02:49:13.5497286Z Jul 13 02:49:13 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)2022-07-13T02:49:13.5497973Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)2022-07-13T02:49:13.5498653Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-13T02:49:13.5499323Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-07-13T02:49:13.5500024Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-07-13T02:49:13.5500655Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-07-13T02:49:13.5501345Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-13T02:49:13.5535107Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-07-13T02:49:13.5535791Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-07-13T02:49:13.5538031Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-07-13T02:49:13.5538994Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)2022-07-13T02:49:13.5539797Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-07-13T02:49:13.5540481Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-13T02:49:13.5541154Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-07-13T02:49:13.5541784Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-07-13T02:49:13.5542410Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-07-13T02:49:13.5543090Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-13T02:49:13.5543930Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-07-13T02:49:13.5544575Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-07-13T02:49:13.5545351Z Jul 13 02:49:13 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-07-13T02:49:13.5546083Z Jul 13 02:49:13 at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)2022-07-13T02:49:13.5546615Z Jul 13 02:49:13 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2022-07-13T02:49:13.5547162Z Jul 13 02:49:13 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2022-07-13T02:49:13.5547713Z Jul 13 02:49:13 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2022-07-13T02:49:13.5548444Z Jul 13 02:49:13 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)2022-07-13T02:49:13.5549641Z Jul 13 02:49:13 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image "flink-configured-jobmanager"2022-07-13T02:49:13.5550318Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)2022-07-13T02:49:13.5551080Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)2022-07-13T02:49:13.5551656Z Jul 13 02:49:13 ... 57 more2022-07-13T02:49:13.5552744Z Jul 13 02:49:13 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout: stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device2022-07-13T02:49:13.5553633Z Jul 13 02:49:13 at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)2022-07-13T02:49:13.5554224Z Jul 13 02:49:13 at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)2022-07-13T02:49:13.5554761Z Jul 13 02:49:13 at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)2022-07-13T02:49:13.5555373Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:252)2022-07-13T02:49:13.5706429Z Jul 13 02:49:13 at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)2022-07-13T02:49:13.5707070Z Jul 13 02:49:13 ... 58 more2022-07-13T02:49:13.5712449Z Jul 13 02:49:13 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout: stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device2022-07-13T02:49:13.5765291Z Jul 13 02:49:13 at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)2022-07-13T02:49:13.5766129Z Jul 13 02:49:13 at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)2022-07-13T02:49:13.5766802Z Jul 13 02:49:13 at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:147)2022-07-13T02:49:13.5767436Z Jul 13 02:49:13 at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)2022-07-13T02:49:13.5768029Z Jul 13 02:49:13 at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)2022-07-13T02:49:13.5768573Z Jul 13 02:49:13 at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)2022-07-13T02:49:13.5769083Z Jul 13 02:49:13 at java.util.concurrent.FutureTask.run(FutureTask.java:266)2022-07-13T02:49:13.5769625Z Jul 13 02:49:13 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2022-07-13T02:49:13.5770223Z Jul 13 02:49:13 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2022-07-13T02:49:13.5771000Z Jul 13 02:49:13 at java.lang.Thread.run(Thread.java:750)2022-07-13T02:49:13.5771336Z Jul 13 02:49:13 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38110&amp;view=logs&amp;j=87489130-75dc-54e4-1f45-80c30aa367a3&amp;t=73da6d75-f30d-5d5a-acbe-487a9dcff678</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28559" opendate="2022-7-15 00:00:00" fixdate="2022-7-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support DataStream PythonKeyedProcessOperator in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.EmbeddedPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.converters.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.runtime.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2856" opendate="2015-10-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink.version property into quickstart archetype</summary>
      <description>With the quickstarts we're currently creating, users have to manually change all the dependencies if they're changing the flink version.I propose to introduce a property for setting the flink version.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.change-version</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28585" opendate="2022-7-18 00:00:00" fixdate="2022-7-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Speculative execution for InputFormat sources</summary>
      <description>This task enables InputFormat sources for speculative execution.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="28586" opendate="2022-7-18 00:00:00" fixdate="2022-7-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Speculative execution for new sources</summary>
      <description>This task enables new sources(FLIP-27) for speculative execution.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorAlignmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.TestingOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorEventGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingSplitEnumeratorContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.TestingSplitEnumerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContextTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.EventReceivingTasks.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SplitAssignmentTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.SubtaskAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.ExecutionSubtaskAccess.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="28588" opendate="2022-7-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance REST API for Speculative Execution</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.AggregatedTaskDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.webmonitor.ClusterOverview.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceOverview.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.messages.ClusterOverviewWithVersion.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.messages.ClusterOverviewWithVersionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.webmonitor.JobDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.MetricDumpSerialization.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.QueryScopeInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalOperatorMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.MutableIOMetrics.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.webmonitor.JobDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.dump.MetricDumpSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.dump.QueryScopeInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedSpeculativeExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfo.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskAttemptHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28589" opendate="2022-7-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enhance Web UI for Speculative Execution</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-backpressure.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.list.task-manager-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.statistic.overview-statistic.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.overview.statistic.overview-statistic.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.overview.ts</file>
    </fixedFiles>
  </bug>
  <bug id="28599" opendate="2022-7-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding FlinkJoinToMultiJoinRule to support translating left/right outer join to multi join</summary>
      <description>Now, Flink use Calcite's rule JOIN_TO_MULTI_JOIN to convert multiple joins into a join set, which can be used by join reorder. However, calcite's rule can not adapte to all outer joins. For left or right outer join, if they meet certain conditions, it can also be converted to multi join. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinToMultiJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.JoinReorderTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkJoinToMultiJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="28608" opendate="2022-7-19 00:00:00" fixdate="2022-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hadoop FS token renewer configurable</summary>
      <description>Please see issue in gist: https://gist.github.com/JackWangCS/0b1ec2c1137c686ab874124569063234</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.HadoopFSDelegationTokenProviderITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.HadoopFSDelegationTokenProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="28617" opendate="2022-7-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SqlGatewayService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28628" opendate="2022-7-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce operation execution plugin</summary>
      <description>Hive dialect may has his own operation execution logic for some operations. So, it'll be better introduce a operation excution plugin to delegate Hive or other dialects's execution logic.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.ParserFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultParserFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.PlannerMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Planner.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28629" opendate="2022-7-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetCatalogs in the HiveServer2 Endpoint</summary>
      <description>Implement to getCatalogs API in the HiveServer2 Endpoint.</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.Session.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.OperationInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="28659" opendate="2022-7-24 00:00:00" fixdate="2022-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-java</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.PartitionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.RequiredParametersTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.PlanGeneratorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.ParameterToolTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.OptionsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.MultipleParameterToolTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.AbstractParameterToolTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.TypeExtractionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.SummaryAggregatorFactoryTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.StringValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.StringSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.ShortValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.ShortSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.LongValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.LongSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.IntegerValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.IntegerSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.FloatValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.FloatSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.DoubleValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.DoubleSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.CompensatedSumTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.BooleanValueSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.BooleanSummaryAggregatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.summarize.aggregation.AggregateCombineHarness.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sampling.RandomSamplerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.SortPartitionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.RightOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.hadoopcompatibility.HadoopUtilsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.io.SequentialFormatTestBase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.io.SerializedFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.CoGroupOperatorCollectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.InnerJoinOperatorBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.ReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionAccumulatorsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionIterationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionWithBroadcastVariableTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.ExecutorDiscoveryAndJobClientTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SelectByFunctionsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesPrecedenceTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CollectionInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.FromElementsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrimitiveInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrintingOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.RowCsvInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.TextInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.TypeSerializerFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.MultipleInvokationsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.NamesTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.AggregateTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.BulkIterationTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.CoGroupSortTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DeltaIterationTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DistinctTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.ReduceTranslationTests.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.UnionTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.AggregateOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DataSinkTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.FirstNOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.FullOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupCombineOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.LeftOuterJoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.OperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="28660" opendate="2022-7-24 00:00:00" fixdate="2022-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify logs of blocklist</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blocklist.DefaultBlocklistTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blocklist.DefaultBlocklistTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blocklist.DefaultBlocklistHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blocklist.BlocklistTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28682" opendate="2022-7-26 00:00:00" fixdate="2022-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support join hint in flink batch</summary>
      <description>Add logic about join hint in batch rules.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQueryTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkLogicalRelFactories.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.StreamNonDeterministicUpdatePlanVisitor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHintStrategies.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHints.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
    </fixedFiles>
  </bug>
  <bug id="28688" opendate="2022-7-26 00:00:00" fixdate="2022-8-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support DataStream PythonWindowOperator in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractTwoInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.converters.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.window.window.operator.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.timerservice.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.runtime.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="28698" opendate="2022-7-27 00:00:00" fixdate="2022-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The display order of aggregated metrics should follow the order of task state transitions</summary>
      <description> Currently, the display order of task state duration is INITIALIZING, CREATED, SCHEDULED, RUNNING, DEPLOYING. I think it would be more reasonable to change to CAEATED, SCHEDULED, DEPLOYING, INITIALIZING, RUNNING, which is follow the order of task state transitions.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.table-aggregated-metrics.table-aggregated-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex.ts</file>
    </fixedFiles>
  </bug>
  <bug id="28701" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimize the explosion range during failover for hybrid shuffle</summary>
      <description>In hybrid shuffle mode, there are currently two strategies to control spilling. For the full spilling strategy, the data is guaranteed to be persistent to the disk after task finished. When a failover occurs, if the upstream has been finished, the data should be recovered directly from the disk file without re-compute. For selective spilling strategy, the entire topology must be restarted.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamExchangeMode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.GlobalStreamExchangeMode.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.BatchShuffleMode.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="28708" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce planner rules to optimize dpp pattern</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdPercentageOriginalRows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.FilterPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="28709" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement dynamic filtering operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SourceTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperatorFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="28745" opendate="2022-7-29 00:00:00" fixdate="2022-8-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support DataStream PythonCoProcessOperator and PythonKeyedCoProcessOperator in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="28752" opendate="2022-7-30 00:00:00" fixdate="2022-8-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add the json plan support in Python UDFs</summary>
      <description>In release-1.15, we removed the json plan support in https://issues.apache.org/jira/browse/FLINK-26060. Since we have updated PyFlink to use the new type system in https://issues.apache.org/jira/browse/FLINK-25231, we need to add the json plan support again.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.ExecNodeMetadataUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="28753" opendate="2022-7-30 00:00:00" fixdate="2022-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve FilterIntoJoinRule which could push some predicate to another side</summary>
      <description>for sql: SELECT * FROM MyTable1 join MyTable2 ON a1 = a2 AND a1 = 2a1 = 2 can be pushed into both left side and right side. but currently only left side will be pushed by FilterIntoJoinRule.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.JoinHintResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.ClearQueryBlockAliasResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.ShuffleMergeJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.ShuffleHashJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.NestLoopJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.JoinHintTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.joinhint.BroadcastJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="28767" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlGatewayServiceITCase.testCancelOperation failed with AssertionFailedError</summary>
      <description>2022-07-31T03:32:09.3148584Z Jul 31 03:32:09 [ERROR] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.268 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceITCase2022-07-31T03:32:09.3205977Z Jul 31 03:32:09 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation Time elapsed: 0.008 s &lt;&lt;&lt; FAILURE!2022-07-31T03:32:09.3207151Z Jul 31 03:32:09 org.opentest4j.AssertionFailedError: expected: &lt;org.apache.flink.table.gateway.api.results.OperationInfo@ab4fed9f&gt; but was: &lt;org.apache.flink.table.gateway.api.results.OperationInfo@ea9e78d5&gt;2022-07-31T03:32:09.3207956Z Jul 31 03:32:09 at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)2022-07-31T03:32:09.3211582Z Jul 31 03:32:09 at org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)2022-07-31T03:32:09.3212267Z Jul 31 03:32:09 at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182)2022-07-31T03:32:09.3212945Z Jul 31 03:32:09 at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:177)2022-07-31T03:32:09.3213607Z Jul 31 03:32:09 at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1141)2022-07-31T03:32:09.3216761Z Jul 31 03:32:09 at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation(SqlGatewayServiceITCase.java:245)2022-07-31T03:32:09.3217645Z Jul 31 03:32:09 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-07-31T03:32:09.3218243Z Jul 31 03:32:09 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-07-31T03:32:09.3218971Z Jul 31 03:32:09 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-07-31T03:32:09.3219622Z Jul 31 03:32:09 at java.lang.reflect.Method.invoke(Method.java:498)2022-07-31T03:32:09.3227901Z Jul 31 03:32:09 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)2022-07-31T03:32:09.3228714Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2022-07-31T03:32:09.3229561Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2022-07-31T03:32:09.3230353Z Jul 31 03:32:09 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2022-07-31T03:32:09.3231409Z Jul 31 03:32:09 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2022-07-31T03:32:09.3235059Z Jul 31 03:32:09 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2022-07-31T03:32:09.3236100Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2022-07-31T03:32:09.3236978Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2022-07-31T03:32:09.3237841Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2022-07-31T03:32:09.3241384Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2022-07-31T03:32:09.3242557Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2022-07-31T03:32:09.3243372Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2022-07-31T03:32:09.3244155Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2022-07-31T03:32:09.3244899Z Jul 31 03:32:09 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2022-07-31T03:32:09.3249715Z Jul 31 03:32:09 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)2022-07-31T03:32:09.3250594Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3251432Z Jul 31 03:32:09 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)2022-07-31T03:32:09.3252291Z Jul 31 03:32:09 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)2022-07-31T03:32:09.3253055Z Jul 31 03:32:09 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)2022-07-31T03:32:09.3256625Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)2022-07-31T03:32:09.3257465Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3258297Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-07-31T03:32:09.3259062Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-07-31T03:32:09.3262851Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-07-31T03:32:09.3263723Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3264528Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-07-31T03:32:09.3265259Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-07-31T03:32:09.3266191Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-07-31T03:32:09.3270216Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)2022-07-31T03:32:09.3271316Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)2022-07-31T03:32:09.3272431Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-07-31T03:32:09.3273258Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3277605Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-07-31T03:32:09.3278454Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-07-31T03:32:09.3279180Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-07-31T03:32:09.3280019Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3280980Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-07-31T03:32:09.3287071Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-07-31T03:32:09.3288036Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-07-31T03:32:09.3289128Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)2022-07-31T03:32:09.3290078Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-07-31T03:32:09.3294254Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3295128Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-07-31T03:32:09.3295912Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-07-31T03:32:09.3296679Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-07-31T03:32:09.3297504Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-07-31T03:32:09.3305416Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-07-31T03:32:09.3306251Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-07-31T03:32:09.3307191Z Jul 31 03:32:09 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-07-31T03:32:09.3308099Z Jul 31 03:32:09 at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)2022-07-31T03:32:09.3308702Z Jul 31 03:32:09 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2022-07-31T03:32:09.3315595Z Jul 31 03:32:09 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2022-07-31T03:32:09.3316450Z Jul 31 03:32:09 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2022-07-31T03:32:09.3317133Z Jul 31 03:32:09 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38962&amp;view=logs&amp;j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&amp;t=cdd32e0b-6047-565b-c58f-14054472f1be</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.SqlGatewayE2ECase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28768" opendate="2022-8-1 00:00:00" fixdate="2022-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update JUnit5 to v5.9.1</summary>
      <description>Junit 5.9.0 is releasedwith release notes https://junit.org/junit5/docs/current/release-notes/#release-notes-5.9.0</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestExternalHandlersITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.test.java.org.apache.flink.dist.DynamicParameterITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SmokeKafkaITCase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="28772" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect supports add jar</summary>
      <description>Flink dialect provides AddJarOperation, and Hive dialect will delegate to Flink's parser for "add jar xxx". But the behavior  between Flink and Hive for add command is different. The main difference is Flink dialect requires quotation but Hive dialect doesn't.For better compatibility,  we need to port it to Hive dialect.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="28773" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hive sink not write a success file after finish writing in batch mode</summary>
      <description>Currently, in stream mode, we allow user to configure commit policy, but in batch mode, we don't provide such way and it will always commit to metastore. But user expect other commit policy such like write a success file in batch mode. So, it'll be better to support write a success file after finish writing.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.PartitionCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionLoader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionCommitPolicy.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28774" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow user to configure whether to enable sort or not when it&amp;#39;s for dynamic parition writing for HiveSource</summary>
      <description>In HiveSource, when it's for inserting into dynamic parition, it'll always add a sort node which may be time consuming.It'll be better to allow users  to configure whether to add a sort or not in the case for inserting into dynamic parition.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="28780" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>function docs of dayofmonth is not correct</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28781" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid Shuffle should support compression</summary>
      <description>Compression is a useful feature for batch jobs, which can significantly reduce disk load and the amount of data transferred over the network. Hybrid shuffle should also support the compression of spilled data, especially under the full spilling strategy.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.ShuffleCompressionITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpillerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpiller.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28782" opendate="2022-8-3 00:00:00" fixdate="2022-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink compaction in PyFlink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.connectors.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.file.system.py</file>
    </fixedFiles>
  </bug>
  <bug id="28785" opendate="2022-8-3 00:00:00" fixdate="2022-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid shuffle consumer thread and upstream thread may have dead lock</summary>
      <description>In hybrid shuffle mode, subpartition view lock will be acquired by consumer thread, and further wait the read lock of MemoryDataManager. But MemoryDataManager may acquire write lock to make a global spilling decision, and then wait subpartition view lock to get consuming offset. In this case, deadlock will occurs.consumer thread : acqurie subpartition lock -&gt; wait read lock.upstream thread  : acquire write lock -&gt; wait subpartition lock.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingSubpartitionViewInternalOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionViewInternalOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="28788" opendate="2022-8-3 00:00:00" fixdate="2022-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SideOutput in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.process.AbstractExternalDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonWindowOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonKeyedCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractTwoInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.DataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="28791" opendate="2022-8-3 00:00:00" fixdate="2022-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-sql-gateway-test does not compile on Java 11</summary>
      <description>Could not find artifact jdk.tools:jdk.tools:jar:1.7</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28793" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetInfo for HiveServer2 Endpoint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28796" opendate="2022-8-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Statement Completement API for sql gateway rest endpoint</summary>
      <description>SQL Gateway supports various clients: sql client, rest, hiveserver2, etc. Given the 1.16 feature freeze date, we won't be able to finish all the endpoints. Thus, we'd exclude one of the rest apis (tracked by this ticket) from FLINK-28163 Introduce the statement related API for REST endpoint - ASF JIRA (apache.org)], which is only needed by the sql client, and still try to complete the remaining of them.In other words, we'd expect the sql gateway to support rest &amp; hiveserver2 apis in 1.16, and sql client in 1.17.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v2.snapshot</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28797" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive source should also support verctor reading for complex data type when with parquet format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="28799" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid shuffle can&amp;#39;t schedule graph contains blocking edge</summary>
      <description>Based on TPC-DS test, we found that hybrid shuffle can't schedule graph contains blocking edge. The reason is that some batch operators will forcibly set the exchange mode to blocking, which breaks ALL_ EDGE_HYBRID‘s constraint makes the scheduling deadlock.We should think of a better way to support the scheduling the graph of all kinds of edges, including hybrid edge.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingPipelinedRegion.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingPipelinedRegionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingPipelinedRegion.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingPipelinedRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="28814" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update postgres driver because of CVE-2022-31197</summary>
      <description>More details about CVE at pgjdbc repo page https://github.com/pgjdbc/pgjdbc/security/advisories/GHSA-r38f-c4h4-hqq2</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28815" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "Real Time Reporting with the Table API" page into Chinese</summary>
      <description>Page "Real Time Reporting with the Table API" need to be translated in Chinese.I will be pleasure to take this PR.PS:Also willing fix a typo on page "Fraud Detection with the DataStream API"  in chinese with the same PR.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.try-flink.table.api.md</file>
      <file type="M">docs.content.zh.docs.try-flink.datastream.md</file>
    </fixedFiles>
  </bug>
  <bug id="28836" opendate="2022-8-5 00:00:00" fixdate="2022-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support broadcast in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonKeyedBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractTwoInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.converters.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="28853" opendate="2022-8-7 00:00:00" fixdate="2022-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-217 Support watermark alignment of source splits</summary>
      <description>This improvement implements FLIP-217 to support watermark alignment of source splits.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.WatermarkToDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.TimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSourceReader.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.IndexedCombinedWatermarkStatus.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarOrderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.fetcher.KafkaSourceFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManagerTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
      <file type="M">docs.content.docs.dev.datastream.sources.md</file>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.sources.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorSplitWatermarkAlignmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.CombinedWatermarkStatus.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherPauseResumeSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.PauseOrResumeSplitsTask.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="28854" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Migrate JDBC table connector to the new LookupFunction interface</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.lookup.cache.DefaultLookupCache.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-jdbc.archunit-violations.e3f99de4-2505-469a-8910-ce53aec7cd1d</file>
    </fixedFiles>
  </bug>
  <bug id="28857" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Document for DataStream Cache API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="28868" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Migrate HBase table connector to the new LookupFunction interface</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.util.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.source.HBaseRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.source.AbstractHBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseLookupOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.source.HBaseRowDataAsyncLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.source.HBaseRowDataAsyncLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.source.HBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.source.HBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.b8900323-6aab-4e7e-9b17-f53b3c3dca46</file>
    </fixedFiles>
  </bug>
  <bug id="28871" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make DPP also works if batch shuffle mode is not ALL_BLOCKING</summary>
      <description>Currently dpp only works when all edges is blocking. Otherwise if the dynamic filtering data collector is located in the same region with the fact source, the fact source would not be started after the data collector task.To fix this issue, we'll force the collector task's output edges to be blocking. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DynamicFilteringDependencyProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
    </fixedFiles>
  </bug>
  <bug id="28873" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make "jobmanager.scheduler" visible in documentation</summary>
      <description>Currently, the option jobmanager.scheduler is still excluded from documentation. But in fact, this option is already used as a public interface (this option needs to be configured by users when using AdaptiveScheduler and AdaptiveBatchScheduler).We should remove the ExcludeFromDocumentation to make it visible in the documentation.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNGFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.ExecutionConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="28889" opendate="2022-8-9 00:00:00" fixdate="2022-10-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Hybrid shuffle should supports multiple consumer</summary>
      <description>Hybrid shuffle does not support multiple consumer for single subpartition data. This will bring some defects, such as the inability to support partition reuse, speculative execution. In particular, it cannot support broadcast optimization, that is, hybrid shuffle writes multiple copies of broadcast data. This will cause a waste of memory and disk space and affect the performance of shuffle write phase. Ideally, for the full spilling strategy, any broadcast data (record or event) should only write one piece of data in the memory, and the same is true for the disk.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.PartitionDescriptorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingSubpartitionViewInternalOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingMemoryDataManagerOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionViewInternalOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsDataView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFullSpillingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingSpillingInfoProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsBufferContextTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSpillingInfoProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSelectiveSpillingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFullSpillingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsBufferContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="28899" opendate="2022-8-10 00:00:00" fixdate="2022-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LOOKUP hint with retry option on async lookup mode</summary>
      <description>Fix the broken path for async lookup with retry options in  LOOKUP hint.Will use a `RetryableAsyncLookupFunctionDelegator` instead of `AsyncWaitOperator`</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.ResultRetryStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="28916" opendate="2022-8-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add e2e test for create function using jar syntax</summary>
      <description>Add e2e test for create function using jar syntax that using remote hdfs jar</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.src.test.java.org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28917" opendate="2022-8-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sql test for adaptive hash join</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashBucketArea.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="28928" opendate="2022-8-11 00:00:00" fixdate="2022-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add E2E test for hybrid shuffle mode</summary>
      <description>Add E2E IT cases for hybrid shuffle mode</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.sql.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-batch-sql-test.src.main.java.org.apache.flink.sql.tests.BatchSQLTestProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BlockingShuffleITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28932" opendate="2022-8-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Builder#withConfiguration instead of deprecated EnvironmentSettings#fromConfiguration</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="28934" opendate="2022-8-11 00:00:00" fixdate="2022-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Source put all the splits to only one parallelism when using Exclusive subscription</summary>
      <description>The image here shows if we start a Flink application with four parallelism and four splits. All the splits would be sent to the first added reader. This is because we don't assign splits by pre-divide splits according to the size of parallelism. The readers are added to the enumerator one by one in the first bootstrap.</description>
      <version>1.14.5,1.15.1,1.16.0</version>
      <fixedVersion>1.16.0,1.14.6,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.CursorPosition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarSerdeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.CollectIteratorAssertions.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.embedded.PulsarEmbeddedRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.f4d91193-72ba-4ce4-ad83-98f780dce581</file>
    </fixedFiles>
  </bug>
  <bug id="28936" opendate="2022-8-12 00:00:00" fixdate="2022-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix RSET endpoint can not serialize CHAR(0)</summary>
      <description>The current implementation doesn't align with the FLIP. We need to introduce a new serializer to fix this.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.StatementCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.cli.SqlGatewayOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.results.JsonResultSetSerDeTest.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ResultSet.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.JsonResultSetSerializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.JsonResultSetDeserializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ColumnInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="28938" opendate="2022-8-12 00:00:00" fixdate="2022-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HiveServer2 Endpoint can not set variable correctly</summary>
      <description>Hive JDBC URL also supports Hive variable replacement. But the current implementation doesn't finish this.   HiveServer2 Endoint should thorw exception to notify users if users tries to fetch logs.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveSetProcessor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.HiveJdbcParameterUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28948" opendate="2022-8-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more test coverage for lookup table full caching</summary>
      <description>Currently there's only IT case for lookup table full caching. We need to add more test cases to guard the correctness of it.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.functions.table.fullcache.FullCacheTestInputFormat.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.lookup.cache.DefaultLookupCacheTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalCacheMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="28955" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNHighAvailabilityITCase compile failed in hadoop3</summary>
      <description>It looks like we have not resolved the problem in hadoop3. This problem was not exposed before due to another test that failed to compile under hadoop3https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39951&amp;view=logs&amp;j=b1fcf054-9138-5463-c73c-a49979b9ac2a&amp;t=9291ac46-dd95-5135-b799-3839e65a86912022-08-14T00:36:43.9971481Z [ERROR] COMPILATION ERROR : 2022-08-14T00:36:43.9972508Z [INFO] -------------------------------------------------------------2022-08-14T00:36:43.9973460Z [ERROR] /__w/3/s/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNHighAvailabilityITCase.java:[53,31] package org.apache.curator.test does not exist2022-08-14T00:36:43.9974493Z [ERROR] /__w/3/s/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNHighAvailabilityITCase.java:[106,20] cannot find symbol2022-08-14T00:36:43.9975371Z symbol: class TestingServer2022-08-14T00:36:43.9975818Z location: class org.apache.flink.yarn.YARNHighAvailabilityITCase2022-08-14T00:36:43.9976253Z [INFO] 2 errors</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28965" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shouldn&amp;#39;t create empty partition when it&amp;#39;s for dynamic partition</summary>
      <description>Can be reproduced by the following code in HiveDialectTest: tableEnv.executeSql( "create table over1k_part_orc(\n" + " si smallint,\n" + " i int,\n" + " b bigint,\n" + " f float)\n" + " partitioned by (ds string, t tinyint) stored as orc");tableEnv.executeSql( "create table over1k(\n" + " t tinyint,\n" + " si smallint,\n" + " i int,\n" + " b bigint,\n" + " f float,\n" + " d double,\n" + " bo boolean,\n" + " s string,\n" + " ts timestamp,\n" + " dec decimal(4,2),\n" + " bin binary)");tableEnv.executeSql( "insert overwrite table over1k_part_orc partition(ds=\"foo\", t)" + " select si,i,b,f,t from over1k where t is null or t=27 order by si") .await();  Althogh it's for dynamic partition, the current code will try to create a partition for it (ds='foo') when there's no data wrotten to it , so the exception will be thrown since the partition spec (ds='foo') is not full pathCaused by: MetaException(message:Invalid partition key &amp; values; keys [ds, t, ], values [foo, ])  </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28971" opendate="2022-8-15 00:00:00" fixdate="2022-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds user documentation for the new LOOKUP hint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.hints.md</file>
    </fixedFiles>
  </bug>
  <bug id="28972" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods of StartCursor and StopCursor to align the Java</summary>
      <description>Add fromPublishTime in the StartCursor classAdd afterEventTime and afterPublishTime in the StopCursor class</description>
      <version>1.14.5,1.15.1,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.connectors.tests.test.pulsar.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="28974" opendate="2022-8-15 00:00:00" fixdate="2022-9-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add doc for the API and Option of sql gateway rest endpoint</summary>
      <description>Add document for the API and Option of sql gateway rest endpoint.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.util.GetInfoHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.util.GetApiVersionHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.statement.FetchResultsHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.statement.ExecuteStatementHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.session.TriggerSessionHeartbeatHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.session.OpenSessionHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.session.GetSessionConfigHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.session.CloseSessionHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.operation.GetOperationStatusHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.operation.CloseOperationHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.operation.CancelOperationHeaders.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28986" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UNNEST function with nested filter fails to generate plan</summary>
      <description>How to reproduceadd the following case to TableEnvironmentITCase@Testdef debug(): Unit = { tEnv.executeSql( s""" |CREATE TEMPORARY TABLE source_kafka_wip_his_all ( | GUID varchar, | OPERATION varchar, | PRODUCTID varchar, | LOTNO varchar, | SERIALNO varchar, | QUERYSERIALNO varchar, | SERIALNO1 varchar, | SERIALNO2 varchar, | WIPORDERNO varchar, | WIPORDERTYPE varchar, | VIRTUALLOT varchar, | PREOPERATION varchar, | NORMALPREOPERATION varchar, | PROCESSID varchar, | EQUIPMENT varchar, | INBOUNDDATE varchar, | OUTBOUNDDATE varchar, | REWORK varchar, | REWORKPROCESSID varchar, | CONTAINER varchar, | WIPCONTENTCLASSID varchar, | STATUSCODE varchar, | WIPSTATUS varchar, | TESTPROCESSID varchar, | TESTORDERTYPE varchar, | TESTORDER varchar, | TEST varchar, | SORTINGPROCESSID varchar, | SORTINGORDERTYPE varchar, | SORTINGORDER varchar, | SORTING varchar, | MINO varchar, | GROUPCODE varchar, | HIGHLOWGROUP varchar, | PRODUCTNO varchar, | FACILITY varchar, | WIPLINE varchar, | CHILDEQUCODE varchar, | STATION varchar, | QTY varchar, | PASS_FLAG varchar, | DEFECTCODELIST varchar, | ISFIRST varchar, | PARALIST ARRAY&lt;ROW(GUID string,WIP_HIS_GUID string,QUERYSERIALNO string,OPERATION string,REWORKPROCESSID string,CHARACTERISTIC string,CHARACTERISTICREVISION string,CHARACTERISTICTYPE string,CHARACTERISTICCLASS string,UPPERCONTROLLIMIT string,TARGETVALUE string,LOWERCONTROLLIMIT string,TESTVALUE string,TESTATTRIBUTE string,TESTINGSTARTDATE string,TESTFINISHDATE string,UOMCODE string,DEFECTCODE string,SPECPARAMID string,STATION string,GP_TIME string,REFERENCEID string,LASTUPDATEON string,LASTUPDATEDBY string,CREATEDON string,CREATEDBY string,ACTIVE string,LASTDELETEON string,LASTDELETEDBY string,LASTREACTIVATEON string,LASTREACTIVATEDBY string,ARCHIVEID string,LASTARCHIVEON string,LASTARCHIVEDBY string,LASTRESTOREON string,LASTRESTOREDBY string,ROWVERSIONSTAMP string)&gt;, | REFERENCEID varchar, | LASTUPDATEON varchar, | LASTUPDATEDBY varchar, | CREATEDON varchar, | CREATEDBY varchar, | ACTIVE varchar, | LASTDELETEON varchar, | LASTDELETEDBY varchar, | LASTREACTIVATEON varchar, | LASTREACTIVATEDBY varchar, | ARCHIVEID varchar, | LASTARCHIVEON varchar, | LASTARCHIVEDBY varchar, | LASTRESTOREON varchar, | LASTRESTOREDBY varchar, | ROWVERSIONSTAMP varchar, | proctime as PROCTIME() | ) with ( | 'connector' = 'datagen' |) |""".stripMargin) tEnv.executeSql( s""" |create TEMPORARY view transform_main_data as |select | r.GUID as wip_his_guid, | r.EQUIPMENT as equipment, | r.WIPLINE as wipline, | r.STATION as station, | cast(r.PROCESSID as decimal) as processid, | r.PRODUCTNO as productno, | t.TESTFINISHDATE as testfinishdate, | t.OPERATION as operation, | t.CHARACTERISTIC as characteristic, | t.LOWERCONTROLLIMIT as lowercontrollimit, | t.UPPERCONTROLLIMIT as uppercontrollimit, | t.TARGETVALUE as targetvalue, | t.DEFECTCODE as defectcode, | t.TESTVALUE as testvalue, | t.CHARACTERISTICTYPE as characteristictype, | proctime | from | (select | GUID, | EQUIPMENT, | WIPLINE, | STATION, | PROCESSID, | PRODUCTNO, | PARALIST, | proctime | FROM source_kafka_wip_his_all) r | cross join | unnest(PARALIST) as t (GUID,WIP_HIS_GUID,QUERYSERIALNO,OPERATION,REWORKPROCESSID,CHARACTERISTIC,CHARACTERISTICREVISION,CHARACTERISTICTYPE,CHARACTERISTICCLASS,UPPERCONTROLLIMIT,TARGETVALUE,LOWERCONTROLLIMIT,TESTVALUE,TESTATTRIBUTE,TESTINGSTARTDATE,TESTFINISHDATE,UOMCODE,DEFECTCODE,SPECPARAMID,STATION,GP_TIME,REFERENCEID,LASTUPDATEON,LASTUPDATEDBY,CREATEDON,CREATEDBY,ACTIVE,LASTDELETEON,LASTDELETEDBY,LASTREACTIVATEON,LASTREACTIVATEDBY,ARCHIVEID,LASTARCHIVEON,LASTARCHIVEDBY,LASTRESTOREON,LASTRESTOREDBY,ROWVERSIONSTAMP) | where t.CHARACTERISTICTYPE = '2' |""".stripMargin) tEnv.executeSql( s""" |explain plan for |select * from transform_main_data |where operation not in ('G1208','G1209','G1211','G1213','G1206','G1207','G1214','G1215','G1282','G1292','G1216') |""".stripMargin).print()} Stacktraceorg.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}])   :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]])   :  +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]])   +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"]:VARCHAR(2147483647) CHARACTER SET "UTF-16LE"), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET "UTF-16LE"))])      +- Uncollect         +- LogicalProject(exprs=[[$cor1.PARALIST]])            +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])This exception indicates that the query uses an unsupported SQL feature.Please check the documentation for the set of currently supported SQL features.org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}]) :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]]) : +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]]) +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET "UTF-16LE"]:VARCHAR(2147483647) CHARACTER SET "UTF-16LE"), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET "UTF-16LE"))]) +- Uncollect +- LogicalProject(exprs=[[$cor1.PARALIST]]) +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])This exception indicates that the query uses an unsupported SQL feature.Please check the documentation for the set of currently supported SQL features.    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)    at scala.collection.Iterator.foreach(Iterator.scala:937)    at scala.collection.Iterator.foreach$(Iterator.scala:937)    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)    at scala.collection.IterableLike.foreach(IterableLike.scala:70)    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)    at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:527)    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:96)    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:51)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:695)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1356)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:733)    at org.apache.flink.table.api.TableEnvironmentITCase.debug(TableEnvironmentITCase.scala:695)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)    at org.junit.runners.Suite.runChild(Suite.java:128)    at org.junit.runners.Suite.runChild(Suite.java:27)    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)    at org.junit.rules.RunRules.evaluate(RunRules.java:20)    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=LOGICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, ModifyKindSetTraitDef=[NONE], UpdateKindTraitDef=[NONE].Missing conversion is Uncollect[convention: NONE -&gt; LOGICAL]There is 1 empty subset: rel#485:RelSubset#4.LOGICAL.any.None: 0.[NONE].[NONE], the relevant part of the original plan is as follows460:Uncollect  458:LogicalProject(subset=[rel#459:RelSubset#3.NONE.any.None: 0.[NONE].[NONE]], PARALIST=[$cor1.PARALIST])    17:LogicalValues(subset=[rel#457:RelSubset#2.NONE.any.None: 0.[NONE].[NONE]], tuples=[[{ 0 }]])</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.UnnestTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="28990" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix BatchPhysicalDynamicFilteringDataCollector with empty output type</summary>
      <description>When dpp fact side have calc node, and partition key index was changed in calc node, BatchPhysicalDynamicFilteringDataCollector will be set with a empty output type, which will throw exception in HiveSourceDynamicFileEnumerator  while check argument:Preconditions.checkArgument(rowType.getFieldCount() == dynamicFilterPartitionKeys.size()); in method setDynamicFilteringData  </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="28992" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Ndv takes the max value instead of sum of all partitions when getting partition table column stats</summary>
      <description>Now, when we obtain column statistics Ndv of the partition table, we use the method of taking sum of Ndv of all partitions, which will cause the obtained stats to be far from the real stats. So now we take the max value instead of sum of all partitions.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemStatisticsReportTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkRecomputeStatisticsProgram.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.plan.stats.TableStatsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.plan.stats.TableStats.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.plan.stats.ColumnStats.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveStatsUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="28994" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable withCredentials for Flink UI</summary>
      <description>Some environments require cookies to authenticate the Flink UI. By enabling the withCredentials flag, Angular will send cookies along with the request.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
    </fixedFiles>
  </bug>
  <bug id="28995" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NPE problem: createRowData will return null while encounter hive default partition</summary>
      <description>createRowData will return null while encounter hive default partition, which will make ‘data.contains(partitionRow)’ throw NPE.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSourceDynamicFileEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceDynamicFileEnumerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="28996" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move parameter parsing into Datadog reporter factory</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporterFactory.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29005" opendate="2022-8-17 00:00:00" fixdate="2022-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet row type reader should not return null value when some child fields is null</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.columnar.vector.heap.HeapRowVector.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.RowColumnReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29007" opendate="2022-8-17 00:00:00" fixdate="2022-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UsingRemoteJarITCase failed with NPE in hadoop3</summary>
      <description>2022-08-17T03:01:45.9844224Z Aug 17 03:01:45 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar Time elapsed: 1.319 s &lt;&lt;&lt; FAILURE!2022-08-17T03:01:45.9844747Z Aug 17 03:01:45 org.opentest4j.MultipleFailuresError: 2022-08-17T03:01:45.9845163Z Aug 17 03:01:45 Multiple Failures (2 failures)2022-08-17T03:01:45.9845669Z Aug 17 03:01:45 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:45.9846199Z Aug 17 03:01:45 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:45.9846801Z Aug 17 03:01:45 at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:196)2022-08-17T03:01:45.9847582Z Aug 17 03:01:45 at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:226)2022-08-17T03:01:45.9848377Z Aug 17 03:01:45 at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:192)2022-08-17T03:01:45.9849199Z Aug 17 03:01:45 at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:79)2022-08-17T03:01:45.9849972Z Aug 17 03:01:45 at org.junit.runner.notification.SynchronizedRunListener.testFinished(SynchronizedRunListener.java:87)2022-08-17T03:01:45.9850715Z Aug 17 03:01:45 at org.junit.runner.notification.RunNotifier$9.notifyListener(RunNotifier.java:225)2022-08-17T03:01:45.9851413Z Aug 17 03:01:45 at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)2022-08-17T03:01:45.9852105Z Aug 17 03:01:45 at org.junit.runner.notification.RunNotifier.fireTestFinished(RunNotifier.java:222)2022-08-17T03:01:45.9852828Z Aug 17 03:01:45 at org.junit.internal.runners.model.EachTestNotifier.fireTestFinished(EachTestNotifier.java:38)2022-08-17T03:01:45.9853510Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:372)2022-08-17T03:01:45.9854156Z Aug 17 03:01:45 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)2022-08-17T03:01:45.9854864Z Aug 17 03:01:45 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)2022-08-17T03:01:45.9855512Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-08-17T03:01:45.9856125Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-08-17T03:01:45.9856751Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-08-17T03:01:45.9857372Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-08-17T03:01:45.9857977Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-08-17T03:01:45.9858581Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-08-17T03:01:45.9859177Z Aug 17 03:01:45 at org.junit.runners.Suite.runChild(Suite.java:128)2022-08-17T03:01:45.9859726Z Aug 17 03:01:45 at org.junit.runners.Suite.runChild(Suite.java:27)2022-08-17T03:01:45.9860299Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-08-17T03:01:45.9860905Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-08-17T03:01:45.9861508Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-08-17T03:01:45.9862134Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-08-17T03:01:45.9862831Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-08-17T03:01:45.9863477Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-08-17T03:01:45.9864128Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-08-17T03:01:45.9864729Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-08-17T03:01:45.9865289Z Aug 17 03:01:45 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)2022-08-17T03:01:45.9865851Z Aug 17 03:01:45 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)2022-08-17T03:01:45.9866564Z Aug 17 03:01:45 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)2022-08-17T03:01:45.9867293Z Aug 17 03:01:45 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)2022-08-17T03:01:45.9868004Z Aug 17 03:01:45 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)2022-08-17T03:01:45.9868759Z Aug 17 03:01:45 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-08-17T03:01:45.9869577Z Aug 17 03:01:45 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-08-17T03:01:45.9870401Z Aug 17 03:01:45 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-08-17T03:01:45.9871369Z Aug 17 03:01:45 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-08-17T03:01:45.9872223Z Aug 17 03:01:45 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-08-17T03:01:45.9872988Z Aug 17 03:01:45 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-08-17T03:01:45.9873698Z Aug 17 03:01:45 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-08-17T03:01:45.9874486Z Aug 17 03:01:45 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-08-17T03:01:45.9875313Z Aug 17 03:01:45 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-08-17T03:01:45.9876099Z Aug 17 03:01:45 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-08-17T03:01:45.9876922Z Aug 17 03:01:45 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-08-17T03:01:45.9877736Z Aug 17 03:01:45 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)2022-08-17T03:01:45.9878501Z Aug 17 03:01:45 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-08-17T03:01:45.9879214Z Aug 17 03:01:45 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-08-17T03:01:45.9879881Z Aug 17 03:01:45 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-08-17T03:01:45.9880543Z Aug 17 03:01:45 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)2022-08-17T03:01:45.9881179Z Aug 17 03:01:45 Suppressed: java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:45.9881755Z Aug 17 03:01:45 at org.junit.Assert.fail(Assert.java:89)2022-08-17T03:01:45.9882395Z Aug 17 03:01:45 at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.createHDFS(UsingRemoteJarITCase.java:88)2022-08-17T03:01:45.9883166Z Aug 17 03:01:45 at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.before(UsingRemoteJarITCase.java:68)2022-08-17T03:01:45.9883816Z Aug 17 03:01:45 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-08-17T03:01:45.9884518Z Aug 17 03:01:45 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-08-17T03:01:45.9885230Z Aug 17 03:01:45 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-08-17T03:01:45.9885853Z Aug 17 03:01:45 at java.lang.reflect.Method.invoke(Method.java:498)2022-08-17T03:01:45.9886488Z Aug 17 03:01:45 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-08-17T03:01:45.9887212Z Aug 17 03:01:45 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-08-17T03:01:45.9887921Z Aug 17 03:01:45 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-08-17T03:01:45.9888632Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)2022-08-17T03:01:45.9889325Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)2022-08-17T03:01:45.9889993Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-08-17T03:01:45.9890671Z Aug 17 03:01:45 at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)2022-08-17T03:01:45.9891333Z Aug 17 03:01:45 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-08-17T03:01:45.9891997Z Aug 17 03:01:45 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-08-17T03:01:45.9892716Z Aug 17 03:01:45 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-08-17T03:01:45.9893333Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-08-17T03:01:45.9893995Z Aug 17 03:01:45 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-08-17T03:01:45.9894654Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-08-17T03:01:45.9895132Z Aug 17 03:01:45 ... 39 more2022-08-17T03:01:45.9895540Z Aug 17 03:01:45 Suppressed: java.lang.NullPointerException2022-08-17T03:01:45.9896168Z Aug 17 03:01:45 at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.destroyHDFS(UsingRemoteJarITCase.java:95)2022-08-17T03:01:45.9896836Z Aug 17 03:01:45 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-08-17T03:01:45.9897442Z Aug 17 03:01:45 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-08-17T03:01:45.9898150Z Aug 17 03:01:45 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-08-17T03:01:45.9898792Z Aug 17 03:01:45 at java.lang.reflect.Method.invoke(Method.java:498)2022-08-17T03:01:45.9899423Z Aug 17 03:01:45 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-08-17T03:01:45.9900141Z Aug 17 03:01:45 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-08-17T03:01:45.9900850Z Aug 17 03:01:45 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-08-17T03:01:45.9901552Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)2022-08-17T03:01:45.9902219Z Aug 17 03:01:45 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)2022-08-17T03:01:45.9902895Z Aug 17 03:01:45 at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)2022-08-17T03:01:45.9903554Z Aug 17 03:01:45 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-08-17T03:01:45.9904220Z Aug 17 03:01:45 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-08-17T03:01:45.9904859Z Aug 17 03:01:45 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-08-17T03:01:45.9905473Z Aug 17 03:01:45 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-08-17T03:01:45.9906283Z Aug 17 03:01:45 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-08-17T03:01:45.9907052Z Aug 17 03:01:45 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-08-17T03:01:45.9907534Z Aug 17 03:01:45 ... 39 more2022-08-17T03:01:45.9907852Z Aug 17 03:01:45 2022-08-17T03:01:46.2525312Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2526850Z Aug 17 03:01:46 [INFO] Results:2022-08-17T03:01:46.2527307Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2527708Z Aug 17 03:01:46 [ERROR] Failures: 2022-08-17T03:01:46.2528297Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateCatalogFunctionUsingRemoteJar2022-08-17T03:01:46.2528903Z Aug 17 03:01:46 [ERROR] Run 1: Multiple Failures (2 failures)2022-08-17T03:01:46.2529519Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2551750Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2552413Z Aug 17 03:01:46 [ERROR] Run 2: Multiple Failures (2 failures)2022-08-17T03:01:46.2553047Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2553670Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2554125Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2554658Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporaryCatalogFunctionUsingRemoteJar2022-08-17T03:01:46.2555270Z Aug 17 03:01:46 [ERROR] Run 1: Multiple Failures (2 failures)2022-08-17T03:01:46.2555862Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2556665Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2557198Z Aug 17 03:01:46 [ERROR] Run 2: Multiple Failures (2 failures)2022-08-17T03:01:46.2557800Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2558407Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2558865Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2559376Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporarySystemFunctionUsingRemoteJar2022-08-17T03:01:46.2559989Z Aug 17 03:01:46 [ERROR] Run 1: Multiple Failures (2 failures)2022-08-17T03:01:46.2560589Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2561194Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2561726Z Aug 17 03:01:46 [ERROR] Run 2: Multiple Failures (2 failures)2022-08-17T03:01:46.2562317Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2562926Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2563375Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2563836Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar2022-08-17T03:01:46.2564373Z Aug 17 03:01:46 [ERROR] Run 1: Multiple Failures (2 failures)2022-08-17T03:01:46.2564979Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2565573Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2566104Z Aug 17 03:01:46 [ERROR] Run 2: Multiple Failures (2 failures)2022-08-17T03:01:46.2566702Z Aug 17 03:01:46 java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration2022-08-17T03:01:46.2567306Z Aug 17 03:01:46 java.lang.NullPointerException: &lt;no message&gt;2022-08-17T03:01:46.2567755Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2568104Z Aug 17 03:01:46 [INFO] 2022-08-17T03:01:46.2568576Z Aug 17 03:01:46 [ERROR] Tests run: 6, Failures: 4, Errors: 0, Skipped: 0https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&amp;view=logs&amp;j=87489130-75dc-54e4-1f45-80c30aa367a3&amp;t=73da6d75-f30d-5d5a-acbe-487a9dcff678</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29009" opendate="2022-8-17 00:00:00" fixdate="2022-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-end-to-end-tests-sql compile failed in hadoop3</summary>
      <description>2022-08-17T00:39:13.9082097Z Dependency convergence error for com.nimbusds:nimbus-jose-jwt:4.41.1 paths to dependency are:2022-08-17T00:39:13.9082987Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT2022-08-17T00:39:13.9083712Z +-org.apache.hadoop:hadoop-common:3.1.32022-08-17T00:39:13.9084340Z +-org.apache.hadoop:hadoop-auth:3.1.32022-08-17T00:39:13.9084963Z +-com.nimbusds:nimbus-jose-jwt:4.41.12022-08-17T00:39:13.9085616Z and2022-08-17T00:39:13.9086212Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT2022-08-17T00:39:13.9086864Z +-org.apache.hadoop:hadoop-common:3.1.32022-08-17T00:39:13.9087499Z +-org.apache.kerby:kerb-simplekdc:1.0.12022-08-17T00:39:13.9088125Z +-org.apache.kerby:kerb-client:1.0.12022-08-17T00:39:13.9088753Z +-org.apache.kerby:token-provider:1.0.12022-08-17T00:39:13.9089381Z +-com.nimbusds:nimbus-jose-jwt:3.102022-08-17T00:39:13.9089596Z 2022-08-17T00:39:13.9090061Z [WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:2022-08-17T00:39:13.9090651Z Failed while enforcing releasability. See above detailed error message.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&amp;view=logs&amp;j=b1fcf054-9138-5463-c73c-a49979b9ac2a&amp;t=9291ac46-dd95-5135-b799-3839e65a8691</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-sql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29012" opendate="2022-8-17 00:00:00" fixdate="2022-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink function doc is not correct</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29020" opendate="2022-8-18 00:00:00" fixdate="2022-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for CTAS feature</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="29022" opendate="2022-8-18 00:00:00" fixdate="2022-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for CREATE FUNCTION USING JAR feature</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="29023" opendate="2022-8-18 00:00:00" fixdate="2022-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Updating Jar statement document</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="29028" opendate="2022-8-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the missing cache api in Python DataStream API</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.content.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="29029" opendate="2022-8-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of CSV format doesn&amp;#39;t work in Thread Mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.datastream.formats.tests.test.csv.py</file>
    </fixedFiles>
  </bug>
  <bug id="29034" opendate="2022-8-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HYBRID_FULL result partition type is not yet reConsumable</summary>
      <description>HYBRID_FULL partitions can be consumed repeatedly, but it does not support concurrent consumption. So re-consumable is false, but double calculation can be avoided during failover. If we regard it as re-consumable, there will be problems when the partition is reused. Therefore, we temporarily set this field false, and reset it to true when HsResultPartition supports downstream concurrent consumption of multiple identical subpartitions.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamExchangeMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="29036" opendate="2022-8-18 00:00:00" fixdate="2022-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Code examples on the Data Sources page have errors</summary>
      <description>While reviewing the Data Source, some examples are slightly out of date.As an example, FutureNotifier doesn't exist any more.This page (as well as some javadoc) could be reviewed for correctness.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReaderContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.ReaderOutput.java</file>
      <file type="M">docs.content.docs.dev.datastream.sources.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.sources.md</file>
    </fixedFiles>
  </bug>
  <bug id="29041" opendate="2022-8-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility to test POJO compliance without any Kryo usage</summary>
      <description>Add a variant of the test util added in FLINK-28636 that additionally asserts that Kryo is not used for any contained field.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.types.PojoTestUtilsTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.types.PojoTestUtils.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug id="29047" opendate="2022-8-20 00:00:00" fixdate="2022-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[3.1] Shade the Flink-Kubernetes Fabric8 Kubernetes dependency with org.apache.flink.shaded prefix</summary>
      <description>For supporting stepDecorators plugin mechanism, we load the plugin via existing Flink plugin from plugins directory. And make it like SPI. So we need to shade the Flink kubernetes(Fabric8) as a whitelist for loading the said Classes during SPI loading instance. Due to some/most part of plugins need depends on the existing Flink Fabric8 dependency. So we need to load the said classes in parent classloader, won't load all classes from Plugin jars.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29053" opendate="2022-8-22 00:00:00" fixdate="2022-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid shuffle has concurrent modification of buffer when compression is enabled</summary>
      <description>When the downstream thread obtains the buffer and consuming it, if the data is compressed in the spilling thread and copied to the original buffer in the same time, since the two threads share the same memory data, the consuming thread will consume incorrect data, causing problems such as deserialize the data disorder.Considering that the downstream consumption is prohibited during compression, or block spilling thread when the downstream consumption is not completed will have a great impact on performance. I think we should move the compression operation to the write thread and store the compressed buffer directly in memory. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpillerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpiller.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="29056" opendate="2022-8-22 00:00:00" fixdate="2022-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw PartitionNotFoundException if the partition file is not readable for hybrid shuffle.</summary>
      <description>If data file is not readable especially data loss, throw PartitionNotFoundException to mark this result partition failed. Otherwise, the partition data is not regenerated, so failover can not recover the job.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.IOUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="29059" opendate="2022-8-22 00:00:00" fixdate="2022-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The existing column stats are deleted incorrectly when analyze table for partial columns</summary>
      <description>If there are three columns named `a, b, c` with column stats already exists,  I just analyze column `a` using `Analyze table xxx FOR COLUMNS a`, the existing column stats of `b, c` will be reset back to empty.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.AnalyzeTableITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="29074" opendate="2022-8-23 00:00:00" fixdate="2022-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use &amp;#39;add jar&amp;#39; in sql client throws "Could not find any jdbc dialect factories that implement"</summary>
      <description>The following step can reproduce this bug:1、 create a source table 't1' in sql-client using jdbc(mysql)2、add a jar with jdbc connector3、select * from 't1'then an exception throws:java.lang.IllegalStateException: Could not find any jdbc dialect factories that implement 'org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory' in the classpath.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcAppendOnlyWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImplTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.dialect.oracle.OraclePreparedStatementTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.MySqlCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.factory.JdbcCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialectLoader.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.MySqlCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtils.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.JdbcCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.factory.JdbcCatalogFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="29081" opendate="2022-8-23 00:00:00" fixdate="2022-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join Hint cannot be identified by lowercase</summary>
      <description>The following sql can reproduce this bug:select /+ bRoadCasT(t1) */ from t1 join t1 as t3 on t1.a = t3.a;</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.JoinHintResolverTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.ClearQueryBlockAliasResolverTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.hints.batch.JoinHintTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHints.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29087" opendate="2022-8-23 00:00:00" fixdate="2022-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jdbc connector sql ITCase failed when run in idea</summary>
      <description>java.lang.NoSuchFieldError: CORRELATE    at org.apache.flink.table.planner.hint.FlinkHintStrategies.createHintStrategyTable(FlinkHintStrategies.java:91)    at org.apache.flink.table.planner.delegation.PlannerContext.lambda$getSqlToRelConverterConfig$1(PlannerContext.java:288)    at java.util.Optional.orElseGet(Optional.java:267)    at org.apache.flink.table.planner.delegation.PlannerContext.getSqlToRelConverterConfig(PlannerContext.java:283)    at org.apache.flink.table.planner.delegation.PlannerContext.createFrameworkConfig(PlannerContext.java:146)    at org.apache.flink.table.planner.delegation.PlannerContext.&lt;init&gt;(PlannerContext.java:124)    at org.apache.flink.table.planner.delegation.PlannerBase.&lt;init&gt;(PlannerBase.scala:121)    at org.apache.flink.table.planner.delegation.StreamPlanner.&lt;init&gt;(StreamPlanner.scala:65)    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:308)    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93)    at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.setup(MySqlCatalogITCase.java:159)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)    at org.junit.runners.Suite.runChild(Suite.java:128)    at org.junit.runners.Suite.runChild(Suite.java:27)    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29091" opendate="2022-8-24 00:00:00" fixdate="2022-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the determinism declaration of the rand function to be consistent with current behavior</summary>
      <description>RAND and RAND_INTEGER are declared as dynamic function (isDynamicFuntion returns true), as the declaration it should only evaluate once at query-level (not per record) for batch mode, FLINK-21713 did the similar fix for temporal functions.But current behavior is completely a non-deterministic function which evaluated per record for both batch and streaming mode, it's not a good choice to break current behavior,  and the determinism of RAND function are also different across vendors:&amp;#91;1&amp;#93; evaluated at query-level though it is treated as non-deterministic function https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-ver16#built-in-function-determinism&amp;#91;2&amp;#93;[ evaluated at row level:  https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand|https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand)]&amp;#91;3&amp;#93; evaluated at row level if not specifies a seed,  e.g., DBMS_RANDOM.normal, DBMS_RANDOM.value(1,10)  https://docs.oracle.com/database/timesten-18.1/TTPLP/d_random.htm#TTPLP71231So just fix the determinism declaration of the rand function to be consistent with the current behavior and make it clear in the documentation.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">docs.content.docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="29095" opendate="2022-8-24 00:00:00" fixdate="2022-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging in SharedStateRegistry</summary>
      <description>with the incremental checkpoint, conceptually, state files that are never used by any checkpoint will be deleted/GC . In practices, state files might be deleted when they are still somehow required by the failover which will lead to Flink job fails.We should add the log for trouble shooting.  </description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="29096" opendate="2022-8-24 00:00:00" fixdate="2022-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>json_value when the path has blank, the result is not right</summary>
      <description>      </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.json.json-value.json</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.JdbcCatalog.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29097" opendate="2022-8-24 00:00:00" fixdate="2022-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Moving json se/deserializers from sql-gateway-api to sql-gateway</summary>
      <description>Considering that the current json se/deserialization rules for results returned by SqlGateway are only used in Rest Endpoint, we migrated the serialization related tools from the flink-sql-gateway-api to the flink-sql-gateway package.</description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.StatementCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.message.statement.FetchResultsResponseBody.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerDeTest.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.results.serde.JsonResultSetSerDeTest.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.serde.JsonResultSetSerializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.serde.JsonResultSetDeserializer.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.RowDataInfo.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ResultSet.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ColumnInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="29113" opendate="2022-8-26 00:00:00" fixdate="2022-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join hint with invalid table name mixed with valid names will not raise an error</summary>
      <description> add a  BROADCAST hint to tpch q2 with a non exists table 'supp' works fine, while a single invalid table name 'supp' will raise an error ```sqlexplain  SELECT /*+ BROADCAST(partsupp,supp) */ s_acctbal,  s_name,  n_name,  p_partkey,  p_mfgr,  s_address,  s_phone,  s_commentFROM  part,  supplier,  partsupp,  nation,  regionWHERE  p_partkey = ps_partkey  AND s_suppkey = ps_suppkey  AND p_size = 15  AND p_type LIKE '%BRASS'  AND s_nationkey = n_nationkey  AND n_regionkey = r_regionkey  AND r_name = 'EUROPE'  AND ps_supplycost = (    SELECT min(ps_supplycost)    FROM      partsupp, supplier,      nation, region    WHERE      p_partkey = ps_partkey      AND s_suppkey = ps_suppkey      AND s_nationkey = n_nationkey      AND n_regionkey = r_regionkey      AND r_name = 'EUROPE'  )ORDER BY  s_acctbal DESC,  n_name,  s_name,  p_partkeyLIMIT 100```</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.hints.batch.JoinHintTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.JoinHintResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="29118" opendate="2022-8-26 00:00:00" fixdate="2022-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove default_catalog in the HiveServer2 Endpoint</summary>
      <description>Hive only has one Catalog. We don't require the default_catalog. Hive JDBC Driver also doesn't support multiple catalogs.  </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.SessionManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.session.SessionEnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.session.SessionEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointStatementITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="29120" opendate="2022-8-26 00:00:00" fixdate="2022-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unexpected join hint propagation into view</summary>
      <description>As expected, Join Hint should only affects the current query block, and does not affect the Join strategy in subquery and view.But current implementation behaviors inconsistently:use source tables of flink-tpch-test, the following join hint takes effect unexpectedlyFlink SQL&gt; create temporary view v1 as SELECT&gt;    p_name,&gt;    p_mfgr,&gt;    p_brand,&gt;    p_type,&gt;    s_name,&gt;    s_address&gt;  FROM&gt;    part,&gt;    supplier&gt;  WHERE p_partkey = s_suppkey;[INFO] Execute statement succeed. Flink SQL&gt; explain SELECT /*+ SHUFFLE_MERGE(part)  */ * from v1;== Abstract Syntax Tree ==LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])+- LogicalFilter(condition=[=($0, $9)])   +- LogicalJoin(condition=[true], joinType=[inner], joinHints=[[[SHUFFLE_MERGE inheritPath:[0, 0] options:[part]]]])      :- LogicalTableScan(table=[[default_catalog, default_database, part]])      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])== Optimized Physical Plan ==Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])+- SortMergeJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])   :- Exchange(distribution=[hash[p_partkey]])   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])   +- Exchange(distribution=[hash[s_suppkey]])      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])== Optimized Execution Plan ==Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])+- SortMergeJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])   :- Exchange(distribution=[hash[p_partkey]])   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])   +- Exchange(distribution=[hash[s_suppkey]])      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address]) without hintFlink SQL&gt; explain SELECT * from v1;== Abstract Syntax Tree ==LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])+- LogicalFilter(condition=[=($0, $9)])   +- LogicalJoin(condition=[true], joinType=[inner])      :- LogicalTableScan(table=[[default_catalog, default_database, part]])      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])== Optimized Physical Plan ==Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])+- HashJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])   +- Exchange(distribution=[broadcast])      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])== Optimized Execution Plan ==Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])+- MultipleInput(readOrder=[1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])\n:- [#1] TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])\n+- [#2] Exchange(distribution=[broadcast])\n])   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])   +- Exchange(distribution=[broadcast])      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])  </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.JoinHintResolverTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.ClearQueryBlockAliasResolverTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.hints.batch.JoinHintTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29121" opendate="2022-8-26 00:00:00" fixdate="2022-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability is failed</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40416&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a42022-08-26T11:03:07.6108823Z Aug 26 11:03:07 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.379 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest2022-08-26T11:03:07.6110033Z Aug 26 11:03:07 [ERROR] org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIVersion)[1] Time elapsed: 0.347 s &lt;&lt;&lt; FAILURE!2022-08-26T11:03:07.6110730Z Aug 26 11:03:07 org.opentest4j.AssertionFailedError: 2022-08-26T11:03:07.6112493Z Aug 26 11:03:07 No compatible call could be found for {"url":"/sessions/:session_handle/:operation_handle/cancel","method":"PUT","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:07.6115158Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/cancel","method":"PUT","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:07.6116664Z Aug 26 11:03:07 Compatibility grade: 7/82022-08-26T11:03:07.6122943Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6123711Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6124489Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6125445Z Aug 26 11:03:07 but was: "/sessions/:session_handle/operations/:operation_handle/cancel"2022-08-26T11:03:07.6128775Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/close","method":"DELETE","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:07.6130953Z Aug 26 11:03:07 Compatibility grade: 6/82022-08-26T11:03:07.6131525Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6132055Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6132735Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6133551Z Aug 26 11:03:07 but was: "/sessions/:session_handle/operations/:operation_handle/close"2022-08-26T11:03:07.6134338Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6134777Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6135124Z Aug 26 11:03:07 but was: "DELETE"2022-08-26T11:03:07.6137281Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/status","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:07.6138587Z Aug 26 11:03:07 Compatibility grade: 6/82022-08-26T11:03:07.6138940Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6139281Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6139687Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6140200Z Aug 26 11:03:07 but was: "/sessions/:session_handle/operations/:operation_handle/status"2022-08-26T11:03:07.6140643Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6141136Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6141622Z Aug 26 11:03:07 but was: "GET"2022-08-26T11:03:07.6144287Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle","method":"DELETE","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:07.6146275Z Aug 26 11:03:07 Compatibility grade: 5/82022-08-26T11:03:07.6147068Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6147651Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6148335Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6149084Z Aug 26 11:03:07 but was: "/sessions/:session_handle"2022-08-26T11:03:07.6149721Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6150272Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6150775Z Aug 26 11:03:07 but was: "DELETE"2022-08-26T11:03:07.6151913Z Aug 26 11:03:07 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:07.6154692Z Aug 26 11:03:07 Rejected by candidate: {"url":"/api_versions","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody","properties":{"versions":{"type":"array","items":{"type":"string"}}}}}.2022-08-26T11:03:07.6157059Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6157436Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6157777Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6158191Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6158618Z Aug 26 11:03:07 but was: "/api_versions"2022-08-26T11:03:07.6159120Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6159445Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6159759Z Aug 26 11:03:07 but was: "GET"2022-08-26T11:03:07.6160504Z Aug 26 11:03:07 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:07.6161015Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6161424Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6162919Z Aug 26 11:03:07 Rejected by candidate: {"url":"/info","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody","properties":{"productName":{"type":"string"},"version":{"type":"string"}}}}.2022-08-26T11:03:07.6164087Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6164470Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6164791Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6165205Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6165633Z Aug 26 11:03:07 but was: "/info"2022-08-26T11:03:07.6165944Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6166278Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6166607Z Aug 26 11:03:07 but was: "GET"2022-08-26T11:03:07.6167384Z Aug 26 11:03:07 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:07.6167902Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6168321Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6170825Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody","properties":{"sessionName":{"type":"string"},"properties":{"type":"object","additionalProperties":{"type":"string"}}}},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody","properties":{"sessionHandle":{"type":"string"}}}}.2022-08-26T11:03:07.6173021Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6173390Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6173735Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6174215Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6174652Z Aug 26 11:03:07 but was: "/sessions"2022-08-26T11:03:07.6174989Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6175300Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6175639Z Aug 26 11:03:07 but was: "POST"2022-08-26T11:03:07.6176375Z Aug 26 11:03:07 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:07.6177054Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6177499Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6179177Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody","properties":{"properties":{"type":"object","additionalProperties":{"type":"string"}}}}}.2022-08-26T11:03:07.6180529Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6180898Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6181219Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6181630Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6182075Z Aug 26 11:03:07 but was: "/sessions/:session_handle"2022-08-26T11:03:07.6182536Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6182860Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6183174Z Aug 26 11:03:07 but was: "GET"2022-08-26T11:03:07.6184091Z Aug 26 11:03:07 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:07.6184595Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6185022Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6186196Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/heartbeat","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"any"}}.2022-08-26T11:03:07.6187190Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6187563Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6187891Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6188286Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6188773Z Aug 26 11:03:07 but was: "/sessions/:session_handle/heartbeat"2022-08-26T11:03:07.6189152Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6189459Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6189791Z Aug 26 11:03:07 but was: "POST"2022-08-26T11:03:07.6190437Z Aug 26 11:03:07 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:07.6191111Z Aug 26 11:03:07 response: [Type of field was changed from 'object' to 'any'.] 2022-08-26T11:03:07.6191536Z Aug 26 11:03:07 expected: "object"2022-08-26T11:03:07.6191867Z Aug 26 11:03:07 but was: "any"2022-08-26T11:03:07.6193723Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/result/:token","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"},{"key":"token"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody","properties":{"results":{"type":"any"},"resultType":{"type":"string"},"nextResultUri":{"type":"string"}}}}.2022-08-26T11:03:07.6195169Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6195538Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6195877Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6196268Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6196954Z Aug 26 11:03:07 but was: "/sessions/:session_handle/operations/:operation_handle/result/:token"2022-08-26T11:03:07.6197421Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6197730Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6198114Z Aug 26 11:03:07 but was: "GET"2022-08-26T11:03:07.6198975Z Aug 26 11:03:07 path-parameters: New path parameter token was added.2022-08-26T11:03:07.6199588Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6200250Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6202961Z Aug 26 11:03:07 Rejected by candidate: {"url":"/sessions/:session_handle/statements","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody","properties":{"statement":{"type":"string"},"executionTimeout":{"type":"integer"},"executionConfig":{"type":"object","additionalProperties":{"type":"string"}}}},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody","properties":{"operationHandle":{"type":"string"}}}}.2022-08-26T11:03:07.6205404Z Aug 26 11:03:07 Compatibility grade: 4/82022-08-26T11:03:07.6206095Z Aug 26 11:03:07 Incompatibilities: 2022-08-26T11:03:07.6206523Z Aug 26 11:03:07 url: 2022-08-26T11:03:07.6207210Z Aug 26 11:03:07 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:07.6207873Z Aug 26 11:03:07 but was: "/sessions/:session_handle/statements"2022-08-26T11:03:07.6208387Z Aug 26 11:03:07 method: 2022-08-26T11:03:07.6208943Z Aug 26 11:03:07 expected: "PUT"2022-08-26T11:03:07.6209508Z Aug 26 11:03:07 but was: "POST"2022-08-26T11:03:07.6210608Z Aug 26 11:03:07 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:07.6211479Z Aug 26 11:03:07 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:07.6212064Z Aug 26 11:03:07 Expecting actual not to be null2022-08-26T11:03:07.6212552Z Aug 26 11:03:07 at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)2022-08-26T11:03:07.6213409Z Aug 26 11:03:07 at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)2022-08-26T11:03:07.6214712Z Aug 26 11:03:07 at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.fail(RestAPIStabilityTestUtils.java:209)2022-08-26T11:03:07.6216196Z Aug 26 11:03:07 at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.assertCompatible(RestAPIStabilityTestUtils.java:138)2022-08-26T11:03:07.6217787Z Aug 26 11:03:07 at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.testStability(RestAPIStabilityTestUtils.java:78)2022-08-26T11:03:07.6218756Z Aug 26 11:03:07 at org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIStabilityTest.java:57)2022-08-26T11:03:07.6219518Z Aug 26 11:03:07 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-08-26T11:03:07.6220092Z Aug 26 11:03:07 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-08-26T11:03:07.6220833Z Aug 26 11:03:07 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-08-26T11:03:07.6221472Z Aug 26 11:03:07 at java.lang.reflect.Method.invoke(Method.java:498)2022-08-26T11:03:07.6222067Z Aug 26 11:03:07 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)2022-08-26T11:03:07.6222767Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2022-08-26T11:03:07.6223567Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2022-08-26T11:03:07.6224456Z Aug 26 11:03:07 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2022-08-26T11:03:07.6225187Z Aug 26 11:03:07 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2022-08-26T11:03:07.6225958Z Aug 26 11:03:07 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)2022-08-26T11:03:07.6226971Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2022-08-26T11:03:07.6227801Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2022-08-26T11:03:07.6228811Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2022-08-26T11:03:07.6229837Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2022-08-26T11:03:07.6230701Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2022-08-26T11:03:07.6231478Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2022-08-26T11:03:07.6232220Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2022-08-26T11:03:07.6233022Z Aug 26 11:03:07 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2022-08-26T11:03:07.6233781Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)2022-08-26T11:03:07.6234656Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6235446Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)2022-08-26T11:03:07.6236487Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)2022-08-26T11:03:07.6237597Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)2022-08-26T11:03:07.6238794Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)2022-08-26T11:03:07.6239601Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6240373Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-08-26T11:03:07.6241101Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-08-26T11:03:07.6241881Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-08-26T11:03:07.6242923Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6243996Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-08-26T11:03:07.6244758Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-08-26T11:03:07.6245627Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-08-26T11:03:07.6246673Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)2022-08-26T11:03:07.6248243Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)2022-08-26T11:03:07.6249734Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)2022-08-26T11:03:07.6251055Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)2022-08-26T11:03:07.6251888Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)2022-08-26T11:03:07.6252628Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2022-08-26T11:03:07.6253463Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6254491Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)2022-08-26T11:03:07.6255342Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6256361Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2022-08-26T11:03:07.6257198Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6257831Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6258620Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6259230Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2022-08-26T11:03:07.6259857Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6260794Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)2022-08-26T11:03:07.6261877Z Aug 26 11:03:07 at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)2022-08-26T11:03:07.6262842Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2022-08-26T11:03:07.6264074Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2022-08-26T11:03:07.6265240Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2022-08-26T11:03:07.6266437Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2022-08-26T11:03:07.6267746Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2022-08-26T11:03:07.6268842Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2022-08-26T11:03:07.6270170Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2022-08-26T11:03:07.6271320Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6272436Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6273549Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2022-08-26T11:03:07.6274812Z Aug 26 11:03:07 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2022-08-26T11:03:07.6275964Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2022-08-26T11:03:07.6277234Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2022-08-26T11:03:07.6278413Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2022-08-26T11:03:07.6279630Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2022-08-26T11:03:07.6280773Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2022-08-26T11:03:07.6281903Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2022-08-26T11:03:07.6283011Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2022-08-26T11:03:07.6284251Z Aug 26 11:03:07 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2022-08-26T11:03:07.6285420Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2022-08-26T11:03:07.6286356Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2022-08-26T11:03:07.6287606Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2022-08-26T11:03:07.6288756Z Aug 26 11:03:07 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2022-08-26T11:03:07.6289721Z Aug 26 11:03:07 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2022-08-26T11:03:07.6290598Z Aug 26 11:03:07 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2022-08-26T11:03:07.6291318Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)2022-08-26T11:03:07.6292107Z Aug 26 11:03:07 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)2022-08-26T11:03:07.6293092Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)2022-08-26T11:03:07.6293968Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6294757Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-08-26T11:03:07.6295473Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-08-26T11:03:07.6296174Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-08-26T11:03:07.6297316Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6298067Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-08-26T11:03:07.6298789Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-08-26T11:03:07.6299667Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-08-26T11:03:07.6300711Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)2022-08-26T11:03:07.6301615Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-08-26T11:03:07.6302559Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6303356Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-08-26T11:03:07.6304135Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-08-26T11:03:07.6304850Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-08-26T11:03:07.6305612Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6306371Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-08-26T11:03:07.6307249Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-08-26T11:03:07.6308365Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-08-26T11:03:07.6309412Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)2022-08-26T11:03:07.6310319Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-08-26T11:03:07.6311225Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6311985Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-08-26T11:03:07.6312884Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-08-26T11:03:07.6314150Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-08-26T11:03:07.6314950Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-08-26T11:03:07.6315874Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-08-26T11:03:07.6316609Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-08-26T11:03:07.6317690Z Aug 26 11:03:07 at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)2022-08-26T11:03:07.6318978Z Aug 26 11:03:07 at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)2022-08-26T11:03:07.6319845Z Aug 26 11:03:07 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2022-08-26T11:03:07.6320479Z Aug 26 11:03:07 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2022-08-26T11:03:07.6321103Z Aug 26 11:03:07 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2022-08-26T11:03:07.6321734Z Aug 26 11:03:07 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)2022-08-26T11:03:07.6322201Z Aug 26 11:03:07 2022-08-26T11:03:07.6498722Z Aug 26 11:03:07 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.022 s - in org.apache.flink.table.gateway.rest.SessionCaseITTest2022-08-26T11:03:08.4973396Z Aug 26 11:03:08 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.396 s - in org.apache.flink.table.gateway.service.session.SessionManagerTest2022-08-26T11:03:08.8545807Z Aug 26 11:03:08 [INFO] 2022-08-26T11:03:08.8546577Z Aug 26 11:03:08 [INFO] Results:2022-08-26T11:03:08.8547189Z Aug 26 11:03:08 [INFO] 2022-08-26T11:03:08.8547603Z Aug 26 11:03:08 [ERROR] Failures: 2022-08-26T11:03:08.8550509Z Aug 26 11:03:08 [ERROR] SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability:57 No compatible call could be found for {"url":"/sessions/:session_handle/:operation_handle/cancel","method":"PUT","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:08.8554432Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/cancel","method":"PUT","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:08.8555944Z Aug 26 11:03:08 Compatibility grade: 7/82022-08-26T11:03:08.8556404Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8556878Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8557341Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8558016Z Aug 26 11:03:08 but was: "/sessions/:session_handle/operations/:operation_handle/cancel"2022-08-26T11:03:08.8560084Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/close","method":"DELETE","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:08.8561585Z Aug 26 11:03:08 Compatibility grade: 6/82022-08-26T11:03:08.8561952Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8562272Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8562682Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8563323Z Aug 26 11:03:08 but was: "/sessions/:session_handle/operations/:operation_handle/close"2022-08-26T11:03:08.8563736Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8564157Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8564499Z Aug 26 11:03:08 but was: "DELETE"2022-08-26T11:03:08.8566446Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/status","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:08.8569300Z Aug 26 11:03:08 Compatibility grade: 6/82022-08-26T11:03:08.8569937Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8570496Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8571215Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8572246Z Aug 26 11:03:08 but was: "/sessions/:session_handle/operations/:operation_handle/status"2022-08-26T11:03:08.8572825Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8573133Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8573460Z Aug 26 11:03:08 but was: "GET"2022-08-26T11:03:08.8575227Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle","method":"DELETE","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody","properties":{"status":{"type":"string"}}}}.2022-08-26T11:03:08.8576635Z Aug 26 11:03:08 Compatibility grade: 5/82022-08-26T11:03:08.8577360Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8578025Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8578743Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8579537Z Aug 26 11:03:08 but was: "/sessions/:session_handle"2022-08-26T11:03:08.8579915Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8580234Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8580549Z Aug 26 11:03:08 but was: "DELETE"2022-08-26T11:03:08.8581230Z Aug 26 11:03:08 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:08.8582810Z Aug 26 11:03:08 Rejected by candidate: {"url":"/api_versions","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody","properties":{"versions":{"type":"array","items":{"type":"string"}}}}}.2022-08-26T11:03:08.8583981Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8584332Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8584663Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8585070Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8585736Z Aug 26 11:03:08 but was: "/api_versions"2022-08-26T11:03:08.8586081Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8586407Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8586722Z Aug 26 11:03:08 but was: "GET"2022-08-26T11:03:08.8587606Z Aug 26 11:03:08 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:08.8588091Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8588516Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8589990Z Aug 26 11:03:08 Rejected by candidate: {"url":"/info","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody","properties":{"productName":{"type":"string"},"version":{"type":"string"}}}}.2022-08-26T11:03:08.8591280Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8591646Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8591959Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8592369Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8592795Z Aug 26 11:03:08 but was: "/info"2022-08-26T11:03:08.8593101Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8593427Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8593756Z Aug 26 11:03:08 but was: "GET"2022-08-26T11:03:08.8594450Z Aug 26 11:03:08 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:08.8594951Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8595371Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8619910Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[]},"query-parameters":{"queryParameters":[]},"request":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody","properties":{"sessionName":{"type":"string"},"properties":{"type":"object","additionalProperties":{"type":"string"}}}},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody","properties":{"sessionHandle":{"type":"string"}}}}.2022-08-26T11:03:08.8622586Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8623158Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8623683Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8624400Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8625066Z Aug 26 11:03:08 but was: "/sessions"2022-08-26T11:03:08.8625574Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8626077Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8626554Z Aug 26 11:03:08 but was: "POST"2022-08-26T11:03:08.8627935Z Aug 26 11:03:08 path-parameters: Existing Path parameter session_handle was removed.2022-08-26T11:03:08.8628783Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8629445Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8632160Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody","properties":{"properties":{"type":"object","additionalProperties":{"type":"string"}}}}}.2022-08-26T11:03:08.8634154Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8634728Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8635242Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8635900Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8637125Z Aug 26 11:03:08 but was: "/sessions/:session_handle"2022-08-26T11:03:08.8637679Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8638215Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8638757Z Aug 26 11:03:08 but was: "GET"2022-08-26T11:03:08.8639897Z Aug 26 11:03:08 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:08.8640744Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8641438Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8643308Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/heartbeat","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"any"}}.2022-08-26T11:03:08.8645016Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8645627Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8646163Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8647000Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8647784Z Aug 26 11:03:08 but was: "/sessions/:session_handle/heartbeat"2022-08-26T11:03:08.8648331Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8648821Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8649321Z Aug 26 11:03:08 but was: "POST"2022-08-26T11:03:08.8650391Z Aug 26 11:03:08 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:08.8651470Z Aug 26 11:03:08 response: [Type of field was changed from 'object' to 'any'.] 2022-08-26T11:03:08.8652215Z Aug 26 11:03:08 expected: "object"2022-08-26T11:03:08.8652757Z Aug 26 11:03:08 but was: "any"2022-08-26T11:03:08.8655846Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/operations/:operation_handle/result/:token","method":"GET","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"},{"key":"operation_handle"},{"key":"token"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"any"},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody","properties":{"results":{"type":"any"},"resultType":{"type":"string"},"nextResultUri":{"type":"string"}}}}.2022-08-26T11:03:08.8658253Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8658849Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8659376Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8660027Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8660918Z Aug 26 11:03:08 but was: "/sessions/:session_handle/operations/:operation_handle/result/:token"2022-08-26T11:03:08.8661621Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8662145Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8662684Z Aug 26 11:03:08 but was: "GET"2022-08-26T11:03:08.8663723Z Aug 26 11:03:08 path-parameters: New path parameter token was added.2022-08-26T11:03:08.8664619Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8665305Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8669057Z Aug 26 11:03:08 Rejected by candidate: {"url":"/sessions/:session_handle/statements","method":"POST","status-code":"200 OK","file-upload":false,"path-parameters":{"pathParameters":[{"key":"session_handle"}]},"query-parameters":{"queryParameters":[]},"request":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody","properties":{"statement":{"type":"string"},"executionTimeout":{"type":"integer"},"executionConfig":{"type":"object","additionalProperties":{"type":"string"}}}},"response":{"type":"object","id":"urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody","properties":{"operationHandle":{"type":"string"}}}}.2022-08-26T11:03:08.8671949Z Aug 26 11:03:08 Compatibility grade: 4/82022-08-26T11:03:08.8672450Z Aug 26 11:03:08 Incompatibilities: 2022-08-26T11:03:08.8672936Z Aug 26 11:03:08 url: 2022-08-26T11:03:08.8673517Z Aug 26 11:03:08 expected: "/sessions/:session_handle/:operation_handle/cancel"2022-08-26T11:03:08.8674326Z Aug 26 11:03:08 but was: "/sessions/:session_handle/statements"2022-08-26T11:03:08.8674933Z Aug 26 11:03:08 method: 2022-08-26T11:03:08.8675409Z Aug 26 11:03:08 expected: "PUT"2022-08-26T11:03:08.8675870Z Aug 26 11:03:08 but was: "POST"2022-08-26T11:03:08.8676947Z Aug 26 11:03:08 path-parameters: Existing Path parameter operation_handle was removed.2022-08-26T11:03:08.8677838Z Aug 26 11:03:08 response: [Field {"type":"string"} was removed.] 2022-08-26T11:03:08.8678425Z Aug 26 11:03:08 Expecting actual not to be null2022-08-26T11:03:08.8678939Z Aug 26 11:03:08 [INFO] 2022-08-26T11:03:08.8679504Z Aug 26 11:03:08 [ERROR] Tests run: 32, Failures: 1, Errors: 0, Skipped: 0</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v1.snapshot</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.operation.CancelOperationHeaders.java</file>
    </fixedFiles>
  </bug>
  <bug id="29126" opendate="2022-8-29 00:00:00" fixdate="2022-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spliting file optimization doesn&amp;#39;t work for orc format</summary>
      <description>FLINK-27338 try to improve file spliting for orc format. But it doesn't work for a making  mistake in judge whether the table is stored as orc format or not. We should fix it.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.PartitionMonitorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSourceFileEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="29148" opendate="2022-8-31 00:00:00" fixdate="2022-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for SQL Gateway</summary>
      <description>Add basic doc for SQL Gateway.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointConfigOptions.java</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.hiveCompatibility.hiveserver2.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hiveCompatibility.hiveserver2.md</file>
    </fixedFiles>
  </bug>
  <bug id="29152" opendate="2022-8-31 00:00:00" fixdate="2022-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe statement resutls is different from the Hive</summary>
      <description>In hive, the results schema is +-----------+------------+----------+| col_name | data_type | comment |+-----------+------------+----------+| a | int | || b | string | |+-----------+------------+----------+but our implementation is 0: jdbc:hive2://localhost:10000/default&gt; describe sink;+-------+-------+-------+-------+---------+------------+| name | type | null | key | extras | watermark |+-------+-------+-------+-------+---------+------------+| a | INT | true | NULL | NULL | NULL |+-------+-------+-------+-------+---------+------------+BTW, it's better we can support DESCRIBE FORMATTED like hive does.+-------------------------------+----------------------------------------------------+-----------------------+| col_name | data_type | comment |+-------------------------------+----------------------------------------------------+-----------------------+| # col_name | data_type | comment || | NULL | NULL || a | int | || b | string | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || Owner: | null | NULL || CreateTime: | Tue Aug 30 06:54:00 UTC 2022 | NULL || LastAccessTime: | UNKNOWN | NULL || Retention: | 0 | NULL || Location: | hdfs://namenode:8020/user/hive/warehouse/sink | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | comment | || | numFiles | 0 || | totalSize | 0 || | transient_lastDdlTime | 1661842440 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL || InputFormat: | org.apache.hadoop.mapred.TextInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+----------------------------------------------------+-----------------------+</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.endpoint.hive.catalog.q</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserLoadSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.operation.HiveLoadDataOperation.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="29156" opendate="2022-8-31 00:00:00" fixdate="2022-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support LISTAGG in the Table API</summary>
      <description>Currently, LISTAGG  are not supported in Table API.table.group_by(col("a"))     .select(         col("a"),        call("LISTAGG", col("b"), ','))</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29185" opendate="2022-9-2 00:00:00" fixdate="2022-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to execute USING JAR in Hive Dialect</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="29188" opendate="2022-9-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The option `sql-gateway.endpint.hiveserver2.catalog.hive-conf-dir` should be requried</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="29196" opendate="2022-9-5 00:00:00" fixdate="2022-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python NOTICE is incorrect</summary>
      <description>We should check the licensing of the 1.16 release.https://cwiki.apache.org/confluence/display/FLINK/Licensing </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.resources.META-INF.licenses.LICENSE.jzlib</file>
      <file type="M">flink-python.src.main.resources.META-INF.licenses.LICENSE.bouncycastle</file>
    </fixedFiles>
  </bug>
  <bug id="29198" opendate="2022-9-5 00:00:00" fixdate="2022-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RetryExtension doesn&amp;#39;t make the test fail if the retries are exhausted</summary>
      <description>FLINK-24627 introduced retry functionality for JUnit5-based tests. It appears that the retry mechanism doesn't have the desired behavior: If the retries are exhausted without the test ever succeeding will result in the test being ignored. I would expect the test to fail in that case. Otherwise, a CI run would succeed without anyone noticing the malfunctioning of the test.</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnFailureExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnExceptionExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.extensions.retry.strategy.RetryOnExceptionStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="29211" opendate="2022-9-6 00:00:00" fixdate="2022-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 2.3.9 NOTICE is incorrect</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="29219" opendate="2022-9-7 00:00:00" fixdate="2022-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE AS statement blocks SQL client&amp;#39;s execution</summary>
      <description>When executing CREATE TABLE AS statement to create a sink table in SQL client, the client could create the table in catalog and submit the job to cluster successfully, but stops emitting new prompts and accepts new inputs, and user has to use SIGTERM (Control + C) to forcefully stop the SQL client. As contrast the behavior of INSERT INTO statement in SQL client is printing "Job is submitted with JobID xxxx" and being ready to accept user's input. From the log it looks like the client was waiting for the job to finish.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.CompiledPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.insert.q</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="29222" opendate="2022-9-7 00:00:00" fixdate="2022-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong behavior for Hive&amp;#39;s load data inpath</summary>
      <description>In hive, `load data inpath` will remove src file, and `load data local inpath` won't remove the src file.But When using the following sql with Hive dialect:load data local inpath 'test.txt' INTO TABLE tab2 The file `test.txt` will be removed, although the expected is not to remove the `test.txt`.The reason is the parameter order is not right when try to call `HiveCatalog#loadTable(...,  isOverWrite, isSourceLocal)`,It'll call it with hiveCatalog.loadTable( ..., hiveLoadDataOperation.isSrcLocal(), // should be isOverwrite hiveLoadDataOperation.isOverwrite()); // should be isSrcLocal   </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="29228" opendate="2022-9-8 00:00:00" fixdate="2022-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align the schema of the HiveServer2 getMetadata with JDBC</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
    </fixedFiles>
  </bug>
  <bug id="29229" opendate="2022-9-8 00:00:00" fixdate="2022-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HiveServer2 Endpoint doesn&amp;#39;t support execute statements in sync mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="29238" opendate="2022-9-9 00:00:00" fixdate="2022-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong index information will be obtained after the downstream failover in hybrid full mode</summary>
      <description>Hybrid shuffle relies on the index to read the disk data. Since the spilled data may be consumed from memory, the readable status is introduced. For the readable buffer, FileDataManager does not pre-load it. However, when the downstream fails, the previous readable status will be used incorrectly, resulting in that some buffer cannot be read correctly.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingMemoryDataManagerOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingFileDataIndex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataIndexImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataIndexImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="2924" opendate="2015-10-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create database state backend</summary>
      <description>The goal is to create a database state backend that can be used with JDBC supporting databases.The backend should support the storage of non-partitioned states, and also the storage of Key-value states with high throughput. As databases provide advanced querying functionality the key-value state can be implemented to be lazily fetched and should scale to "arbitrary" state sizes by not storing the non-active key-values on heap.An adapter class will be provided that can help bridge the gap between different sql implementations.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.NonKeyedWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.FileStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryHeapKvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsHeapKvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.state.DerbyAdapter.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.state.DBStateCheckpointingTest.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.state.DbStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.MySqlAdapter.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.LazyDbKvState.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateHandle.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateBackend.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbBackendConfig.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29253" opendate="2022-9-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DefaultJobmanagerRunnerRegistry#localCleanupAsync calls close instead of closeAsync</summary>
      <description>DefaultJobmanagerRunnerRegistry#localCleanupAsync is meant to be called from the main thread. The current implementation calls close on the JobManagerRunner instead of closeAsync. This results in a blocking call on the Dispatcher's main thread which we want to avoid.Thanks for identifying this issue, chesnay</description>
      <version>1.16.0,1.17.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="29282" opendate="2022-9-13 00:00:00" fixdate="2022-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decouple Quickstart E2E test from Elasticsearch</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.Elasticsearch7SinkExample.scala</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.java.org.apache.flink.quickstarts.test.Elasticsearch7SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29299" opendate="2022-9-14 00:00:00" fixdate="2022-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the network memory size calculation issue in fine-grained resource mode</summary>
      <description>After FLINK-28663, one intermediate dataset can be consumed by multiple consumers, there is a case where one vertex can consume one intermediate dataset multiple times. However, currently in fine-grained resource mode, when computing the required network buffer size, the intermediate dataset is used as key to record the size of network buffer per input gate, which means it may allocate less network buffers than needed if two input gate of the same vertex consumes the same intermediate dataset.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.TaskInputsOutputsDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="29349" opendate="2022-9-20 00:00:00" fixdate="2022-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use state ttl instead of timer to clean up state in proctime unbounded over aggregate</summary>
      <description>Currently we rely on the timer based state cleaning in proctime over aggregate, this can be optimized to use state ttl for a more efficienct way</description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="29350" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a section for moving planner jar in Hive dependencies page</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.queries.transform.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.queries.sub-queries.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.queries.lateral-view.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.queries.join.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.overview.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.insert.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.drop.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.create.md</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.alter.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.queries.transform.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.queries.sub-queries.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.queries.lateral-view.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.queries.join.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.drop.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.alter.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="29352" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CONVERT_TZ built-in function in Table API</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29353" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support UNIX_TIMESTAMP built-in function in Table API</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29354" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TO_DATE and TO_TIMESTAMP built-in function in the Table API</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29386" opendate="2022-9-22 00:00:00" fixdate="2022-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix fail to compile flink-connector-hive when profile is hive3</summary>
      <description>The compile will fail in hive3. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41238&amp;view=logs&amp;j=b1fcf054-9138-5463-c73c-a49979b9ac2a&amp;t=9291ac46-dd95-5135-b799-3839e65a8691Introduced by FLINK-29152 which introduces org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_SERIALIZATION_FORMAT,  TableType.INDEX_TABLE, ErrorMsg.SHOW_CREATETABLE_INDEX.    But they don't exist in Hive3.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveShowTableUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="29389" opendate="2022-9-22 00:00:00" fixdate="2022-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation of JDBC and HBase lookup table for new caching options</summary>
      <description>Update documentation of JDBC and HBase lookup table for new caching options</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="29408" opendate="2022-9-26 00:00:00" fixdate="2022-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveCatalogITCase failed with NPE</summary>
      <description>2022-09-25T03:41:07.4212129Z Sep 25 03:41:07 [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf Time elapsed: 0.098 s &lt;&lt;&lt; ERROR!2022-09-25T03:41:07.4212662Z Sep 25 03:41:07 java.lang.NullPointerException2022-09-25T03:41:07.4213189Z Sep 25 03:41:07 at org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf(HiveCatalogUdfITCase.java:109)2022-09-25T03:41:07.4213753Z Sep 25 03:41:07 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-09-25T03:41:07.4224643Z Sep 25 03:41:07 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-09-25T03:41:07.4225311Z Sep 25 03:41:07 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-09-25T03:41:07.4225879Z Sep 25 03:41:07 at java.lang.reflect.Method.invoke(Method.java:498)2022-09-25T03:41:07.4226405Z Sep 25 03:41:07 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-09-25T03:41:07.4227201Z Sep 25 03:41:07 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-09-25T03:41:07.4227807Z Sep 25 03:41:07 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-09-25T03:41:07.4228394Z Sep 25 03:41:07 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2022-09-25T03:41:07.4228966Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-09-25T03:41:07.4229514Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4230066Z Sep 25 03:41:07 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-09-25T03:41:07.4230587Z Sep 25 03:41:07 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-09-25T03:41:07.4231258Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-09-25T03:41:07.4231823Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-09-25T03:41:07.4232384Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-09-25T03:41:07.4232930Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)2022-09-25T03:41:07.4233511Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)2022-09-25T03:41:07.4234039Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-09-25T03:41:07.4234546Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-09-25T03:41:07.4235057Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-09-25T03:41:07.4235573Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-09-25T03:41:07.4236087Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-09-25T03:41:07.4236635Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-09-25T03:41:07.4237314Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-09-25T03:41:07.4238211Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4238775Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4239277Z Sep 25 03:41:07 at org.junit.rules.RunRules.evaluate(RunRules.java:20)2022-09-25T03:41:07.4239769Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-09-25T03:41:07.4240265Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-09-25T03:41:07.4240731Z Sep 25 03:41:07 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)2022-09-25T03:41:07.4241196Z Sep 25 03:41:07 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)2022-09-25T03:41:07.4241715Z Sep 25 03:41:07 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)2022-09-25T03:41:07.4242316Z Sep 25 03:41:07 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)2022-09-25T03:41:07.4242904Z Sep 25 03:41:07 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)2022-09-25T03:41:07.4243528Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-09-25T03:41:07.4244201Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-09-25T03:41:07.4244883Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-09-25T03:41:07.4245801Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-09-25T03:41:07.4246600Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-09-25T03:41:07.4247226Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-09-25T03:41:07.4247808Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-09-25T03:41:07.4248449Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-09-25T03:41:07.4249567Z Sep 25 03:41:07 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-09-25T03:41:07.4250222Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-09-25T03:41:07.4250889Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-09-25T03:41:07.4251559Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-09-25T03:41:07.4252193Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-09-25T03:41:07.4252776Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-09-25T03:41:07.4253335Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-09-25T03:41:07.4253884Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=5acec1b4-945b-59ca-34f8-168928ce5199</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29425" opendate="2022-9-27 00:00:00" fixdate="2022-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid full spilling strategy triggering spilling frequently</summary>
      <description>In hybrid shuffle mode, we have an internal config option 'DEFAULT_FULL_STRATEGY_NUM_BUFFERS_TRIGGER_SPILLED' to control spilling frequency in the full spilling strategy. Unfortunately, the default value(10) is too small. As a result, frequent disk spilling calls are observed in the TPC-DS test, which seriously affects performance. When we increase the value, the query performance is improved significantly. We should set a more reasonable default value, or adopt an adaptive strategy to determine the spilling frequency.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.TestingSpillingStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSelectiveSpillingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFullSpillingStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSpillingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSelectiveSpillingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFullSpillingStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="29427" opendate="2022-9-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LookupJoinITCase failed with classloader problem</summary>
      <description>2022-09-27T02:49:20.9501313Z Sep 27 02:49:20 Caused by: org.codehaus.janino.InternalCompilerException: Compiling "KeyProjection$108341": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.2022-09-27T02:49:20.9502654Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)2022-09-27T02:49:20.9503366Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)2022-09-27T02:49:20.9504044Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)2022-09-27T02:49:20.9504704Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)2022-09-27T02:49:20.9505341Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)2022-09-27T02:49:20.9505965Z Sep 27 02:49:20 at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)2022-09-27T02:49:20.9506584Z Sep 27 02:49:20 at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)2022-09-27T02:49:20.9507261Z Sep 27 02:49:20 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)2022-09-27T02:49:20.9507883Z Sep 27 02:49:20 ... 30 more2022-09-27T02:49:20.9509266Z Sep 27 02:49:20 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.2022-09-27T02:49:20.9510835Z Sep 27 02:49:20 at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)2022-09-27T02:49:20.9511760Z Sep 27 02:49:20 at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)2022-09-27T02:49:20.9512456Z Sep 27 02:49:20 at java.lang.Class.forName0(Native Method)2022-09-27T02:49:20.9513014Z Sep 27 02:49:20 at java.lang.Class.forName(Class.java:348)2022-09-27T02:49:20.9513649Z Sep 27 02:49:20 at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)2022-09-27T02:49:20.9514339Z Sep 27 02:49:20 at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)2022-09-27T02:49:20.9514990Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)2022-09-27T02:49:20.9515659Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)2022-09-27T02:49:20.9516337Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)2022-09-27T02:49:20.9516989Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)2022-09-27T02:49:20.9517632Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)2022-09-27T02:49:20.9518319Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)2022-09-27T02:49:20.9519018Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)2022-09-27T02:49:20.9519680Z Sep 27 02:49:20 at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)2022-09-27T02:49:20.9520386Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)2022-09-27T02:49:20.9521042Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)2022-09-27T02:49:20.9521677Z Sep 27 02:49:20 at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)2022-09-27T02:49:20.9522299Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)2022-09-27T02:49:20.9522929Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)2022-09-27T02:49:20.9523658Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)2022-09-27T02:49:20.9524365Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)2022-09-27T02:49:20.9525030Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)2022-09-27T02:49:20.9525750Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)2022-09-27T02:49:20.9526383Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)2022-09-27T02:49:20.9527069Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)2022-09-27T02:49:20.9527832Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)2022-09-27T02:49:20.9528560Z Sep 27 02:49:20 at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)2022-09-27T02:49:20.9529217Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)2022-09-27T02:49:20.9529862Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)2022-09-27T02:49:20.9530427Z Sep 27 02:49:20 ... 37 more2022-09-27T02:49:20.9530852Z Sep 27 02:49:20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41369&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.functions.table.fullcache.LookupFullCacheTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.functions.table.fullcache.inputformat.InputFormatCacheLoaderTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="29455" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OperatorIdentifier</summary>
      <description>Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowSavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadataV2.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowSavepointReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="29457" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a uid(hash) remapping function</summary>
      <description>Expose functionality for modifying the uid&amp;#91;hash&amp;#93; of a state.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorIdentifier.java</file>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="29458" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When two tables have the same field, do not specify the table name,Exception will be thrown：SqlValidatorException :Column &amp;#39;currency&amp;#39; is ambiguous</summary>
      <description>When two tables are join, the two tables have the same field. When querying select, an exception will be thrown if the table name is not specifiedexception contentColumn 'currency' is ambiguous。 </description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.joins.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.joins.md</file>
    </fixedFiles>
  </bug>
  <bug id="2946" opendate="2015-10-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add orderBy() to Table API</summary>
      <description>In order to implement a FLINK-2099 prototype that uses the Table APIs code generation facilities, the Table API needs a sorting feature.I would implement it the next days. Ideas how to implement such a sorting feature are very welcome. Is there any more efficient way instead of .sortPartition(...).setParallism(1)? Is it better to sort locally on the nodes first and finally sort on one node afterwards?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29468" opendate="2022-9-29 00:00:00" fixdate="2022-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jackson-bom to 2.13.4</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.15.3,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="29477" opendate="2022-9-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when collect primitive array to Python</summary>
      <description>How to reproduce this bug:ds = env.from_collection([1, 2], type_info=Types.PRIMITIVE_ARRAY(Types.INT()))ds.execute_and_collect()got:java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object</description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="29478" opendate="2022-9-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink sql Connector hive to support 3.1.3</summary>
      <description>Currently , flink-connector hive support flink-sql-connector-hive-3.1.2 as highest version ! hive 3.1.3 released on 08 April 2022Proposal :- We should think of adding support for 3.1.3. </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.main.resources.modules-defining-excess-dependencies.modulelist</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.reflectasm</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.minlog</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.kryo</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.jodd</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.javolution</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTemporalJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">docs.content.docs.dev.table.sql.load.md</file>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.load.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="29479" opendate="2022-9-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support whether using system PythonPath for PyFlink jobs</summary>
      <description>It exists PYTHONPATH env in system,like yarn/k8s images, it will cause conflict with users python depdendency sometimes. so i suggest add a config to do whether using system env of PYTHONPATH</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="29483" opendate="2022-9-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink python udf arrow in thread model bug</summary>
      <description></description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
    </fixedFiles>
  </bug>
  <bug id="29496" opendate="2022-10-3 00:00:00" fixdate="2022-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Configuration for STS endpoint when using ASSUME_ROLE credential provider</summary>
      <description>When using Kinesis connector with credentials provider configured as ASSUME_ROLE in the job running in VPC without internet connection, credentials provider logic tries to access global STS endpoint, sts.amazonaws.com. However, only regional endpoints for STS are available in that case.Connector need support for configuring STS endpoint to allow such use-case.</description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSGeneralUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.main.java.org.apache.flink.connector.aws.util.AWSGeneralUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.main.java.org.apache.flink.connector.aws.config.AWSConfigConstants.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="29502" opendate="2022-10-4 00:00:00" fixdate="2022-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Hadoop implementation for filesystems to 3.3.4</summary>
      <description>Flink currently uses Hadoop version 3.3.2 for the Flink filesystem implementations. Upgrading this to version 3.3.4 will resolve some CVEs like https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25168 (which Flink is not affected by)</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29508" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some NOTICE files are not checked for correctness</summary>
      <description>We have 3 modules that are not being deployed (and thus auto-excluded since FLINK-29301) which are still relevant for production though.We should amend the checker to take into account whether the non-deployed module is bundled by another deployed module.</description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.NoticeFileCheckerTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="29511" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort properties/schemas in OpenAPI spec</summary>
      <description>The properties/schema order is currently based on whatever order they were looked up, which varies as the spec is being extended.Sort them by name to prevent this.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.sql.gateway.yml</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29513" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka version to 3.2.3</summary>
      <description>Kafka 3.2.3 contains certain security fixes (see https://downloads.apache.org/kafka/3.2.3/RELEASE_NOTES.html). We should upgrade the dependency in Flink</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29514" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Minikdc to v3.2.4</summary>
      <description>Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29517" opendate="2022-10-6 00:00:00" fixdate="2022-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation about DATE_FORMAT to state that it&amp;#39;s supported in Table API</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2955" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add operations introduction in Table API page.</summary>
      <description>On the Table API page, there is no formal introduction of current supported operations, it should be nice to have it.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="29567" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert sink output metric names from numRecordsSend back to numRecordsOut</summary>
      <description>As discussed in the mailing list, all sink metrics with name “numXXXOut” defined in FLIP-33 are replace by “numXXXSend” in FLINK-26126 and FLINK-26492. Considering metric names are public APIs, this is a breaking change to end users and not backward compatible. We need to revert these metric names back. </description>
      <version>1.16.0,1.15.3</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.InternalOperatorIOMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.SinkWriterMetricGroup.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.TestSinkInitContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29568" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary whitespace in request/response blocks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="29569" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of deprecated expand shortcode</summary>
      <description>The expand shortcode is deprecated; use &lt;details&gt; instead.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.assets..custom.scss</file>
    </fixedFiles>
  </bug>
  <bug id="29576" opendate="2022-10-11 00:00:00" fixdate="2022-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serialized OperatorCoordinators are collected in a not thread-safe ArrayList even though serialization was parallelized in FLINK-26675</summary>
      <description>There's a build failure being caused by SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution on master:Oct 11 01:45:36 [ERROR] Errors: Oct 11 01:45:36 [ERROR] SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution:89 » Runtime Fail...Oct 11 01:45:36 [INFO] Oct 11 01:45:36 [ERROR] Tests run: 1931, Failures: 0, Errors: 1, Skipped: 4The actual cause might be a missing OperatorCoordinatorHolder in DefaultOperatorCoordinatorHandler (see attached Maven logs that were extracted from the linked build):01:44:28,248 [flink-akka.actor.default-dispatcher-5] WARN org.apache.flink.runtime.taskmanager.Task [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (1/4)#0 (9babb402557eb959216c28116aabddbe_1dd2eb40b0971d6d849b9e4a69494c88_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator bc764cd8ddf7a0cff126f51c16239658 at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:117) at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031) at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588) at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29590" opendate="2022-10-12 00:00:00" fixdate="2022-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix literal issue in HiveDialect</summary>
      <description>in FLINK-26474, we try to fold constant, but it brings a issue that the folded constant like `Double.NAN` and no-primitive type  can't be convert into calcite literal in method  `HiveParserRexNodeConverter#convertConstant`.For example, the following code will throw an exception "org.apache.hadoop.hive.ql.parse.SemanticException: NaN" in method `HiveParserRexNodeConverter#convertConstant`// hive dialectSELECT asin(2); To fix it, we need to figure out such case and then not to fold constant . in FLINK-27017, we use Hive's `GenericUDFOPDivide` to do divide for better compatibility, but it bring a issue that when use a int/long literal as divisor, the result type passed and inferred type may not match.The fix it, we need to make the result type match the inferred type. </description>
      <version>1.16.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserTypeCheckProcFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="29624" opendate="2022-10-13 00:00:00" fixdate="2022-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0</summary>
      <description>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0 to avoid being falsely flagged for CVEs CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvCommons.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29628" opendate="2022-10-13 00:00:00" fixdate="2022-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump aws-java-sdk-s3 to 1.12.319</summary>
      <description>As reported by Dependabot in https://github.com/apache/flink/pull/20285</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2963" opendate="2015-11-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependence on SerializationUtils#deserialize() should be avoided</summary>
      <description>There is a problem with `SerializationUtils` from Apache CommonsLang. Here is an open issue where the class will throw a`ClassNotFoundException` even if the class is in the classpath in amultiple-classloader environment:https://issues.apache.org/jira/browse/LANG-1049 state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/NonKeyedWindowOperator.java state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java return SerializationUtils.deserialize(message);./flink-streaming-java/src/main/java/org/apache/flink/streaming/util/serialization/JavaDefaultStringSchema.java T copied = SerializationUtils.deserialize(SerializationUtils./flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockOutput.javaWe should move away from SerializationUtils.deserialize()</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29638" opendate="2022-10-14 00:00:00" fixdate="2022-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jackson bom because of CVE-2022-42003</summary>
      <description>There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1https://nvd.nist.gov/vuln/detail/CVE-2022-42003P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915</description>
      <version>1.16.0,1.17.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="29639" opendate="2022-10-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add ResourceId in TransportException for debugging</summary>
      <description>When the taskmanager is lost, only the host and port are shown in the exception. It is hard to find the exactly taskmanger by resourceId. Add ResourceId info will help a lot in debugging the job.</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.NettyShuffleDescriptorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.ConnectionID.java</file>
    </fixedFiles>
  </bug>
  <bug id="29644" opendate="2022-10-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reference Kubernetes operator from Flink Kubernetes deploy docs</summary>
      <description>Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="29684" opendate="2022-10-19 00:00:00" fixdate="2022-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[UI] Upgrade runtime web Angular framework and associated deps to v14</summary>
      <description>Angular framework and NG-ZORRO both have released their stable v14 versions. It is a good time to bump them to the latest.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.tsconfig.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.resize.resize.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.navigation.navigation.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.status.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.submit.submit.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.web-dashboard.angular.json</file>
    </fixedFiles>
  </bug>
  <bug id="2974" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add periodic offset commit to Kafka Consumer if checkpointing is disabled</summary>
      <description>Flink only writes the offsets from the consumer into ZK if checkpointing is enabled.We should have a similar feature to Kafka's autocommit in our consumer.Issue reported by user: http://stackoverflow.com/questions/33501574/flink-kafka-why-am-i-losing-messages</description>
      <version>None</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.java</file>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="29740" opendate="2022-10-24 00:00:00" fixdate="2022-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-265] Deprecate all customer-facing Scala APIs</summary>
      <description>Annotate all @Public, @PublicEvolving and @Experimental Scala APIs as @Deprecated.  Remove all Scala API code examples from the codebase</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.WordCountSQLExampleITCase.java</file>
      <file type="M">flink-walkthroughs.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.src.main.scala.FraudDetector.scala</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.src.main.scala.FraudDetectionJob.scala</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.src.main.resources.log4j2.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.WordCountITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.WebLogAnalysisITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.TransitiveClosureITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.PageRankITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.EnumTriangleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.ConnectedComponentsITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.test.resources.projects.testArtifact.goal.txt</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.test.resources.projects.testArtifact.archetype.properties</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.DataStreamJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.resources.log4j2.properties</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.pom.xml</file>
      <file type="M">docs.content.zh.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.scala.api.extensions.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.learn-flink.datastream.api.md</file>
      <file type="M">docs.content.zh.docs.try-flink.datastream.md</file>
      <file type="M">docs.content.zh.docs.try-flink.table.api.md</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.scala.api.extensions.md</file>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.docs.learn-flink.datastream.api.md</file>
      <file type="M">docs.content.docs.try-flink.datastream.md</file>
      <file type="M">docs.content.docs.try-flink.table.api.md</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.QuickstartExample.scala</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.clustering.KMeans.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.graph.ConnectedComponents.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.graph.DeltaPageRank.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.graph.EnumTriangles.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.graph.PageRankBasic.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.graph.TransitiveClosureNaive.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.misc.PiEstimation.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.relational.TPCHQuery10.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.relational.TPCHQuery3.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.relational.WebLogAnalysis.scala</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.scala.org.apache.flink.examples.scala.wordcount.WordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.async.AsyncClient.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.async.AsyncIOExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.iteration.IterateExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.join.WindowJoin.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.join.WindowJoinSampleData.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.socket.SocketWindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.GroupedProcessingTimeWindowExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.SessionWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.util.CarSource.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.wordcount.util.CLI.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.wordcount.WordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.scala.examples.windowing.TopSpeedWindowingExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.socket.SocketWindowWordCountITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.scala.org.apache.flink.streaming.scala.examples.StreamingExamplesITCase.scala</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.GettingStartedExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.StreamTableExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.basics.WordCountSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.GettingStartedExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.scala.basics.StreamSQLExampleITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2978" opendate="2015-11-5 00:00:00" fixdate="2015-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate web submission interface into the new dashboard</summary>
      <description>As discussed in http://mail-archives.apache.org/mod_mbox/flink-dev/201511.mbox/%3CCAL3J2zQg6UBKNDnm=8TshPZ6r4p2Jvx7NRLOm7cAAJrb9S6PYA@mail.gmail.com%3E, we should integrate job submission from the web into the dashboard.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobManagerConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractExecutionGraphRequestHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="29788" opendate="2022-10-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StatefulJobWBroadcastStateMigrationITCase failed in native savepoints</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29812" opendate="2022-10-31 00:00:00" fixdate="2022-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Netty API usages</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.KeepAliveWrite.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29834" opendate="2022-11-1 00:00:00" fixdate="2022-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear static Jackson TypeFactory cache on CL release</summary>
      <description>The Jackson TypeFactory contains a singleton instance that is at times used by Jackson, potentially containing user-classes for longer than necessary.https://github.com/FasterXML/jackson-databind/issues/1363We could clear this cache whenever a user code CL is being released similar to what was done in BEAM-6460.</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="29849" opendate="2022-11-2 00:00:00" fixdate="2022-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Event time temporal join on an upsert source may produce incorrect execution plan</summary>
      <description>For current implementation, the execution plan is incorrect when do event time temporal join on an upsert source. There's two problems:1. for an upsert source, we should not add a ChangelogNormalize node under a temporal join input, or it will damage the versions of the version table. For versioned tables, we use a single-temporal mechanism which relies sequencial records of a same key to ensure the valid period of each version, so if the ChangelogNormalize was added then an UB message will be produced based on the previous UA or Insert message, and all the columns are totally same include event time, e.g., original upsert input+I (key1, '2022-11-02 10:00:00', a1)+U (key1, '2022-11-02 10:01:03', a2)the versioned data should be:v1 [~, '2022-11-02 10:00:00')v2 ['2022-11-02 10:00:00', '2022-11-02 10:01:03')after ChangelogNormalize's processing, will output:+I (key1, '2022-11-02 10:00:00', a1)-U (key1, '2022-11-02 10:00:00', a1)+U (key1, '2022-11-02 10:01:03', a2)versions are incorrect:v1 ['2022-11-02 10:00:00', '2022-11-02 10:00:00') // invalid periodv2 ['2022-11-02 10:00:00', '2022-11-02 10:01:03')2. semantically, a filter cannot be pushed into an event time temporal join, otherwise, the filter may also corrupt the versioned table</description>
      <version>1.16.0,1.15.3</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="2985" opendate="2015-11-6 00:00:00" fixdate="2015-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow different field names for unionAll() in Table API</summary>
      <description>The recently merged `unionAll` operator checks if the field names of the left and right side are equal. Actually, this is not necessary. The union operator in SQL checks only the types and uses the names of left side.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29852" opendate="2022-11-2 00:00:00" fixdate="2022-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive Scheduler duplicates operators for each parallel instance in the Web UI</summary>
      <description>All the operators in the DAG are shown repeatedly</description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="29859" opendate="2022-11-3 00:00:00" fixdate="2022-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test with adaptive batch scheduler failed due to oo non-empty .out files.</summary>
      <description>Nov 03 02:02:12 &amp;#91;FAIL&amp;#93; 'TPC-DS end-to-end test with adaptive batch scheduler' failed after 21 minutes and 44 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&amp;view=logs&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="29879" opendate="2022-11-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce operators for files merging in batch mode</summary>
      <description>Similar to streaming mode, we introuce the following four operators:BatchFileWriter -&gt; BatchCompactCoordinator  -&gt;  BatchCompactOperator -&gt; BatchPartitionCommitter</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.AbstractCompactTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.compact.CompactMessages.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.SingleDirectoryWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionWriterFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionTempFileManager.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionLoader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.GroupedPartitionWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.DynamicPartitionWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="29880" opendate="2022-11-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive sink supports merge files in batch mode</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.compact.CompactOperator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.batch.compact.CompactFileUtils.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.batch.compact.BatchCompactOperator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.batch.compact.BatchCompactCoordinator.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="29913" opendate="2022-11-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared state would be discarded by mistake when maxConcurrentCheckpoint&gt;1</summary>
      <description>When maxConcurrentCheckpoint&gt;1, the shared state of Incremental rocksdb state backend would be discarded by registering the same name handle. See https://github.com/apache/flink/pull/21050#discussion_r1011061072cc roman </description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistryImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateHandleReuseITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategyTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBRestoreResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalLocalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29920" opendate="2022-11-7 00:00:00" fixdate="2022-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor reformat Kafka connector documentation</summary>
      <description>We used some HTML tag in the documentation which does not interpret Markdown format nicely. This fixes this by replacing with Markdown tags.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="29992" opendate="2022-11-11 00:00:00" fixdate="2022-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive lookupJoin execution plan parsing error</summary>
      <description>//tableEnv.executeSql(" CREATE CATALOG hive WITH (\n" + " 'type' = 'hive',\n" + " 'default-database' = 'flinkdebug',\n" + " 'hive-conf-dir' = '/programe/hadoop/hive-3.1.2/conf'\n" + " )");tableEnv.executeSql("create table datagen_tbl (\n" + "id STRING\n" + ",name STRING\n" + ",age bigint\n" + ",ts bigint\n" + ",`par` STRING\n" + ",pro_time as PROCTIME()\n" + ") with (\n" + " 'connector'='datagen'\n" + ",'rows-per-second'='10'\n" + " \n" + ")");String dml1 = "select * " + " from datagen_tbl as p " + " join hive.flinkdebug.default_hive_src_tbl " + " FOR SYSTEM_TIME AS OF p.pro_time AS c" + " ON p.id = c.id";// Execution succeeded  System.out.println(tableEnv.explainSql(dml1));String dml2 = "select p.id " + " from datagen_tbl as p " + " join hive.flinkdebug.default_hive_src_tbl " + " FOR SYSTEM_TIME AS OF p.pro_time AS c" + " ON p.id = c.id";// Throw an exception System.out.println(tableEnv.explainSql(dml2)); org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: FlinkLogicalCalc(select=[id]) +- FlinkLogicalJoin(condition=[=($0, $1)], joinType=[inner])    :- FlinkLogicalCalc(select=[id])    :  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, datagen_tbl]], fields=[id, name, age, ts, par])    +- FlinkLogicalSnapshot(period=[$cor1.pro_time])       +- FlinkLogicalTableSourceScan(table=[[hive, flinkdebug, default_hive_src_tbl, project=[id]]], fields=[id])This exception indicates that the query uses an unsupported SQL feature. Please check the documentation for the set of currently supported SQL features.    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)     at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)</description>
      <version>1.16.0,1.17.0,1.14.7,1.15.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="29996" opendate="2022-11-11 00:00:00" fixdate="2022-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Link to the task manager&amp;#39;s thread dump page in the backpressure tab</summary>
      <description>Currently, we have a complex steps to find the thread dump of backpressured tasks, however, this could be simplified with a link in the backpressure tab of web UI.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="29997" opendate="2022-11-11 00:00:00" fixdate="2022-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Link to the taskmanager page in the expired sub-task of checkpoint tab</summary>
      <description>Currently, when we debug why some of the sub-tasks cannot complete the checkpoints in time, we have a complex steps to find which task manager containing such logs. This could be simplified via a direct link.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="29998" opendate="2022-11-11 00:00:00" fixdate="2022-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the backpressure tab could be sort by the busy percent</summary>
      <description>Currently, we cannot sort the backpressure tab to see which task is busiest.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="30002" opendate="2022-11-12 00:00:00" fixdate="2022-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the alignmentTimeout to alignedCheckpointTimeout</summary>
      <description>The alignmentTimeout has been changed to alignedCheckpointTimeout in FLINK-23041 .But some fields or methods still use alignmentTimeout. They should be renamed to alignedCheckpointTimeout.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.BarrierHandlerState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingWaitingForFirstBarrierUnaligned.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingWaitingForFirstBarrier.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriersUnaligned.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="30006" opendate="2022-11-13 00:00:00" fixdate="2022-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot remove columns that are incorrectly considered constants from an Aggregate In Streaming</summary>
      <description>In Streaming, columns generated by dynamic functions are incorrectly considered constants and removed from an Aggregate via optimization rule `CoreRules.AGGREGATE_PROJECT_PULL_UP_CONSTANTS` (inside the RelMdPredicates, it only considers the non-deterministic functions, but this doesn't applicable for streaming)an example query: @Test def testReduceGroupKey(): Unit = { util.tableEnv.executeSql(""" |CREATE TABLE t1( | a int, | b varchar, | cat VARCHAR, | gmt_date DATE, | cnt BIGINT, | PRIMARY KEY (cat) NOT ENFORCED |) WITH ( | 'connector' = 'values' |) |""".stripMargin) util.verifyExecPlan(s""" |SELECT | cat, gmt_date, SUM(cnt), count(*) |FROM t1 |WHERE gmt_date = current_date |GROUP BY cat, gmt_date |""".stripMargin) }the wrong plan:Calc(select=[cat, CAST(CURRENT_DATE() AS DATE) AS gmt_date, EXPR$2, EXPR$3])+- GroupAggregate(groupBy=[cat], select=[cat, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3]) +- Exchange(distribution=[hash[cat]]) +- Calc(select=[cat, cnt], where=[=(gmt_date, CURRENT_DATE())]) +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, cnt, gmt_date], metadata=[]]], fields=[cat, cnt, gmt_date])expect plan:GroupAggregate(groupBy=[cat, gmt_date], select=[cat, gmt_date, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])+- Exchange(distribution=[hash[cat, gmt_date]]) +- Calc(select=[cat, gmt_date, cnt], where=[(gmt_date = CURRENT_DATE())]) +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, gmt_date, cnt], metadata=[]]], fields=[cat, gmt_date, cnt])In addition to this issue, we need to check all optimization rules in streaming completely to avoid similar problems.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlTimestampFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUpsertKeys.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.StreamNonDeterministicUpdatePlanVisitor.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcMergeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcMergeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.common.CalcMergeTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterCalcMergeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="30041" opendate="2022-11-16 00:00:00" fixdate="2022-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup conjars https mirror</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.google-mirror-settings.xml</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30044" opendate="2022-11-16 00:00:00" fixdate="2022-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deduplicate plugin parser loops</summary>
      <description>The flink-ci-tools parse the output of various modules and each have their own looping logic to that end, that are pretty much identical though.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.utils.deploy.DeployParserTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.shade.ShadeParser.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.deploy.DeployParser.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.dependency.DependencyParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="3005" opendate="2015-11-12 00:00:00" fixdate="2015-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Commons-collections object deserialization remote command execution vulnerability</summary>
      <description>http://foxglovesecurity.com/2015/11/06/what-do-weblogic-websphere-jboss-jenkins-opennms-and-your-application-have-in-common-this-vulnerability/TL;DR: If you have commons-collections on your classpath and accept and process Java object serialization data, then you may have an exploitable remote command execution vulnerability.Brief search in code base for ObjectInputStream reveals several places where the vulnerability exists.</description>
      <version>None</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30079" opendate="2022-11-18 00:00:00" fixdate="2022-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop using deprecated TM options in doc</summary>
      <description>The option ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY was deprecated and configuring it should have no effect now. However, in the documentation we still reference it and show in example code. This can be replaced with TaskManagerOptions.MANAGED_MEMORY_FRACTION.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.dataset.local.execution.md</file>
      <file type="M">docs.content.zh.docs.dev.dataset.local.execution.md</file>
    </fixedFiles>
  </bug>
  <bug id="30113" opendate="2022-11-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Compression for operator state</summary>
      <description>It has been requested in the ML to be able to enable compression for broadcast statehttps://lists.apache.org/thread/2kylgj0fdmn21jk7x63696mgdvd1csxo</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendSerializationProxy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="30116" opendate="2022-11-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t Show Env Vars in Web UI</summary>
      <description>As discussed and agreed upon in &amp;#91;1&amp;#93;, we'd like to revert &amp;#91;2&amp;#93; and not show any environment variables in the Web UI for security reasons. &amp;#91;1&amp;#93; https://lists.apache.org/thread/rjgk15bqttvblp60zry4n5pw4xjw7q9k &amp;#91;2&amp;#93; https://issues.apache.org/jira/browse/FLINK-28311</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobManagerJobEnvironmentHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.EnvironmentInfo.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="30169" opendate="2022-11-23 00:00:00" fixdate="2022-1-23 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Adds version switcher in PyFlink API doc</summary>
      <description>Adds version switcher in PyFlink API doc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30180" opendate="2022-11-24 00:00:00" fixdate="2022-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the description of Filesystem supporting for the lookup in the Supported Connectors section</summary>
      <description>In the Supported Connectors section, the graph shows that the Filesystem connector supports the lookup source. Actually, it doesn't.So I want to remove this description.  </description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="30185" opendate="2022-11-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Provide the flame graph to the subtask level</summary>
      <description>FLINK-13550 supported for CPU FlameGraphs in web UI.As Flink doc mentioned:https://nightlies.apache.org/flink/flink-docs-master/docs/ops/debugging/flame_graphs/#sampling-processNote: Stack trace samples from all threads of an operator are combined together. If a method call consumes 100% of the resources in one of the parallel tasks but none in the others, the bottleneck might be obscured by being averaged out.There are plans to address this limitation in the future by providing “drill down” visualizations to the task level.  The flame graph at the subtask level is very useful when a small number of subtasks are bottlenecked. So we should provide the flame graph to the subtask level   </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.FlameGraphTypeQueryParameter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.ThreadInfoSample.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.TaskThreadInfoResponse.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="30189" opendate="2022-11-24 00:00:00" fixdate="2022-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HsSubpartitionFileReader may load data that has been consumed from memory</summary>
      <description>In order to solve the problem that data cannot be read from the disk correctly after failover, we changed the calculation logical of the buffer's readable state in FLINK-29238.  Buffers that are greater than consumingOffset and have been released can be pre-load from file. However, the update of consumingOffset is asynchronous, If it lags behind the actual consumption progress, the buffer will have a chance to be load from the disk again. IMO, we can record the consumed status of buffer by each consumer in the InternalRegion. Only the buffers that have not been consumed and have been released will be considered as readable. In the case of failover, a new consumerId will be generated, so all buffers will be considered as unconsumed and can be correctly read from the disk too.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="30239" opendate="2022-11-29 00:00:00" fixdate="2022-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph doesn&amp;#39;t work due to groupExecutionsByLocation has bug</summary>
      <description>The flame graph cannot be generated forever when multiple tasks in the same TM. It's caused by FLINK-26074 Root cause:A Set cannot be converted to an ImmutableSet during the aggregation of ExecutionAttemptIDs. It will cause only the first ExecutionAttemptID of the TM to be added to the set, the second ExecutionAttemptID will fail.     Exception Info: </description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="30250" opendate="2022-11-30 00:00:00" fixdate="2022-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph type is wrong</summary>
      <description>When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.Root cause:When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type. code link</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.flame-graph.flame-graph.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="30251" opendate="2022-11-30 00:00:00" fixdate="2022-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move the IO with DFS during abort checkpoint to an asynchronous thread.</summary>
      <description>Currently when the checkpoint fails, we process the abort message in the Task's mailbox. We will close the output stream and delete the file on DFS.  However, when the checkpoint failure is caused by a DFS system failure (for example, the namenode failure of HDFS), this operation may take a long time or hang, and the task will not be able to process the data at this time. So I think we can put the operation of deleting files in an asynchronous thread just like uploading checkpoint data asynchronously.</description>
      <version>1.16.0,1.15.2</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="30286" opendate="2022-12-2 00:00:00" fixdate="2022-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run rat plugin in validate phase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">tools.ci.compile.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30339" opendate="2022-12-8 00:00:00" fixdate="2022-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a unified delegation token manager</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.layouts.shortcodes.generated.security.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.security.auth.kerberos.section.html</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="3034" opendate="2015-11-17 00:00:00" fixdate="2015-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redis SInk Connector</summary>
      <description>Flink does not provide a sink connector for Redis.See FLINK-3033</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.fault.tolerance.md</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="30358" opendate="2022-12-10 00:00:00" fixdate="2022-12-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Show the task manager id on the exception history page</summary>
      <description>At present, the web UI exception history page only displays the TM host and port. However, we generally need to search for problems according to the pod name or container ID.Therefore, it is more convenient to add resource id (pod name on k8s, container id on yarn) to the location column and a link to the task manager id to jump to the task manager page.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug id="30375" opendate="2022-12-12 00:00:00" fixdate="2022-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlClient leaks flink-table-planner jar under /tmp</summary>
      <description>SqlClient leaks flink-table-planner jar under /tmp [root@ip-172-1-1-3 lib]# ls -lrt /tmp/flink-table-planner_*-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:08 /tmp/flink-table-planner_acada33f-a10b-4a4a-ad16-6bca25a67e10.jar-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:17 /tmp/flink-table-planner_fb2f6e31-48a0-4c1e-ab7b-16129e776125.jar-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:22 /tmp/flink-table-planner_83499393-1621-43de-953c-2000bc6967ce.jar-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:24 /tmp/flink-table-planner_3aa798da-e794-4c6c-ad9f-49b4574da64b.jar-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:36 /tmp/flink-table-planner_ea52d9ea-3148-4fc3-8a58-f83063bb14d5.jar-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:44 /tmp/flink-table-planner_fdc64e21-6bc7-4e4a-b8b2-2a97629d0727.jar-rw-rw-r-- 1 hadoop hadoop 39137545 Dec 12 11:05 /tmp/flink-table-planner_84c00b13-c6b5-4e8b-80cb-5631a2fa3150.jar-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:10 /tmp/flink-table-planner_d00b8a7b-e615-46f7-bd21-b5efa684c184.jar-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:11 /tmp/flink-table-planner_a413520d-ce15-41f9-aa5e-05a30f7eaff5.jar</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-loader.src.test.java.org.apache.flink.table.planner.loader.LoaderITCase.java</file>
      <file type="M">flink-table.flink-table-planner-loader.src.main.java.org.apache.flink.table.planner.loader.PlannerModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="30383" opendate="2022-12-12 00:00:00" fixdate="2022-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UseLogicalIdentifier makes datadog consider metric as custom</summary>
      <description>Custom metrics in the DD context are meant for metrics not exported by the standard integrations (which includes our reporter), but seemingly enabling the useLogicalIdentifier flag breaks whatever detection mechanism they have for the reporter.This results in additional costs to users.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="30386" opendate="2022-12-12 00:00:00" fixdate="2022-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column constraint lacks primary key not enforced check</summary>
      <description>Currently, only table constraint performs the enforced check. Not sure if it is by design or a bug.The following case can be reproduced on Flink 1.16.0, 1.15.3, and 1.15.2. I think the earlier version might also reveal it.Flink SQL&gt; create table T (f0 int not null primary key, f1 string) with ('connector' = 'datagen');[INFO] Execute statement succeed.Flink SQL&gt; explain select * from T;== Abstract Syntax Tree ==LogicalProject(f0=[$0], f1=[$1])+- LogicalTableScan(table=[[default_catalog, default_database, T]])== Optimized Physical Plan ==TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])== Optimized Execution Plan ==TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])Flink SQL&gt; create table S (f0 int not null, f1 string, primary key(f0)) with ('connector' = 'datagen');[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode</description>
      <version>1.16.0,1.15.2,1.15.3</version>
      <fixedVersion>1.17.0,hbase-3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.SqlConstraintValidator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30392" opendate="2022-12-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default value of cluster.thread-dump.stacktrace-max-depth</summary>
      <description>Currently, the thread dump function still have the default stack-trace depth as 8, which is the same as before. However, the default value is really too small for developers to know the actual thread info.From our experiences, we can set this value as 24.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="30397" opendate="2022-12-13 00:00:00" fixdate="2022-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Pulsar connector from master branch</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.delayer.MessageDelayer.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.source.FailoverSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.source.ExclusiveSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSinkE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.PulsarContainerTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.util.ConfigurationOptionLocator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.licences.LICENSE.bouncycastle</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.protobuf.sample.message.proto</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.PulsarSourceTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.KeyedPulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.cases.SharedSubscriptionContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.cases.KeySharedSubscriptionContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.sink.PulsarSinkTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.sink.PulsarPartitionDataReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.SampleData.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.SameThreadOrderedSafeExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.NonClosableMockBookKeeper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.BlankBrokerInterceptor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContextFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestCommonUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.function.ControlSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.extension.TestOrderlinessExtension.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.extension.SubType.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplitSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarUnorderedSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRangeTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicPartitionTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.TopicRangeUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.SplitRangeGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicProducerRegisterTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListenerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.router.RoundRobinTopicRouterTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.router.KeyHashTopicRouterTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.committer.PulsarCommittableSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTypeSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTypeInformationTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.factories.ProtobufSchemaFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.factories.ProtobufNativeSchemaFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.factories.KeyValueSchemaFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.factories.JSONSchemaFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.factories.AvroSchemaFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigValidatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplitState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplitSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarOrderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarTypeInformationWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaInitializationContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRange.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicPartition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.TopicRangeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.SplitRangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.RangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.FullRangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.FixedRangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.FixedKeysRangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.TopicPatternSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.TopicListSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.BasePulsarSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.PublishTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.NeverStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.MessageIdStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.LatestMessageStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.EventTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.TimestampStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.CursorPosition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.KeySharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarConsumerBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.CursorVerification.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicProducerRegister.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListener.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.serializer.PulsarSerializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.serializer.PulsarSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.serializer.PulsarSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.TopicRoutingMode.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.TopicRouter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.RoundRobinTopicRouter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.MessageKeyHash.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.KeyHashTopicRouter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.message.PulsarMessageBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.message.PulsarMessage.java</file>
      <file type="M">docs.setup.docs.sh</file>
      <file type="M">docs.config.toml</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.admin.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.consumer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.producer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.source.configuration.html</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.3ac3a1dc-681f-4213-9990-b7b3298a20bc</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.f4d91193-72ba-4ce4-ad83-98f780dce581</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigValidator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.metrics.MetricNames.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.metrics.ProducerMetricsInterceptor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.AvroSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.BaseStructSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.JSONSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.KeyValueSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.PrimitiveSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.ProtobufNativeSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.ProtobufSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.factories.StringSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTypeInformation.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTypeSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarSerdeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarTransactionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.committer.PulsarCommittable.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.committer.PulsarCommittableSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSink.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.context.PulsarSinkContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.context.PulsarSinkContextImpl.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.delayer.FixedMessageDelayer.java</file>
    </fixedFiles>
  </bug>
  <bug id="30424" opendate="2022-12-15 00:00:00" fixdate="2022-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add source operator restore readerState log to distinguish split is from newPartitions or split state</summary>
      <description>When a job start firstly, we can find 'assignPartitions' from log。but if source recover from state, we can not distinguish the newPartitions is from timed discover thread or from reader task state.  We can add a helper log to distinguish and confirm the reader using split state in recover situation.  it's very useful for troubleshooting.  </description>
      <version>1.16.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="30425" opendate="2022-12-15 00:00:00" fixdate="2022-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize token receive side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdaterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30461" opendate="2022-12-20 00:00:00" fixdate="2022-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some rocksdb sst files will remain forever</summary>
      <description>In rocksdb incremental checkpoint mode, during file upload, if some files have been uploaded and some files have not been uploaded, the checkpoint is canceled due to checkpoint timeout at this time, and the uploaded files will remain. Impact: The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written.However only 50k files are available, the other 950k files should be cleaned up.Root cause:If an exception is thrown during the checkpoint async phase, flink will clean up metaStateHandle, miscFiles and sstFiles.However, when all sst files are uploaded, they are added together to sstFiles. If some sst files have been uploaded and some sst files are still being uploaded, and  the checkpoint is canceled due to checkpoint timeout at this time, all sst files will not be added to sstFiles. The uploaded sst will remain on hdfs.code linkSolution:Using the CloseableRegistry as the tmpResourcesRegistry. If the async phase is failed, the tmpResourcesRegistry will cleanup these temporary resources. POC code:https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1   </description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="30491" opendate="2022-12-23 00:00:00" fixdate="2022-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive table partition supports to deserialize later during runtime</summary>
      <description></description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.HivePartitionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceDynamicFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="30517" opendate="2022-12-28 00:00:00" fixdate="2022-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decrease log output interval while waiting for YARN JobManager be allocated</summary>
      <description>Flink Client will retrieve the application status every 250ms after submitting to YARN. If JobManager does not start in 60 seconds, it will log "Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster" every 250ms. This will lead to too many logs. We can keep the check interval at 250ms, but log the message every 1 minute.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="30557" opendate="2023-1-4 00:00:00" fixdate="2023-1-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove Kinesis Data Streams connectors from Flink master branch</summary>
      <description>Remove: flink-connector-kinesis flink-connector-aws-kinesis-streams flink-sql-connector-kinesis flink-sql-connector-aws-kinesis-streams Corresponding e2e tests</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.resources.filter-large-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.model.Order.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.main.java.org.apache.flink.streaming.kinesis.test.KinesisExampleTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.main.java.org.apache.flink.streaming.kinesis.test.KinesisExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.src.test.resources.send-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.test.KinesisStreamsTableApiIT.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.test.java.org.apache.flink.connectors.kinesis.PackagingITCase.java</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.profile</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.9-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.9-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.8-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.8-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.16-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.16-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.15-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.15-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.14-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.14-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.13-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.13-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.12-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.12-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.11-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.11-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.10-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.kinesis-consumer-migration-test-flink1.10-empty-snapshot</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.WatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.UniformShardAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.RecordEmitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2UtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestSourceContext.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestRuntimeContext.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcherForShardConsumerException.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableFlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.KinesisShardIdGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.KinesisPubsubClient.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.KinesisEventsGeneratorProducerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.ExactlyOnceValidatingConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.AlwaysThrowsDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2FactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardHandleTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.StartingPositionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.SentinelSequenceNumberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.model.DynamoDBStreamsShardHandleTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.metrics.ShardConsumerMetricsReporterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.metrics.PollingRecordPublisherMetricsReporterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualConsumerProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.KinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerFanOutTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordBatchTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.StreamConsumerRegistrarTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherConfigurationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.examples.ConsumeFromKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.examples.ConsumeFromDynamoDBStreams.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.org.apache.flink.kinesis.shaded.software.amazon.awssdk.global.handlers.execution.interceptors</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.services.org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpService</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.WatermarkTracker.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.UniformShardAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.TimeoutLatch.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTracker.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.BeanDeserializerModifierForIgnorables.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2Util.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.RowDataKinesisDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisConsumerOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisDeserializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.DynamoDBStreamsSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Interface.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Factory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyInterface.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.GetShardListResult.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.FullJitterBackoff.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardHandle.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.SequenceNumber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.SentinelSequenceNumber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.KinesisStreamShardState.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.KinesisStreamShard.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.DynamoDBStreamsShardHandle.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.metrics.ShardConsumerMetricsReporter.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.metrics.PollingRecordPublisherMetricsReporter.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.metrics.KinesisConsumerMetricConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.KinesisShardAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.KinesisPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordPublisherFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordBatch.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.AdaptivePollingRecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.StreamConsumerRegistrar.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcher.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.archunit-violations.75596a92-3816-4a44-85ac-7c96e72f443a</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.archunit-violations.7e2560a3-23eb-40cc-8669-e7943e393b88</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.archunit-violations.84abeb9c-8355-4165-96aa-dda65b04e5e7</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsException.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.PartitionKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.FixedKinesisPartitionKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisPartitionKeyGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.RandomKinesisPartitionKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.RowDataFieldsKinesisPartitionKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.table.util.KinesisStreamsConnectorOptionsUtils.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.resources.log4j2.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.examples.SinkIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.RowDataFieldsKinesisPartitionKeyGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.util.KinesisProducerOptionsMapperTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.resources.archunit.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.resources.profile</file>
      <file type="M">flink-connectors.flink-connector-kinesis.archunit-violations.28f0499c-3213-4ec2-97f7-970f052922b3</file>
      <file type="M">flink-connectors.flink-connector-kinesis.archunit-violations.4c963703-6b45-4782-825a-5cc6ba1556dd</file>
      <file type="M">flink-connectors.flink-connector-kinesis.archunit-violations.stored.rules</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.AWSConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ProducerConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkDynamoDBStreamsConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisException.java</file>
    </fixedFiles>
  </bug>
  <bug id="3056" opendate="2015-11-21 00:00:00" fixdate="2015-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show bytes sent/received as MBs/GB and so on in web interface</summary>
      <description>It would be great if the web interface would round show the bytes in an appropriate (=human readable) unit.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.index.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug id="30567" opendate="2023-1-5 00:00:00" fixdate="2023-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong insert overwrite behavior when the table contains uppercase character with Hive dialect</summary>
      <description>If use hive dialect, the data would be wrong if we rerun the following job multiple tables.INSERT overwrite TABLE db_name.target_tB_name partition(p_date = '20230101')select author_idfrom db_name.source_tb_nameWHERE p_date = '20230101'and author_id &lt;&gt; '0'The framework has a minor bug when determining whether an override is required if the target name contains uppercase character.</description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="30579" opendate="2023-1-6 00:00:00" fixdate="2023-1-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introducing cofigurable option to enable hive native function</summary>
      <description>Currently, hive native function implementation can't assign behavior with hive udaf, so we should introduce an configurable option to allow enable this optimization, the default behavior is disabled.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.session.SessionEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.endpoint.hive.module.q</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModuleFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSumAggFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.archunit-violations.26d337fc-45c4-4d03-a84a-6692c37fafbc</file>
    </fixedFiles>
  </bug>
  <bug id="30586" opendate="2023-1-6 00:00:00" fixdate="2023-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix calcCodeGen failed if calc with like condition contains double quotation mark</summary>
      <description>If I write a sql like "SELECT * FROM MyTable WHERE b LIKE '%"%'" in Flink-1.16 as'like' condition contains double quotation mark, it will cause code gen failed because wrong code generated by codeGen.   </description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="30617" opendate="2023-1-10 00:00:00" fixdate="2023-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong data type for null literal with cast in Hive dialect</summary>
      <description>When in hive dialect, it's found that the expression `cast null as bigint` will result in string type. Then it will cause the following sql fail:create table t(a ARRAY&lt;BIGINT&gt;)insert into t select array(cast(null as bigint)) The exception is:org.apache.flink.table.api.ValidationException: Column types of query result and sink for 'test-catalog.default.t' do not match.Cause: Incompatible types for sink column 'a' at position 0.Query schema: [_o__c0: ARRAY&lt;STRING&gt;]Sink schema:  [a: ARRAY&lt;BIGINT&gt;] Introduce by FLINK-26474, after that we will fold `cast null as bigint` to `null `which will then considered as the null type. And for null type as argument, the function `array` will get a result type of array&lt;string&gt;. </description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="30636" opendate="2023-1-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo fix: "to to" -&gt; "to"</summary>
      <description>There's a surprising number of occurrences of "to to" in JavaDoc and the like, where it actually should just be "to".</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ProcessingTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelShuttles.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLegacySink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBFullSnapshotResources.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.TaskOperatorEventGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationHeadTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.TaskEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.ListState.java</file>
      <file type="M">docs.content.docs.dev.table.hive-compatibility.hive-dialect.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.hive-compatibility.hive-dialect.insert.md</file>
    </fixedFiles>
  </bug>
  <bug id="30761" opendate="2023-1-20 00:00:00" fixdate="2023-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JVM asserts from leader election code</summary>
      <description>assert is not enabled in the test run. We should using Preconditions</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.ManualLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
    </fixedFiles>
  </bug>
  <bug id="30808" opendate="2023-1-28 00:00:00" fixdate="2023-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleInputITCase failed with AdaptiveBatch Scheduler</summary>
      <description>MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="30814" opendate="2023-1-29 00:00:00" fixdate="2023-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The parallelism&amp;maxParallelism of sort after a global partitioning is not forced to be 1</summary>
      <description>The parallelism&amp;maxParallelism of sort after a global partitioning is not forced to be 1. The may lead to the parallelism to be changed by adaptive batch scheduler, which is unexpected.</description>
      <version>1.16.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.ParallelismSettingTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLimit.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecScriptTransform.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLimit.java</file>
    </fixedFiles>
  </bug>
  <bug id="30819" opendate="2023-1-29 00:00:00" fixdate="2023-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sql client print an empty line between the multi-line statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.util.CliClientTestUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ClientResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.parser.SqlMultiLineParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="30896" opendate="2023-2-3 00:00:00" fixdate="2023-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce usage of CatalogViewImpl in planner</summary>
      <description>Most of the work was done under https://issues.apache.org/jira/browse/FLINK-21801 However there are still some usages of CatalogViewImpl</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.hint.OptionsHintTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="30966" opendate="2023-2-8 00:00:00" fixdate="2023-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL IF FUNCTION logic error</summary>
      <description>my data is //{ "before": { "status": "sent" }, "after": { "status": "succeed" }, "op": "u", "ts_ms": 1671926400225, "transaction": null } my sql is  CREATE TABLE t( before ROW ( status varchar (32) ), after ROW ( status varchar (32) ), ts_ms bigint, op string, kafka_timestamp timestamp METADATA FROM 'timestamp',-- @formatter:off proctime AS PROCTIME()-- @formatter:on) WITH ( 'connector' = 'kafka',-- 'topic' = '', 'topic' = 'test', 'properties.bootstrap.servers' = ' ', 'properties.group.id' = '', 'format' = 'json', 'scan.topic-partition-discovery.interval' = '60s', 'scan.startup.mode' = 'earliest-offset', 'json.ignore-parse-errors' = 'true' );create table p( status STRING , before_status STRING , after_status STRING , metadata_operation STRING COMMENT '源记录操作类型', dt STRING)WITH ( 'connector' = 'print' );INSERT INTO pSELECT IF(op &lt;&gt; 'd', after.status, before.status), before.status, after.status, op AS metadata_operation, DATE_FORMAT(kafka_timestamp, 'yyyy-MM-dd') AS dtFROM t;  my local env output is   +I[null, sent, succeed, u, 2023-02-08]   my produtionc env output is +I[sent, sent, succeed, u, 2023-02-08] why?  This look like a bug. </description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.IfCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.type.NumericOrDefaultReturnTypeInference.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.type.FlinkReturnTypes.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="30984" opendate="2023-2-9 00:00:00" fixdate="2023-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove explicit cast required by 3.1.x janino</summary>
      <description>This is a follow up task.Currently in 3.1.x Janino there is  https://github.com/janino-compiler/janino/issues/188 leading to fail several Flink tests. Once it is fixed on janino side WAs should be removed together with janino's update</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-formats.flink-protobuf.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31543" opendate="2023-3-21 00:00:00" fixdate="2023-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkStatement for jdbc driver</summary>
      <description>Introduce `FlinkStatement` for jdbc driver</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkResultSetMetaDataTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkConnectionTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSetMetaData.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSet.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkConnection.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31544" opendate="2023-3-21 00:00:00" fixdate="2023-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkDatabaseMetaData for jdbc driver</summary>
      <description>Introduce `FlinkDatabaseMetaData`</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkConnectionTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSetMetaData.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSet.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkConnection.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.DriverUri.java</file>
    </fixedFiles>
  </bug>
  <bug id="31588" opendate="2023-3-23 00:00:00" fixdate="2023-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The unaligned checkpoint type is wrong at subtask level</summary>
      <description>FLINK-20488 supported show checkpoint type for each subtask, and it based on received `CheckpointOptions` and it's right.However, FLINK-27251 supported timeout aligned to unaligned checkpoint barrier in the output buffers. It means the received `CheckpointOptions` can be converted from aligned checkpoint to unaligned checkpoint.So, the unaligned checkpoint type may be wrong at subtask level. For example, as shown in the figure below, Unaligned checkpoint type is false, but it is actually Unaligned checkpoint (persisted data &gt; 0).  </description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="31604" opendate="2023-3-24 00:00:00" fixdate="2023-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce usage of CatalogTableImpl in planner</summary>
      <description>The task is similar to https://issues.apache.org/jira/browse/FLINK-30896 however about CatalogTableImpl which is deprecated</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogConstraintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31912" opendate="2023-4-24 00:00:00" fixdate="2023-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade bytebuddy to 14.4.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31915" opendate="2023-4-24 00:00:00" fixdate="2023-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python API incorrectly passes env.java.opts as single argument</summary>
      <description>The python API passes all java options as a single string argument, which typically means that the JVM will reject them.</description>
      <version>1.16.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug id="31934" opendate="2023-4-25 00:00:00" fixdate="2023-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove mocking in RocksDB tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="32027" opendate="2023-5-8 00:00:00" fixdate="2023-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch jobs could hang at shuffle phase when max parallelism is really large</summary>
      <description>In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:It would hang for a long time and show "No bytes sent": After some time to debug, we can see the downstream operator did not receive the end-of-partition event.</description>
      <version>1.16.0,1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="3216" opendate="2016-1-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
