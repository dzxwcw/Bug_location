<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="11103" opendate="2018-12-7 00:00:00" fixdate="2018-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set a default uncaught exception handler</summary>
      <description>We should set a default uncaught exception handler in Flink via Thread.setDefaultUncaughtExceptionHandler() which at least logs the exceptions. Ideally, we would even fail the job (could make this configurable) but users may have some ill-behaving threads, e.g. through libraries, which they would want to tolerate and we don't want to change behaviour now.(FLINK-5232 added this for the JobManager, we need it for the TaskManager)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="13973" opendate="2019-9-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="17401" opendate="2020-4-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Labels to Mesos TM taskinfo object</summary>
      <description>Currently labels are not set in the task info object. A lot of companies can have logic specific to labels on TM. For example in criteo, based on labels we set kerberos env variables and mesos debug capabilities. It is critical for us to be able to pass these label values to the task manager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18934" opendate="2020-8-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idle stream does not advance watermark in connected stream</summary>
      <description>Per Flink documents, when a stream is idle, it will allow watermarks of downstream operator to advance. However, when I connect an active data stream with an idle data stream, the output watermark of the CoProcessOperator does not increase.Here's a small test that reproduces the problem.https://github.com/kien-truong/flink-idleness-testing</description>
      <version>1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamSourceContextIdleDetectionTests.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.MockStreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapWrapperOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.CountingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Input.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Output.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TwoInputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ChainingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2Test.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.CollectorOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.FirstInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.OneInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.BroadcastingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.CopyingSecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.FirstInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.SecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.output.BlackHoleOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SortingBoundedInputITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonTimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingBroadcastingOutputCollector.java</file>
    </fixedFiles>
  </bug>
  <bug id="1902" opendate="2015-4-17 00:00:00" fixdate="2015-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface reports false (the default) jobmanager.rpc.port on YARN</summary>
      <description>Running Flink as YARN session mode I was completely confused by the web interface reporting a false jobmanager.rpc.port (the default).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19445" opendate="2020-9-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests for HBase connector 1.4 failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest) Time elapsed: 0.031 sec &lt;&lt;&lt; ERROR!2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.2020-09-28T21:28:31.0370707Z 2020-09-28T21:28:31.0370976Z Table options are:2020-09-28T21:28:31.0371204Z 2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'2020-09-28T21:28:31.0371871Z 'table-name'='my_table'2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'2020-09-28T21:28:31.0372812Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)2020-09-28T21:28:31.0373359Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)2020-09-28T21:28:31.0373905Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)2020-09-28T21:28:31.0374390Z at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)2020-09-28T21:28:31.0375224Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)2020-09-28T21:28:31.0375867Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)2020-09-28T21:28:31.0376479Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0377077Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)2020-09-28T21:28:31.0377593Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0378114Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)2020-09-28T21:28:31.0378622Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)2020-09-28T21:28:31.0379132Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)2020-09-28T21:28:31.0379872Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)2020-09-28T21:28:31.0380477Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)2020-09-28T21:28:31.0381128Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)2020-09-28T21:28:31.0381666Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)2020-09-28T21:28:31.0382264Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)2020-09-28T21:28:31.0382968Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)2020-09-28T21:28:31.0383550Z at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)2020-09-28T21:28:31.0384172Z at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)2020-09-28T21:28:31.0384700Z at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)2020-09-28T21:28:31.0385201Z at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)2020-09-28T21:28:31.0385717Z at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)2020-09-28T21:28:31.0386166Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:28:31.0386575Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:28:31.0387257Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:28:31.0387822Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:28:31.0388229Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-28T21:28:31.0388718Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-28T21:28:31.0389198Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-28T21:28:31.0389745Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-28T21:28:31.0390262Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)2020-09-28T21:28:31.0390732Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-28T21:28:31.0391179Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:28:31.0391582Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:28:31.0391964Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-28T21:28:31.0392382Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-28T21:28:31.0393053Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-28T21:28:31.0393617Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-28T21:28:31.0393997Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-28T21:28:31.0394407Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-28T21:28:31.0394817Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-28T21:28:31.0395211Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-28T21:28:31.0395608Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-28T21:28:31.0396041Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)2020-09-28T21:28:31.0396517Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)2020-09-28T21:28:31.0397026Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-28T21:28:31.0397512Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)2020-09-28T21:28:31.0398245Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)2020-09-28T21:28:31.0398778Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)2020-09-28T21:28:31.0399251Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V2020-09-28T21:28:31.0400340Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)2020-09-28T21:28:31.0400756Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)2020-09-28T21:28:31.0401304Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)2020-09-28T21:28:31.0401869Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)2020-09-28T21:28:31.0402307Z ... 50 more2020-09-28T21:28:31.0402624Z 2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.4448287Z 2020-09-28T21:28:31.4448950Z Results :2020-09-28T21:28:31.4449082Z 2020-09-28T21:28:31.4449270Z Tests in error: 2020-09-28T21:28:31.4450556Z HBaseDynamicTableFactoryTest.testTableSourceFactory:104-&gt;createTableSource:332 Â» Validation2020-09-28T21:28:31.4451232Z HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...2020-09-28T21:28:31.4451851Z HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19844" opendate="2020-10-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Python UDAF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="20165" opendate="2020-11-16 00:00:00" fixdate="2020-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionFIFOITCase.checkForProhibitedLogContents: Error occurred during initialization of boot layer java.lang.IllegalStateException: Module system already initialized</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9597&amp;view=logs&amp;j=298e20ef-7951-5965-0e79-ea664ddc435e&amp;t=8560c56f-9ec1-5c40-4ff5-9d3eaaaa882d2020-11-15T22:42:03.3053212Z 22:42:03,303 [ Time-limited test] INFO org.apache.flink.yarn.YARNSessionFIFOITCase [] - Finished testDetachedMode()2020-11-15T22:42:37.9020133Z [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 2, Time elapsed: 67.485 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOSecuredITCase2020-11-15T22:42:37.9022015Z [ERROR] testDetachedMode(org.apache.flink.yarn.YARNSessionFIFOSecuredITCase) Time elapsed: 12.841 s &lt;&lt;&lt; FAILURE!2020-11-15T22:42:37.9023701Z java.lang.AssertionError: 2020-11-15T22:42:37.9025649Z Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-1_0/application_1605480087188_0002/container_1605480087188_0002_01_000002/taskmanager.out with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:2020-11-15T22:42:37.9026730Z [2020-11-15T22:42:37.9027080Z Error occurred during initialization of boot layer2020-11-15T22:42:37.9027623Z java.lang.IllegalStateException: Module system already initialized2020-11-15T22:42:37.9033278Z java.lang.IllegalStateException: Module system already initialized2020-11-15T22:42:37.9033825Z ]2020-11-15T22:42:37.9034291Z at org.junit.Assert.fail(Assert.java:88)2020-11-15T22:42:37.9034971Z at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:479)2020-11-15T22:42:37.9035814Z at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:83)</description>
      <version>1.11.3,1.13.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="20310" opendate="2020-11-24 00:00:00" fixdate="2020-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Debezium, Canal, Raw support for Filesystem connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="20423" opendate="2020-11-30 00:00:00" fixdate="2020-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug id="2044" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implementation of Gelly HITS Algorithm</summary>
      <description>Implementation of Hits Algorithm in Gelly API using Java. the feature branch can be found here: (https://github.com/JavidMayar/flink/commits/HITS)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="20467" opendate="2020-12-3 00:00:00" fixdate="2020-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Example in Python DataStream Doc</summary>
      <description>Currently the example of MapFunction can't work. We need to fix it.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.md</file>
    </fixedFiles>
  </bug>
  <bug id="2050" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pipelining mechanism for chainable transformers and estimators</summary>
      <description>The key concept of an easy to use ML library is the quick and simple construction of data analysis pipelines. Scikit-learn's approach to define transformers and estimators seems to be a really good solution to this problem. I propose to follow a similar path, because it makes FlinkML flexible in terms of code reuse as well as easy for people coming from Scikit-learn to use the FlinkML.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery10ITCase.java</file>
      <file type="M">flink-tests.src.test.assembly.test-streamingclassloader-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-kmeans-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-custominput-assembly.xml</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.examples.TPCHQuery3.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.preprocessing.StandardScalerITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.experimental.SciKitPipelineSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.CoCoASuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs.libs.ml.als.md</file>
      <file type="M">docs.libs.ml.cocoa.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.polynomial.base.feature.mapper.md</file>
      <file type="M">docs.libs.ml.standard.scaler.md</file>
      <file type="M">flink-clients.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery3.java</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AvgAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.package-info.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.Job.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.Job.scala</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.LocalJob.java</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.YarnJob.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JarFileCreatorTest.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-staging.flink-avro.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-language-binding.flink-python.src.main.java.org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.java</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedPredictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.KMeans.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Offset.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Scaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.CanCopy.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20550" opendate="2020-12-9 00:00:00" fixdate="2020-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong savepoint config in some docs</summary>
      <description>Fix config 'state.savepoint.dir' into 'state.savepoints.dir' in docs</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20567" opendate="2020-12-10 00:00:00" fixdate="2020-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Error in UDTF section</summary>
      <description>itemContentDocumentLinkpartInner Join with Table Function (UDTF)originTableFunction&lt;String&gt; split = new MySplitUDTF();change toTableFunction&lt;Tuple3&lt;String,String,String&gt;&gt; split = new MySplitUDTF();I have run the following the codes successfully that contain all the contents from the above.①InnerJoinwithTableFunction.java②MySplitUDTF.javaReason:In this part, it says:joinLateral(call("split", $("c")).as("s", "t", "v"))it means:the udtf has 1 input "c",and 3 outputs "s", "t", "v"So:these outputs should have 3 types.such as TableFunction&lt;Tuple3&lt;String,String,String&gt;&gt;instead of only TableFunction&lt;String&gt; split</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="20588" opendate="2020-12-14 00:00:00" fixdate="2020-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docker-compose as appendix to Mesos documentation</summary>
      <description>Dockerfile and docker-compose.yml can be added to the Mesos documentation as appendix to provide an easy entry point for starting to work with Mesos/Marathon.</description>
      <version>1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug id="20613" opendate="2020-12-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update TableResult.collect()/TableResult.print() to the new type system</summary>
      <description>Currently, TableResult.collect()/TableResult.print() use old sink interfaces and old type system. Those methods are very important in the API and should be updated.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.SelectTableSinkSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.SelectTableSinkBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.ExternalCatalogTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SelectSinkOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.SelectResultProvider.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformations.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformation.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.transforms.LegacyDecimalTypeTransformation.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ExternalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ExternalSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
    </fixedFiles>
  </bug>
  <bug id="20615" opendate="2020-12-16 00:00:00" fixdate="2020-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local recovery and sticky scheduling end-to-end test timeout with "IOException: Stream Closed"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=0b23652f-b18b-5b6e-6eb6-a11070364610It tried to restart many times, and the final error was following:2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 ... 16 more </description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="2067" opendate="2015-5-21 00:00:00" fixdate="2015-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained streaming operators should not throw chained exceptions</summary>
      <description>The exceptions that come from chained operators have an non-intuitive chaining structure, that makes the stack traces harder to understand.For every chained task, there is a "Failed to forward record" exception, before the actual exception comes as a cause.In the Batch API, we use a special "ExceptionInChainedStubException" that is recognized and un-nested to make chained operator exceptions surface as root exceptions. We should do the same for the streaming API.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug id="20680" opendate="2020-12-19 00:00:00" fixdate="2020-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fails to call var-arg function with no parameters</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20694" opendate="2020-12-21 00:00:00" fixdate="2020-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Declarative resource management can get stuck in a loop</summary>
      <description>We've seen a few tests where the coordination layer gets stuck in a request slot -&gt; offer slot -&gt; reject slot -&gt; request slot loop.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="20695" opendate="2020-12-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper node under leader and leaderlatch is not deleted after job finished</summary>
      <description>I used flink 1.11 in standalone cluster mode for batch job. The enviornment was configured as zookeeper HA mode.After job was commited, flink runtime created nodes under /flink/default/leader and /flink/default/leaderlatch with job id.  Though jobs were finished, these nodes  were remaining in zookeeper path forever. After a period of running, more and more jobs had been executed and there were a greate number of nodes under /flink/default/leader and slowed down the performance of zookeeper. Why not delete the nodes after job finished? Flink runtime could get job status by listeners and delete the leader nodes for job immidiately.</description>
      <version>1.9.0,1.11.3,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.1,1.12.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingManualHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.AbstractNonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="20703" opendate="2020-12-22 00:00:00" fixdate="2020-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveSinkCompactionITCase test timeout</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11096&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=62110053-334f-5295-a0ab-80dd7e2babbfhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11136&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=62110053-334f-5295-a0ab-80dd7e2babbfThe phenomenon is: All failure are related to the hive tests. All tests timeout when fail.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20731" opendate="2020-12-22 00:00:00" fixdate="2020-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Source</summary>
      <description>This is our implementation based on FLIP-27.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="20750" opendate="2020-12-23 00:00:00" fixdate="2020-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port stream python group aggregate nodes to Java</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20751" opendate="2020-12-23 00:00:00" fixdate="2020-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port batch python group aggregate nodes to Java</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="20779" opendate="2020-12-28 00:00:00" fixdate="2020-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for row-based operation in Python Table API</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="20783" opendate="2020-12-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of BatchExec nodes for Join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.SortSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20790" opendate="2020-12-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generated classes should not be put under src/ directory</summary>
      <description>The flink-parquet and flink-confluent-schema-registry modules generate classes and put them into the src/ directory.Standard convention is to put generated sources under target/generated-sources, which has the main advantage that they are deleted when doing a mvn clean.</description>
      <version>1.13.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-state-evolution-test.src.main.java.org.apache.flink.avro.generated.Address.java</file>
      <file type="M">flink-end-to-end-tests.flink-state-evolution-test.src.main.avro.Address.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-state-evolution-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.avro.ComplexPayloadAvro.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.avro.user.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20803" opendate="2020-12-29 00:00:00" fixdate="2020-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Version mismatch between spotless-maven-plugin and google-java-format plugin</summary>
      <description>The spotless-maven-plugin uses version 1.7 of the google-java-format, while the IntelliJ google-java-format plugin uses 1.9, resulting in inconsistent formatting.We cannot bump the version used by the spotless plugin because it requires java 11, so instead we have to downgrade the intellij plugin to 1.7.0.5 .</description>
      <version>1.11.4,1.12.1,1.13.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="20807" opendate="2020-12-29 00:00:00" fixdate="2020-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup checkstyle suppressions files</summary>
      <description>With spotless formatting having landed this is a good opportunity to remove or narrow down various checkstyle suppressions.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-runtime.xml</file>
      <file type="M">tools.maven.suppressions-optimizer.xml</file>
      <file type="M">tools.maven.suppressions-core.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20868" opendate="2021-1-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pause the idle/back pressure timers during processing mailbox actions</summary>
      <description>FLINK-20717 introduced a bug, where any time spent on processing mails, when task is idle or back pressured, will be accounted to idle or back pressured time instead of the busy time.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxDefaultAction.java</file>
    </fixedFiles>
  </bug>
  <bug id="20921" opendate="2021-1-11 00:00:00" fixdate="2021-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Date/Time/Timestamp in Python DataStream</summary>
      <description>Currently the Date/Time/Timestamp type doesn't works in Python DataStream.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="20933" opendate="2021-1-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config Python Operator Use Managed Memory In Python DataStream</summary>
      <description>Now the way to set `Python DataStream Operator` to use managed memory is to set a hook in the `execute` method of `Python StreamExecutionEnvironment` to traverse the `StreamGraph` and set the `Python Operator` to use managed memory.But when the user’s job uses `from_data_stream` to convert the `DataStream` to a `Table`, the `TableEnvironment.execute` method is used at the end rather than `StreamExecutionEnvironment.execute`, so the `Python DataStream` related operators will not have `Managed Memory` set.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="20940" opendate="2021-1-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LOCALTIME/LOCALTIMSTAMP functions should use session time zone</summary>
      <description>LOCALTIMELOCALTIME TIME(0) NOT NULL #session timezone: UTC 08:52:52 #session timezone: UTC+8 08:52:52wall clock: UTC+8:2020-12-29 08:52:52|LOCALTIMESTAMPLOCALTIMESTAMP TIMESTAMP(0) NOT NULL #session timezone: UTC 2020-12-29T08:52:52 #session timezone: UTC + 8 LOCALTIMESTAMP TIMESTAMP(0) NOT NULL 2020-12-29T08:52:52wall clock: UTC+8:2020-12-29 08:52:52|</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20942" opendate="2021-1-12 00:00:00" fixdate="2021-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Digest of FLOAT literals throws UnsupportedOperationException</summary>
      <description>The recent refactoring of Calcite's digests might have caused a regression for FLOAT literals. org.apache.calcite.rex.RexLiteral#appendAsJava throws a UnsupportedOperationException for the following query:def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val source = env.fromElements( (1.0f, 11.0f, 12.0f), (2.0f, 21.0f, 22.0f), (3.0f, 31.0f, 32.0f), (4.0f, 41.0f, 42.0f), (5.0f, 51.0f, 52.0f) ) val settings = EnvironmentSettings.newInstance() .inStreamingMode() .useBlinkPlanner() .build() val tEnv = StreamTableEnvironment.create(env, settings) tEnv.createTemporaryView("myTable", source, $("id"), $("f1"), $("f2")) val query = """ |select * from myTable where id in (1.0, 2.0, 3.0) |""".stripMargin tEnv.executeSql(query).print()}Stack trace:Exception in thread "main" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.type.SqlTypeName: FLOAT at org.apache.calcite.util.Util.needToImplement(Util.java:1075) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:703) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexLiteral.toLiteral(RexLiteral.java:737) at org.apache.calcite.rex.RexLiteral.lambda$printSarg$4(RexLiteral.java:710) at org.apache.calcite.util.RangeSets$Printer.singleton(RangeSets.java:397) at org.apache.calcite.util.RangeSets.forEach(RangeSets.java:237) at org.apache.calcite.util.Sarg.lambda$printTo$0(Sarg.java:110) at org.apache.calcite.linq4j.Ord.forEach(Ord.java:157) at org.apache.calcite.util.Sarg.printTo(Sarg.java:106) at org.apache.calcite.rex.RexLiteral.printSarg(RexLiteral.java:709) at org.apache.calcite.rex.RexLiteral.lambda$appendAsJava$1(RexLiteral.java:652) at org.apache.calcite.util.Util.asStringBuilder(Util.java:2502) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:651) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:971) at org.apache.calcite.rex.RexBuilder.makeSearchArgumentLiteral(RexBuilder.java:1066) at org.apache.calcite.rex.RexSimplify$SargCollector.fix(RexSimplify.java:2786) at org.apache.calcite.rex.RexSimplify.lambda$simplifyOrs$6(RexSimplify.java:1843) at java.util.ArrayList.forEach(ArrayList.java:1257) at org.apache.calcite.rex.RexSimplify.simplifyOrs(RexSimplify.java:1843) at org.apache.calcite.rex.RexSimplify.simplifyOr(RexSimplify.java:1817) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:313) at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:282) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:257) at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:213) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:707) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1107) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:666) at org.apache.flink.table.examples.scala.basics.WordCountTable$.main(WordCountTable.scala:59)</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20946" opendate="2021-1-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Optimize Python ValueState Implementation In PyFlink</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="20954" opendate="2021-1-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize State with Cross Bundle State Cache In PyFlink</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="20964" opendate="2021-1-13 00:00:00" fixdate="2021-3-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce PythonStreamGroupWindowAggregateOperator</summary>
      <description>Adds PythonStreamGroupWindowAggregateOperator to support running General Python Stream Group Window Aggregate Function</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  <bug id="20968" opendate="2021-1-14 00:00:00" fixdate="2021-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy exec nodes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodePlanDumper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMiniBatchAssigner.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DeadlockBreakupProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputPriorityGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.LegacyBatchExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.LegacyExecNodeBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.LegacyStreamExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.FlinkPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExpand.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecValues.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDropUpdateBefore.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="20975" opendate="2021-1-14 00:00:00" fixdate="2021-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTableSourceITCase.testPartitionFilter fails on AZP</summary>
      <description>The test HiveTableSourceITCase.testPartitionFilter fails on AZP with the following exception:java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertFalse(Assert.java:64) at org.junit.Assert.assertFalse(Assert.java:74) at org.apache.flink.connectors.hive.HiveTableSourceITCase.testPartitionFilter(HiveTableSourceITCase.java:278) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21005" opendate="2021-1-18 00:00:00" fixdate="2021-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce new provider for unified Sink API and implement in planner</summary>
      <description>FLIP-143 &amp;#91;1&amp;#93; introduced the unified sink API, we should add a SinkRuntimeProvider for it and support it in planner. So that Table SQL users can also use the unified sink APIs. &amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestFileSourceFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.sink.DataStreamSinkProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="21026" opendate="2021-1-19 00:00:00" fixdate="2021-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align column list specification with Hive in INSERT statement</summary>
      <description>HIVE-9481 allows column list specification in INSERT statement. The syntax is:INSERT INTO TABLE table_name [PARTITION (partcol1[=val1], partcol2[=val2] ...)] [(column list)] select_statement FROM from_statementIn the MeanWhile, flink introduces PARTITION syntax that the PARTITION clause appears after the COLUMN LIST clause. It looks weird and luckily we don't support COLUMN LIST clause now.  I think it'a good chance to align this with Hive now.     </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="21028" opendate="2021-1-19 00:00:00" fixdate="2021-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming application didn&amp;#39;t stop properly</summary>
      <description>I have a Flink job running on YARN with a disjoint graph, i.e. a single job contains two independent and isolated pipelines.From time to time, I stop the job with a savepoint like so:flink stop -p ${SAVEPOINT_BASEDIR}/${FLINK_JOB_NAME}/SAVEPOINTS --yarnapplicationId=${FLINK_YARN_APPID} ${ID}A few days ago, this job suddenly didn't stop properly as usual but ran into a possible race condition.On the CLI with stop, I received a simple timeout:org.apache.flink.util.FlinkException: Could not stop with a savepoint job "f23290bf5fb0ecd49a4455e4a65f2eb6". at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:495) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:864) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:487) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:931) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:493) ... 9 more The root of the problem however is that on a taskmanager, I received an exception in shutdown, which lead to restarting (a part) of the pipeline and put it back to running state, thus the console command for stopping timed out (as the job was (partially) back in running state). the exception which looks like a race condition for me in the logs is:2021-01-12T06:15:15.827877+01:00 WARN org.apache.flink.runtime.taskmanager.Task Source: rawdata_source1 -&gt; validation_source1 -&gt; enrich_source1 -&gt; map_json_source1 -&gt; Sink: write_to_kafka_source1) (3/18) (bc68320cf69dd877782417a3298499d6) switched from RUNNING to FAILED.java.util.concurrent.ExecutionException: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302) at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:576) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:544) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:642) at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41) at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator$WatermarkEmitter.emitWatermark(TimestampsAndWatermarksOperator.java:165) at org.apache.flink.streaming.runtime.operators.util.AssignerWithPeriodicWatermarksAdapter.onPeriodicEmit(AssignerWithPeriodicWatermarksAdapter.java:54) at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.close(TimestampsAndWatermarksOperator.java:125) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:205) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:203) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:177) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:90) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155) ... 13 moreCaused by: java.lang.RuntimeException at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:123) at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:41) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:570) at org.apache.flink.streaming.api.operators.ProcessOperator.processWatermark(ProcessOperator.java:72) at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.emitWatermark(OperatorChain.java:638) ... 25 moreCaused by: java.lang.IllegalStateException at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179) at org.apache.flink.runtime.io.network.buffer.BufferBuilder.append(BufferBuilder.java:83) at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.copyToBufferBuilder(SpanningRecordSerializer.java:90) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:136) at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:80) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.emitWatermark(RecordWriterOutput.java:121) ... 29 moreI already raised a question regarding this bug on the user mailing list with the conclusion to just open a ticket here. Original on user mailing list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Bugs-in-Streaming-job-stopping-Weird-graceful-stop-restart-for-disjoint-job-graph-td40610.htmledit:In Flink 1.12.x this bug can probably lead to corrupted data stream and all kinds of deserialisation errors on the downstream task.Also the same bug can leave any code (regardless if it's Flink's network stack, user code, or some 3rd party library) that sleeps interruptible in an invalid state.</description>
      <version>1.11.2,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21041" opendate="2021-1-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ExecNodeGraph to wrap the ExecNode topology</summary>
      <description>Currently, we use List&lt;ExecNode&lt;?&gt; to represent the ExecNode topology, as we will introduce more features (such as serialize/deserialize {{ExecNode}}s), It's better we can introduce an unified class to represent the topology.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodePlanDumper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DeadlockBreakupProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DAGProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecGraphGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21047" opendate="2021-1-20 00:00:00" fixdate="2021-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the correct registered/free resources information in SlotManager</summary>
      <description>In FLINK-16640, we extend ResourceOverview and TaskManager(Details)Info for registered/free resources. However, the implementation is based on the assumption that all the registered task executors are homogeneous with the default resource spec. This assumption will be broken in standalone mode when user manually starts heterogeneous task executors.We need to calculate the registered/free resources according to the exact defaultSlotResourceProfile and totalResourceProfile of TaskExecutorRegistrations.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21048" opendate="2021-1-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor documentation related to switch memory allocator</summary>
      <description>Since we decide to change the switch of memory allocator from command to environment variable in FLINK-21034, we should also change related documentation.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="2106" opendate="2015-5-28 00:00:00" fixdate="2015-9-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add outer joins to Runtime</summary>
      <description>Add left/right/full outer join methods to the runtime of Flink.Initially, the execution strategy should be a sort-merge outer join (FLINK-2105) but can later be extended to hash joins for left/right outer joins.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.AbstractSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.ReusingMergeOuterJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NonReusingMergeOuterJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.AbstractMergeOuterJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.AbstractMergeIterator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.iterations.ConnectedComponentsCoGroupTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.examples.RelationalQueryCompilerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.java.JoinTranslationTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.SortMergeJoinDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.costs.CostEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21067" opendate="2021-1-21 00:00:00" fixdate="2021-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the logic of computing tasks to trigger/wait/commit to consider finished tasks</summary>
      <description>To support checkpoint after tasks finished, for each checkpoint we would like to trigger the new "root" tasks, and wait / commit for all the running tasks. Thus we would need to modify the logic of identifying the tasks to trigger / wait / commit</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ExecutionAttemptMappingProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointPlanCalculator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointPlan.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21076" opendate="2021-1-21 00:00:00" fixdate="2021-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the declarative scheduler and its limitations</summary>
      <description>We need to document how to use the declarative scheduler and what its limitations are. Ideally, we already create tickets for the limitations we want to fix in the foreseeable future so that we can link them.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="21078" opendate="2021-1-21 00:00:00" fixdate="2021-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DeclarativeSlotPoolService</summary>
      <description>In order to leverage FLINK-21077, we need to implement a DeclarativeSlotPoolService which encapsulates the DeclarativeSlotPool.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeResourceDeclarationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21080" opendate="2021-1-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Handle UnionListState with finished operators</summary>
      <description>Most legacy source operators would record the offset for each partitions, and after recovery it would read from the recorded offset. If before a checkpoint some subtasks are finished, the corresponding partition offsets would be deserted in the checkpoint. Then if the job recover with this checkpoint, the legacy source would re-discovery all the partitions and for those finished tasks, the legacy source would re-read them since their offsets are not recorded. Therefore, we would like to fail the checkpoint if some legacy source operators have part of subtasks finished. </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnableTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointPlan.java</file>
    </fixedFiles>
  </bug>
  <bug id="21088" opendate="2021-1-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>CheckpointCoordinator pass the flag about whether a operator is fully finished on recovery</summary>
      <description>For recovery using checkpoints after tasks finished, the flags about whether an operator is fully finished is required for skip the execution of fully finished operators. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskExecutionDecorationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21089" opendate="2021-1-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Skip the execution of the fully finished operators after recovery</summary>
      <description>After recovery we should skip the execution of fully finished operators to keep consistent: For sources (including legacy source and new source), they should directly emit EndOfPartition. For both source and non-source operators, their lifecycle methods like open/endOfInput/close should be skipped.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21100" opendate="2021-1-22 00:00:00" fixdate="2021-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DeclarativeScheduler skeleton</summary>
      <description>Add the DeclarativeScheduler skeleton as proposed in FLIP-160. The skeleton will contain the basic state machine and rudimentary functionality of the scheduler.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNGFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNGFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.declarative.allocator.SharedSlotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.SchedulerNGFactoryFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.declarative.allocator.SharedSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.declarative.allocator.JobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.SchedulerNGFactoryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="21101" opendate="2021-1-22 00:00:00" fixdate="2021-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set up cron job to run CI with declarative scheduler</summary>
      <description>Once the declarative scheduler has been merged, we should create a Cron job to run all CI profiles with this scheduler in order to find all remaining test failures.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ProcessingTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.LocalRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeAllWindowCheckpointingITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkITBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21102" opendate="2021-1-22 00:00:00" fixdate="2021-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ScaleUpController</summary>
      <description>Add the ScaleUpController according to the definition in FLIP-160.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs..includes.generated.expert.scheduling.section.html</file>
      <file type="M">docs..includes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="21103" opendate="2021-1-22 00:00:00" fixdate="2021-3-22 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>E2e tests time out on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12377&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529Creating worker2 ... doneJan 22 13:16:17 Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...Jan 22 13:16:22 Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...Jan 22 13:16:27 Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...Jan 22 13:16:32 Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...Jan 22 13:16:37 Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...Jan 22 13:16:43 Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...Jan 22 13:16:48 Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...Jan 22 13:16:53 Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...Jan 22 13:16:58 Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...Jan 22 13:17:03 Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...Jan 22 13:17:08 We only have 0 NodeManagers up. We have been trying for 0 seconds, retrying ...21/01/22 13:17:10 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.19.0.3:803221/01/22 13:17:11 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.19.0.3:10200Jan 22 13:17:11 We now have 2 NodeManagers up.============================================================================================= WARNING: This E2E Run took already 80% of the allocated time budget of 250 minutes ====================================================================================================================================================================================================== WARNING: This E2E Run will time out in the next few minutes. Starting to upload the log output =========================================================================================================##[error]The task has timed out.Async Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.0' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactAsync Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.1' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactAsync Command Start: Upload ArtifactUploading 1 filesFile upload succeed.Upload '/tmp/_e2e_watchdog.output.2' to file container: '#/11824779/e2e-timeout-logs'Associated artifact 140921 with build 12377Async Command End: Upload ArtifactFinishing: Run e2e tests</description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21106" opendate="2021-1-23 00:00:00" fixdate="2021-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>google-java-format Intellij Plugin 1.7.0.5 causes UnsupportedOperationException in IntelliJ</summary>
      <description>There's a problem with google-java-format Intellij plugin version 1.7.0.5 that causes an UnsupportedOperationException when creating a new Java class file. Besides the exception, an error dialog pops up and the newly created file is not properly formatted. A simple reformat solves the issue.This problem is caused by a bug that got fixed in the google-java-format plugin's codebase in 45fb41a.Unfortunately, this fix got released with the plugin version 1.8.0.1 which we cannot upgrade to due to our limitations on sticking to Java 8 for now (see FLINK-20803).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.zh.docs.flinkDev.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="21127" opendate="2021-1-25 00:00:00" fixdate="2021-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Stores finished status for fully finished operators</summary>
      <description>Once an operator is fully finished in one checkpoint, we need to persist the information inside the checkpoint so that after recovery, we could skip the execution for these operators.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointMetadataTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2Serializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21128" opendate="2021-1-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rethink how Kubernetes interfaces with our docker image</summary>
      <description>Our native Kubernetes integration assembles a java command that is passed to the docker-script, which first does some magical stuff to create the classpath# Set the Flink related environmentsexport _FLINK_HOME_DETERMINED=true. $FLINK_HOME/bin/config.shexport FLINK_CLASSPATH="`constructFlinkClassPath`:$INTERNAL_HADOOP_CLASSPATHS"and then evaluates the given java command.This magical stuff is a bit problematic for a few reasons:1) it is only required for Kubernetes to work; it should not be executed in any other case2) it appears to be required for running anything in Flink that is not part of the docker-entrypoint API (e.g., jobmanager) when it isn't.3) it unnecessarily spreads kubernetes-specific logic across the flink and flink-docker repositories.A simple change would be to introduce dedicated Kubernetes scripts into the distribution, which for starters will just import config.sh, export FLINK_CLASSPATH and evaluate the given command.In the long-run it would be good if we would not create a java command but instead relied on the existing scripts which do exactly that already.As is stands we're duplicating code across the distribution and the Kubernetes module, which already caused troubles when we did the log4j2 migration.Ideally we find a way to reuse the existing code in the Kubernetes module for generating these commands in the existing scripts, e.g., by moving them to the BashJavaUtils.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesTaskManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesFactoryWithPodTemplateTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21131" opendate="2021-1-25 00:00:00" fixdate="2021-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show alignment timeout in checkpoint configuration (web UI)</summary>
      <description> </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="21149" opendate="2021-1-26 00:00:00" fixdate="2021-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated CatalogTable.getProperties</summary>
      <description>CatalogTable.getProperties has been deprecated in 1.11. It is time to remove it, to reduce confusion and potential bugs by calling the wrong method.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.OperationMatchers.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.JavaCatalogTableTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogBaseTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTablePropertiesOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogViewImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableProperties.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveTable.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactoryBase.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="21168" opendate="2021-1-28 00:00:00" fixdate="2021-3-28 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add support for general python group window aggregate function in Physical Rule and Node</summary>
      <description>Add support for general python group window aggregate function in Physical Rule and Node</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.PythonGroupWindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.PythonGroupWindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalPythonGroupWindowAggregateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21170" opendate="2021-1-28 00:00:00" fixdate="2021-3-28 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add internal state hierarchy in PyFlink</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="21185" opendate="2021-1-28 00:00:00" fixdate="2021-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new interface for catalog to listen on temporary object operations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21188" opendate="2021-1-28 00:00:00" fixdate="2021-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce wrapper class to hold information besides ArchivedExecutionGraph</summary>
      <description>Instead of returning only the ArchivedExecutionGraph, the plan is to introduce a wrapper class ExecutionGraphInfo that would hold the exception history besides the execution graph.This change might also be necessary as we could support returning a collection of ArchivedExecutionGraphs as part of the work on the declarative scheduler (FLIP-160).</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingExecutionGraphCache.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.NoOpExecutionGraphCache.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingDispatcher.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationClusterEntryPoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.ArchivedExecutionGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherJob.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherJobResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.HistoryServerArchivist.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JsonResponseHistoryServerArchivist.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MemoryArchivedExecutionGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.PartialDispatcherServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.PartialDispatcherServicesWithJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.VoidHistoryServerArchivist.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractJobVertexHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ExecutionGraphCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.history.JsonArchivist.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherJobTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21189" opendate="2021-1-28 00:00:00" fixdate="2021-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend exception history to collect concurrent task failure that have a common root cause</summary>
      <description>We want to collect task failures that were caused by the same root cause.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.ExceptionHistoryEntryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExceptionHistoryEntry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.exceptionhistory.ExceptionHistoryEntryMatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Failing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="21192" opendate="2021-1-28 00:00:00" fixdate="2021-3-28 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support setting namespace in RemoteKeyedStateBackend</summary>
      <description>Currently, RemoteKeyedStateBackend only support VoidNamespace. We need to add the support for setting namespace so that we can support window state.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="21213" opendate="2021-1-31 00:00:00" fixdate="2021-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>e2e test fail with &amp;#39;As task is already not running, no longer decline checkpoint&amp;#39;</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12686&amp;view=logs&amp;j=68a897ab-3047-5660-245a-cce8f83859f6&amp;t=d47e27f5-9721-5d5f-1cf3-62adbf3d115d Checking for non-empty .out files... No non-empty .out files.  </description>
      <version>1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
    </fixedFiles>
  </bug>
  <bug id="21215" opendate="2021-1-31 00:00:00" fixdate="2021-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint was declined because one input stream is finished</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12691&amp;view=logs&amp;j=34f41360-6c0d-54d3-11a1-0292a2def1d9&amp;t=2d56e022-1ace-542f-bf1a-b37dd63243f2&amp;l=9146  &amp;#91;ERROR&amp;#93; Errors: &amp;#91;ERROR&amp;#93; UnalignedCheckpointITCase.execute&amp;#91;parallel pipeline with remote channels, p = 5&amp;#93; » JobExecution   ... 4 more Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold. at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:98) at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:84) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1930) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1007) at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$declineCheckpoint$9(SchedulerBase.java:1009) at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$processCheckpointCoordinatorMessage$10(SchedulerBase.java:1025) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)          </description>
      <version>1.11.4,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedControllerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnableTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
    </fixedFiles>
  </bug>
  <bug id="21216" opendate="2021-1-31 00:00:00" fixdate="2021-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamPandasConversionTests Fails</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12699&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3 =================================== FAILURES =================================== _______________ StreamPandasConversionTests.test_empty_to_pandas _______________  self = &lt;pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_empty_to_pandas&gt;   def test_empty_to_pandas(self): &gt; table = self.t_env.from_pandas(self.pdf, self.data_type)  pyflink/table/tests/test_pandas_conversion.py:144: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ pyflink/table/table_environment.py:1462: in from_pandas arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False) pyarrow/types.pxi:1315: in pyarrow.lib.Schema.from_pandas ??? .tox/py37-cython/lib/python3.7/site-packages/pyarrow/pandas_compat.py:519: in dataframe_to_types type_ = pa.lib.ndarray_to_arrow_type(values, type) pyarrow/array.pxi:53: in pyarrow.lib._ndarray_to_arrow_type ??? pyarrow/array.pxi:64: in pyarrow.lib._ndarray_to_type ??? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  &gt; ??? E pyarrow.lib.ArrowTypeError: Did not pass numpy.dtype object  pyarrow/error.pxi:108: ArrowTypeError _________________ StreamPandasConversionTests.test_from_pandas _________________  self = &lt;pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_from_pandas&gt;   def test_from_pandas(self): &gt; table = self.t_env.from_pandas(self.pdf, self.data_type, 5)  pyflink/table/tests/test_pandas_conversion.py:120: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</description>
      <version>1.11.4,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="21272" opendate="2021-2-4 00:00:00" fixdate="2021-2-4 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test&amp;#39; Fail</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12893&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529[FAIL] 'Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test' failed after 0 minutes and 37 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnableTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="21277" opendate="2021-2-4 00:00:00" fixdate="2021-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLClientSchemaRegistryITCase fails to download testcontainers/ryuk:0.3.0</summary>
      <description>Tests using testcontainers fail from time to time downloading required images. Most probably caused by: https://github.com/testcontainers/testcontainers-java/issues/3574We should upgrade testcontainers to 1.15.1https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12890&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12874&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529Feb 03 19:14:14 java.lang.RuntimeException: Could not build the flink-dist imageFeb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)Feb 03 19:14:14 at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.&lt;init&gt;(SQLClientSchemaRegistryITCase.java:88)Feb 03 19:14:14 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)Feb 03 19:14:14 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)Feb 03 19:14:14 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)Feb 03 19:14:14 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)Feb 03 19:14:14 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)Feb 03 19:14:14 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)Feb 03 19:14:14 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)Feb 03 19:14:14 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)Feb 03 19:14:14 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)Feb 03 19:14:14 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)Feb 03 19:14:14 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)Feb 03 19:14:14 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)Feb 03 19:14:14 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)Feb 03 19:14:14 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Feb 03 19:14:14 at java.lang.Thread.run(Thread.java:748)Feb 03 19:14:14 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.NotFoundException: Status 404: {"message":"No such image: testcontainers/ryuk:0.3.0"}Feb 03 19:14:14 Feb 03 19:14:14 at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)Feb 03 19:14:14 at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)Feb 03 19:14:14 at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:309)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:268)Feb 03 19:14:14 ... 20 moreFeb 03 19:14:14 Caused by: com.github.dockerjava.api.exception.NotFoundException: Status 404: {"message":"No such image: testcontainers/ryuk:0.3.0"}Feb 03 19:14:14 Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:241)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.post(DefaultInvocationBuilder.java:125)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:33)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.CreateContainerCmdExec.execute(CreateContainerCmdExec.java:13)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.exec.AbstrSyncDockerCmdExec.exec(AbstrSyncDockerCmdExec.java:21)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.command.AbstrDockerCmd.exec(AbstrDockerCmd.java:35)Feb 03 19:14:14 at org.testcontainers.shaded.com.github.dockerjava.core.command.CreateContainerCmdImpl.exec(CreateContainerCmdImpl.java:595)Feb 03 19:14:14 at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:91)Feb 03 19:14:14 at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:203)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.imageExists(FlinkContainer.java:316)Feb 03 19:14:14 at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:301)Feb 03 19:14:14 ... 21 more</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21294" opendate="2021-2-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support state access API for the map/flat_map operation of Python ConnectedStreams</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoFlatMapOperator.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="21297" opendate="2021-2-5 00:00:00" fixdate="2021-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;LOAD/UNLOAD MODULE&amp;#39; syntax</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21298" opendate="2021-2-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;USE MODULES&amp;#39; syntax</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21299" opendate="2021-2-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;SHOW [FULL] MODULES&amp;#39; syntax</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableResult.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2130" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ source does not fail when failing to retrieve elements</summary>
      <description>The RMQ source only logs when elements cannot be retrieved. Failures are not propagated.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="21300" opendate="2021-2-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update module documentation</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.use.md</file>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.docs.dev.table.modules.md</file>
      <file type="M">docs.content.docs.dev.python.table.table.environment.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.use.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.modules.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.table.environment.md</file>
    </fixedFiles>
  </bug>
  <bug id="21301" opendate="2021-2-5 00:00:00" fixdate="2021-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decouple window aggregate allow lateness with state ttl configuration</summary>
      <description>Currently, state retention time config will also effect state clean behavior of Window Aggregate, which is unexpected for most users.E.g for the following example,  User would set `MinIdleStateRetentionTime` to 1 Day to clean state in `deduplicate` . However, it will also effects clean behavior of window aggregate. For example, 2021-01-04 data would clean at 2021-01-06 instead of 2021-01-05. SELECT DATE_FORMAT(tumble_end(ROWTIME ,interval '1' DAY),'yyyy-MM-dd') as stat_time, count(1) first_phone_numFROM ( SELECT ROWTIME, user_id, row_number() over(partition by user_id, pdate order by ROWTIME ) as rn FROM source_kafka_biz_shuidi_sdb_crm_call_record ) cal where rn =1group by tumble(ROWTIME,interval '1' DAY);It's better to decouple window aggregate allow lateness with `MinIdleStateRetentionTime` .</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowEmitStrategy.scala</file>
    </fixedFiles>
  </bug>
  <bug id="21321" opendate="2021-2-8 00:00:00" fixdate="2021-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change RocksDB incremental checkpoint re-scaling to use deleteRange</summary>
      <description>In FLINK-8790, it was suggested to use RocksDB's deleteRange API to more efficiently clip the databases for the desired target group.During the PR for that ticket, #5582, the change did not end up using the deleteRange method as it was an experimental feature in RocksDB.At this point deleteRange is in a far less experimental state now but I believe is still formally "experimental". It is heavily by many others like CockroachDB and TiKV and they have teased out several bugs in complex interactions over the years.For certain re-scaling situations where restores trigger restoreWithScaling and the DB clipping logic, this would likely reduce an O&amp;#91;n&amp;#93; operation (N = state size/records) to O(1). For large state apps, this would potentially represent a non-trivial amount of time spent for re-scaling. In the case of my workplace, we have an operator with 100s of billions of records in state and re-scaling was taking a long time (&gt;&gt;30min, but it has been awhile since doing it).</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksIncrementalCheckpointRescalingBenchmarkTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21333" opendate="2021-2-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce stopping with savepoint state</summary>
      <description>The declarative scheduler is also affected by the problem described in FLINK-21030. We want to solve this problem by introducing a separate state when are taking a savepoint for stopping Flink.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.RestartingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.MockStateWithExecutionGraphContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.FailingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21338" opendate="2021-2-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relax test naming constraints</summary>
      <description>Issues like FLINK-21337 or FLINK-21031 show that accidents happen where tests were added with incorrect names, causing them to not be run on CI, potentially hiding regressions.rmetzger had the neat idea to relax the constrainst such that in the verify phase we just run everything that is not end on Test.java.The subtasks are tests that are currently not being run, and potentially broken.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.CheckForbiddenMethodsUsage.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21342" opendate="2021-2-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various classes extending TestBases cannot be instantiated</summary>
      <description>Various tests (mostly serializer tests) use a pattern where they have an inner class extending a test base. When the naming conventions are relaxed these are picked up as test classes, but they lack a public constructor.Putting aside how jank this approach is, we can add @Ignore as a workaround.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.LocalRecoveryITCase.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTestInstance.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TraversableSerializerTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.ScalaSpecialTypesSerializerTest.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.RowSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.LegacyRowSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.EitherSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.CompositeSerializerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21343" opendate="2021-2-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation with possible migration strategies</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="21344" opendate="2021-2-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support switching from/to rocks db with heap timers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.RocksSavepointStateBackendSwitchTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.BackendSwitchSpecs.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBFullSnapshotResources.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.iterator.RocksStatesPerKeyGroupMergeIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSnapshotRestoreWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapMetaInfoRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.StateConfigUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionInternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21348" opendate="2021-2-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for StateWithExecutionGraph</summary>
      <description>This ticket is about adding dedicated tests for the StateWithExecutionGraph class.This is a follow up from https://github.com/apache/flink/pull/14879#discussion_r573707768</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StopWithSavepointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.RestartingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.FailingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CancelingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.OperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21351" opendate="2021-2-10 00:00:00" fixdate="2021-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental checkpoint data would be lost once a non-stop savepoint completed</summary>
      <description>FLINK-10354 counted savepoint as retained checkpoint so that job could failover from latest position. I think this operation is reasonable, however, current implementation would let incremental checkpoint data lost immediately once a non-stop savepoint completed.Current general phase of incremental checkpoints: once a newer checkpoint completed, it would be added to checkpoint store. And if the size of completed checkpoints larger than max retained limit, it would subsume the oldest one. This lead to the reference of incremental data decrease one and data would be deleted once reference reached to zero. As we always ensure to register newer checkpoint and then unregister older checkpoint, current phase works fine as expected.However, if a non-stop savepoint (a median manual trigger savepoint) is completed, it would be also added into checkpoint store and just subsume previous added checkpoint (in default retain one checkpoint case), which would unregister older checkpoint without newer checkpoint registered, leading to data lost.Thanks for banmoy reporting this problem first.</description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="21353" opendate="2021-2-10 00:00:00" fixdate="2021-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FS-based StateChangelog implementation</summary>
      <description>Detailed design: https://docs.google.com/document/d/1vifa8cqZqirVr6Ke1A0FclLa2aNltOQb25GZon79btM/edit?usp=sharing </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.TestingStateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.BatchingStateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogOptions.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateMemoryStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateHashMapTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateFileStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateEmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21354" opendate="2021-2-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ChangelogStateBackend (proxy-everything)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.StateConfigUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.java</file>
      <file type="M">flink-state-backends.pom.xml</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotTransformerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="21355" opendate="2021-2-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Send changes to the state changelog (still proxy everything)</summary>
      <description>Subtasks: Changelog instantiation (including configuration) (FLINK-21804) Connecting Changelog with ProxyBackend Connecting Proxy-State objects with Changelog Serializing changes in ProxyState objects and sending changes to Changelog no metadata logging (FLINK-22808) Unit test coverage</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyGroupedPriorityQueue.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.AbstractChangelogState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21357" opendate="2021-2-10 00:00:00" fixdate="2021-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add periodic materialization</summary>
      <description>Including cleanup on shutdown.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimerException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsynchronousException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncExceptionHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.resources.log4j2.properties</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendLoadingTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateStateTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateMemoryStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateHashMapTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateFileStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateEmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.HashMapStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorageFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.layouts.shortcodes.generated.common.state.backends.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21366" opendate="2021-2-12 00:00:00" fixdate="2021-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector documentation should mentions Maxwell as CDC mechanism</summary>
      <description>The current Kafka connector changelog section of the documentation mentions Debezium and Canal CDC tools but not the recently added Maxwell format.This PR linked to this ticket edits the text to add it.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="21367" opendate="2021-2-12 00:00:00" fixdate="2021-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support objectReuse in JDBC sink</summary>
      <description>Currently, if object reuse is enabled then JDBC sink factories will throw an exception. This prevents some use cases including recent one with Stateful Functions &amp;#91;1&amp;#93;.To overcome this, JdbcBatchingOutputFormat can implement InputTypeConfigurable and use the obtained serializer to copy the record if object reuse is enabled.&amp;#91;1&amp;#93; http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Stateful-Functions-JDBC-Sink-Problems-td41265.html cc: igal, jark</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="21369" opendate="2021-2-12 00:00:00" fixdate="2021-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Checkpoint Storage</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.content.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.docs.learn-flink.fault.tolerance.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.custom.serialization.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.oss.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.azure.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
    </fixedFiles>
  </bug>
  <bug id="21376" opendate="2021-2-15 00:00:00" fixdate="2021-1-15 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Failed state might not provide failureCause</summary>
      <description>Task.executionState and Task.failureCause are not set atomically. This became an issue when implementing the exception history (FLINK-21187) where we relied on the invariant that a failureCause is present when the Task failed.Adding this check to Task.notifyFinalStage() will reveal the race condition.TaskExecutorSlotLifetimeTest becomes unstable when adding this invariant. The reason is that the test starts a task but does not wait for the task to be finished. The task finalization and the cancellation of the task triggered through stopping the TaskManager shutdown compete with each other and could cause the executionState to be set to FAILED while the failureCause still being null. This is then forwarded to Execution through Task.notifyFinalState.We should set failureCause while setting the executionState to failed to not miss any caught error.</description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureHandlingResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="21382" opendate="2021-2-16 00:00:00" fixdate="2021-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone K8s documentation does not explain usage of standby JobManagers</summary>
      <description>Our standalone K8s documentation mentions how to configure K8s HA services. It does not mention that this only works with a single JobManager. When using standby JobManagers, then the given deployment yamls won't work because the jobmanager.rpc.address is configured to be the jobmanager service.Changing the configuration to work is surprisingly difficult because of a lack of documentation. Moreover, it is quite difficult to pass in custom configuration values when using a ConfigMap for sharing Flink's flink-conf.yaml. The problem is that mounted ConfigMaps are not writable from a pod perspective. See this answer for how one could achieve it.I think we could improve our documentation to explain our users how to configure a standalone HA cluster with standby JobManagers.</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="21386" opendate="2021-2-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FromElements ignores returns()</summary>
      <description>StreamExEnv#fromElements eagerly serializes data with the automatically determined serializer. This can result in errors for example when generic avro records are used, because it will default to Kryo.Subsequent calls to returns() have no effect because the typeinformation is never forwarded to the function.Annoyingly the fact that it serializes data isn't logged anywhere. and there doesn't seem to be a way to change the serializer except by using fromCollection() instead.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.FromElementsFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21417" opendate="2021-2-19 00:00:00" fixdate="2021-2-19 01:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Separate type specific memory segments.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.UnsafeMemorySegment.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.CaseClassNormalizedKeySortingTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.HashTableRecordWidthCombinations.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.collections.binary.BytesHashMapTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.TestMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.SortUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.binary.BinarySegmentUtilsTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.BinaryStringDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.BinaryRowDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.BinaryArrayDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.ResettableExternalBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.FileChannelUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.collections.binary.AbstractBytesHashMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.RawValueDataSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.MapDataSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryRowDataSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMapSpillMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelWriterOutputView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelReaderInputView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedBlockChannelWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedBlockChannelReader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashBucketArea.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.formats.raw.RawFormatSerializationSchema.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.formats.raw.RawFormatDeserializationSchema.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.writer.BinaryRowWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.writer.BinaryArrayWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.writer.AbstractBinaryWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.binary.BinaryRowDataUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.NestedRowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.BinaryStringData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.BinaryRowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.BinaryRawValueData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.BinaryMapData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.binary.BinaryArrayData.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.TestAllocator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListSerializerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.SkipListKeyComparatorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteSkipListStateMapBasicOpTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.main.java.org.apache.flink.runtime.state.heap.SkipListUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.main.java.org.apache.flink.runtime.state.heap.SkipListKeySerializer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-heap-spillable.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteSkipListStateMap.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BloomFilterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BitSetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.MutableHashTableTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTablePerformanceComparison.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.CompactingHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemorySegmentSimpleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestSubpartitionProducer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestBufferFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputChannelTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageClientSideSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferConsumerWithPartialRecordLengthTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AbstractCollectingResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningWrapperTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HybridMemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.CrossSegmentTypeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.EndiannessAccessChecks.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.HybridOffHeapDirectMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.HybridOffHeapUnsafeMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.HybridOnHeapMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.MemorySegmentChecksTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.MemorySegmentFactoryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.MemorySegmentUndersizedTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.OperationsOnFreedSegmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HeapMemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.DirectMemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.OffHeapMemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPos.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.DataInputOutputSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.DirectMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.HeapMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.UnsafeMemorySegmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.NormalizableKeyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileBasedBufferIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferCompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferDecompressor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NetworkBufferAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.FileChannelBoundedData.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousBufferFileWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.BufferFileWriterFileSegmentReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.BufferFileWriterReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerAsyncTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.PagedViewsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21419" opendate="2021-2-19 00:00:00" fixdate="2021-3-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove GC cleaner mechanism for unsafe memory segments</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.UnsafeMemoryBudgetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.UnsafeMemoryBudget.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.JavaGcCleanerWrapperTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.JavaGcCleanerWrapper.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemoryUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HybridMemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="21452" opendate="2021-2-23 00:00:00" fixdate="2021-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-27 sources cannot reliably downscale</summary>
      <description>Sources currently store their registered readers into the snapshot. However, when downscaling, there are unmatched readers we violate a couple of invariants.The solution is to not store registered readers - they are re-registered anyways on restart.To keep it backward compatible, the best option is to always store an empty set of readers while writing the snapshot and discard any recovered readers from the snapshot.</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContextTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorSerdeUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="21453" opendate="2021-2-23 00:00:00" fixdate="2021-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BoundedOneInput.endInput is NOT called when doing stop with savepoint WITH drain</summary>
      <description>In FLINK-21132 we disable endInput calls when stopping with savepoint. However as discussed in FLINK-21133, stop with savepoint with drain (stop-with-savepoint --drain), should be calling endOfInput().</description>
      <version>1.11.4,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.RestoreStreamTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.StatefulOperatorChainedTaskTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskExecutionDecorationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceExternalCheckpointTriggerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointTypeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.CheckpointSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointSequenceValidator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.ValidatingCheckpointHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21458" opendate="2021-2-23 00:00:00" fixdate="2021-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add numRestarts metric</summary>
      <description>Reference: org.apache.flink.runtime.scheduler.SchedulerBase#registerJobMetrics</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21464" opendate="2021-2-23 00:00:00" fixdate="2021-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ADD JAR command in sql client</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21481" opendate="2021-2-24 00:00:00" fixdate="2021-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move git-commit-id-plugin execution to flink-runtime</summary>
      <description>The properties set by the git-commit-id-plugin are only accessed in flink-runtime (see EnvironmentInformation), so we should use the execution of this plugin into flink-runtime to save some build time.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.PseudoRandomValueSelector.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21488" opendate="2021-2-24 00:00:00" fixdate="2021-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jdbc XA sink - XID generation conflicts between jobs</summary>
      <description>I'm using Flink 1.13's JDBC XA sink to write data to oracle DB using exactly once semantics.I want to have two jobs doing this. One is working right now. When starting second one, I encountered errors:org.apache.flink.util.FlinkRuntimeException: unable to start XA transaction, xid: 201:0600000000000000:9b1d1b84e8ce79bb, error -3: resource manager error has occurred. &amp;#91;XAErr (-3): A resource manager error has occured in the transaction branch. ORA-2079 SQLErr (0)&amp;#93;Oracle description:ORA-02079: cannot join a committing distributed transaction    Cause: Once a transaction branch is prepared, no more new transaction branches are allowed to start, nor is the prepared transaction branch allowed to be joined.    Action: Check the application code as this is an XA protocol violation.I've looked at the implementation of XID generation and noticed following line:private transient byte[] gtridBuffer; // globalTransactionId = checkpoint id (long)My hypothesis is that second job generated xid that referred to global transaction id that the first job created. If I'm right, then I'd suppose fix would rely on embedding part of job id inside of gtridBuffer.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.xa.XidGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.xa.SemanticXidGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21489" opendate="2021-2-24 00:00:00" fixdate="2021-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hugo docs add two anchor links to headers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.static.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug id="21502" opendate="2021-2-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Reduce frequency of global re-allocate resources</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="21503" opendate="2021-2-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests time out on azure</summary>
      <description>Both test_ci legacy_slot_management and test_ci finegrained_resource_management time out on azurehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13731&amp;view=logs&amp;j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21505" opendate="2021-2-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Enforce common savepoint format at the operator level</summary>
      <description>Currently, we are relying on the fact that all keyed backends would use the same strategy for savepoints.We should be forcing them at the API level to ensure that all exiting and future state backends will creat savepoints in the same format.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SnapshotStrategyRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.SavepointStateBackendSwitchTestBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapSavepointStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.HeapPriorityQueuesManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointableKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBFullSnapshotResources.java</file>
    </fixedFiles>
  </bug>
  <bug id="21506" opendate="2021-2-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Option for setting mesos task user</summary>
      <description>Mesos tasks can be run with a specific Unix user, which can be passed through mesos API. When tasksmanagers are running on mesos, we want them to run as a specific user sometimes, thus there should be an option to specify such user.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21517" opendate="2021-2-26 00:00:00" fixdate="2021-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test harnesses are bypassing serialization stack for events</summary>
      <description>Since FLINK-19297 (https://github.com/apache/flink/pull/13447/files#diff-e5c3ecec28e8d4c2f7f62bc8a4c9ed88c30141d44a570059849a3b2120e4d2b50) we accidentally removed a test coverage for event (de)serialization from a lot of the unit tests, that were/are using test harnesses. For example because of that I almost broke 1.12.2 release, since `stop-with-savepoint --drain` was only tested using test harnesses (didn't have an ITCases and/or end to end test).</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordOrEventCollectingResultPartitionWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2152" opendate="2015-6-3 00:00:00" fixdate="2015-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide zipWithIndex utility in flink-contrib</summary>
      <description>We should provide a simple utility method for zipping elements in a data set with a dense index.its up for discussion whether we want it directly in the API or if we should provide it only as a utility from flink-contrib.I would put it in flink-contrib.See my answer on SO: http://stackoverflow.com/questions/30596556/zipwithindex-on-apache-flink</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="21521" opendate="2021-2-26 00:00:00" fixdate="2021-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pretty print K8s specifications</summary>
      <description>When turning on the DEBUG log level, then Flink's K8s integration logs the specifications of started K8s resources (e.g. JobManager/TaskManager pods). From a user's perspective this is pretty helpful because it tells him what Flink is launching. In order to better read the specification it would be even better if we could print the yaml with proper indentation (pretty print).cc fly_in_gis</description>
      <version>1.12.1,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21535" opendate="2021-3-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointITCase.execute failed with "OutOfMemoryError: Java heap space"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13866&amp;view=logs&amp;j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&amp;t=a99e99c7-21cd-5a1f-7274-585e62b72f562021-02-27T02:11:41.5659201Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-02-27T02:11:41.5659947Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)2021-02-27T02:11:41.5660794Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)2021-02-27T02:11:41.5661618Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2021-02-27T02:11:41.5662356Z at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)2021-02-27T02:11:41.5663104Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-02-27T02:11:41.5664016Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-02-27T02:11:41.5664817Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)2021-02-27T02:11:41.5665638Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-02-27T02:11:41.5666405Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-02-27T02:11:41.5667609Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-02-27T02:11:41.5668358Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-02-27T02:11:41.5669218Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)2021-02-27T02:11:41.5669928Z at akka.dispatch.OnComplete.internal(Future.scala:264)2021-02-27T02:11:41.5670540Z at akka.dispatch.OnComplete.internal(Future.scala:261)2021-02-27T02:11:41.5671268Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2021-02-27T02:11:41.5671881Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2021-02-27T02:11:41.5672512Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-02-27T02:11:41.5673219Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)2021-02-27T02:11:41.5674085Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2021-02-27T02:11:41.5674794Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2021-02-27T02:11:41.5675466Z at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)2021-02-27T02:11:41.5676181Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)2021-02-27T02:11:41.5676977Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)2021-02-27T02:11:41.5677717Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)2021-02-27T02:11:41.5678409Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)2021-02-27T02:11:41.5679071Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-02-27T02:11:41.5679776Z at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)2021-02-27T02:11:41.5680576Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)2021-02-27T02:11:41.5681383Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-02-27T02:11:41.5682167Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-02-27T02:11:41.5683040Z at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)2021-02-27T02:11:41.5683759Z at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)2021-02-27T02:11:41.5684493Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)2021-02-27T02:11:41.5685238Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)2021-02-27T02:11:41.5686193Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2021-02-27T02:11:41.5686901Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2021-02-27T02:11:41.5687621Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2021-02-27T02:11:41.5688337Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2021-02-27T02:11:41.5689199Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)2021-02-27T02:11:41.5690155Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130)2021-02-27T02:11:41.5691115Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81)2021-02-27T02:11:41.5692140Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)2021-02-27T02:11:41.5693174Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)2021-02-27T02:11:41.5694037Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)2021-02-27T02:11:41.5694882Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:696)2021-02-27T02:11:41.5695679Z at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)2021-02-27T02:11:41.5696679Z at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:433)2021-02-27T02:11:41.5697369Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-02-27T02:11:41.5698136Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-02-27T02:11:41.5699013Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-02-27T02:11:41.5699822Z at java.lang.reflect.Method.invoke(Method.java:498)2021-02-27T02:11:41.5700518Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)2021-02-27T02:11:41.5701297Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)2021-02-27T02:11:41.5702092Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)2021-02-27T02:11:41.5702872Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)2021-02-27T02:11:41.5703579Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2021-02-27T02:11:41.5704234Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2021-02-27T02:11:41.5704897Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2021-02-27T02:11:41.5705584Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2021-02-27T02:11:41.5706268Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2021-02-27T02:11:41.5706950Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-02-27T02:11:41.5707627Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-02-27T02:11:41.5708276Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2021-02-27T02:11:41.5708920Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2021-02-27T02:11:41.5709572Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2021-02-27T02:11:41.5710192Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2021-02-27T02:11:41.5710808Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2021-02-27T02:11:41.5711511Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2021-02-27T02:11:41.5712074Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2021-02-27T02:11:41.5712561Z ... 4 more2021-02-27T02:11:41.5713267Z Caused by: java.lang.OutOfMemoryError: Java heap space2021-02-27T02:11:41.5713740Z at java.util.Arrays.copyOf(Arrays.java:3236)2021-02-27T02:11:41.5714369Z at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)2021-02-27T02:11:41.5714949Z at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)2021-02-27T02:11:41.5715639Z at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)2021-02-27T02:11:41.5716279Z at com.esotericsoftware.kryo.io.Output.flush(Output.java:163)2021-02-27T02:11:41.5716902Z at com.esotericsoftware.kryo.io.Output.require(Output.java:142)2021-02-27T02:11:41.5717398Z at com.esotericsoftware.kryo.io.Output.writeLong(Output.java:501)2021-02-27T02:11:41.5717941Z at com.twitter.chill.java.BitSetSerializer.write(BitSetSerializer.java:79)2021-02-27T02:11:41.5718501Z at com.twitter.chill.java.BitSetSerializer.write(BitSetSerializer.java:35)2021-02-27T02:11:41.5719419Z at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:505)2021-02-27T02:11:41.5720114Z at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:266)2021-02-27T02:11:41.5720879Z at org.apache.flink.runtime.state.ArrayListSerializer.copy(ArrayListSerializer.java:75)2021-02-27T02:11:41.5721624Z at org.apache.flink.runtime.state.PartitionableListState.&lt;init&gt;(PartitionableListState.java:64)2021-02-27T02:11:41.5722507Z at org.apache.flink.runtime.state.PartitionableListState.deepCopy(PartitionableListState.java:76)2021-02-27T02:11:41.5723353Z at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:77)2021-02-27T02:11:41.5724425Z at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:36)2021-02-27T02:11:41.5725434Z at org.apache.flink.runtime.state.SnapshotStrategyRunner.snapshot(SnapshotStrategyRunner.java:82)2021-02-27T02:11:41.5726280Z at org.apache.flink.runtime.state.DefaultOperatorStateBackend.snapshot(DefaultOperatorStateBackend.java:230)2021-02-27T02:11:41.5727152Z at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:220)2021-02-27T02:11:41.5728043Z at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:163)2021-02-27T02:11:41.5728921Z at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371)2021-02-27T02:11:41.5729844Z at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:691)2021-02-27T02:11:41.5730849Z at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:612)2021-02-27T02:11:41.5731913Z at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:575)2021-02-27T02:11:41.5733435Z at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)2021-02-27T02:11:41.5734991Z at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:1020)2021-02-27T02:11:41.5735786Z at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$663/1514851121.run(Unknown Source)2021-02-27T02:11:41.5736587Z at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)2021-02-27T02:11:41.5737425Z at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1004)2021-02-27T02:11:41.5738230Z at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:960)2021-02-27T02:11:41.5739112Z at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:115)2021-02-27T02:11:41.5740088Z at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.handleBarrier(SingleCheckpointBarrierHandler.java:182)</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21542" opendate="2021-3-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for supporting INSERT INTO specific columns</summary>
      <description>We have supported INSERT INTO specific columns in FLINK-18726, but no add documentation yet.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
    </fixedFiles>
  </bug>
  <bug id="21545" opendate="2021-3-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running Kerberized YARN per-job on Docker test stalls on azure</summary>
      <description>For some reason the test started taking 10x more time than beforehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13921&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13920&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13918&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13919&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529[PASS] 'Running Kerberized YARN per-job on Docker test (default input)' passed after 40 minutes and 34 seconds! Test exited with exit code 0.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21561" opendate="2021-3-2 00:00:00" fixdate="2021-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce cache for binaries downloaded by bash tests</summary>
      <description>It seems that archive.apache.org is currently very slow, causing the bash e2e tests to timeout, because they need a lot of time downloading dependencies.This ticket is for tracking an improvement to the bash-based e2e tests to cache the binaries in Azure pipelines Caches &amp;#91;1&amp;#93;.&amp;#91;1&amp;#93; https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="21571" opendate="2021-3-2 00:00:00" fixdate="2021-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp reference version does not exist</summary>
      <description>In FLINK-21570 the japicmp reference version was set to 1.13.0 . Since this version does not exist the japicmp checks have effectively been disabled.[INFO] --- japicmp-maven-plugin:0.11.0:cmp (default) @ flink-metrics-core ---[WARNING] Could not resolve org.apache.flink:flink-metrics-core:jar:1.13.0[WARNING] Could not resolve dependency with descriptor 'org.apache.flink:flink-metrics-core:1.13.0'.[WARNING] Please provide at least one resolvable old version using one of the configuration elements &lt;oldVersion/&gt; or &lt;oldVersions/&gt;.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21611" opendate="2021-3-4 00:00:00" fixdate="2021-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp fails with "Could not resolve org.apache.flink:flink-core:jar:1.12.0"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14139&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=9b1a0f88-517b-5893-fc93-76f4670982b4[ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-core: Could not resolve org.apache.flink:flink-core:jar:1.12.0 -&gt; [Help 1]</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21620" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support abbreviation TIMESTAMP_LTZ FOR TIMESTAMP WITH LOCAL TIME ZONE</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.ComparableInputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeParser.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.DataTypes.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">docs.content.docs.dev.table.types.md</file>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.json.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.debezium.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.canal.md</file>
      <file type="M">docs.content.docs.connectors.table.datagen.md</file>
      <file type="M">docs.content.zh.docs.dev.table.types.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.json.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.debezium.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.canal.md</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LocalZonedTimestampType.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21621" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TIMESTAMP_LTZ arithmetic</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="21622" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce function TO_TIMESTAMP_LTZ(numeric [, precision])</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="21623" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce CURRENT_ROW_TIMESTAMP() function</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="21624" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct FLOOR/CEIL (TIMESTAMP/TIMESTAMP_LTZ/DATE TO WEEK) functions</summary>
      <description>While using a simple query such as this SELECT   `ts`,   FLOOR(`ts` TO WEEK) as `week_start`,   CEIL(`ts` TO WEEK) as `week_end`FROM some_table I get some weird results like these: 2021-03-01T00:00|    2021-02-25T00:00|    2021-03-04T00:00 Which is obviously wrong since March 1st is on Monday, February 25th is Thursday as well as March 04th. I've tried different combinations of timezone configurations and with both timestamps and dates, with the same results. Is there anything obviously wrong in that query? Is there any configuration to keep in mind for the start of week day? from user ML: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-SQL-FLOORing-OR-CEILing-a-DATE-or-TIMESTAMP-to-WEEK-uses-Thursdays-as-week-start-td41838.html</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="21628" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Tumbling Window</summary>
      <description>Support Python UDAF in Tumbling Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="21629" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Sliding Window</summary>
      <description>Support Python UDAF in Sliding Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.assigner.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="21630" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDAF in Session Window</summary>
      <description>Support Python UDAF in Session Window</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.assigner.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="21632" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add NamespacedStateView and PerWindowStateDataViewStore</summary>
      <description>Currently we only support KeyedStateView and PerKeyStateDataViewStore. We need to Add NamespacedStateView and PerWindowStateDataViewStore for support setting namespace.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug id="21633" opendate="2021-3-5 00:00:00" fixdate="2021-3-5 01:00:00" resolution="Done">
    <buginformation>
      <summary>Index pending task managers in TaskManagerTracker</summary>
      <description>This would accelerate the progress of finding matching pending task manager in the task manager registration.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingTaskManagerResourceInfoProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerResourceInfoProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21659" opendate="2021-3-8 00:00:00" fixdate="2021-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointSettings not properly exposed for initializing jobs</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14232&amp;view=logs&amp;j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&amp;t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a“Completed checkpoint” is more than two times(42 times in the "flink-vsts-standalonejob-2-fv-az83-563.log") but the test still fail.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CreatedTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.factories.TestingJobMasterServiceProcessFactoryOld.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.factories.TestingJobMasterServiceProcessFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.JobGraphJobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceProcessFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug id="21696" opendate="2021-3-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala code in "Writing WatermarkGenerators" documentation missing/deprecated</summary>
      <description>Hi all,I belive the scala code in the "Writing a Periodic WatermarkGenerator" and "Writing a Punctuated WatermarkGenerator" sections (Event Time -&gt; Generating Watermark) is outdated/wrong.And it's also missing in the previous section "Writing WatermarkGenerators".It uses undefined arguments (output) &amp;#8211; the onEvent and onPeriodicEmit methods have the wrong signatures.If it's alright I can put up a PR with the scala code on how to extend WatermarkGenerator class and some comment.Also sorry if I misplaced this Issue, feel free to improve/correctGiack</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug id="21698" opendate="2021-3-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable problematic cast conversion between NUMERIC type and TIMESTAMP type</summary>
      <description>The cast conversion  between NUMERIC type and TIMESTAMP type is problematic , we should disable it, NUMERIC type and TIMESTAMP_LTZ type cast conversion is valid.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21702" opendate="2021-3-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support option `sql-client.verbose` to print the exception stack</summary>
      <description>By enable this option, users can get the full exception stack.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SqlClientOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">docs.layouts.shortcodes.generated.sql.client.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21714" opendate="2021-3-10 00:00:00" fixdate="2021-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use TIMESTAMP_LTZ as return type for function PROCTIME()</summary>
      <description>When users use a PROCTIME() in SQL, the return value of PROCTIME() has a timezone offset with the wall-clock time in users' local time zone, users need to add their local time zone offset manually to get expected local timestamp(e.g: Users in Germany need to +1h to get expected local timestamp). This issue to correct PROCTIME()  functionfunctionexisted problemcurrent behaviorproposed changesPROCTIME()returns UTC timestamp, but user expects current timestamp in session time zonereturn type: TIMESTAMP  PROCTIME#session timezone: UTC2020-12-28 23:52:52#session timezone: UTC+82020-12-28 23:52:52return current timestamp in session time zone for PROCTIME(), the return type should be TIMESTAMP  WITH LOCAL TIME ZONE *PROCTIME*#session timezone: UTC2020-12-28 23:52:52#session timezone: UTC+82020-12-29 07:52:52</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.types.LogicalTypeAssignableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorContractTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.WindowedSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.TumblingSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssignerTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.HoppingSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.CumulativeSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataStructureConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.TableAggregateWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.WindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.TumblingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.CumulativeWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.AggregateWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceUnsharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.util.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.conversion.DataStructureConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.SelectTableSinkSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableToDataStreamITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalTableFunctionJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.TemporalTableFunctionJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.TemporalFunctionJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSnapshot.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.content.zh.docs.dev.table.types.md</file>
      <file type="M">docs.content.docs.dev.table.types.md</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperatorTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.view.q</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.TemporalTableFunctionImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LocalZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.ZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.FlinkRexBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerProctimeAttribute.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SingleValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.ProctimeMaterializeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
    </fixedFiles>
  </bug>
  <bug id="21715" opendate="2021-3-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit cast conversion between timestamp and timestamp_ltz</summary>
      <description>The existed UDFs using `LocalDateTime` or `java.sql.Timestamp` as parameters type, if the data type of parameter column in SQL changed from TIMESTAMP to TIMESTAMP_LTZ, these UDFs must be rewritten, because TIMESTAMP_LTZ data type does not support `LocalDateTime` or `java.sql.Timestamp` as conversion class. The first approach is to support the two conversion classes  for TIMESTAMP_LTZ  data type, to obtain the correct result, the local time zone information is necessary when conversion happens between  SQL internal data structure `TimestampData` and external conversion class `LocalDateTime` or `java.sql.Timestamp`.In this approach, after a e2e POC, we found that there are more changes than we thought before. It is difficult to cover with tests and it is easy to introduce bugs. Thus, to resolve the UDF compatibility issue, we consider support the implicit cast conversion between timestamp and timestamp_ltz after some offline discuss with jark and ykt836,This way is clean and lightweight way to resolve the UDF compatibility issue and doesn't change any public interface as well.  BTW the  implicit cast conversion is supported in Oracle&amp;#91;1&amp;#93;  &amp;#91;1&amp;#93;https://docs.oracle.com/cd/B19306_01/server.102/b14225/ch4datetime.htm </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="21728" opendate="2021-3-11 00:00:00" fixdate="2021-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DegreesWithExceptionITCase crash</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&amp;view=logs&amp;j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&amp;t=f266c805-9429-58ed-2f9e-482e7b82f58b</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="21741" opendate="2021-3-12 00:00:00" fixdate="2021-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SHOW JARS statement in SQL Client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21742" opendate="2021-3-12 00:00:00" fixdate="2021-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support REMOVE JAR statement in SQL Client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21794" opendate="2021-3-15 00:00:00" fixdate="2021-3-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support retrieving slot details via rest api</summary>
      <description>It would be helpful to allow retrieving detail information of slots via rest api. JobID that the slot is assigned to Slot resources (for dynamic slot allocation)Such information should be displayed on webui, once fine-grained resource management is enabled in future.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug id="21798" opendate="2021-3-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Guard MemorySegment against multiple frees.</summary>
      <description>As discussed in FLINK-21419, freeing a memory segment for multiple times usually indicates the ownership of the segment is unclear. It would be good to gradually getting rid of all such multiple-frees.This ticket serves as an umbrella for detected multiple-free cases.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21847" opendate="2021-3-17 00:00:00" fixdate="2021-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce DESC grammar in sql parser</summary>
      <description>DESC is abbreviation of the DESCRIBE statement. DESC currently is only supported in the sql client.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21850" opendate="2021-3-18 00:00:00" fixdate="2021-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve document and config description of sort-merge blocking shuffle</summary>
      <description>After the improvement of FLINK-19614, some of the previous document description for sort-merge blocking shuffle is not accurate, we need to improve the corresponding document.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="21853" opendate="2021-3-18 00:00:00" fixdate="2021-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) end-to-end test could not finished in 900 seconds</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14921&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=03dbd840-5430-533d-d1a7-05d0ebe03873&amp;l=7318Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host fv-az232-135.grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 15744Killed TM @ 19625Test (pid: 9232) did not finish after 900 seconds.</description>
      <version>1.11.3,1.13.0</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ha.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21867" opendate="2021-3-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend UI to expose also concurrent task failures besides the root cause</summary>
      <description>The UI only exposes the root cause of failures caught and handled by Flink. This subtasks is about extending the UI to fully support the feature of concurrently captured task failures which is subject to the related FLINK-21189 subtask.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-exception.ts</file>
    </fixedFiles>
  </bug>
  <bug id="21880" opendate="2021-3-19 00:00:00" fixdate="2021-2-19 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>UnalignedCheckpointRescaleITCase fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15037&amp;view=logs&amp;j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&amp;t=5360d54c-8d94-5d85-304e-a89267eb785a&amp;l=9404Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster. at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316) at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster. at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.startJobMasterServiceSafely(JobManagerRunnerImpl.java:390) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.startJobMaster(JobManagerRunnerImpl.java:351) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.verifyJobSchedulingStatusAndStartJobManager(JobManagerRunnerImpl.java:329) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.lambda$grantLeadership$1(JobManagerRunnerImpl.java:306) at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49) at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:731) at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2023) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.grantLeadership(JobManagerRunnerImpl.java:302) at org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService$GrantLeadershipCall.run(EmbeddedLeaderService.java:557) at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.FileNotFoundException: Cannot find meta data file '_metadata' in directory 'file:/tmp/junit3214690414199099741/junit526862574608820717/894d38376ec2541a38da0a1d73831c16/chk-10'. Please try to load the checkpoint/savepoint directly from the metadata file instead of the directory. at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:290) at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1626) at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:381) at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:308) at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:218) at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:130) at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:124) at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:108) at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:339) at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:316) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:94) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:39) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.startJobMasterServiceSafely(JobManagerRunnerImpl.java:363) ... 13 more</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21881" opendate="2021-3-19 00:00:00" fixdate="2021-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support local global optimization for window aggregation in plan</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowRankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.WindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalGroupWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.logical.WindowingStrategy.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.logical.WindowAttachedWindowingStrategy.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerWindowProperty.java</file>
    </fixedFiles>
  </bug>
  <bug id="21896" opendate="2021-3-22 00:00:00" fixdate="2021-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GroupAggregateJsonPlanTest.testDistinctAggCalls fail</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&amp;view=logs&amp;j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&amp;t=1abe556e-1530-599d-b2c7-b8c00d549e53&amp;l=6364 at org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.testDistinctAggCalls(GroupAggregateJsonPlanTest.java:148) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.ZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.YearMonthIntervalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarCharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarBinaryType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.UserDefinedType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.UnresolvedUserDefinedType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TypeInformationRawType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TinyIntType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TimeType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.SymbolType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.StructuredType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.SmallIntType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RowType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RawType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.NullType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.MultisetType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.MapType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LocalZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LegacyTypeInformationType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.IntType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.FloatType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DoubleType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DistinctType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DecimalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DayTimeIntervalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DateType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.CharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.BooleanType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.BinaryType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.BigIntType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.ArrayType.java</file>
    </fixedFiles>
  </bug>
  <bug id="21899" opendate="2021-3-22 00:00:00" fixdate="2021-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SOURCE_WATERMARK built-infunction to preserve watermark from source</summary>
      <description>The SOURCE_WATERMARK function doesn't have concrete implementation. The eval() function should throw a meaningful exception message to indicate users the function should only be used in DDL to generate a watermark preserved from source system.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="21918" opendate="2021-3-23 00:00:00" fixdate="2021-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add execution.runtime-mode setter in StreamExecutionEnvironment</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="21929" opendate="2021-3-23 00:00:00" fixdate="2021-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-statebackend-rocksdb crashes with Error occurred in starting fork</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9001&amp;view=results2021-03-23T09:11:12.1861967Z [INFO] BUILD FAILURE2021-03-23T09:11:12.1863007Z [INFO] ------------------------------------------------------------------------2021-03-23T09:11:12.1863492Z [INFO] Total time: 42:35 min2021-03-23T09:11:12.1864171Z [INFO] Finished at: 2021-03-23T09:11:12+00:002021-03-23T09:11:12.8003245Z [INFO] Final Memory: 137M/806M2021-03-23T09:11:12.8006310Z [INFO] ------------------------------------------------------------------------2021-03-23T09:11:12.8082409Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-statebackend-rocksdb_2.11: There are test failures.2021-03-23T09:11:12.8086652Z [ERROR] 2021-03-23T09:11:12.8092462Z [ERROR] Please refer to /__w/1/s/flink-state-backends/flink-statebackend-rocksdb/target/surefire-reports for the individual test results.2021-03-23T09:11:12.8096948Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.2021-03-23T09:11:12.8101388Z [ERROR] ExecutionException Error occurred in starting fork, check output in log2021-03-23T09:11:12.8105868Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log2021-03-23T09:11:12.8110518Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)2021-03-23T09:11:12.8115518Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)2021-03-23T09:11:12.8120811Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)2021-03-23T09:11:12.8126356Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)2021-03-23T09:11:12.8127129Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)2021-03-23T09:11:12.8131291Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)2021-03-23T09:11:12.8132369Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)2021-03-23T09:11:12.8133397Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2021-03-23T09:11:12.8134116Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2021-03-23T09:11:12.8134793Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2021-03-23T09:11:12.8135621Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2021-03-23T09:11:12.8136323Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2021-03-23T09:11:12.8141570Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2021-03-23T09:11:12.8142374Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2021-03-23T09:11:12.8145665Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2021-03-23T09:11:12.8146407Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2021-03-23T09:11:12.8148835Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2021-03-23T09:11:12.8151299Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2021-03-23T09:11:12.8152244Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2021-03-23T09:11:12.8152806Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2021-03-23T09:11:12.8155818Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-03-23T09:11:12.8159757Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-03-23T09:11:12.8177288Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-03-23T09:11:12.8178021Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2021-03-23T09:11:12.8179802Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2021-03-23T09:11:12.8183929Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2021-03-23T09:11:12.8187563Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2021-03-23T09:11:12.8192413Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2021-03-23T09:11:12.8196538Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log2021-03-23T09:11:12.8201660Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)2021-03-23T09:11:12.8203999Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)2021-03-23T09:11:12.8204879Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)2021-03-23T09:11:12.8205665Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)2021-03-23T09:11:12.8206513Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)2021-03-23T09:11:12.8207169Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2021-03-23T09:11:12.8209376Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2021-03-23T09:11:12.8209955Z [ERROR] at java.lang.Thread.run(Thread.java:748)2021-03-23T09:11:12.8211107Z [ERROR] -&gt; [Help 1]2021-03-23T09:11:12.8211559Z [ERROR] 2021-03-23T09:11:12.8309016Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.2021-03-23T09:11:12.8310211Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.2021-03-23T09:11:12.8311401Z [ERROR] 2021-03-23T09:11:12.8311976Z [ERROR] For more information about the errors and possible solutions, please read the following articles:2021-03-23T09:11:12.8312635Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2021-03-23T09:11:12.8313300Z [ERROR] 2021-03-23T09:11:12.8314248Z [ERROR] After correcting the problems, you can resume the build with the command2021-03-23T09:11:12.8315462Z [ERROR] mvn &lt;goals&gt; -rf :flink-statebackend-rocksdb_2.112021-03-23T09:11:12.8687280Z Process exited with EXIT CODE: 1.2021-03-23T09:11:12.8687830Z Trying to KILL watchdog (359).2021-03-23T09:11:12.8696302Z /__w/1/s/tools/ci/watchdog.sh: line 100: 359 Terminated watchdog2021-03-23T09:11:16.4147696Z Searching for .dump, .dumpstream and related files in '/__w/1/s'2021-03-23T09:11:21.6569652Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun2.dump' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6645588Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dump' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6683479Z Moving '/__w/1/s/flink-runtime/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6726888Z Moving '/__w/1/s/flink-core/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6772031Z Moving '/__w/1/s/flink-metrics/flink-metrics-core/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6807831Z Moving '/__w/1/s/flink-java/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6847619Z Moving '/__w/1/s/flink-runtime-web/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:21.6882147Z Moving '/__w/1/s/flink-state-backends/flink-statebackend-rocksdb/core.18126' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1251457Z Moving '/__w/1/s/flink-state-backends/flink-statebackend-rocksdb/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1287286Z Moving '/__w/1/s/flink-test-utils-parent/flink-test-utils/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1322850Z Moving '/__w/1/s/flink-clients/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1360757Z Moving '/__w/1/s/flink-streaming-java/java_pid5009.hprof' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1421701Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun2.dump' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1460325Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1492941Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805-jvmRun1.dump' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1523043Z Moving '/__w/1/s/flink-streaming-java/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1554119Z Moving '/__w/1/s/flink-optimizer/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1589618Z Moving '/__w/1/s/flink-scala/target/surefire-reports/2021-03-23T08-28-44_805.dumpstream' to target directory ('/__w/_temp/debug_files/')2021-03-23T09:11:22.1638143Z Compressing debug filesI'm not sure if this includes one or two issues.One issue seems to be in the flink-streaming-java tests:# Created at 2021-03-23T09:03:49.666Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'java.lang.OutOfMemoryError: Java heap space'.java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'java.lang.OutOfMemoryError: Java heap space'. at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.&lt;init&gt;(ForkClient.java:507) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177) at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88) at java.lang.Thread.run(Thread.java:748)# Created at 2021-03-23T09:03:49.667Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Dumping heap to java_pid5009.hprof ...'.java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Dumping heap to java_pid5009.hprof ...'. at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.&lt;init&gt;(ForkClient.java:507) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177) at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88) at java.lang.Thread.run(Thread.java:748)# Created at 2021-03-23T09:03:49.693Corrupted STDOUT by directly writing to native stream in forked JVM 1. Stream 'Heap dump file created [2834354 bytes in 0.027 secs]'.java.lang.IllegalArgumentException: Stream stdin corrupted. Expected comma after third character in command 'Heap dump file created [2834354 bytes in 0.027 secs]'. at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$OperationalData.&lt;init&gt;(ForkClient.java:507) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:210) at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:177) at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88) at java.lang.Thread.run(Thread.java:748)There's a coredump included: Looks like the testInitialSizeCompoutation test is causing the / a failure.The second issue is (which is reported as a test failure): # Created at 2021-03-23T09:09:39.376Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError# Created at 2021-03-23T09:09:39.535Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError# Created at 2021-03-23T09:11:10.037pure virtual method called# Created at 2021-03-23T09:11:10.037terminate called without an active exception# Created at 2021-03-23T09:11:12.170Aborted (core dumped)From the coredump, I see the following:(gdb) where#0 0x00007f9343508438 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54#1 0x00007f934350a03a in __GI_abort () at abort.c:89#2 0x00007f9341b8e84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6#3 0x00007f9341b8c6b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6#4 0x00007f9341b8c701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6#5 0x00007f9341b8d23f in __cxa_pure_virtual () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6#6 0x00007f92ed39c0d5 in rocksdb::DBImpl::CloseHelper() () from /tmp/librocksdbjni8334465068904677424.so#7 0x00007f92ed3a717b in rocksdb::DBImpl::~DBImpl() () from /tmp/librocksdbjni8334465068904677424.so#8 0x00007f92ed3a7451 in rocksdb::DBImpl::~DBImpl() () from /tmp/librocksdbjni8334465068904677424.so#9 0x00007f932d801aa8 in ?? ()#10 0x0000000085677fe8 in ?? ()#11 0x00007f931c8da910 in ?? ()#12 0x00007f931c8da968 in ?? ()#13 0x00007f932d007ffd in ?? ()#14 0x0000000000000000 in ?? ()</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="21930" opendate="2021-3-23 00:00:00" fixdate="2021-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in Hive temporal join example</summary>
      <description>  fix hive dim doc replace order to o</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="21936" opendate="2021-3-23 00:00:00" fixdate="2021-1-23 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Rescale pointwise connections for unaligned checkpoints</summary>
      <description>We currently do not have any hard guarantees on pointwise connection regarding data consistency. However, since data was structured implicitly in the same way as any preceding source or keyby, some users relied on this behavior to divide compute-intensive tasks into smaller chunks while relying on ordering guarantees.As long as the parallelism does not change, unaligned checkpoints (UC) retains these properties. With the implementation of rescaling of UC (FLINK-19801), that has changed. For most exchanges, there is a meaningful way to reassign state from one channel to another (even in random order). For some exchanges, the mapping is ambiguous and requires post-filtering. However, for point-wise connections, it's impossible while retaining these properties.Consider, source -&gt; keyby -&gt; task1 -&gt; forward -&gt; task2. Now if we want to rescale from parallelism p = 1 to p = 2, suddenly the records inside the keyby channels need to be divided into two channels according to the keygroups. That is easily possible by using the keygroup ranges of the operators and a way to determine the key(group) of the record (independent of the actual approach). For the forward channel, we completely lack the key context. No record in the forward channel has any keygroup assigned; it's also not possible to calculate it as there is no guarantee that the key is still present.The root cause for this limitation is the conceptual mismatch between what we provide and what some users assume we provide (or we assume that the users assume). For example, it's impossible to use (keyed) state in task2 right now, because there is no key context, but we still want to guarantee orderness in respect to that key context.For 1.13, the easiest solution is to disable channel state in pointwise connections. For any non-trivial application with at least one shuffle, the number of pointwise channels (linear to p) is quickly dwarfed by all-to-all connections (quadratic to p). I'd add some alternative ideas to the discussion.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamEdge.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.SubtaskStateMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="21937" opendate="2021-3-24 00:00:00" fixdate="2021-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support batch mode in Python DataStream API for basic operations</summary>
      <description>This ticket is dedicated to support batch mode for basic operations such as map/flatmap/filter, etc in Python DataStream API. For the other operations such as reduce, etc, we will support them in separate tickets.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.util.utils.py</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.state.backend.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.state.backend.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.dataset.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
      <file type="M">flink-python.pyflink.common.restart.strategy.py</file>
      <file type="M">flink-python.pyflink.common.execution.config.py</file>
      <file type="M">flink-python.pyflink.common.configuration.py</file>
    </fixedFiles>
  </bug>
  <bug id="21938" opendate="2021-3-24 00:00:00" fixdate="2021-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to test Python UDFs</summary>
      <description>It should be similar to the Java UDFs:https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/testing.html#testing-user-defined-functions</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="21939" opendate="2021-3-24 00:00:00" fixdate="2021-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support batch mode in Python DataStream API for process and reduce operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.OneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="2194" opendate="2015-6-10 00:00:00" fixdate="2015-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type extractor does not support Writable type</summary>
      <description>The type extractor supports sub types of Writable but not Writable itself. The reason is that it wants to create a WritableTypeInfo which is not possible for the interface Writable, though. A solution would be to treat the Writable interface like a generic type.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="21942" opendate="2021-3-24 00:00:00" fixdate="2021-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KubernetesLeaderRetrievalDriver not closed after terminated which lead to connection leak</summary>
      <description>Looks like KubernetesLeaderRetrievalDriver is not closed even if the KubernetesLeaderElectionDriver is closed and job reach globally terminated.This will lead to many configmap watching be still active with connections to K8s.When the connections exceeds max concurrent requests, those new configmap watching can not be started. Finally leads to all new jobs submitted timeout.fly_in_gis trohrmann This may be related to FLINK-20695, could you confirm this issue?But when many jobs are running in same session cluster, the config map watching is required to be active. Maybe we should merge all config maps watching?</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdActions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21945" opendate="2021-3-24 00:00:00" fixdate="2021-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable checkpointing of inflight data in pointwise connections for unaligned checkpoints</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.partitioner.BinaryHashPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.StreamPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ShufflePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RebalancePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.GlobalPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointOptionsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.CheckpointBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.SnapshotUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21947" opendate="2021-3-24 00:00:00" fixdate="2021-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TIMESTAMP_LTZ type in CSV format</summary>
      <description>Currently CSV format does not support  TIMESTAMP_LTZ yet, and the code exists some bug, we should support the TIMESTAMP_LTZ type properly and correct the timestamp conversion.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerDerTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.TimestampFormat.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.TimeFormats.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.RowDataToJsonConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDecodingFormat.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDecodingFormat.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowDataToCsvConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSchemaConverter.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21974" opendate="2021-3-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to match quoted values for &amp;#39;SET key=val&amp;#39; statement</summary>
      <description>When execute sql,SET key_placeholder='test name ';the value of the key_placeholder is test_name rather than 'test name'</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.ParserImplTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.command.SetOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21978" opendate="2021-3-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable cast conversion between Numeric type and TIMESTAMP_LTZ type</summary>
      <description>Currently we has supported the cast conversion between Numeric type and TIMESTAMP_LTZ type, we suppose the numeric value e.g `Long type 1000L` as epoch seconds and then cast  to TIMESTAMP_LTZ, but the java.lang.Long is a conversion class of `LocalZonedTimestampType`  and treats as milliseconds.To avoid the inconsistency, we should disable it and encourage user to use `TO_TIMESTAMP_LTZ(numeric, precisoon)` function.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.DecimalDataUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="21984" opendate="2021-3-26 00:00:00" fixdate="2021-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change precision argument from optional to required in TO_TIMESTAMP_LTZ(numeric, precision)</summary>
      <description>To avoid the confusing the numeric value is in second or millisecond, we'd better set the precision argument to required. The background is that `LocalZonedTimestampType` always treats `Integer` conversion class as second, treats `Long` as millisecond. </description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="21986" opendate="2021-3-26 00:00:00" fixdate="2021-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>taskmanager native memory not release timely after restart</summary>
      <description>I run a regular join job with flink_1.12.1 , and find taskmanager native memory not release timely after restart cause by exceeded checkpoint tolerable failure threshold.problem job information： job first restart cause by exceeded checkpoint tolerable failure threshold. then taskmanager be killed by yarn many times in this case，tm heap is set to 7.68G，bug all tm heap size is under 4.2G nonheap size increase after restart，but still under 160M. taskmanager process memory increase 3-4G after restart（this figure show one of taskmanager）  my guess：RocksDB wiki mentioned ：Many of the Java Objects used in the RocksJava API will be backed by C++ objects for which the Java Objects have ownership. As C++ has no notion of automatic garbage collection for its heap in the way that Java does, we must explicitly free the memory used by the C++ objects when we are finished with them.So, is it possible that RocksDBStateBackend not call AbstractNativeReference#close() to release memory use by RocksDB C++ Object ?I make a change:        Actively call System.gc() and System.runFinalization() every minute. And run this test again: taskmanager process memory no obvious increase job run for several days，and restart many times，but no taskmanager killed by yarn like before Summary： first，there is some native memory can not release timely after restart in this situation I guess it maybe RocksDB C++ object，but I hive not check it from source code of RocksDBStateBackend </description>
      <version>1.11.3,1.12.1,1.13.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21989" opendate="2021-3-26 00:00:00" fixdate="2021-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a SupportsSourceWatermark ability interface</summary>
      <description>FLINK-21899 added a dedicated function that can be used in watermark definitions. Currently, the generated watermark strategy is invalid because of the exception that we throw in the function’s implementation. We should integrate this concept deeper into the interfaces instead of the need to implement some expression analyzing utility for every source.We propose the following interface:SupportsSourceWatermark { void applySourceWatermark()} </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.scalar.SourceWatermarkFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.ShortcutUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.abilities.source.SourceAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.ScanTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsWatermarkPushDown.java</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="21994" opendate="2021-3-26 00:00:00" fixdate="2021-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FLIP-27 based FileSource connector in PyFlink DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.time.py</file>
    </fixedFiles>
  </bug>
  <bug id="22003" opendate="2021-3-29 00:00:00" fixdate="2021-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointITCase fail</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15601&amp;view=logs&amp;j=119bbba7-f5e3-5e08-e72d-09f1529665de&amp;t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc&amp;l=4142[ERROR] execute[parallel pipeline with remote channels, p = 5](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase) Time elapsed: 60.018 s &lt;&lt;&lt; ERROR!org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1859) at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1822) at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:138) at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:184) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22006" opendate="2021-3-29 00:00:00" fixdate="2021-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Could not run more than 20 jobs in a native K8s session when K8s HA enabled</summary>
      <description>Currently, if we start a native K8s session cluster when K8s HA enabled, we could not run more than 20 streaming jobs.  The latest job is always initializing, and the previous one is created and waiting to be assigned. It seems that some internal resources have been exhausted, e.g. okhttp thread pool , tcp connections or something else.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="22022" opendate="2021-3-30 00:00:00" fixdate="2021-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the ExecNode scan scope to improve performance when converting json plan to ExecNodeGraph</summary>
      <description>Change the scan package from org.apache.flink to org.apache.flink.table.planner.plan.nodes.exec, which can reduce a lot of useless scan</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.MetadataHandlerConsistencyTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22038" opendate="2021-3-30 00:00:00" fixdate="2021-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update TopN node to be without rowNumber if rowNumber field is projected out after TopN</summary>
      <description>As describe in article sql_queries, an optimization way to improve performance of TopN is omitting rownum field in the outer SELECT clause of the Top-N query. However, some queries generated unexpected plan, even though we have followed the instructions in the documentation.@Testdef testRowNumberFiltered(): Unit = { util.addDataStream[(String, Long, Long, Long)]( "T", 'uri, 'reqcount, 'start_time, 'bucket_id) val sql = """ |SELECT | uri, | reqcount, | start_time |FROM | ( | SELECT | uri, | reqcount, | rownum_2, | start_time | FROM | ( | SELECT | uri, | reqcount, | start_time, | ROW_NUMBER() OVER ( | PARTITION BY start_time | ORDER BY | reqcount DESC | ) AS rownum_2 | FROM | ( | SELECT | uri, | reqcount, | start_time, | ROW_NUMBER() OVER ( | PARTITION BY start_time, bucket_id | ORDER BY | reqcount DESC | ) AS rownum_1 | FROM T | ) | WHERE | rownum_1 &lt;= 100000 | ) | WHERE | rownum_2 &lt;= 100000 |) |""".stripMargin util.verifyExecPlan(sql)}For example, we expect both outer and inner TopN could use without rowNumber optimization in the above queries, however inner TopN is not as we expected.The logical plan and physical plan as following, we could find even though the rowNumber field is projected out after inner topN, inner topN still with rowNumber.  </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ConstantRankNumberColumnRemoveRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.RedundantRankNumberColumnRemoveRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="22051" opendate="2021-3-30 00:00:00" fixdate="2021-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better document the distinction between stop-with-savepoint and stop-with-savepoint-with-drain</summary>
      <description>The Flink documentation only contains very few details about the difference between stop-with-savepoint and stop-with-savepoint-with-drain. We should better explain the semantic differences.</description>
      <version>1.11.3,1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="22070" opendate="2021-3-31 00:00:00" fixdate="2021-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  <bug id="22076" opendate="2021-4-1 00:00:00" fixdate="2021-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python Test failed with "OSError: [Errno 12] Cannot allocate memory"</summary>
      <description>https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=443dc6bf-b240-56df-6acf-c882d4b238da&amp;l=21533Python Test failed with "OSError: &amp;#91;Errno 12&amp;#93; Cannot allocate memory" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug id="22094" opendate="2021-4-1 00:00:00" fixdate="2021-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release scripts after documentation switch to Hugo</summary>
      <description>The release scripts has not been upgraded after migration to Hugo.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">tools.releasing.create.snapshot.branch.sh</file>
      <file type="M">tools.releasing.create.release.branch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22098" opendate="2021-4-2 00:00:00" fixdate="2021-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bug for window join: plan is wrong if join condition contain &amp;#39;IS NOT DISTINCT FROM&amp;#39;</summary>
      <description>Plan test is wrong for window join if join condition contain 'IS NOT DISTINCT FROM'. </description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="22099" opendate="2021-4-2 00:00:00" fixdate="2021-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bug for semi/anti window join.</summary>
      <description>Fix bug for Semi/Anti WindowJoin.//代码占位符@Testdef testSemiJoinIN(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE L.a IN ( |SELECT a FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end) """.stripMargin util.verifyRelPlan(sql)}@Testdef testSemiExist(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE EXISTS ( |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end AND L.a = R.a) """.stripMargin util.verifyRelPlan(sql)}@Testdef testAntiJoinNotExist(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE NOT EXISTS ( |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end AND L.a = R.a) """.stripMargin util.verifyRelPlan(sql)}@Testdef testAntiJoinNotIN(): Unit = { val sql = """ |SELECT * FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) L WHERE L.a NOT IN ( |SELECT a FROM ( | SELECT | a, | window_start, | window_end, | window_time, | count(*) as cnt, | count(distinct c) AS uv | FROM TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) | GROUP BY a, window_start, window_end, window_time |) R |WHERE L.window_start = R.window_start AND L.window_end = R.window_end) """.stripMargin util.verifyRelPlan(sql)}Now run the above sql, an `ArrayIndexOutOfBoundsException` would be thrown out.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="22102" opendate="2021-4-2 00:00:00" fixdate="2021-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw meaningful exceptions for features we don&amp;#39;t support</summary>
      <description>As reported by user in this comment, we should error out in such cases.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="22107" opendate="2021-4-2 00:00:00" fixdate="2021-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include antlr into hive connector uber jars</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22113" opendate="2021-4-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UniqueKey constraint is lost with multiple sources join in SQL</summary>
      <description>Hi team,   We have a use case to join multiple data sources to generate a continuous updated view. We defined primary key constraint on all the input sources and all the keys are the subsets in the join condition. All joins are left join.   In our case, the first two inputs can produce JoinKeyContainsUniqueKey input sepc, which is good and performant. While when it comes to the third input source, it's joined with the intermediate output table of the first two input tables, and the intermediate table does not carry key constraint information(although the thrid source input table does), so it results in a NoUniqueKey input sepc. Given NoUniqueKey inputs has dramatic performance implications per the Force Join Unique Key email thread, we want to know if there is any mitigation solution for this. Example:Take the example from https://github.com/ververica/flink-sql-cookbook/blob/master/joins/05/05_star_schema.mdCREATE TEMPORARY TABLE passengers ( passenger_key STRING, first_name STRING, last_name STRING, update_time TIMESTAMP(3), PRIMARY KEY (passenger_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'passengers', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE stations ( station_key STRING, update_time TIMESTAMP(3), city STRING, PRIMARY KEY (station_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'stations', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE booking_channels ( booking_channel_key STRING, update_time TIMESTAMP(3), channel STRING, PRIMARY KEY (booking_channel_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'booking_channels', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'raw', 'value.format' = 'json');CREATE TEMPORARY TABLE train_activities ( scheduled_departure_time TIMESTAMP(3), actual_departure_date TIMESTAMP(3), passenger_key STRING, origin_station_key STRING, destination_station_key STRING, booking_channel_key STRING, PRIMARY KEY (booking_channel_key, origin_station_key, destination_station_key) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'train_activities', 'properties.bootstrap.servers' = 'localhost:9092', 'key.format' = 'json', 'value.format' = 'json');SELECT t.actual_departure_date, p.first_name, p.last_name, b.channel, os.city AS origin_station, ds.city AS destination_stationFROM train_activities_1 tLEFT JOIN booking_channels b ON t.booking_channel_key = b.booking_channel_keyLEFT JOIN passengers pON t.passenger_key = p.passenger_keyLEFT JOIN stations osON t.origin_station_key = os.station_keyLEFT JOIN stations dsON t.destination_station_key = ds.station_key  The query will generate exeuction plan of: Flink SQL&gt; explain&gt; SELECT&gt; t.actual_departure_date,&gt; p.first_name,&gt; p.last_name,&gt; b.channel,&gt; os.city AS origin_station,&gt; ds.city AS destination_station&gt; FROM train_activities_1 t&gt; LEFT JOIN booking_channels b&gt; ON t.booking_channel_key = b.booking_channel_key&gt; LEFT JOIN passengers p&gt; ON t.passenger_key = p.passenger_key&gt; LEFT JOIN stations os&gt; ON t.origin_station_key = os.station_key&gt; LEFT JOIN stations ds&gt; ON t.destination_station_key = ds.station_key;== Abstract Syntax Tree ==LogicalProject(actual_departure_date=[$1], first_name=[$10], last_name=[$11], channel=[$8], origin_station=[$15], destination_station=[$18])+- LogicalJoin(condition=[=($4, $16)], joinType=[left]) :- LogicalJoin(condition=[=($3, $13)], joinType=[left]) : :- LogicalJoin(condition=[=($2, $9)], joinType=[left]) : : :- LogicalJoin(condition=[=($5, $6)], joinType=[left]) : : : :- LogicalTableScan(table=[[default_catalog, default_database, train_activities_1]]) : : : +- LogicalWatermarkAssigner(rowtime=[update_time], watermark=[-($1, 10000:INTERVAL SECOND)]) : : : +- LogicalTableScan(table=[[default_catalog, default_database, booking_channels]]) : : +- LogicalTableScan(table=[[default_catalog, default_database, passengers]]) : +- LogicalTableScan(table=[[default_catalog, default_database, stations]]) +- LogicalTableScan(table=[[default_catalog, default_database, stations]])== Optimized Physical Plan ==Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])+- Join(joinType=[LeftOuterJoin], where=[=(destination_station_key, station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) :- Exchange(distribution=[hash[destination_station_key]]) : +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city]) : +- Join(joinType=[LeftOuterJoin], where=[=(origin_station_key, station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : :- Exchange(distribution=[hash[origin_station_key]]) : : +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name]) : : +- Join(joinType=[LeftOuterJoin], where=[=(passenger_key, passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : :- Exchange(distribution=[hash[passenger_key]]) : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel]) : : : +- Join(joinType=[LeftOuterJoin], where=[=(booking_channel_key, booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : : :- Exchange(distribution=[hash[booking_channel_key]]) : : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : : +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key]) : : : : +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]]) : : : : +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- Calc(select=[booking_channel_key, channel]) : : : +- ChangelogNormalize(key=[booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- Calc(select=[passenger_key, first_name, last_name]) : : +- ChangelogNormalize(key=[passenger_key]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time]) : +- Exchange(distribution=[hash[station_key]]) : +- Calc(select=[station_key, city]) : +- ChangelogNormalize(key=[station_key]) : +- Exchange(distribution=[hash[station_key]]) : +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city]) +- Exchange(distribution=[hash[station_key]]) +- Calc(select=[station_key, city]) +- ChangelogNormalize(key=[station_key]) +- Exchange(distribution=[hash[station_key]]) +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city])== Optimized Execution Plan ==Calc(select=[actual_departure_date, first_name, last_name, channel, city AS origin_station, city0 AS destination_station])+- Join(joinType=[LeftOuterJoin], where=[(destination_station_key = station_key)], select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city, station_key, city0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) :- Exchange(distribution=[hash[destination_station_key]]) : +- Calc(select=[actual_departure_date, destination_station_key, channel, first_name, last_name, city]) : +- Join(joinType=[LeftOuterJoin], where=[(origin_station_key = station_key)], select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name, station_key, city], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : :- Exchange(distribution=[hash[origin_station_key]]) : : +- Calc(select=[actual_departure_date, origin_station_key, destination_station_key, channel, first_name, last_name]) : : +- Join(joinType=[LeftOuterJoin], where=[(passenger_key = passenger_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel, passenger_key0, first_name, last_name], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : :- Exchange(distribution=[hash[passenger_key]]) : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, channel]) : : : +- Join(joinType=[LeftOuterJoin], where=[(booking_channel_key = booking_channel_key0)], select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key, booking_channel_key0, channel], leftInputSpec=[HasUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey]) : : : :- Exchange(distribution=[hash[booking_channel_key]]) : : : : +- Calc(select=[actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : : +- ChangelogNormalize(key=[booking_channel_key, origin_station_key, destination_station_key]) : : : : +- Exchange(distribution=[hash[booking_channel_key, origin_station_key, destination_station_key]]) : : : : +- TableSourceScan(table=[[default_catalog, default_database, train_activities_1]], fields=[scheduled_departure_time, actual_departure_date, passenger_key, origin_station_key, destination_station_key, booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- Calc(select=[booking_channel_key, channel]) : : : +- ChangelogNormalize(key=[booking_channel_key]) : : : +- Exchange(distribution=[hash[booking_channel_key]]) : : : +- TableSourceScan(table=[[default_catalog, default_database, booking_channels, watermark=[-($1, 10000:INTERVAL SECOND)]]], fields=[booking_channel_key, update_time, channel]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- Calc(select=[passenger_key, first_name, last_name]) : : +- ChangelogNormalize(key=[passenger_key]) : : +- Exchange(distribution=[hash[passenger_key]]) : : +- TableSourceScan(table=[[default_catalog, default_database, passengers]], fields=[passenger_key, first_name, last_name, update_time]) : +- Exchange(distribution=[hash[station_key]])(reuse_id=[1]) : +- Calc(select=[station_key, city]) : +- ChangelogNormalize(key=[station_key]) : +- Exchange(distribution=[hash[station_key]]) : +- TableSourceScan(table=[[default_catalog, default_database, stations]], fields=[station_key, update_time, city]) +- Reused(reference_id=[1])  </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
    </fixedFiles>
  </bug>
  <bug id="22117" opendate="2021-4-6 00:00:00" fixdate="2021-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not print stack trace for checkpoint trigger failure if not all tasks are started.</summary>
      <description>Currently the stack trace is printed compared with the previous versions, but it might cover the actual exception that user want to locate. </description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22125" opendate="2021-4-6 00:00:00" fixdate="2021-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit the return value of MapState.get when a key doesn&amp;#39;t exist</summary>
      <description>Currently, it will thrown KeyError if the key doesn't exist for MapState in Python DataStream API. However, it returns null in the Java DataStream API. Maybe we should keep the behavior the same across Python DataStream API and Java DataStream API.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
    </fixedFiles>
  </bug>
  <bug id="22127" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enrich error message of read buffer request timeout exception</summary>
      <description>Enrich error message of read buffer request timeout exception to tell the user how to solve the timeout exception.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22136" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Device application for unaligned checkpoint test on cluster</summary>
      <description>To test unaligned checkpoints, we should use a few different applications that use different features: Mixing forward/rescale channels with keyby or other shuffle operations Unions 2 or n-ary operators Associated state ((keyed) process function) Correctness verificationsThe sinks should not be mocked but rather should be able to induce a fair amount of backpressure into the system. Quite possibly, it would be a good idea to have a way to add more backpressure to the sink by running the respective system on the cluster and be able to add/remove parallel instances.Things to check in the application Inflight data is restored to the correct keygroups -&gt; can be checked with keyed state in a process function Correctness: Completeness (no lost records) + no duplicates Orderness of data for keyed exchanges (we guarantee that records with the same key retain orderness across keyed operators) (To detect errors early, we can also use magic headers)</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22137" opendate="2021-4-7 00:00:00" fixdate="2021-1-7 01:00:00" resolution="Done">
    <buginformation>
      <summary>Execute unaligned checkpoint test on a cluster</summary>
      <description>Start application and at some point cancel/induce failure, the user needs to restart from a retained checkpoint with lower same higher degree of parallelism.To enable unaligned checkpoints, set execution.checkpointing.unaligned: true execution.checkpointing.alignment-timeout to 0s, 10s, 1min (for high backpressure)The primary objective is to check if all data is recovered properly and if the semantics is correct (does state match input?).The secondary objective is to check if Flink UI shows the information correctly: unaligned checkpoint enabled on job level timeout on job level for each checkpoint, if it's unaligned or not; how much data was written</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.SqlUnparseUtils.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableDropConstraint.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.ColumnReferenceFinder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableDropConstraintOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
    </fixedFiles>
  </bug>
  <bug id="22138" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow casting between row and structured type</summary>
      <description>There are still some minor barriers that prevent using toDataStream to its full extent.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.StructuredRelDataType.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.StructuredType.java</file>
    </fixedFiles>
  </bug>
  <bug id="22142" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove console logging for Kafka connector for AZP runs</summary>
      <description>For the Kafka connector we do log to the console. These logging statements clutter the AZP output considerably. I propose to remove this logic. Moreover, we still have some DEBUG logging for FLINK-16383 which has been fixed.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22144" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Done">
    <buginformation>
      <summary>Test display last n exceptions/causes for job restarts in Web UI</summary>
      <description>This is the testing task for FLINK-6042. We should test whether the root causes for multiple restarts are properly displayed in the web UI.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsHeaders.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.web.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="22151" opendate="2021-4-8 00:00:00" fixdate="2021-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for agg functions</summary>
      <description>Update avg, count, min, max, sum, sum0, stddevPop, stddevSamp, varPop, varSamp.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.aggregations.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeMerging.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ComparableTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.OperatorBindingCallContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.LookupCallContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.CallBindingCallContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.AbstractSqlCallContext.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.utils.CallContextMock.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.utils.UnknownCallContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.utils.AdaptedCallContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SubsequenceInputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.CallContext.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22152" opendate="2021-4-8 00:00:00" fixdate="2021-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of same timers are registered multiple times</summary>
      <description>The same timer will be registered multiple times. We need to deduplicate same timers</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.input.handler.py</file>
      <file type="M">flink-python.pyflink.fn.execution.timerservice.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
    </fixedFiles>
  </bug>
  <bug id="22155" opendate="2021-4-8 00:00:00" fixdate="2021-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EXPLAIN statement should validate insert and query separately</summary>
      <description>When explain insert statement, the validator validate the whole statement rather than validate the query. But when execute insert statement, the planner only validate the query part of the insert statement. It may brings the result of the explan is different from the actual plan.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainInsert1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainInsert0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="22159" opendate="2021-4-8 00:00:00" fixdate="2021-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new window TVF based operations</summary>
      <description>In this 1.13 version, we have supported window TVF based aggregation and TopN of FLIP-145. We should add documentation for them. We may also need to restructure the "Queries" page.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
    </fixedFiles>
  </bug>
  <bug id="22171" opendate="2021-4-9 00:00:00" fixdate="2021-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the doc about SQL Client</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="22172" opendate="2021-4-9 00:00:00" fixdate="2021-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of shared resource among Python Operators of the same slot is not released</summary>
      <description>The problem was discussed here:https://issues.apache.org/jira/browse/FLINK-20663?focusedCommentId=17317706&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17317706In general, the python process should be terminated when the shared resource is released as many times as requested. However, this reference counting logic has been implemented twice. Consequently, the shared resource may be requested multiple times but only released once.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.PythonSharedResources.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22177" opendate="2021-4-9 00:00:00" fixdate="2021-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for consistent time functions and compatibility specification</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.time.attributes.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.time.attributes.md</file>
    </fixedFiles>
  </bug>
  <bug id="22180" opendate="2021-4-9 00:00:00" fixdate="2021-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RM reclaims slots while batch job is in progress</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16271&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=f508e270-48d6-5f1e-3138-42a17e0714f0&amp;l=40362021-04-09T09:51:37.9803869Z [INFO] Running org.apache.flink.test.iterative.ConnectedComponentsWithSolutionSetFirstITCase2021-04-09T09:51:42.3003700Z Job execution failed.2021-04-09T09:51:42.3125900Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-04-09T09:51:42.3128088Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)2021-04-09T09:51:42.3129160Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)2021-04-09T09:51:42.3130010Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2021-04-09T09:51:42.3130773Z at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)2021-04-09T09:51:42.3131511Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-04-09T09:51:42.3137024Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-04-09T09:51:42.3138081Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)2021-04-09T09:51:42.3138945Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-04-09T09:51:42.3139739Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-04-09T09:51:42.3140492Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-04-09T09:51:42.3141212Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-04-09T09:51:42.3141999Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)2021-04-09T09:51:42.3142892Z at akka.dispatch.OnComplete.internal(Future.scala:264)2021-04-09T09:51:42.3143460Z at akka.dispatch.OnComplete.internal(Future.scala:261)2021-04-09T09:51:42.3144072Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2021-04-09T09:51:42.3144857Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2021-04-09T09:51:42.3145476Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-04-09T09:51:42.3146230Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)2021-04-09T09:51:42.3147022Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2021-04-09T09:51:42.3147857Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2021-04-09T09:51:42.3148552Z at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)2021-04-09T09:51:42.3149277Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)2021-04-09T09:51:42.3150172Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)2021-04-09T09:51:42.3233576Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)2021-04-09T09:51:42.3234583Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)2021-04-09T09:51:42.3235304Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-04-09T09:51:42.3236015Z at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)2021-04-09T09:51:42.3237181Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)2021-04-09T09:51:42.3238165Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-04-09T09:51:42.3239041Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-04-09T09:51:42.3239798Z at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)2021-04-09T09:51:42.3240522Z at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)2021-04-09T09:51:42.3241216Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)2021-04-09T09:51:42.3241986Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)2021-04-09T09:51:42.3243232Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2021-04-09T09:51:42.3243930Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2021-04-09T09:51:42.3244748Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2021-04-09T09:51:42.3245429Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2021-04-09T09:51:42.3246212Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy2021-04-09T09:51:42.3247118Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)2021-04-09T09:51:42.3248306Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)2021-04-09T09:51:42.3249304Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:206)2021-04-09T09:51:42.3250180Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:196)2021-04-09T09:51:42.3251112Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:187)2021-04-09T09:51:42.3252068Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:680)2021-04-09T09:51:42.3253223Z at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)2021-04-09T09:51:42.3254507Z at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1462)2021-04-09T09:51:42.3255444Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1112)2021-04-09T09:51:42.3256235Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1052)2021-04-09T09:51:42.3257025Z at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:883)2021-04-09T09:51:42.3289873Z at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:598)2021-04-09T09:51:42.3290986Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-04-09T09:51:42.3291964Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-04-09T09:51:42.3293107Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2021-04-09T09:51:42.3294152Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)2021-04-09T09:51:42.3295130Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)2021-04-09T09:51:42.3295943Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)2021-04-09T09:51:42.3296927Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)2021-04-09T09:51:42.3297919Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2021-04-09T09:51:42.3298763Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2021-04-09T09:51:42.3299880Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2021-04-09T09:51:42.3300709Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2021-04-09T09:51:42.3301554Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2021-04-09T09:51:42.3302472Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-04-09T09:51:42.3303309Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-04-09T09:51:42.3304144Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2021-04-09T09:51:42.3304918Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2021-04-09T09:51:42.3305541Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2021-04-09T09:51:42.3306224Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2021-04-09T09:51:42.3306711Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2021-04-09T09:51:42.3307516Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2021-04-09T09:51:42.3308219Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2021-04-09T09:51:42.3308813Z ... 4 more2021-04-09T09:51:42.3309940Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID eb55ab4ea4591c142078631839e1216f and allocation ID 802cf7ad875c5d2c3220b55a32b00401.2021-04-09T09:51:42.3311414Z at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)2021-04-09T09:51:42.3311964Z at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)2021-04-09T09:51:42.3312509Z at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)2021-04-09T09:51:42.3313227Z at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)2021-04-09T09:51:42.3313894Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-04-09T09:51:42.3314805Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-04-09T09:51:42.3315888Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)2021-04-09T09:51:42.3317047Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-04-09T09:51:42.3318060Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-04-09T09:51:42.3319103Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-04-09T09:51:42.3320125Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-04-09T09:51:42.3321082Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1064)2021-04-09T09:51:42.3321711Z at akka.dispatch.OnComplete.internal(Future.scala:263)2021-04-09T09:51:42.3322067Z at akka.dispatch.OnComplete.internal(Future.scala:261)2021-04-09T09:51:42.3322537Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2021-04-09T09:51:42.3322940Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2021-04-09T09:51:42.3323322Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-04-09T09:51:42.3323789Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)2021-04-09T09:51:42.3324269Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2021-04-09T09:51:42.3324728Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2021-04-09T09:51:42.3325177Z at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)2021-04-09T09:51:42.3325638Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)2021-04-09T09:51:42.3326176Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)2021-04-09T09:51:42.3326634Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)2021-04-09T09:51:42.3327697Z at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)2021-04-09T09:51:42.3328547Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-04-09T09:51:42.3329434Z at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)2021-04-09T09:51:42.3330625Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)2021-04-09T09:51:42.3331784Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-04-09T09:51:42.3333847Z at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2021-04-09T09:51:42.3334886Z at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)2021-04-09T09:51:42.3335978Z at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)2021-04-09T09:51:42.3336866Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)2021-04-09T09:51:42.3337874Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)2021-04-09T09:51:42.3338622Z ... 4 more2021-04-09T09:51:42.3339740Z Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID eb55ab4ea4591c142078631839e1216f and allocation ID 802cf7ad875c5d2c3220b55a32b00401.2021-04-09T09:51:42.3340997Z at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:593)2021-04-09T09:51:42.3341839Z at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)2021-04-09T09:51:42.3342854Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-04-09T09:51:42.3343581Z at java.lang.reflect.Method.invoke(Method.java:498)2021-04-09T09:51:42.3344553Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)2021-04-09T09:51:42.3374800Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)2021-04-09T09:51:42.3375800Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)2021-04-09T09:51:42.3376525Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2021-04-09T09:51:42.3377185Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2021-04-09T09:51:42.3377950Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2021-04-09T09:51:42.3378595Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2021-04-09T09:51:42.3379277Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2021-04-09T09:51:42.3379924Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-04-09T09:51:42.3380572Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-04-09T09:51:42.3381245Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2021-04-09T09:51:42.3381877Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2021-04-09T09:51:42.3382769Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2021-04-09T09:51:42.3383336Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2021-04-09T09:51:42.3383880Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2021-04-09T09:51:42.3384422Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2021-04-09T09:51:42.3384921Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2021-04-09T09:51:42.3385319Z ... 4 more</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManagerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22191" opendate="2021-4-11 00:00:00" fixdate="2021-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlinkStreamUserDefinedFunctionTests.test_udf_in_join_condition_2 fail due to NPE</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16326&amp;view=logs&amp;j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&amp;t=f5211ead-5e53-5af8-f827-4dbf08df26bb&amp;l=21130</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.PythonSharedResources.java</file>
    </fixedFiles>
  </bug>
  <bug id="22217" opendate="2021-4-12 00:00:00" fixdate="2021-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quotes around job names</summary>
      <description>Quotes could be neat here:Starting execution of job State machine job [..]</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="22227" opendate="2021-4-12 00:00:00" fixdate="2021-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job name, Job ID and receiving Dispatcher should be logged by the client</summary>
      <description>Surprisingly we don't log for job submission where we submit them to or what the job ID/name is.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22232" opendate="2021-4-12 00:00:00" fixdate="2021-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve test coverage for network stack</summary>
      <description>This is a clone of FLINK-20103 to improve test coverage in the shorter term and target specifically network stack.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.ExpectedTestException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.logger.NetworkActionsLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.RecoveredChannelStateHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22236" opendate="2021-4-12 00:00:00" fixdate="2021-4-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document opt-in behavior of flamegraph</summary>
      <description>The flamegraph feature added in FLINK-13550 must be explicitly enabled, but this isn't documented.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.flame.graphs.md</file>
    </fixedFiles>
  </bug>
  <bug id="22243" opendate="2021-4-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reactive Mode parallelism changes are not shown in the job graph visualization in the UI</summary>
      <description>As reported here FLINK-22134, the parallelism in the visual job graph on top of the detail page is not in sync with the parallelism listed in the task list below, when reactive mode causes a parallelism change.</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22244" opendate="2021-4-12 00:00:00" fixdate="2021-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify Reactive Mode documentation wrt to timeouts</summary>
      <description>In the release testing of Reactive Mode (FLINK-22134) we found that the documentation of the timeouts needs some clarification.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="2225" opendate="2015-6-15 00:00:00" fixdate="2015-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Erroneous scheduling of co-located tasks</summary>
      <description>While running the ConnectedComponents example on a testing cluster, I encountered spurious failures of the job execution. The error was that the scheduler could sometimes not schedule co-located tasks because it did not find any suitable slots for deployment. This error might be related to FLINK-1952 where a similar error was observed. The error might have been reintroduced by FLINK-2183 which fixed another scheduling error.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotSharingGroupAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Slot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Instance.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22252" opendate="2021-4-13 00:00:00" fixdate="2021-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backquotes are not rendered correctly in config option descriptions</summary>
      <description>This is how the config options are rendered in the 1.13 docs:`none`, `off`, `disable`: No restart strategy.`fixeddelay`, `fixed-delay`: Fixed delay restart strategy. More details can be found here.`failurerate`, `failure-rate`: Failure rate restart strategy. More details can be found here..https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/#state-backendHere's the rendering in the old docs.https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#restart-strategy</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.description.HtmlFormatter.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.stream.pipeline.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.security.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.restart.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.history.server.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.fixed.delay.restart.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.failure.rate.restart.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.exponential.delay.restart.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.security.ssl.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.state.backends.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.checkpointing.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.blob.server.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22255" opendate="2021-4-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>AdaptiveScheduler improvements/bugs</summary>
      <description>This ticket collects the improvements/bugs for the AdaptiveScheduler.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="22266" opendate="2021-4-14 00:00:00" fixdate="2021-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden JobMasterStopWithSavepointITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16451&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=f508e270-48d6-5f1e-3138-42a17e0714f0&amp;l=3884[ERROR] throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase) Time elapsed: 0.154 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithoutRestartsHelper(JobMasterStopWithSavepointITCase.java:154) at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(JobMasterStopWithSavepointITCase.java:138) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StopWithSavepointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.RestartingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.FailingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CancelingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
    </fixedFiles>
  </bug>
  <bug id="22273" opendate="2021-4-14 00:00:00" fixdate="2021-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for General Python Group Window Aggregation in Python Table API</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="22286" opendate="2021-4-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix picklebytescoder doesn&amp;#39;t support custom python class</summary>
      <description>Currently picklebytescoder use pickle to serialize data, which won't work when the data is Python custom class.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="22289" opendate="2021-4-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update JDBC XA sink docs</summary>
      <description>According to https://issues.apache.org/jira/browse/FLINK-22141?focusedCommentId=17321934&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17321934(except code changes) cc:ym</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="22297" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perform early check to ensure that the length of the result is the same as the input for Pandas UDF</summary>
      <description>For Pandas UDF, the input type for each input argument is Pandas.Series and the result type is also of type Pandas.Series. Besides, the length of the result should be the same as the inputs. If this is not the case, currently the behavior is unclear. We should perform early check for this and provide a clear error message.See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-Vectorized-UDF-throws-NullPointerException-td42952.html and http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  <bug id="22298" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The ExecNode&amp;#39;s id should always start from 1 in json plan test</summary>
      <description>The ExecNode's id should always start from 1 in json plan test, which could make the test more stable.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testRegisterByClass.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
    </fixedFiles>
  </bug>
  <bug id="22301" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Statebackend and CheckpointStorage type is not shown in the Web UI</summary>
      <description>Currently we have already have the Rest API /jobs/:jobid/checkpoints/config that returns the statebackend name and checkpoint storage type:{ "mode":"exactly_once", "interval":20, "timeout":600000, "min_pause":0, "max_concurrent":1, "externalization":{ "enabled":false, "delete_on_cancellation":true }, "state_backend":"EmbeddedRocksDBStateBackend", "checkpoint_storage":"FileSystemCheckpointStorage", "unaligned_checkpoints":false, "tolerable_failed_checkpoints":0}But the two fields does not shown in the Web UI of checkpoint configuration:</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
    </fixedFiles>
  </bug>
  <bug id="22302" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restructure SQL "Queries" documentation into one page per operation</summary>
      <description>Currently, the "Queries" page has been very large and it's getting longger when we supporting more features. We already have separate pages for Joins and CEP. I would propose to separate "Queries" into one page per operation. This way we can easily add more detailed informations for the operations and more examples.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.drop.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.versioned.tables.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.match.recognize.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.joins.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.md</file>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.docs.dev.table.sql.hints.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.joins.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.docs.dev.table.tuning.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries..index.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.set-ops.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.group-agg.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.deduplication.md</file>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.docs.dev.table.sql.drop.md</file>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tuning.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.set-ops.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.group-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.deduplication.md</file>
    </fixedFiles>
  </bug>
  <bug id="22305" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve log messages of sort-merge blocking shuffle</summary>
      <description>Currently, the default value of taskmanager.network.sort-shuffle.min-buffers is 64, which is pretty small. As suggested, we'd like to increase the default value of taskmanager.network.sort-shuffle.min-buffers. By increasing the default taskmanager.network.sort-shuffle.min-buffers, the corner case of very small in-memory sort-buffer and write-buffer can be avoid, which is better for performance.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="22307" opendate="2021-4-16 00:00:00" fixdate="2021-4-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Increase the default value of data writing cache size (not configurable) for sort-merge blocking shuffle</summary>
      <description>Currently, the data writing cache is 8M, which is not enough if data compression is enabled. By increasing the cache size to 16M, the performance of our benchmark job can be increased by about 20%. (We may make it configurable in the future)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="2231" opendate="2015-6-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a Serializer for Scala Enumerations</summary>
      <description>Scala Enumerations are currently serialized with Kryo, but should be efficiently serialized by just writing the initial.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.ScalaSpecialTypesSerializerTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeDescriptors.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeAnalyzer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="22311" opendate="2021-4-16 00:00:00" fixdate="2021-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink JDBC XA connector need to set maxRetries to 0 to properly working</summary>
      <description>Hi,We're using XA connector from Flink 1.13 in one of our projects and we were able to create duplicates of records during write to Oracle.The reason was that default MAX_RETRIES in JdbcExecutionOptions is 3 and this can cause duplicates in DB.I think we should at least mention this in docs or even validate this option when creating XA Sink.In documentation we're using defaults.https://github.com/apache/flink/pull/10847/files#diff-a585e56c997756bb7517ebd2424e5fab5813cee67d8dee3eab6ddd0780aff627R88</description>
      <version>1.13.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="22315" opendate="2021-4-16 00:00:00" fixdate="2021-12-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support ADD column/constraint/watermark for ALTER TABLE statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Schema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableSchemaOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableAddConstraintOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableSchema.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableModify.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="22316" opendate="2021-4-16 00:00:00" fixdate="2021-12-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support MODIFY column/constraint/watermark for ALTER TABLE statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
    </fixedFiles>
  </bug>
  <bug id="22318" opendate="2021-4-16 00:00:00" fixdate="2021-12-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support RENAME column name for ALTER TABLE statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.AlterSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterTableRenameColumn.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="22335" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Increase default resource wait timeout for the adaptive scheduler</summary>
      <description>As discussed in FLINK-22135, the current default value 10s for jobmanager.adaptive-scheduler.resource-wait-timeout is too short and can easily lead to job failures when working with active resource managers. We'd like to increase the default value to 5min, aligning with slot.request.timeout.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22338" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear watermark output when test finished for FromElementSourceFunctionWithWatermark</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
    </fixedFiles>
  </bug>
  <bug id="22341" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table doesn&amp;#39;t work with hive dialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22345" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CoordinatorEventsExactlyOnceITCase hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16731&amp;view=logs&amp;j=02c4e775-43bf-5625-d1cc-542b5209e072&amp;t=e5961b24-88d9-5c77-efd3-955422674c25&amp;l=9896"main" #1 prio=5 os_prio=0 tid=0x00007fa8c800b800 nid=0x58b3 waiting on condition [0x00007fa8cfd1c000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x000000008147a7e8&gt; (a java.util.concurrent.CompletableFuture$Signaller) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:802) at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:187) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="22348" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Python operators of Python DataStream API doesn&amp;#39;t use managed memory in execute_and_collect</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="22349" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LOCALTIMESTAMP doesn&amp;#39;t return correct result when setting session time zone to &amp;#39;UTC-10:00&amp;#39;</summary>
      <description>set execution.runtime-mode = batch;set sql-client.execution.result-mode = tableau;set table.local-time-zone = 'UTC-10:00';select LOCALTIME, CURRENT_DATE, LOCALTIMESTAMP, CURRENT_TIME, CURRENT_TIMESTAMP, NOW(), CURRENT_ROW_TIMESTAMP(), PROCTIME()from (values (1), (2), (3), (4), (5)) as T(a);+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+| LOCALTIME | CURRENT_DATE | LOCALTIMESTAMP | CURRENT_TIME | CURRENT_TIMESTAMP | EXPR$5 | EXPR$6 | EXPR$7 |+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+| 09:00:58.905 | 2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 || 09:00:58.905 | 2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 || 09:00:58.905 | 2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 || 09:00:58.905 | 2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 || 09:00:58.905 | 2021-04-19 | 2021-04-19 09:00:58.905 | 09:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:58.905 | 2021-04-18 23:00:59.033 | 2021-04-18 23:00:59.033 |+--------------+--------------+-------------------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+5 rows in set</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.TimeWindowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.TumblingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.util.python.PythonTableUtilsTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableConfigTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="22350" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the doc of user-defined window in Python DataStream API</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.md</file>
    </fixedFiles>
  </bug>
  <bug id="22352" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Mesos Support in documentation</summary>
      <description>According to the discussion on the dev mailing list ([SURVEY] Remove Mesos support and [VOTE] Deprecating Mesos support), the community decided to deprecate Mesos support in Apache Flink (see (RESULT)[VOTE]Deprecating Mesos support).This issue covers adding corresponding warnings/notices: Log statements - Mesos-related ClusterEntrypoint implementations Warning in the docs @Deprecated in relevant classes</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosResourceAllocation.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosConfiguration.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServerImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactResolver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.StandaloneMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.ZooKeeperMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.StandaloneMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServicesUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.AbstractMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.RegisteredMesosWorkerNode.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosWorkerResourceSpecFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactoryImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActions.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs.content.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="22354" opendate="2021-4-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to define watermark on computed column of CURRENT_TIMESTAMP and LOCALTIMESTAMP</summary>
      <description>It is very common to define watermarks on localtimestamp and current_tiemstamp to support ingestion time. However, the following DDLs failed in v1.13. I also tested the following DDLs can pass in v1.12.1, so this is a regression. The root cause may be introduced by FLINK-21435 which adds a strict check to only allow precision = 3 (precision = 0 should also can be time attribute), however the precision of return type of current_timestamp and localtimestamp are 0 (another long-lived bug). We should fix them both. Flink SQL&gt; create table datagen_source (&gt; order_number BIGINT,&gt; price int,&gt; buyer string,&gt; ts as current_timestamp,&gt; proctime as proctime(),&gt; watermark for ts as ts - interval '0.001' second&gt; ) with (&gt; 'connector' = 'datagen',&gt; 'rows-per-second' = '1'&gt; );[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Invalid data type of time field for watermark definition. The field must be of type TIMESTAMP(3) or TIMESTAMP_LTZ(3).Flink SQL&gt; create table datagen_source (&gt; order_number BIGINT,&gt; price int,&gt; buyer string,&gt; ts as cast(current_timestamp as timestamp_ltz(3)),&gt; proctime as proctime(),&gt; watermark for ts as ts - interval '0.001' second&gt; ) with (&gt; 'connector' = 'datagen',&gt; 'rows-per-second' = '1'&gt; );[INFO] Execute statement succeed.Flink SQL&gt; create table datagen_source2 (&gt; order_number BIGINT,&gt; price int,&gt; buyer string,&gt; ts as localtimestamp,&gt; proctime as proctime(),&gt; watermark for ts as ts - interval '0.001' second&gt; ) with (&gt; 'connector' = 'datagen',&gt; 'rows-per-second' = '1'&gt; );[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Invalid data type of time field for watermark definition. The field must be of type TIMESTAMP(3) or TIMESTAMP_LTZ(3).</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlCurrentTimestampFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.java</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PrintUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TimestampStringUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22358" opendate="2021-4-19 00:00:00" fixdate="2021-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing stability annotation to Split Reader API classes</summary>
      <description>The Split Reader API currently has no stability annotations, it is unclear which classes are public API, which are internal, which are stable, and which are evolving.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.utils.SerdeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitsChange.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitsAddition.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderOptions.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsWithSplitIds.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsBySplits.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTask.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.FetchTask.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="22368" opendate="2021-4-20 00:00:00" fixdate="2021-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnalignedCheckpointITCase hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&amp;l=10144</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="22378" opendate="2021-4-20 00:00:00" fixdate="2021-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type mismatch when declaring SOURCE_WATERMARK on TIMESTAMP_LTZ column</summary>
      <description>The following schema cannot be resolved currently:Schema.newBuilder() .columnByMetadata("rowtime", DataTypes.TIMESTAMP_LTZ(3)) .watermark("rowtime", "SOURCE_WATERMARK()") .build()It leads to:The watermark output type TIMESTAMP(3) is different from input time filed type TIMESTAMP_LTZ(3).</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sink.OutputConversionOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.ParserImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.PlannerMocks.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.ParserImplTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.abilities.source.FilterPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.CallExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.ParserImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.DefaultParserFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.SqlExprToRexConverterFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.SqlExprToRexConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.DynamicTableSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ParserMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExpressionResolverMocks.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.utils.ValuesOperationTreeBuilderTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.resolver.ExpressionResolverTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogBaseTableResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.SqlExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveSqlCallRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Parser.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="22386" opendate="2021-4-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce cache for docker images</summary>
      <description>Tests that use testcontainers occassionally fail when downloading images from docker hub. It would be nice to have a caching local proxy to the docker hub.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="22396" opendate="2021-4-21 00:00:00" fixdate="2021-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in sql hbase-1.4 connector NOTICE file</summary>
      <description>The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle: commons-configuration:commons-configuration:1.7 org.apache.hbase:hbase-prefix-tree:1.4.3 org.apache.hbase:hbase-procedure:1.4.3</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2240" opendate="2015-6-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use BloomFilter to minimize probe side records which are spilled to disk in Hybrid-Hash-Join</summary>
      <description>In Hybrid-Hash-Join, while small table does not fit into memory, part of the small table data would be spilled to disk, and the counterpart partition of big table data would be spilled to disk in probe phase as well. If we build a BloomFilter while spill small table to disk during build phase, and use it to filter the big table records which tend to be spilled to disk, this may greatly reduce the spilled big table file size, and saved the disk IO cost for writing and further reading.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingHashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingHashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.MutableHashTablePerformanceBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.TestTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildSecondHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReusingBuildFirstHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildSecondHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="22407" opendate="2021-4-22 00:00:00" fixdate="2021-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump log4j to 2.14.1</summary>
      <description>Flink is currently relying on log4j 2.12.1 .Unfortunately, this vesrion has a bug related to json layout that prevents a user from adding additional log fields, as reported here https://issues.apache.org/jira/browse/LOG4J2-2652 as well as here: https://stackoverflow.com/questions/57003440/why-is-log4j2-jsonlayout-keyvaluepair-printing-empty-logevent-messagesThe problem is fixed in Log4j 2.13.1.Is there a good reason to keep Log4j 2.12.1, or can we upgrade?As an illustration, the presence of  additional1 in the snippet below: rootLogger.level = INFOrootLogger.appenderRef.console.ref = LogConsole appender.console.name = LogConsoleappender.console.type = CONSOLEappender.console.layout.type = JsonLayoutappender.console.layout.complete = falseappender.console.layout.compact = trueappender.console.layout.eventEol = trueappender.console.layout.properties = trueappender.console.layout.includeStacktrace=trueappender.console.layout.stacktraceAsString=true appender.console.layout.additional1.type=KeyValuePairappender.console.layout.additional1.key=timestampappender.console.layout.additional1.value=$${date:yyyy-MM-dd'T'HH:mm:ss.SSSZ}  leads to missing fields in the resulting logs, e.g.:{"logEvent":"Recover all persisted job graphs.","timestamp":"2021-04-21T16:50:31.722+0000"}{"logEvent":"Successfully recovered 0 persisted job graphs.","timestamp":"2021-04-21T16:50:31.723+0000"}{"logEvent":"Starting the SlotManager.","timestamp":"2021-04-21T16:50:31.732+0000"}{"logEvent":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","timestamp":"2021-04-21T16:50:31.822+0000"}  Removing the additional1 resolves the issue and yield json logs containing all expected fields:{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess","message":"Successfully recovered 0 persisted job graphs.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":216868000},"threadId":48,"contextMap":{},"threadPriority":5}{"thread":"flink-akka.actor.default-dispatcher-3","level":"INFO","loggerName":"org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl","message":"Starting the SlotManager.","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":313130000},"threadId":19,"contextMap":{},"threadPriority":5}{"thread":"cluster-io-thread-1","level":"INFO","loggerName":"org.apache.flink.runtime.rpc.akka.AkkaRpcService","message":"Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .","endOfBatch":false,"loggerFqcn":"org.apache.logging.slf4j.Log4jLogger","instant":{"epochSecond":1619080838,"nanoOfSecond":408714000},"threadId":48,"contextMap":{},"threadPriority":5}   </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="22413" opendate="2021-4-22 00:00:00" fixdate="2021-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide Checkpointing page in the UI for batch jobs</summary>
      <description>This is a follow up to https://github.com/apache/flink/commit/8dca9fa852c72984ac873eae9a96bbd739e502f3#commitcomment-49744255Batch jobs don't need a Checkpointing page, because there is no information for these jobs available there.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.JsonJobGraphGenerationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonGeneratorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
    </fixedFiles>
  </bug>
  <bug id="22471" opendate="2021-4-26 00:00:00" fixdate="2021-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use formatted descriptions for connector options</summary>
      <description>The documentation for connector options is currently not yet generated. This leads to a duplication, and in general there are deviations between the documentation and the description of the &lt;ConfigOption&gt;s.Importantly, these descriptions are in many places not suitable for the automatic generation due to hard-coded linebreaks, and in general not making use of the Description/Formatter infrastructure yet.A lot of them also duplicate things like "required" or default values in the description.</description>
      <version>1.12.2,1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptions.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="22472" opendate="2021-4-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The real partition data produced time is behind meta(_SUCCESS) file produced</summary>
      <description>I test write some data to csv file by flink filesystem connector, but after the success file produced, the data file is still un-committed, it's very weird to me.bang@mac db1.db $ll /var/folders/55/cw682b314gn8jhfh565hp7q00000gp/T/junit8642959834366044048/junit484868942580135598/test-partition-time-commit/d\=2020-05-03/e\=12/total 8drwxr-xr-x 4 bang staff 128 4 25 19:57 ./drwxr-xr-x 8 bang staff 256 4 25 19:57 ../-rw-r--r-- 1 bang staff 12 4 25 19:57 .part-b703d4b9-067a-4dfe-935e-3afc723aed56-0-4.inprogress.b7d9cf09-0f72-4dce-8591-b61b1d23ae9b-rw-r--r-- 1 bang staff 0 4 25 19:57 _MY_SUCCESS After some debug I found I have to set  sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval parameters, the default value of the two parameters is pretty big(128M and 30min). It's not convenient for test/demo. I think we can improve this. As the doc&amp;#91;1&amp;#93; described, for row formats (csv, json), you can set the parameter sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval in the connector properties and parameter execution.checkpointing.interval in flink-conf.yaml together if you don’t want to wait a long period before observe the data exists in file system.&amp;#91;1&amp;#93; https://ci.apache.org/projects/flink/flink-docs-master/docs/connectors/table/filesystem/#rolling-policy</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.stream.StreamingFileWriterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.ProcTimeCommitTrigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.PartitionTimeCommitTrigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitTrigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.AbstractStreamingWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="2248" opendate="2015-6-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of sdtout logging output</summary>
      <description>Currently when a job is submitted through the CLI we get in stdout all the log output about each stage in the job.It would useful to have an easy way to disable this output when submitting the job, as most of the time we are only interested in the log output if something goes wrong.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="22489" opendate="2021-4-27 00:00:00" fixdate="2021-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>subtask backpressure indicator shows value for entire job</summary>
      <description>In the backpressure tab of the web UI, the OK/LOW/HIGH indication is displaying the job-level backpressure for every subtask, rather than the individual subtask values (effectively showing max back pressure from all of the subtasks of the given task for every subtask, instead of the individual values).</description>
      <version>1.9.3,1.10.3,1.11.3,1.12.2,1.13.0</version>
      <fixedVersion>1.11.4,1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="22511" opendate="2021-4-28 00:00:00" fixdate="2021-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of non-composite result type in Python TableAggregateFunction</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="22512" opendate="2021-4-28 00:00:00" fixdate="2021-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t call current_timestamp with hive dialect for hive-3.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22523" opendate="2021-4-29 00:00:00" fixdate="2021-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TUMBLE TVF should throw helpful exception when specifying second interval parameter</summary>
      <description>Currently, the following query can run and no exception is thrown. However, the second interval parameter (i.e. the offset parameter) is not supported yet. We should throw a exception for this. select date_format(window_end, 'yyyy-MM-dd') as date_str, date_format(window_end, 'HH:mm') as time_str, count(distinct user_id) as uvfrom table(tumble(table user_behavior, descriptor(ts), interval '10' minute, interval '1' day))group by window_start, window_end;</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlTumbleTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlHopTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlCumulateTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="22537" opendate="2021-4-30 00:00:00" fixdate="2021-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation how to interact with DataStream API</summary>
      <description>Add documentation for FLIP-136.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="22556" opendate="2021-5-4 00:00:00" fixdate="2021-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend license checker to scan for traces of (L)GPL licensed code</summary>
      <description>This is a follow up to FLINK-22555.The goal is to catch this and similar cases automatically.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.JarFileCheckerTest.java</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="22566" opendate="2021-5-5 00:00:00" fixdate="2021-5-5 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Running Kerberized YARN application on Docker test (default input) fails with no resources</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529&amp;l=8745May 05 01:29:04 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 120000 msMay 05 01:29:04 at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]May 05 01:29:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]May 05 01:29:04 ... 4 more</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22573" opendate="2021-5-5 00:00:00" fixdate="2021-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncIO can timeout elements after completion</summary>
      <description>AsyncIO emits completed elements over the mailbox at which any timer is also canceled. However, if the mailbox cannot process (heavy backpressure), it may be that the timer still triggers on a completed element.</description>
      <version>1.11.3,1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22581" opendate="2021-5-6 00:00:00" fixdate="2021-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keyword &amp;#39;CATALOG&amp;#39; is missing in sql client doc</summary>
      <description>Excerpt from sql client doc:```CREATE CATALOG MyCatalog WITH ( 'type' = 'hive' );USE MyCatalog;```The statement `USE MyCatalog' should be 'USE CATALOG MyCatalog'</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="22586" opendate="2021-5-6 00:00:00" fixdate="2021-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve precision derivation for decimal arithmetics</summary>
      <description>Currently the precision and scale derivation is not properly for decimal data arithmetics, e.g,considering the following example:select cast('10.1' as decimal(38, 19)) * cast('10.2' as decimal(38, 19)) from Tthe result is `null`, which may confuses use a lot, because the result is actually not that big.The root cause is the precision derivation for the above multiplication is:(38, 19) * (38, 19) -&gt; (38, 38)So there is no space for integral digits, which leads to null results. </description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeMerging.java</file>
    </fixedFiles>
  </bug>
  <bug id="2259" opendate="2015-6-22 00:00:00" fixdate="2015-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support training Estimators using a (train, validation, test) split of the available data</summary>
      <description>When there is an abundance of data available, a good way to train models is to split the available data into 3 parts: Train, Validation and Test.We use the Train data to train the model, the Validation part is used to estimate the test error and select hyperparameters, and the Test is used to evaluate the performance of the model, and assess its generalization &amp;#91;1&amp;#93;This is a common approach when training Artificial Neural Networks, and a good strategy to choose in data-rich environments. Therefore we should have some support of this data-analysis process in our Estimators.&amp;#91;1&amp;#93; Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="22590" opendate="2021-5-7 00:00:00" fixdate="2021-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scala implicit conversions for new API methods</summary>
      <description>FLINK-19980 should also be exposed through Scala's implicit conversions. To allow a fluent API such as `table.toDataStream(...)`</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DataStreamScalaITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.package.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.DataStreamConversions.scala</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="22596" opendate="2021-5-7 00:00:00" fixdate="2021-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active timeout is not triggered if there were no barriers</summary>
      <description>The condition in the active timeout timer is often incorrect, because we do not reset the allBarriersReceivedFuture on barrier announcements.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2021-5-8 00:00:00" fixdate="2021-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify the usage of SessionState</summary>
      <description>Hive SessionState is used by hive dialect. Starting a SessionState involves some heavy operations like creating scratch folders and instantiating an HMS client. We should investigate how to reduce these operations.It's of course better to completely get rid of SessionState. But that's difficult to achieve because some hive functions rely on it.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQB.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22636" opendate="2021-5-11 00:00:00" fixdate="2021-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group job specific ZooKeeper HA services under common jobs/&lt;JobID&gt; zNode</summary>
      <description>In order to better clean up Zookeeper HA services, I suggest grouping job-specific services under a common jobs/&lt;JobID&gt; zNode. That way, it becomes trivial to clean up the job-specific Zookeeper data (simply deleting the jobs/&lt;JobID&gt; node.Currently, our Zookeeper structure is not really structured well. The current layout looks like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; &lt;job-id&gt; -&gt; checkpoint-1 -&gt; checkpoint-counter -&gt; &lt;job-id&gt; -&gt; counter -&gt; leaderlatch -&gt; dispatcher_lock -&gt; resourc_emanager_lock -&gt; &lt;job-id&gt; -&gt; leader -&gt; dispatcher_lock -&gt; resource_manager_lock -&gt; &lt;job-id&gt;The new layout could look like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; jobs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; checkpoint-1 -&gt; checkpoint_id_counter -&gt; counter -&gt; leader -&gt; latch -&gt; connection_info -&gt; leader -&gt; dispatcher -&gt; latch -&gt; connection_info -&gt; resource_manager -&gt; latch -&gt; connection_info</description>
      <version>1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.zk.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22640" opendate="2021-5-12 00:00:00" fixdate="2021-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataGen SQL Connector does not support defining fields min/max option of decimal type field</summary>
      <description>When defining fields' min/max option of decimal type field will fail to create DataGen SQL Connector.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22655" opendate="2021-5-13 00:00:00" fixdate="2021-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When using -i &lt;init.sql&gt; option to initialize SQL Client session It should be possible to annotate the script with --</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliStatementSplitterTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStatementSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22683" opendate="2021-5-17 00:00:00" fixdate="2021-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The total Flink/process memory of memoryConfiguration in /taskmanagers can be null or incorrect value</summary>
      <description>In FLINK-14435, we add the `memoryConfiguration` to /taskmanagers REST API. However, that field can be `null` if it is not configured by the user or the incorrect value in fine-grained resource management. We need to calculate them proactively.</description>
      <version>1.13.0,1.12.4</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="22725" opendate="2021-5-20 00:00:00" fixdate="2021-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SlotManagers should unregister metrics at the start of suspend()</summary>
      <description>Slotmanagers register metrics in start(), but only unregister them in close().This has 2 issues:a) If the SM is restarted it cannot re-register the metrics because the old ones are still present; this isn't a critical problem (because the old ones still work) but it produces unnecesasry logging noiseb) The metric may produce an NPE when accessed by a reporter during suspend().</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22726" opendate="2021-5-20 00:00:00" fixdate="2021-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive GROUPING__ID returns different value in older versions</summary>
      <description>Hive's GROUPING__ID changed since 2.3.0. In current implementation we use Calcite's GROUPING_ID operator which yields same value as hive &gt;= 2.3.0. When integrate with older hive, we should follow the previous behavior.Refer to:https://issues.apache.org/jira/browse/HIVE-16102https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup#EnhancedAggregation,Cube,GroupingandRollup-Grouping__IDfunction(beforeHive2.3.0)</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.query-test.grouping.set.q</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="22733" opendate="2021-5-21 00:00:00" fixdate="2021-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API</summary>
      <description>See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html for more details.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.1,1.12.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="22769" opendate="2021-5-25 00:00:00" fixdate="2021-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>yarnship do not support symbolic directory</summary>
      <description>If we pass `-yt ` a symbolic directory, we will get an exception:Please assign to me, I spent a whole day working on it, and I'd like to be a contributor (already implemented by me).</description>
      <version>1.12.2,1.13.0,1.13.1</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationFileUploader.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FileUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22774" opendate="2021-5-25 00:00:00" fixdate="2021-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kinesis SQL connector&amp;#39;s Guava to 27.0-jre</summary>
      <description>`flink-coonector-kinesis` uses a very old version (18.0) for compatibility reasons. However, since we don't expose Guava and relocate it in SQL connectors, we can use a newer version and avoid security concerns raised by vulnerability tools.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22778" opendate="2021-5-26 00:00:00" fixdate="2021-5-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade to JUnit 4.13</summary>
      <description>The latest JUnit4 release has useful improvements and fixes: https://github.com/junit-team/junit4/blob/HEAD/doc/ReleaseNotes4.13.md</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22780" opendate="2021-5-26 00:00:00" fixdate="2021-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 25.05</summary>
      <description>Tests such as: multiInputMapSink multiInputOneIdleMapSink readFileSplitshow regressions.Regression in run for range: 80ad5b3b51-bb597ea-1621977169It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
    </fixedFiles>
  </bug>
  <bug id="22789" opendate="2021-5-27 00:00:00" fixdate="2021-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.3.7</summary>
      <description>flink support for Hive version 2.3.7 releases.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="22795" opendate="2021-5-28 00:00:00" fixdate="2021-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw better exception when executing remote SQL file in SQL Client</summary>
      <description>hi, allWhen I executed following command in Flink 1.13bin/sql-client.sh -f hdfs:/user/test.sqlThe exception is unclear to me:  Exception in thread "main" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.  at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)  at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)  Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Fail to read content from the /opt/flink-1.13.0/hdfs:/user/test.sql.  at org.apache.flink.table.client.SqlClient.readFromURL(SqlClient.java:250)  at org.apache.flink.table.client.SqlClient.readExecutionContent(SqlClient.java:239)  at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:153)  at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)  at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)  ... 1 more  Caused by: java.io.FileNotFoundException: /opt/flink-1.13.0/hdfs:/user/test.sql (No such file or directory)  at java.io.FileInputStream.open0(Native Method)  at java.io.FileInputStream.open(FileInputStream.java:195)  at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)  at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:93)  at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)  at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)  at java.net.URL.openStream(URL.java:1045)  at org.apache.commons.io.IOUtils.toString(IOUtils.java:2764)  at org.apache.flink.table.client.SqlClient.readFromURL(SqlClient.java:247)  ... 5 more Shutting down the session...  done.I think we should improve the exception message at first, and then it's great if we can support load file from HDFS.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="22802" opendate="2021-5-28 00:00:00" fixdate="2021-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Fabric8 Kubernetes Client to &gt;= 5.X</summary>
      <description>Hi All, Currently, Flink is using version 4.9.2 of the Fabric8 Kubernetes Client which does not support new versions of the Kubernetes API such as 1.19 or 1.20 as pointed out by their Compatibility Matrix which is support in 5.4.0 or above.As far as I have seen in the Flink documentation, Flink supports `Kubernetes &gt;= 1.9.` but due to this dependency, it might not be the case. Is there a plan to update this dependency?What is the plan moving forwards when new versions of Kubernetes are released?I am raising this because I have been testing Flink HA Session Cluster on Kubernetes 1.19 and 1.20 and I have encountered some frequent errors that force the JM pods to restart or even result in an unrecoverable state.Thanks!</description>
      <version>1.13.0,1.13.1,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedDispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.KubernetesSharedWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-session.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22808" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log metadata</summary>
      <description>In FLINK-21355, logging state changes was added. This ticket is about storing metadata in log so that changes can be applied on recovery (spun off from FLINK-21355 to ease review).</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PriorityQueueStateChangeLoggerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.KvStateChangeLoggerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.AbstractStateChangeLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="22810" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Elasticsearch modules</summary>
      <description>Remove references to flink-table-planner in all Elasticsearch modules.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22811" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Avro module</summary>
      <description>Remove references to flink-table-planner in the Avro module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.table.runtime.batch.AvroTypesITCase.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22812" opendate="2021-5-31 00:00:00" fixdate="2021-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hive issues in sql-connector-hive jars</summary>
      <description>Since we provide uber jars for hive connector, this gives us a chance to fix some hive issues on our side.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22813" opendate="2021-5-31 00:00:00" fixdate="2021-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Hive module</summary>
      <description>Remove references to flink-table-planner in the Hive module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22864" opendate="2021-6-3 00:00:00" fixdate="2021-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the legacy planner code base</summary>
      <description>This removes the legacy planner code base. In particular, it removes the content of flink-table-planner.An equally named module will be reintroduced in a later subtask and will contain the content of flink-table-planner-blink which is the one and only planner from Flink 1.14 onwards.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.SelectTableSinkSchemaConverter.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testStreamTableEnvironmentExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testStatementSetExecutionExplain1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testStatementSetExecutionExplain0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testStatementSet1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testStatementSet0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testSqlUpdateAndToDataStream.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testMultipleInserts1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testMultipleInserts.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testJoin1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testJoin0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testInsert1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testInsert.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testFromToDataStreamAndSqlUpdate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testFilterStream0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testFilter1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testFilter0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExplainSqlWithSelect1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExplainSqlWithSelect0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExplainSqlWithInsert1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExplainSqlWithInsert0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainSelect1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainSelect0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainInsert1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testExecuteSqlWithExplainInsert0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.resources.testBatchTableEnvironmentExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.util.python.PythonTableUtilsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.testTableSinks.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TestFilterableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TestContextTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.LogicalPlanFormatUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.InputTypeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.InMemoryTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.typeutils.TypeCheckUtilsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsCollectionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsClusterTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.StreamITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.SortTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.CommonTestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.types.CRowSerializerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.types.CRowComparatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.RetractionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessOperatorWithWatermarkDelayTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.operators.KeyedCoProcessOperatorWithWatermarkDelayTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.SortProcessFunctionHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.MatchHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.BaseTwoInputStreamOperatorWithStateRetentionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.AggFunctionHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.SumWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.SumAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.Sum0WithRetractAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.Sum0AggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.CountAggFunctionWithNonParamTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.CountAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.CollectAggFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.AvgFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.SplitPythonConditionFromJoinRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.SplitPythonConditionFromCorrelateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramRewriterTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCorrelateSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.NormalizationRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.CalcPythonCorrelateTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestWildcardFormatTableSourceFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestTableSourceFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestTableFormatFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestTableFormat.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestSerializationSchema.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestFixedFormatTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestDeserializationSchema.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.utils.TestAmbiguousTableFormatFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.TableSourceFactoryServiceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.factories.TableFormatFactoryServiceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.ScalarOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.RowTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.CompositeAccessValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.RowTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ArrayTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.LiteralTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.KeywordParseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.DateTimeFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.TestTableDescriptor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.TableDescriptorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.SchemaValidatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.SchemaTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.OldCsvTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.LiteralValueTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.FunctionDescriptorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.FileSystemTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.ClassInstanceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.dataview.MapViewSerializerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.dataview.ListViewSerializerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.FlinkTypeFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSchemaValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.InlineTableValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.ColumnFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableAggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowTableAggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.TableAggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.SetOperatorsStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.GroupWindowTableAggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.GroupWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.ColumnFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.SortValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.TableFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SortTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.MatchRecognizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.SortValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SortStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.JoinStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.ColumnFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.SortValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.map-view-serializer-1.12.test-data</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.map-view-serializer-1.12.serializer-snapshot</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.map-view-serializer-1.11.test-data</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.map-view-serializer-1.11.serializer-snapshot</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.logback-test.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.list-view-serializer-1.12.test-data</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.list-view-serializer-1.12.serializer-snapshot</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.list-view-serializer-1.11.test-data</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.list-view-serializer-1.11.serializer-snapshot</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.types.python.ExamplePointUserDefinedType.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaStreamTestData.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaPojos.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.JavaTableSourceITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.factories.utils.TestLegacyCatalogFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.factories.CatalogFactoryServiceTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.dataview.MapViewSerializerUpgradeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.dataview.ListViewSerializerUpgradeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.ViewExpansionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.TestExternalTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.PathResolutionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.DatabaseCalciteSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogManagerTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.StreamTableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.validate.ValidationResult.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.MatchUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.Logging.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.JavaScalaConversionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.DummyNoOpOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.DummyExecutionEnvironment.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.types.CRowTypeInfo.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.types.CRowSerializer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.types.CRowComparator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.types.CRow.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.triggers.StateCleaningCountTrigger.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.TableFunctionCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.TableAggregateCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.RowtimeProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.RowKeySelector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.OutputRowtimeProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.outerJoinRunners.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.outerJoinGroupReduceRunners.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.operators.KeyedProcessOperatorWithWatermarkDelay.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.operators.KeyedCoProcessOperatorWithWatermarkDelay.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.MinusCoGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.match.PatternProcessFunctionRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.match.EventRowComparator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.MapSideJoinRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.MapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.MapJoinRightRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.MapJoinLeftRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.LimitFilterFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.TimeBoundedStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.TemporalRowtimeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.TemporalProcessTimeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.RowTimeBoundedStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.ProcTimeBoundedStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.OuterJoinPaddingUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowInnerJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.EmitAwareCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.CRowWrappingMultiOutputCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.CountingCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.join.BaseTwoInputStreamOperatorWithStateRetention.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.io.ValuesInputFormat.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.io.CRowValuesInputFormat.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.IntersectCoGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.functions.DateTimeFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.FlatMapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.FlatJoinRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowWrappingCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowProcessRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowOutputProcessRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowMapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowKeySelector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CRowCorrelateProcessRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CountPartitionFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.CorrelateFlatMapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.conversion.CRowToScalaTupleMapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.conversion.CRowToScalaTupleMapFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.conversion.CRowToRowMapFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.conversion.CRowToJavaTupleMapRunner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.conversion.CRowToJavaTupleMapFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeSortProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeSortProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRowsOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.KeyedProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupTableAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GeneratedAggregations.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DistinctReduce.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggMapFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideTimeWindowAggFlatMapFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregatePreProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetPreAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetFinalAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.CoProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.CleanupState.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramRewriter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexDefaultVisitor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.TreeNode.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.StreamOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.RowSchema.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.MultisetRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.MapRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.GenericRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.FlinkTableFunctionImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.CompositeRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.ArrayRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushProjectIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.DecomposeGroupingSetRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamValuesRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamTableAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamRetractionRules.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamGroupWindowTableAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetValuesRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetMinusRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetIntersectRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetDistinctRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.BatchTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.WindowAggregateReduceFunctionsRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.common.ConvertToNotInOrInRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.Optimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalValues.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalUnion.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTemporalTableJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalOverWindow.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalMinus.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalIntersect.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalDataSetScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.LogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.FlinkConventions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.retractionTraits.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.retractionTraitDefs.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamValues.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalTableJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalJoinToCoProcessTranslator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamSort.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoinToCoProcessTranslator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetValues.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSingleRowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetMinus.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetIntersect.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetDistinct.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonSort.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonMatchRecognize.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.TableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTemporalTableJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.MatchRecognize.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.FlinkDefaultRelMetadataProvider.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.DataSetCostFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.DataSetCost.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.BatchOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.DataStreamConversions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.Conversions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.PlannerQueryOperation.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.sql.StreamRecordTimestampSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.sql.ProctimeSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumWithRetractAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.Sum0WithRetractAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.Ordering.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.CollectAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.windowProperties.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.symbols.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpression.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.overOffsets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.ordering.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.logic.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.InputTypeSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.hashExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.ExpressionBridge.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.composite.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.comparison.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.collection.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.cast.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.dataview.StateMapView.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.dataview.StateListView.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.Indenter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.Compiler.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenException.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.TrimCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.TimestampDiffCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.RandCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.NotCallGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.MultiTypeMethodCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.MethodCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.HashCalcCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.DateFormatCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.CurrentTimePointCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ConstantCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.CallGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.BasicOperatorTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelOptClusterFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilderFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.exceptions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.dataview.DataViewSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.bridge.scala.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.bridge.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.janino</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.icu4j</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.util.DummyStreamExecutionEnvironment.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.shaded.dependencies.sh</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.jdbc.CalciteSchemaBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalSnapshot.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalTableScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rex.RexLiteral.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rex.RexSimplify.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.AuxiliaryConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.SqlGroupedWindowFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.api.bridge.java.package-info.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CatalogCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CatalogManagerCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.QueryOperationCatalogViewTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.executor.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.executor.StreamExecutorFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.explain.Node.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.explain.PlanJsonParser.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.PlannerTypeInferenceUtilImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.AbstractRegexParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.ClearOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.ExtendedParser.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.ExtendedParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.HelpOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.QuitOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.ResetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.ParserImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.StreamPlannerFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.AbstractPythonCorrelateRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.batch.DataSetPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.CalcPythonCorrelateTransposeRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.FlinkFilterJoinRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.stream.DataStreamPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.runtime.types.CRowSerializerSnapshot.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.BatchSelectTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="22906" opendate="2021-6-7 00:00:00" fixdate="2021-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add build time to Flink documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.zh..index.md</file>
    </fixedFiles>
  </bug>
  <bug id="22934" opendate="2021-6-9 00:00:00" fixdate="2021-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions for using the " &amp;#39; " escape syntax of SQL client</summary>
      <description>FLINK-22921</description>
      <version>1.13.0,1.13.1</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="22942" opendate="2021-6-9 00:00:00" fixdate="2021-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable upsert into syntax in Flink SQL</summary>
      <description>I found we can write  insert into and upsert into in Flink SQL， but the later syntax's semantic and behavior is never discussed, currently they have same implementation.We should disable the later one util we support  `upsert into ` with correct behavior.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22984" opendate="2021-6-14 00:00:00" fixdate="2021-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnsupportedOperationException when using Python UDF to generate watermark</summary>
      <description>Hi,I'm trying to use output of Python UDF (parse_data) to set watermark for the tableCREATE TABLE test ( data BYTES, ts as parse_data(data).ts, WATERMARK for ts as ts) WITH ( 'connector' = 'kafka', 'topic' = 'test', 'properties.bootstrap.servers' = 'localhost:9092', 'properties.group.id' = 'flink', 'scan.startup.mode' = 'earliest-offset', 'format' = 'raw')Then running SELECT on this table gives me exceptionPy4JJavaError: An error occurred while calling o311.hasNext.: java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:829)Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ... 13 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) ... 15 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137) at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680) at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658) at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174) ... 15 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435) at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Generated WatermarkGenerator fails to generate for row: +I([2, 10, ..., 23]). at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:172) at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:150) at org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithWatermarkGenerator.onEvent(KafkaTopicPartitionStateWithWatermarkGenerator.java:82) at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:368) at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:183) at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:142) at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)Caused by: java.lang.UnsupportedOperationException: This method is a placeholder and should not be called. at org.apache.flink.table.functions.python.PythonScalarFunction.eval(PythonScalarFunction.java:69) at WatermarkGenerator$14.currentWatermark(Unknown Source) at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:166) ... 9 more</description>
      <version>1.13.0,1.13.1</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanAcrossCalcRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="23001" opendate="2021-6-15 00:00:00" fixdate="2021-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-avro-glue-schema-registry lacks scala suffix</summary>
      <description>The dependency on flink-streaming-java implies a need for a scala suffix.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="23054" opendate="2021-6-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct upsert optimization by upsert keys</summary>
      <description>After FLINK-22901.We can use upsert keys to fix upsert join, upsert rank, and upsert sink. For join and rank: if input has no upsert keys, do not use upsert optimization. For upsert sink: if input has unique keys but no upsert keys, we need add a materialize operator to produce upsert records.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.RankProcessStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="23059" opendate="2021-6-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update playgrounds for Flink 1.13</summary>
      <description>The various playgrounds in apache/flink-playgrounds all need an update for the 1.13 release.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
    </fixedFiles>
  </bug>
  <bug id="23064" opendate="2021-6-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose connector/format options as @PublicEvolving</summary>
      <description>For built-in connectors, we need to refactor their corresponding *Options classes to … not contain any internal things … be marked PublicEvolving</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.RowDataFieldsKinesisPartitionerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvOptions.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroOptions.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerDerTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.RowDataToJsonConverters.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.filesystem.stream.StreamingFileWriterTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.filesystem.FileSystemTableFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.StreamingSink.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileWriter.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.ProcTimeCommitPredicate.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionTimeCommitPredicate.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitTrigger.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitter.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.PartitionCommitPredicate.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.DefaultPartTimeExtractor.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.AbstractFileSystemTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestFileFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcConnectionOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcFullTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcAppendOnlyWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.SequenceGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTemporalJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="23087" opendate="2021-6-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AddressResolution should be a top-level class</summary>
      <description>Make AddressResolution a top-level class so that it can be moved more easily to a separate module, because it is required both by the Akka RPC service and flink-runtime.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.AddressResolutionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23088" opendate="2021-6-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink-rpc-core module</summary>
      <description>Add a new module containing the fundamental interfaces required for implementing an RPC service.Flink-runtime will depend on this module, as will the new akka-specific rpc module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ScheduledFutureAdapterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.StartStoppable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcTimeout.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcSystemUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcSystem.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.PermanentlyFencedRpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.UnfencedMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.RunAsync.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.RpcInvocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteHandshakeMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteFencedMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.LocalRpcInvocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.LocalFencedMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.HandshakeSuccessMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.FencedMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.messages.CallAsync.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadValidatorUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.FencedRpcGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.FencedRpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.FencedMainThreadExecutable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.FatalErrorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.exceptions.RpcRuntimeException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.exceptions.RpcException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.exceptions.RpcConnectionException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.exceptions.HandshakeException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.exceptions.FencingTokenException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.AddressResolution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistryConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.ScheduledFutureAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23092" opendate="2021-6-22 00:00:00" fixdate="2021-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Built-in UDAFs could not be mixed use with Python UDAF in group window</summary>
      <description></description>
      <version>1.13.0</version>
      <fixedVersion>1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalPythonGroupWindowAggregateRule.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="23093" opendate="2021-6-22 00:00:00" fixdate="2021-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit number of I/O pool and Future threads in Mini Cluster</summary>
      <description>When running tests on CI via the minicluster, the mini cluster typically spawns 100s of I/O threads, both in the MiniCluster I/O pool and in the TM I/O pool.The standard rule for the maximum pool size is 4*num-cores, but the number of cores can be fairly large these days. Various Java versions also mess up core counting when running in containers (JVM container might have been given 2 cores as resource limits, but the JVM counts the system as a whole, like 64/128 cores).This is both a nuisance for debugging, and a big waste of memory (each thread takes by default around 1MB when spawned, so the test JVM wastes 100s of MBs for nothing).I would suggest to set a default of 8 I/O threads for the Mini Cluster. The scaling-with-cores is important for proper TM/JM deployments, but not for the Mini Cluster.</description>
      <version>1.13.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23118" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop Mesos support</summary>
      <description>Following the discussion on the ML , remove Mesos support.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManagerLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.package-info.java</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.runtime.akka.FSMSpec.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.Utils.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.TasksTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.TaskMonitorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.util.TestingMesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.util.MesosResourceAllocationTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.scheduler.TestingSchedulerDriver.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.scheduler.OfferTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.store.TestingMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStoreTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.services.TestingMesosServices.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorkerTest.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.Tasks.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.TaskMonitor.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ConnectionMonitor.scala</file>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosResourceAllocation.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosConfiguration.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServerImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactResolver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.TaskSchedulerBuilder.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.SchedulerProxy.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.Offer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.StatusUpdate.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.SlaveLost.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ResourceOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ReRegistered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Registered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.OfferRescinded.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.FrameworkMessage.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ExecutorLost.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Error.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Disconnected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Connected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.AcceptOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.LaunchableTask.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.StandaloneMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.ZooKeeperMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.StandaloneMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServicesUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.AbstractMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.RegisteredMesosWorkerNode.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosWorkerResourceSpecFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactoryImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActions.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-jepsen.test.jepsen.flink.zookeeper.test.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.mesos.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.flink.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.README.md</file>
      <file type="M">flink-jepsen.docker.test-specs.mesos-session.edn</file>
      <file type="M">flink-jepsen.docker.run-tests.sh</file>
      <file type="M">flink-jepsen.docker.Dockerfile-db</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.DistributedCacheDfsTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.mesos.wordcount.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.mesos.multiple.submissions.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.mesos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster-job.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
      <file type="M">docs.content.zh.docs.concepts.flink-architecture.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.zh.docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.content.zh.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.upgrading.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.docs.concepts.flink-architecture.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.content.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.upgrading.md</file>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.high.availability.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.miscellaneous.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.core.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.zk.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.security.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.configuration.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="23133" opendate="2021-6-24 00:00:00" fixdate="2021-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API</summary>
      <description>The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
    </fixedFiles>
  </bug>
  <bug id="23208" opendate="2021-7-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Late processing timers need to wait 1ms at least to be fired</summary>
      <description>The problem is from the codes below:public static long getProcessingTimeDelay(long processingTimestamp, long currentTimestamp) { // delay the firing of the timer by 1 ms to align the semantics with watermark. A watermark // T says we won't see elements in the future with a timestamp smaller or equal to T. // With processing time, we therefore need to delay firing the timer by one ms. return Math.max(processingTimestamp - currentTimestamp, 0) + 1;}Assuming a Flink job creates 1 timer per millionseconds, and is able to consume 1 timer/ms. Here is what will happen: Timestmap1(1st ms): timer1 is registered and will be triggered on Timestamp2. Timestamp2(2nd ms): timer2 is registered and timer1 is triggered Timestamp3(3rd ms): timer3 is registered and timer1 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer2, and timer2 will be triggered on Timestamp4(wait 1ms at least) Timestamp4(4th ms): timer4 is registered and timer2 is triggered Timestamp5(5th ms): timer5 is registered and timer2 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer3, and timer3 will be triggered on Timestamp6(wait 1ms at least)As we can see here, the ability of the Flink job is consuming 1 timer/ms, but it's actually able to consume 0.5 timer/ms. And another problem is that we cannot observe the delay from the lag metrics of the source(Kafka). Instead, what we can tell is that the moment of output is much later than expected. I've added a metrics in our inner version, we can see the lag of the timer triggering keeps increasing: In another word, we should never let the late processing timer wait 1ms, I think a simple change would be as below:return Math.max(processingTimestamp - currentTimestamp, -1) + 1;</description>
      <version>1.11.0,1.11.3,1.13.0,1.14.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23356" opendate="2021-7-12 00:00:00" fixdate="2021-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase delegation token expired after 7 days</summary>
      <description>FLINK-6376 has solved the problem for HDFS but HBase still has the issue.The root cause of the issue is that HBase delegation token expires after 7 days and Flink is not re-obtaining any kind of token at the moment.</description>
      <version>1.13.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="23496" opendate="2021-7-26 00:00:00" fixdate="2021-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven snapshot build does not use MAVEN_GLOBAL_OPTIONS</summary>
      <description>On CI we have a collection of useful settings for maven that we generally use everywhere, but the deployment of maven snapshot artifacts currently doesn't.This leads to some duplication and noisy logs.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
    </fixedFiles>
  </bug>
  <bug id="23519" opendate="2021-7-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate State Backend Latency by State Level</summary>
      <description>To make metrics aggregation easier, there should be a config to expose something like `state.backend.rocksdb.metrics.column-family-as-variable` that rocksdb provides to do aggregation across column families (https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/config/#state-backend-rocksdb-metrics-column-family-as-variable). In this case of state backend latency, the variable exposed would be state level instead column family. This makes it easier to aggregate by the various state levels that are reported.</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.StateLatencyMetricBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingValueState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingStateConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingMapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metrics.LatencyTrackingAggregatingState.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.StateBackendOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.latency.tracking.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23569" opendate="2021-7-31 00:00:00" fixdate="2021-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>There is a spelling error in a word in the document</summary>
      <description>I was browsing the documentation and found a word misspelled. This causes an exception when introducing libs in the pom.I checked the latest code and the problem still exists.</description>
      <version>1.13.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
    </fixedFiles>
  </bug>
  <bug id="23570" opendate="2021-8-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation lists incorrect scala suffixes</summary>
      <description>Some of the maven dependencies in the documentation seem to have some problems and cannot be used directly.Page: DataStream Connectors -&gt; File Sink/Streaming FIle Sink </description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="23936" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug id="23995" opendate="2021-8-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect: `insert overwrite table partition if not exists` will throw exception when tablename is like &amp;#39;database.table&amp;#39;</summary>
      <description>when run such hive sql insert overwrite table default.dest2 partition (p1=1,p2='static') if not exists select x from src it will throw exceptionCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found default</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="24031" opendate="2021-8-27 00:00:00" fixdate="2021-1-27 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>I am trying to deploy Flink in kubernetes but when I launch the taskManager in other container I get a Exception</summary>
      <description> I explain here -&gt; https://github.com/apache/flink/pull/17020I have a problem when I try to run Flink in k8s with the follow manifestsI have the following exception JobManager :2021-08-27 09:16:57,917 ERROR akka.remote.EndpointWriter [] - dropping message &amp;#91;class akka.actor.ActorSelectionMessage&amp;#93; for non-local recipient [Actor&amp;#91;akka.tcp://flink@jobmanager-hs:6123/&amp;#93;] arriving at &amp;#91;akka.tcp://flink@jobmanager-hs:6123&amp;#93; inbound addresses are &amp;#91;akka.tcp://flink@cluster:6123&amp;#93; 2021-08-27 09:17:01,255 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:01,284 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.netty.NettyTransport [] - Remote connection to &amp;#91;/172.17.0.1:34827&amp;#93; was disconnected because of &amp;#91;id: 0x13ae1d03, /172.17.0.1:34827 :&gt; /172.17.0.23:6123&amp;#93; DISCONNECTED 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.ProtocolStateActor [] - Association between local &amp;#91;tcp://flink@cluster:6123&amp;#93; and remote &amp;#91;tcp://flink@172.17.0.1:34827&amp;#93; was disassociated because the ProtocolStateActor failed: Unknown 2021-08-27 09:17:10,009 WARN akka.remote.ReliableDeliverySupervisor [] - Association with remote system &amp;#91;akka.tcp://flink@172.17.0.24:6122&amp;#93; has failed, address is now gated for &amp;#91;50&amp;#93; ms. Reason: &amp;#91;Disassociated&amp;#93;TaskManager:INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_. INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_.Best regards,Julio</description>
      <version>1.13.0,1.13.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.idea.vcs.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24049" opendate="2021-8-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description></description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug id="24317" opendate="2021-9-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description></description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug id="24318" opendate="2021-9-17 00:00:00" fixdate="2021-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="24540" opendate="2021-10-14 00:00:00" fixdate="2021-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Resource leak due to Files.list</summary>
      <description>Files.list will open dir and we should close it</description>
      <version>1.13.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.DirectoryBasedPluginFinder.java</file>
    </fixedFiles>
  </bug>
  <bug id="24662" opendate="2021-10-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24676" opendate="2021-10-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2468" opendate="2015-8-3 00:00:00" fixdate="2015-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamCheckpointingITCase failure</summary>
      <description>Two of my recent builds failed on this case.https://travis-ci.org/sachingoel0101/flink/jobs/73851862https://travis-ci.org/apache/flink/jobs/73861390</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpoinedITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24708" opendate="2021-10-29 00:00:00" fixdate="2021-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>`ConvertToNotInOrInRule` has a bug which leads to wrong result</summary>
      <description>A user report this bug in maillist, I paste the content here.We are in the process of upgrading from Flink 1.9.3 to 1.13.3.  We have noticed that statements with either where UPPER(field) or LOWER(field) in combination with an IN do not always evaluate correctly.  The following test case highlights this problem.  import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.Schema;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;public class TestCase { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); TestData testData = new TestData(); testData.setField1("bcd"); DataStream&lt;TestData&gt; stream = env.fromElements(testData); stream.print(); // To prevent 'No operators' error final StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env); tableEnvironment.createTemporaryView("testTable", stream, Schema.newBuilder().build()); // Fails because abcd is larger than abc tableEnvironment.executeSql("select *, '1' as run from testTable WHERE lower(field1) IN ('abcd', 'abc', 'bcd', 'cde')").print(); // Succeeds because lower was removed tableEnvironment.executeSql("select *, '2' as run from testTable WHERE field1 IN ('abcd', 'abc', 'bcd', 'cde')").print(); // These 4 succeed because the smallest literal is before abcd tableEnvironment.executeSql("select *, '3' as run from testTable WHERE lower(field1) IN ('abc', 'abcd', 'bcd', 'cde')").print(); tableEnvironment.executeSql("select *, '4' as run from testTable WHERE lower(field1) IN ('abc', 'bcd', 'abhi', 'cde')").print(); tableEnvironment.executeSql("select *, '5' as run from testTable WHERE lower(field1) IN ('cde', 'abcd', 'abc', 'bcd')").print(); tableEnvironment.executeSql("select *, '6' as run from testTable WHERE lower(field1) IN ('cde', 'abc', 'abcd', 'bcd')").print(); // Fails because smallest is not first tableEnvironment.executeSql("select *, '7' as run from testTable WHERE lower(field1) IN ('cdef', 'abce', 'abcd', 'ab', 'bcd')").print(); // Succeeds tableEnvironment.executeSql("select *, '8' as run from testTable WHERE lower(field1) IN ('ab', 'cdef', 'abce', 'abcdefgh', 'bcd')").print(); env.execute("TestCase"); } public static class TestData { private String field1; public String getField1() { return field1; } public void setField1(String field1) { this.field1 = field1; } }} The job produces the following output:Empty set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              2---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              3---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              4---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              5---------------------------------++-------------------------------1 row in set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              6---------------------------------++-------------------------------1 row in setEmpty set---------------------------------++-------------------------------op                         field1                            run---------------------------------++-------------------------------+I                            bcd                              8---------------------------------++-------------------------------1 row in set  </description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkRexBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="24798" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="24954" opendate="2021-11-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reset read buffer request timeout on buffer recycling for sort-shuffle</summary>
      <description>Currently, the read buffer request timeout implementation of sort-shuffle is a little aggressive. As reported in the mailing list: https://lists.apache.org/thread/bd3s5bqfg9oxlb1g1gg3pxs3577lhf88. The TimeoutException may be triggered if there is data skew and the downstream task is slow. Actually, we can further improve this case by reseting the request timeout on buffer recycling.</description>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25091" opendate="2021-11-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12、1.13、1.14 。。。&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="25143" opendate="2021-12-2 00:00:00" fixdate="2021-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ITCase for Generalized incremental checkpoints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointFormatITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="25240" opendate="2021-12-10 00:00:00" fixdate="2021-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update log4j2 version to 2.15.0</summary>
      <description>2.0 &lt;= Apache log4j2 &lt;= 2.14.1 have a RCE zero day.https://www.cyberkendra.com/2021/12/worst-log4j-rce-zeroday-dropped-on.htmlhttps://www.lunasec.io/docs/blog/log4j-zero-day/</description>
      <version>1.13.0</version>
      <fixedVersion>1.11.5,1.12.6,1.14.1,1.13.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25277" opendate="2021-12-13 00:00:00" fixdate="2021-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce explicit shutdown signalling between TaskManager and JobManager</summary>
      <description>We need to introduce shutdown signalling between TaskManager and JobManager for fast &amp; graceful shutdown in reactive scheduler mode.In Flink 1.14 and earlier versions, the JobManager tracks the availability of a TaskManager using a hearbeat. This heartbeat is by default configured with an interval of 10 seconds and a timeout of 50 seconds &amp;#91;1&amp;#93;. Hence, the shutdown of a TaskManager is recognized only after about 50-60 seconds. This works fine for the static scheduling mode, where a TaskManager only disappears as part of a cluster shutdown or a job failure. However, in the reactive scheduler mode (FLINK-10407), TaskManagers are regularly added and removed from a running job. Here, the heartbeat-mechanisms incurs additional delays.To remove these delays, we add an explicit shutdown signal from the TaskManager to the JobManager. &amp;#91;1&amp;#93;https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#heartbeat-timeout</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriverTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerRunnerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25385" opendate="2021-12-20 00:00:00" fixdate="2021-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden function serialization in JSON plan</summary>
      <description>Similar to FLINK-25230, we should revisit how functions are serialized into the JSON plan. No legacy in plan No Java serialization in plan Consider config option regarding catalog objects Helpful exceptions for the unsupported cases Use function version 1 for all functions as a first step</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ContextResolvedTableSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ResolvedCatalogTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.CatalogManagerMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.BuiltInSqlOperator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.typeutils.SymbolUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.typeutils.SymbolUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
    </fixedFiles>
  </bug>
  <bug id="25386" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden table serialization in JSON plan</summary>
      <description>Similar to previous subtasks, we should revisit the JSON plan according to FLIP-190: Consider config option regarding catalog objects Helpful exceptions for the unsupported cases</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.UnionJsonPlanTest.jsonplan.testUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testOverwrite.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestDynamicTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.RexNodeExpression.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.CatalogTableSpecBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.TemporalTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.PlannerMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.jsonplan.testGetJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testChangelogSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
    </fixedFiles>
  </bug>
  <bug id="25388" opendate="2021-12-20 00:00:00" fixdate="2021-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Identify topology changing config options for all StreamExec nodes</summary>
      <description>Add an annotation to very ExecNode that we currently support Identify consumed exec config options</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeMetadata.java</file>
    </fixedFiles>
  </bug>
  <bug id="25391" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Updating existing table factories for mutable table options</summary>
      <description>Update all existing factories for FLINK-25390.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.json.md</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.AbstractFileSystemTable.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro.md</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro-confluent.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBase.java</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.formats.csv.md</file>
    </fixedFiles>
  </bug>
  <bug id="25392" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new STATEMENT SET syntax</summary>
      <description>Support:EXECUTE STATEMENT SETBEGIN INSERT INTO pageview_pv_sink SELECT page_id, count(1) FROM clicks GROUP BY page_id; INSERT INTO pageview_uv_sink SELECT page_id, count(distinct user_id) FROM clicks GROUP BY page_id;END;EXPLAIN STATEMENT SETBEGIN INSERT INTO pageview_pv_sink SELECT page_id, count(1) FROM clicks GROUP BY page_id; INSERT INTO pageview_uv_sink SELECT page_id, count(distinct user_id) FROM clicks GROUP BY page_id;END;This time we should add this to the SQL parser. We need to figure out a solution for the interactive SQL Client.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStatementSetExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.StatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.EndStatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.BeginStatementSetOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="25418" opendate="2021-12-22 00:00:00" fixdate="2021-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dir_cache is specified in the flink task. When there is no network, you will still download the python third-party library</summary>
      <description>Specified in Python code set_python_requirements(requirements_cache_dir=dir_cache)During task execution, priority will be given to downloading Python third-party packages from the network，Can I directly use the python package in the cache I specify when I specify the cache value and don't want the task task to download the python package from the network</description>
      <version>1.13.0,1.14.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonEnvironmentManagerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="26540" opendate="2022-3-9 00:00:00" fixdate="2022-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support handle join involving complex types in on condition</summary>
      <description>Now, the Hive dialect can't handle join involving complex types in on condition, which can be reproduced using the following code in HiveDialectITCase:tableEnv.executeSql("CREATE TABLE test2a (a ARRAY&lt;INT&gt;)");tableEnv.executeSql("CREATE TABLE test2b (a INT)");List&lt;Row&gt; results = CollectionUtil.iteratorToList( tableEnv.executeSql( "select * from test2b join test2a on test2b.a = test2a.a[1]") .collect());</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserJoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="27108" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="28457" opendate="2022-7-8 00:00:00" fixdate="2022-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink add JobStatusHook support</summary>
      <description>/** * Hooks provided by users on job status changing. */@Internalpublic interface JobStatusHook extends Serializable { /** When Job become CREATED status. It would only be called one time. */ void onCreated(JobID jobId); /** When job finished successfully. */ void onFinished(JobID jobId); /** When job failed finally. */ void onFailed(JobID jobId, Throwable throwable); /** When job get canceled by users. */ void onCanceled(JobID jobId);} Introduce JM-side execution state change hook</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="28459" opendate="2022-7-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL supports non-atomic CREATE TABLE AS SELECT(CTAS)</summary>
      <description>Non-atomic CTAS support in Streaming and Batch modes.Does not actively delete created target tables when a job fails or is cancelled</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
