<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10009" opendate="2018-7-31 00:00:00" fixdate="2018-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the casting problem for function TIMESTAMPADD in Table</summary>
      <description>There seems to be a bug in TIMESTAMPADD function. For example, TIMESTAMPADD(MINUTE, 1, DATE '2016-06-15') throws a ClassCastException ( java.lang.Integer cannot be cast to java.lang.Long). Actually, it tries to cast an integer date to a long timestamp in RexBuilder.java:1524 - return TimestampString.fromMillisSinceEpoch((Long) o).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10075" opendate="2018-8-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTTP connections to a secured REST endpoint flood the log</summary>
      <description>When connecting with a browser (or other client tool) to a secured REST endpoint, the decoder throws many exceptions indicating that the received data is not an SSL record.This massively floods the log, drowning out everything else (see below).Proposed SolutionIf a NotSslRecordException is caught, Netty should send a response HTTP 301 with a new location of https://host:port/The response would need to bypass the SSL handler because it must come in plain text.Fallback SolutionIf the proper solution cannot work, we should reduce the log level for that particular exception to TRACE.Sample Log OutputLog message that is written per each request (there are many per web UI page)2018-08-06 19:07:57,734 WARN org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Unhandled exceptionorg.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: org.apache.flink.shaded.netty4.io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 474554202f7061727469616c732f6f766572766965772e68746d6c20485454502f312e310d0a486f73743a206c6f63616c686f73743a383038310d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a4163636570743a20746578742f68746d6c0d0a557365722d4167656e743a204d6f7a696c6c612f352e3020285831313b204c696e7578207838365f363429204170706c655765624b69742f3533372e333620284b48544d4c2c206c696b65204765636b6f29204368726f6d652f34372e302e323532362e313131205361666172692f3533372e33360d0a526566657265723a20687474703a2f2f6c6f63616c686f73743a383038312f0d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174652c20736463680d0a4163636570742d4c616e67756167653a20656e2d55532c656e3b713d302e382c64653b713d302e360d0a49662d4d6f6469666965642d53696e63653a204d6f6e2c2030362041756720323031382031353a34343a313720474d540d0a0d0a at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.5.2,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10076" opendate="2018-8-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.18</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10115" opendate="2018-8-9 00:00:00" fixdate="2018-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Content-length limit is also applied to FileUploads</summary>
      <description>Uploading jar files via WEB UI not working. After initializing upload... it only shows saving... and file never shows up on UI to be able to submit it</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10124" opendate="2018-8-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use ByteArrayDataInput/OutputView instead of stream + wrapper</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBAppendingState.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayDataInputView.java</file>
    </fixedFiles>
  </bug>
  <bug id="10127" opendate="2018-8-12 00:00:00" fixdate="2018-8-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add TypeInformation and serializers for JDK8 Instant</summary>
      <description>Currently Flink's basic types include all Java primitives and their boxed form, plus void, String, Date, BigDecimal, and BigInteger. New JDK8 Instance type should be added as well</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.Types.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.BasicTypeInfoTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.Types.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="10135" opendate="2018-8-13 00:00:00" fixdate="2018-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Certain cluster-level metrics are no longer exposed</summary>
      <description>In the documentation for metrics in the Flink 1.5.0 release, it says that the following metrics are reported by the JobManager:numRegisteredTaskManagersnumRunningJobstaskSlotsAvailabletaskSlotsTotalIn the job manager REST endpoint (http://&lt;job-manager&gt;:8081/jobmanager/metrics), those metrics don't appear.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="10142" opendate="2018-8-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="10150" opendate="2018-8-15 00:00:00" fixdate="2018-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained batch operators interfere with each other other</summary>
      <description>The flink web ui displays an inconsistent number of "Records received" / "Records sent” in the job overview "Subtasks" view.When I run the example wordcount batch job with a small input file on flink 1.3.2 I get 3 records sent by the first subtask and 3 records received by the second subtaskThis is the result I would expect.If I run the same job on flink 1.4.0 / 1.5.2 / 1.6.0 I get 13 records sent by the first subtask and 3 records received by the second subtaskIn real life jobs the numbers are much more strange.</description>
      <version>1.4.0,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="10164" opendate="2018-8-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for resuming from savepoints to StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="10176" opendate="2018-8-20 00:00:00" fixdate="2018-8-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Replace ByteArrayData[Input|Output]View with Data[Output|InputDe]Serializer</summary>
      <description>I have found that the functionality of ByteArrayData&amp;#91;Input|Output&amp;#93;View is very similar to the already existing Data&amp;#91;Output|InputDe&amp;#93;Serializer. With some very small additions, we can replace the former with the later.</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SerializedCheckpointData.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBCachingPriorityQueueSet.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.iterator.RocksStateKeysIterator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBAppendingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateSnapshotTransformer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.DataInputDeserializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayDataOutputView.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayDataInputView.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.serialization.TypeInformationSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationKeyValueSerializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="10181" opendate="2018-8-20 00:00:00" fixdate="2018-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add anchor link to individual rest requests</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="10207" opendate="2018-8-24 00:00:00" fixdate="2018-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump checkstyle-plugin to 8.9</summary>
      <description>Our current checkstyle version (8.4) is incompatible with java 9, the earliest version to work properly is 8.9.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">docs.flinkDev.ide.setup.md</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTaskManagerRunnerFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.retriever.impl.AkkaJobManagerRetrieverTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.AbstractParameterToolTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10208" opendate="2018-8-24 00:00:00" fixdate="2018-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump mockito to 2.0+</summary>
      <description>Mockito only properly supports java 9 with version 2. We have to bump the dependency and fix various API incompatibilities.Additionally we could investigate whether we still need powermock after bumping the dependency (which we'd also have to bump otherwise).</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.ProcessingTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.EventTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.DynamicProcessingTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.DynamicEventTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferAlignmentLimitTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.windowing.functions.InternalWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TimerServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.ConnectionUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateClientHandlerTest.java</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.Utils.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroOutputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.MethodForwardingTestUtil.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10209" opendate="2018-8-24 00:00:00" fixdate="2018-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk.tools dependency from hadoop when running with java 9</summary>
      <description>hadoop-common has a jdk.tools dependency which cannot be resolved on java 9. At least for compiling we have to exclude this dependency.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10227" opendate="2018-8-27 00:00:00" fixdate="2018-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of javax.xml.bind.DatatypeConverter</summary>
      <description>In java 9 javax.xml.bind.DatatypeConverter is no longer accessible by default. Since this calss is only used in 3 instances (and only the single method parseHexBinary) we should replace it with another implementation.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.TriggerId.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertexID.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.JobID.java</file>
    </fixedFiles>
  </bug>
  <bug id="10243" opendate="2018-8-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to reduce latency metrics granularity</summary>
      <description>The latency is currently tracked separately from each operator subtask to each source subtask. The total number of latency metrics in the cluster is thus (# of sources) * (# of operators) * parallelism², i.e. quadratic scaling.If we'd ignore the source subtask the scaling would be a lot more manageable.</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.LatencyStats.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10245" opendate="2018-8-29 00:00:00" fixdate="2018-7-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an upsert table sink factory for HBase</summary>
      <description>Design documentation: https://docs.google.com/document/d/1of0cYd73CtKGPt-UL3WVFTTBsVEre-TNRzoAt5u2PdQ/edit?usp=sharing This issue is aiming to introduce a HBase upsert table sink (for append-only and updating queries) and HBase table factory.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseTestingClusterAutostarter.java</file>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseLookupFunctionITCase.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.table.descriptors.HBaseValidator.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.util.HBaseReadHelper.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableFactory.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseRowInputFormat.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseLookupFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="10247" opendate="2018-8-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService in separate thread pool</summary>
      <description>In order to make the MetricQueryService run independently of the main Flink components, it should get its own dedicated thread pool assigned.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.SerializableOptional.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10253" opendate="2018-8-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService with lower priority</summary>
      <description>We should run the MetricQueryService with a lower priority than the main Flink components. An idea would be to start the underlying threads with a lower priority.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10257" opendate="2018-8-30 00:00:00" fixdate="2018-1-30 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Incorrect CHAR type support in Flink SQL and Table API</summary>
      <description>Despite that we officially do not support CHAR type, this type is visible and accessible for the users. First of all, string literals have default type of CHAR in SQL. Secondly users can always cast expressions/columns to CHAR.Problem is that we do not support CHAR correctly. We mishandle it in: comparisons and functions writing values to sinksAccording to SQL standard (and as almost all of the other databases do), CHAR comparisons should ignore white spaces. On the other hand, functions like CONCAT or LENGTH shouldn't: http://troels.arvin.dk/db/rdbms/#data_types-char .Currently in In Flink we completely ignore those rules. Sometimes we store internally CHAR with padded spaces sometimes without. This results with semi random behaviour with respect to comparisons/functions/writing to sinks. For example following query:tEnv.sqlQuery("SELECT CAST(s AS CHAR(10)) FROM sourceTable").insertInto("targetTable")env.execute()Where `sourceTable` has single VARCHAR(10) column with values: "Hi", "Hello", "Hello world", writes to sink not padded strings (correctly), but following query:tEnv.sqlQuery("SELECT * FROM (SELECT CAST(s AS CHAR(10)) c FROM sourceTable) WHERE c = 'Hi'") .insertInto("targetTable")env.execute()Incorrectly filters out all of the results, because CAST(s AS CHAR(10)) is a NOOP in Flink, while 'Hi' constant handed by Calcite to us will be padded with 8 spaces.On the other hand following query produces strings padded with spaces:tEnv.sqlQuery("SELECT CASE l WHEN 1 THEN 'GERMANY' WHEN 2 THEN 'POLAND' ELSE 'this should not happen' END FROM sourceTable") .insertInto("targetTable")env.execute()val expected = Seq( "GERMANY", "POLAND", "POLAND").mkString("\n")org.junit.ComparisonFailure: Different elements in arrays: expected 3 elements and received 3expected: [GERMANY, POLAND, POLAND]received: [GERMANY , POLAND , POLAND ]To make matter even worse, Calcite's constant folding correctly performs comparisons, while if same comparisons are performed by Flink, they yield different results. In other words in SQL:SELECT 'POLAND' = 'POLAND 'return true, but same expression performed on columnsSELECT CAST(country as CHAR(10)) = CAST(country_padded as CHAR(10)) FROM countriesreturns false.To further complicated things, in SQL our string literals have CHAR type, while in Table API our literals have String type (effectively VARCHAR) making results inconsistent between those two APIs. CC twalthr fhueske hequn8128</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeGeneralizationTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeGeneralization.java</file>
    </fixedFiles>
  </bug>
  <bug id="10263" opendate="2018-8-30 00:00:00" fixdate="2018-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>User-defined function with LITERAL paramters yields CompileException</summary>
      <description>When using a user-defined scalar function only with literal parameters, a CompileException is thrown. For exampleSELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))public class MyFunc extends ScalarFunction { public int eval(float lon, float lat) { // do something }}results in [ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name "com"The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10282" opendate="2018-9-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide separate thread-pool for REST endpoint</summary>
      <description>The REST endpoints currently share their thread-pools with the RPC system, which can cause the Dispatcher to become unresponsive if the REST parts are overloaded.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ExecutorThreadFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10291" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint currently generates the JobGraph from the user code when being started. Due to the nature of how the JobGraph is generated, it will get a random JobID assigned. This is problematic in case of a failover because then, the JobMaster won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the JobID assignment or make it configurable.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.StreamingPlan.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetrieverTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10295" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="10309" opendate="2018-9-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancel with savepoint fails with java.net.ConnectException when using the per job-mode</summary>
      <description>The problem occurs when using the Yarn per-job detached mode. Trying to cancel with savepoint fails with the following exception before being able to retrieve the savepoint path:exception stack trace : org.apache.flink.util.FlinkException: Could not cancel job xxxx.        at org.apache.flink.client.cli.CliFrontend.lambda$cancel$4(CliFrontend.java:585)        at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:960)        at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:577)        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1034)        at java.lang.Thread.run(Thread.java:748)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)        at org.apache.flink.client.program.rest.RestClusterClient.cancelWithSavepoint(RestClusterClient.java:398)        at org.apache.flink.client.cli.CliFrontend.lambda$cancel$4(CliFrontend.java:583)        ... 6 moreCaused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$5(FutureUtils.java:213)        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:274)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)        ... 1 moreCaused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connect refuse: xxx/xxx.xxx.xxx.xxx:xxx        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)        ... 16 moreCaused by: java.net.ConnectException: Connect refuse: xxx/xxx.xxx.xxx.xxx:xxx        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281)        ... 7 moresome discussion in mailing list : http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Cancel-flink-job-occur-exception-td24056.html</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AsynchronousJobOperationKey.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="1032" opendate="2014-7-23 00:00:00" fixdate="2014-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend POJO Field Selectors to Support Accessing Nested Objects</summary>
      <description>Right now you can only use fields of the top-level object as key fields in groupings, joins, and co-group. This should be extended to allow using nested fields. We have to be careful with null fields, though.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfo.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.testjar.KMeansForTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobs.kmeans.udfs.ComputeDistance.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.PartitionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithRankComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithRankAndDanglingComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.customdanglingpagerank.types.VertexWithAdjacencyListComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.exampleScalaPrograms.PageRankITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.exampleScalaPrograms.EnumTriangleOptITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.exampleJavaPrograms.WordCountPOJOITCase.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorISD3Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorISD2Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorISD1Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorILDXC2Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorILDX1Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorILDC3Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorILD3Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleComparatorILD2Test.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.JoinOperatorTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.CoGroupOperatorTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.unfinishedKeyPairOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassComparator.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.crossDataSet.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaAggregateOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.StringPairComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.IntPairComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.IntListComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MassiveStringSortingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.ReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.ReduceCombineDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.GroupReduceDriverTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeInformationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericPairComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.WritableTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ValueTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeInfoParser.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.java.io.AvroInputFormatTypeExtractionTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.JavaApiPostPass.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.WorksetIterationsJavaApiCompilerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.InvalidTypesException.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.InvalidProgramException.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CoGroupOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.JoinOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.ReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.CollectionExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.CompositeType.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.NothingTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.TypeInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BasicTypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.GenericPairComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.record.RecordComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypePairComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.KeyFieldOutOfBoundsException.java</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.example.java.remotecollectoroutputformat.RemoteCollectorOutputFormatExample.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.example.java.environments.CollectionExecutionExample.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.AggregateOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoField.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.RecordTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericTypeComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimePairComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.ValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.WritableComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10331" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of flush requests to the network stack</summary>
      <description>With the re-design of the record writer interaction with the result(sub)partitions, flush requests can currently pile up in these scenarios: a previous flush request has not been completely handled yet and/or is still enqueued or the network stack is still polling from this subpartition and doesn't need a new notificationThese lead to increased notifications in low latency settings (low output flusher intervals) which can be avoided.</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="10357" opendate="2018-9-17 00:00:00" fixdate="2018-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed with mismatch</summary>
      <description>The Streaming File Sink end-to-end test failed on an Amazon instance with the following result: FAIL File Streaming Sink: Output hash mismatch. Got f2000bbc18a889dc8ec4b6f2b47bf9f5, expected 6727342fdd3aae2129e61fc8f433fb6f.head hexdump of actual:0000000 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n0000010 8 \n 9 \n0000014</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10358" opendate="2018-9-17 00:00:00" fixdate="2018-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink kinesis connector could throw NPE during getRecords() call</summary>
      <description>When extending the flink kinesis connector to consume from a dynamodb stream, it was found NPE could be thrown at here . This is because the getRecords API in dynamodb streams does not return the millisBehindLatest field and has it set to null.  Null check is probably needed here.See FLINK-4582 for the context of building dynamodb streams connector on top of the Kinesis connector.  </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10393" opendate="2018-9-22 00:00:00" fixdate="2018-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy entrypoints from startup scripts</summary>
      <description>Remove the legacy entrypoints from the startup scripts.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10394" opendate="2018-9-22 00:00:00" fixdate="2018-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy mode testing profiles from Travis config</summary>
      <description>Remove the legacy mode testing profiles from Travis config.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1040" opendate="2014-7-31 00:00:00" fixdate="2014-12-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make type() call in projections optional (or remove it)</summary>
      <description>I think the type() call should be optional. The compiler can also cast the data set directly and the result type is computed from the input types anyways.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.WebLogAnalysis.java</file>
      <file type="M">docs.programming.guide.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.SumMinMaxITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ProjectITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.FirstNITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.DistinctITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CrossITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.AggregateITCase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DeltaIterationTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java8.src.main.java.org.apache.flink.examples.java8.relational.TPCHQuery10.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery10.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.NestedIterationsTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.OpenIterationTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.DeltaIterationDependenciesTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.IterationsCompilerTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.CustomPartitioningGlobalOptimizationTest.java</file>
      <file type="M">docs.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="10414" opendate="2018-9-25 00:00:00" fixdate="2018-10-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add skip to next strategy</summary>
      <description>Add skip to next strategy, that should discard all partial matches that started with the same element as found match.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.AfterMatchSkipITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipToLastStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipToFirstStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.SkipPastLastStrategy.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.aftermatch.AfterMatchSkipStrategy.java</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="10416" opendate="2018-9-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add files generated by jepsen tests to rat excludes</summary>
      <description>Currently jepsen generates some files that results in rat plugin failures.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10421" opendate="2018-9-25 00:00:00" fixdate="2018-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shaded Hadoop S3A end-to-end test failed on Travis</summary>
      <description>https://api.travis-ci.org/v3/job/432916761/log.txt</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10440" opendate="2018-9-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CassandraPojoOutputFormat</summary>
      <description>Add a CassandraPojoOutputFormat to write Pojo in Cassandra from batch API like the CassandraPojoSink for streaming API</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.batch.connectors.cassandra.example.BatchPojoExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="10457" opendate="2018-9-28 00:00:00" fixdate="2018-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SequenceFile for StreamingFileSink</summary>
      <description>SequenceFile is major file format in Hadoop eco system.It is simple to manage file and easy to combine with other tools.So we are still needed SequenceFile format, even if the file format supports Parquet and ORC.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.FiniteTestSource.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-sequencefile.src.test.java.org.apache.flink.formats.sequencefile.SequenceFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.main.java.org.apache.flink.formats.sequencefile.SequenceFileWriterFactory.java</file>
      <file type="M">flink-formats.flink-sequencefile.src.main.java.org.apache.flink.formats.sequencefile.SequenceFileWriter.java</file>
      <file type="M">flink-formats.flink-sequencefile.pom.xml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.testutils.FiniteTestSource.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.avro.ParquetStreamingFileSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10465" opendate="2018-9-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: runit supervised sshd is stopped on tear down</summary>
      <description>When tearing down the DB, we tear down all services supervised by runit. However when running the tests in Docker, sshd is under supervision by runit. When sshd is stopped, the tests cannot be continued because the control node cannot interact with the DB nodes anymore.How to reproduceRun command below in control-node container:./docker/run-tests.sh 1 [...]/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgzExpected behaviorsshd should never be stopped</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10471" opendate="2018-10-1 00:00:00" fixdate="2018-2-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>State TTL cleanup using RocksDb compaction filter</summary>
      <description>In case of rocksdb backend, we can piggy back compaction using custom compaction filter which checks our last access timestamp and discards expired values. It requires contributing a C++ Flink TTL specific filter to Rocksdb, like for cassandra. At the moment RocksDB does not support compaction filter plugins (see PR discussion), it is under development. Meanwhile, we can apply to strategies to enable this feature in Flink: Release and maintain a temporary fork of RocksDB for Flink: FRocksDB and merge TTL filter into this fork (used in Flink 1.8) Build C++ TTL filter separately, pack this C++ lib into its JNI java client jar and load it in Flink additionally to vanila RocksDB (Flink RocksDB extensions, planned for Flink 1.9)The second strategy is more flexible in the long run.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBTtlStateTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksIteratorWrapper.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitor.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlValueStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestContextBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlReducingStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStatePerElementTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStateAllEntriesTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlListStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlFoldingStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlAggregatingStateTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.StateBackendTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockTtlTimeProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.MonotonicTTLTimeProvider.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.DataStreamStateTTLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="10474" opendate="2018-10-1 00:00:00" fixdate="2018-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t translate IN with Literals to JOIN with VALUES for streaming queries</summary>
      <description>IN predicates with literals are translated to JOIN with VALUES if the number of elements in the IN clause exceeds a certain threshold. This should not be done, because a streaming join is very heavy and materializes both inputs (which is fine for the VALUES) input but not for the other.There are two ways to solve this: don't translate IN to a JOIN at all translate it to a JOIN but have a special join strategy if one input is bound and final (non-updating)Option 1. should be easy to do, option 2. requires much more effort.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10509" opendate="2018-10-8 00:00:00" fixdate="2018-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop flink-storm</summary>
      <description>Following the discussion on the ML (http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/DISCUSS-Dropping-flink-storm-tt23485.html#a23527) this JIRA tracks the removal of flink-storm.Given that the feature freeze is only 2 weeks away I suggest to delay the removal for 1.8.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormConfig.java</file>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">README.md</file>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.TestContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.StormTupleTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SetupOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.FlinkTopologyContextTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestSink.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummySpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummyBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.StormStreamSelectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.SpoutOutputCollectorObserverTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.NullTerminatingSpoutTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.FiniteTestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.AbstractTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.StormTuple.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SetupOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.MergedInputsBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.FlinkTopologyContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.AbstractStormCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormStreamSelector.java</file>
      <file type="M">docs.dev.libs.storm.compatibility.md</file>
      <file type="M">docs.redirects.storm.compat.md</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.README.md</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.operators.ExclamationBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.RandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.VerifyAndEnrichBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.SpoutSplitExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractBoltSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractLineSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.BoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.BoltPrintSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FiniteFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FiniteInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.InMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.OutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.TupleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojo.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNames.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounterByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizer.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizerByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountDataPojos.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountDataTuple.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.util.WordCountData.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.ExclamationWithBoltITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.ExclamationWithSpoutITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.util.ExclamationData.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojoITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNamesITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.SpoutSourceWordCountITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.README.md</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.FiniteSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.NullTerminatingSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SplitStreamMapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SplitStreamType.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SpoutOutputCollectorObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="10512" opendate="2018-10-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy REST API docs</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="10514" opendate="2018-10-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change Tachyon to Alluxio</summary>
      <description>Since the Tachyon renamed to Alluxio, we should change doc as well.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-fs-tests.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  <bug id="10517" opendate="2018-10-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add stability test for the REST API</summary>
      <description>The the versioning scheme introduced in FLINK-7551 we should add a test that no API breaking changes occur within a given version.</description>
      <version>1.7.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.versioning.RestAPIVersion.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10532" opendate="2018-10-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken links in documentation</summary>
      <description>https://travis-ci.org/apache/flink/builds/440115490#L599http://localhost:4000/dev/stream/operators.html:Remote file does not exist -- broken link!!!--http://localhost:4000/dev/table/streaming/sql.html:Remote file does not exist -- broken link!!!http://localhost:4000/dev/table/streaming.html:Remote file does not exist -- broken link!!!</description>
      <version>1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.stream.python.md</file>
    </fixedFiles>
  </bug>
  <bug id="10537" opendate="2018-10-12 00:00:00" fixdate="2018-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Network throughput performance regression after broadcast changes</summary>
      <description>There is a slight network throughput regression introduced in: https://issues.apache.org/jira/browse/FLINK-9913It is visible in the following benchmark:http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=networkThroughput.1,100ms&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=on(drop in the chart that happened since 21st September.)</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10551" opendate="2018-10-15 00:00:00" fixdate="2018-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy REST handlers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.DoubleAccumulator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.TaskManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.SubtaskMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobVertexMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingTaskManagersMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingSubtasksMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingJobsMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AbstractAggregatingMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobStoppingHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobPlanHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.DashboardConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.CurrentJobIdsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.ClusterOverviewHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.ClusterConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsSubtaskDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.RequestHandlerException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.RequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.SubtaskMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarAccessDeniedHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarActionHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarAccessDeniedHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarActionHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarListHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarPlanHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarRunHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.AbstractExecutionGraphRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.AbstractJobVertexRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.AbstractJsonRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.AbstractSubtaskAttemptRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.AbstractSubtaskRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ClusterConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationWithSavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobStoppingHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AbstractAggregatingMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingJobsMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingSubtasksMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.AggregatingTaskManagersMetricsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10554" opendate="2018-10-15 00:00:00" fixdate="2018-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded dependency version</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1056" opendate="2014-8-17 00:00:00" fixdate="2014-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build jar in generated archetypes using the maven-assembly-plugin</summary>
      <description>By using the "maven-assembly-plugin" with the "jar-with-dependencies" descriptor, we can ensure that the user's dependencies are always bundled into the jar file of the user's job.We may have to define some exclusion rules to avoid flink dependencies being packed as well (jar size) but that should be doable.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.META-INF.maven.archetype-metadata.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">docs.scala.api.quickstart.md</file>
      <file type="M">docs.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="10567" opendate="2018-10-16 00:00:00" fixdate="2018-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lost serialize fields when ttl state store with the mutable serializer</summary>
      <description>In TtlStateSerializer, when it is duplicate, it will lost the long field serializer which will lead to exception when createInstance, which can easily be reproduced by the test case</description>
      <version>1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10569" opendate="2018-10-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Staged">
    <buginformation>
      <summary>Clean up uses of Scheduler and Instance in valid tests</summary>
      <description>Legacy class Scheduler and Instance are still used in some valid tests like ExecutionGraphRestartTest. We should replace them with FLIP-6 schedule mode. The best way I can find is use SimpleSlotProvider.Note that we need not to remove all use points among all files since most of them stay in legacy codebase like JobManager.scala and would be removed later.</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraintTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitionerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerIsolatedTasksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RestartPipelinedRegionStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRescalingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphConstructionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10582" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make REST executor thread priority configurable</summary>
      <description>With FLINK-10282, we introduced a dedicated thread pool for the REST server endpoint. The thread priority was set to Thread.MIN_PRIORITY. This, however, might affect existing users by making some of the REST calls no longer responsive (e.g. if the other components' threads take all the time). Therefore, I propose to set the default thread priority to Thread.NORM_PRIORITY and make it additionally configurable such that users can change it.</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10583" opendate="2018-10-17 00:00:00" fixdate="2018-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for state retention to the Processing Time versioned joins.</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.TemporalProcessTimeJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10584" opendate="2018-10-17 00:00:00" fixdate="2018-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for state retention to the Event Time versioned joins.</summary>
      <description>Open PR here</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.TemporalRowtimeJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.BaseTwoInputStreamOperatorWithStateRetention.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10591" opendate="2018-10-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add functions to return TimeIndicators from MATCH_RECOGNIZE</summary>
      <description>In order to be able to apply windowing on top of results from MATCH_RECOGNIZE clause we have to provide a way to return proper TimeIndicator. We cannot simply return rowtime/proctime for any row of the match, cause match can be finalized e.g. by the first non matching row (in case of greedy), so the TimeIndicator will be equal to the timestamp of that non-matching row.The suggestion is to provide functions: MATCH_ROWTIME() MATCH_PROCTIME()that will output rowtime/proctime indicators.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="10596" opendate="2018-10-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add access to timerService in Pattern(Flat)SelectFunction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.utils.NFATestHarness.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAStatusChangeITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.IterativeCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.functions.PatternProcessFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.context.TimerContext.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.utils.OutputAsserter.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CepOperatorTestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimestampedSideOutputCollector.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.test.scala.org.apache.flink.cep.scala.PatternStreamScalaJavaAPIInteroperabilityTest.scala</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="10597" opendate="2018-10-18 00:00:00" fixdate="2018-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable UDFs support in MATCH_RECOGNIZE</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchOperatorValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.MatchUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="10600" opendate="2018-10-18 00:00:00" fixdate="2018-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide End-to-end test cases for modern Kafka connectors</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaEventSchema.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.KafkaEvent.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10602" opendate="2018-10-18 00:00:00" fixdate="2018-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricFetcher in metrics ActorSystem</summary>
      <description>After letting the MetricQueryService run into a separate ActorSystem, we should also let the MetricFetcher use the same ActorSystem to fetch the metrics. That way it will be possible to configure the metric ActorSystem different from the RpcService ActorSystem (e.g. different frame size).</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="10607" opendate="2018-10-19 00:00:00" fixdate="2018-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify to remove duplicated NoOpResultPartitionConsumableNotifier</summary>
      <description>Currently there exists two same NoOpResultPartitionConsumableNotifier implementations for different tests. We can deduplicate the common codes and public it for unified usages. And it will also bring benefits for future new tests.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10608" opendate="2018-10-19 00:00:00" fixdate="2018-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add avro files generated by datastream-allround-test to RAT exclusions</summary>
      <description>With the 1.5.5 and 1.6.2 release-candidates it was discovered that files generated during the build are not covered by the apache-rat-plugin license-header check.As a result the first compilation succeeds but a subsequent one may fail.This is a bit surprising considering that the plugin is executed in the verify phase, which is executed after the compilation and execution of tests.This is because the plugin is only run in the flink-parent project before anything is compiled. The plugin-configuration is explicitly disables inheritance.I'm re-purposing this Jira to add the avro classes to the exclusion list.However, in the long term I'd suggest to enable inheritance for the plugin and define the module-specific exclusions in each module respectively. This will allows to run the plugin in the verify phase of each module which would've caught this error.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10623" opendate="2018-10-21 00:00:00" fixdate="2018-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend streaming SQL end-to-end test to test MATCH_RECOGNIZE</summary>
      <description>We should extend the existing test_streaming_sql.sh to test the newly added MATCH_RECOGNIZE functionality.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10625" opendate="2018-10-21 00:00:00" fixdate="2018-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MATCH_RECOGNIZE documentation</summary>
      <description>The newly added MATCH_RECOGNIZE functionality needs to be documented.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="10628" opendate="2018-10-21 00:00:00" fixdate="2018-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for REST communication encryption</summary>
      <description>With FLINK-10371 we added support for mutual authentication for the REST communication. We should adapt one of the existing end-to-end tests to require this feature for the REST communication.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="10631" opendate="2018-10-21 00:00:00" fixdate="2018-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jepsen tests to run with multiple slots</summary>
      <description>After fixing FLINK-9455, we should update the Jepsen tests to run with multiple slots per TaskExecutor.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10633" opendate="2018-10-21 00:00:00" fixdate="2018-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Prometheus reporter</summary>
      <description>Add an end-to-end test using the PrometheusReporter to verify that all metrics are properly reported. Additionally verify that the newly introduce RocksDB metrics are accessible (see FLINK-10423).</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10638" opendate="2018-10-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid table scan resolution for temporal join queries</summary>
      <description>Registered tables that contain a temporal join are not properly resolved when performing a table scan.A planning error occurs when registering a table with a temporal join and reading from it again.LogicalProject(amount=[*($0, $4)]) LogicalFilter(condition=[=($3, $1)]) LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{2}]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalTableFunctionScan(invocation=[Rates(CAST($cor0.rowtime):TIMESTAMP(3) NOT NULL)], rowType=[RecordType(VARCHAR(65536) currency, BIGINT rate, TIME ATTRIBUTE(ROWTIME) rowtime)], elementType=[class [Ljava.lang.Object;])</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10639" opendate="2018-10-22 00:00:00" fixdate="2018-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix java syntax error in document</summary>
      <description>Due to the  StreamTableSourceFactory  is a trait. So the java example in the document should using "implements" keyword.    </description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="10669" opendate="2018-10-24 00:00:00" fixdate="2018-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions &amp; errors are not properly checked in logs in e2e tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1067" opendate="2014-8-26 00:00:00" fixdate="2014-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Stratosphere" occurance in docs FAQ</summary>
      <description>The name Stratosphere occurs in the answer to the fault-tolerance question of the FAQ.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="10670" opendate="2018-10-24 00:00:00" fixdate="2018-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Correlate codegen error</summary>
      <description>TableFunctionCollector should handle reuseInitCode and reuseMemberCode</description>
      <version>None</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10676" opendate="2018-10-25 00:00:00" fixdate="2018-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;as&amp;#39; method for OverWindowWithOrderBy in Java API</summary>
      <description>The preceding clause of OVER Window in the traditional database is optional. The default is UNBOUNDED. So we can add the "as" method to OverWindowWithOrderBy. This way OVERWindow is written more easily. e.g.:.window(Over partitionBy 'c orderBy 'proctime preceding UNBOUNDED_ROW as 'w)Can be simplified as follows:.window(Over partitionBy 'c orderBy 'proctime as 'w)What do you think? </description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="10678" opendate="2018-10-25 00:00:00" fixdate="2018-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a switch to run_test to configure if logs should be checked for errors/excepions</summary>
      <description>After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.queryable.state.restart.tm.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10681" opendate="2018-10-25 00:00:00" fixdate="2018-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>elasticsearch6.ElasticsearchSinkITCase fails if wrong JNA library installed</summary>
      <description>The elasticsearch6.ElasticsearchSinkITCase fails on systems where a wrong JNA library is installed.There is an incompatible JNA native library installed on this systemExpected: 5.2.0Found: 4.0.0/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib.To resolve this issue you may do one of the following: - remove or uninstall the offending library - set the system property jna.nosys=true - set jna.boot.library.path to include the path to the version of the jnidispatch library included with the JNA jar file you are using at com.sun.jna.Native.&lt;clinit&gt;(Native.java:199) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45) at org.elasticsearch.bootstrap.BootstrapInfo.isMemoryLocked(BootstrapInfo.java:50) at org.elasticsearch.monitor.process.ProcessProbe.processInfo(ProcessProbe.java:130) at org.elasticsearch.monitor.process.ProcessService.&lt;init&gt;(ProcessService.java:44) at org.elasticsearch.monitor.MonitorService.&lt;init&gt;(MonitorService.java:48) at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:363) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$PluginNode.&lt;init&gt;(EmbeddedElasticsearchNodeEnvironmentImpl.java:85) at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.start(EmbeddedElasticsearchNodeEnvironmentImpl.java:53) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(ElasticsearchSinkTestBase.java:73) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:113) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)I propose to solve the problem by setting the system property jna.nosys=true to prefer the bundled JNA library.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10689" opendate="2018-10-26 00:00:00" fixdate="2018-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port UDFs in Table API extension points to flink-table-common</summary>
      <description>After FLINK-10687 and FLINK-10688 have been resolved, we should also port the remaining extension points of the Table API to flink-table-common. This includes interfaces for UDFs and the external catalog interface.This ticket is for porting UDFs. This jira does NOT depend on FLINK-16088 so it can be started at anytime.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunction.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.functions.TableFunction.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.functions.ScalarFunction.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionContext.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.functions.AggregateFunction.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TableFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.ScalarFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.FunctionContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10690" opendate="2018-10-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests leak resources via Files.list</summary>
      <description>Files.list has the unfortunate property that is has to be explicitly closed to cleanup the underlying DirectoryStream. This is not done automatically by collectors.Several tests don't close the stream.</description>
      <version>1.5.4,1.6.1,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointIT.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.java.org.apache.flink.fs.s3.common.utils.RefCountedFileTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-distributed-cache-via-blob-test.src.main.java.org.apache.flink.streaming.tests.DistributedCacheViaBlobTestProgram.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FileUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10692" opendate="2018-10-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden Confluent schema E2E test</summary>
      <description>The Confluent schema E2E test starts a Confluent schema registry to run against. If the schema registry cannot be started, the test proceeds and simply dead locks. In order to improve the situation I suggest to fail if we cannot start the Confluent schema registry.</description>
      <version>1.6.1,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10711" opendate="2018-10-29 00:00:00" fixdate="2018-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-end-to-end-tests can fail silently</summary>
      <description>Because they are written in bash and they are not settingset -eat the beginning, errors can be swallowed silently.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="10717" opendate="2018-10-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SimpleSerializerSnapshot as replacement for ParameterlessTypeSerializerConfig</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerSerializationUtilTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.VoidSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.StringValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.StringSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlTimestampSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlTimeSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.SqlDateSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ShortValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ShortSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.NullValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.LongValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.LongSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.IntValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.IntSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.InstantSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.FloatValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.FloatSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DoubleValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DoubleSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DateSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CharValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CharSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ByteValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ByteSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BooleanValueSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BooleanSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BigIntSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.BigDecSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.StringArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.ShortPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.LongPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.IntPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.FloatPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.DoublePrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.CharPrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.array.BooleanPrimitiveArraySerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1072" opendate="2014-8-27 00:00:00" fixdate="2014-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Travis testing to more Hadoop versions.</summary>
      <description>http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Extend-Travis-CI-build-matrix-td1516.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="10720" opendate="2018-10-29 00:00:00" fixdate="2018-11-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add stress deployment end-to-end test</summary>
      <description>In order to test Flink's scalability, I suggest to add an end-to-end test which tests the deployment of a job which is very demanding. The job should have large TaskDeploymentDescriptors (e.g. a job using union state or having a high degree of parallelism). That way we can test that the serialization overhead of the TDDs does not affect the health of the cluster (e.g. heartbeats are not affected because the serialization does not happen in the main thread).</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10725" opendate="2018-10-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for Java 11 (LTS)</summary>
      <description>Java 8 is over 5 years old and will be end of life in 2019/2020. Java 11, the latest long-term support release, became GA in September 2018. Given that FLINK-8033 still hasn't been resolved and that Java 9 was end of life (discontinued / no longer publically available or supported) since March 2018, it doesn't make sense to continue trying to add Java 9 support when both Java 9 and Java 10 are end-of-life.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10726" opendate="2018-10-30 00:00:00" fixdate="2018-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming SQL end-to-end test fails due to recent flink-table-common changes</summary>
      <description>The streaming SQL end-to-end test fails with:java.lang.NoClassDefFoundError: org/apache/flink/table/api/TableException at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:85) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813) at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050) at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.api.TableException at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 18 moreThis has to do with the recent dependency changes for introducing a flink-table-common module.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10742" opendate="2018-10-31 00:00:00" fixdate="2018-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let Netty use Flink&amp;#39;s buffers directly in credit-based mode</summary>
      <description>For credit-based flow control, we always have buffers available for data that is sent to use. We could thus use them directly and not copy the network stream into Netty buffers first and then into our buffers.</description>
      <version>1.7.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10754" opendate="2018-11-1 00:00:00" fixdate="2018-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Jepsen tests to run with activated local recovery</summary>
      <description>With FLINK-9635 Flink now supports to run properly with local recovery activated. We should update the Jepsen tests to run with this feature in order to give it more test exposure.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10764" opendate="2018-11-2 00:00:00" fixdate="2018-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add S3 entropy injection end-to-end/IT test</summary>
      <description>It would be good to add an IT/end-to-end test which verifies the entropy injection for the S3 filesystems introduce by FLINK-9061.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.7.0,1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-core.src.test.resources.META-INF.services.org.apache.flink.core.fs.FileSystemFactory</file>
    </fixedFiles>
  </bug>
  <bug id="10765" opendate="2018-11-2 00:00:00" fixdate="2018-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify one of the s3 tests to use the Presto S3 s3p schema</summary>
      <description>We should modify one end-to-end test which uses S3 to explicitly use the Presto S3 filesystem by specifying the s3p schema. This was introduced with FLINK-10563.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="10797" opendate="2018-11-5 00:00:00" fixdate="2018-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"IntelliJ Setup" link is broken in Readme.md</summary>
      <description>The link points to https://github.com/apache/flink/blob/master/docs/internals/ide_setup.md#intellij-idea which is a 404 not found.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10803" opendate="2018-11-6 00:00:00" fixdate="2018-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about S3 support by the StreamingFileSink</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.ops.filesystems.md</file>
    </fixedFiles>
  </bug>
  <bug id="10809" opendate="2018-11-7 00:00:00" fixdate="2018-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using DataStreamUtils.reinterpretAsKeyedStream produces corrupted keyed state after restore</summary>
      <description>I've tried using DataStreamUtils.reinterpretAsKeyedStream for results of windowed aggregation: DataStream&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;&gt; eventStream4 = eventStream2.keyBy(Event::getKey) .window(SlidingEventTimeWindows.of(Time.milliseconds(150 * 3), Time.milliseconds(150))) .apply(new WindowFunction&lt;Event, Tuple2&lt;Integer, List&lt;Event&gt;&gt;, Integer, TimeWindow&gt;() { private static final long serialVersionUID = 3166250579972849440L; @Override public void apply( Integer key, TimeWindow window, Iterable&lt;Event&gt; input, Collector&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;&gt; out) throws Exception { out.collect(Tuple2.of(key, StreamSupport.stream(input.spliterator(), false).collect(Collectors.toList()))); } }); DataStreamUtils.reinterpretAsKeyedStream(eventStream4, events-&gt; events.f0) .flatMap(createSlidingWindowCheckMapper(pt)) .addSink(new PrintSinkFunction&lt;&gt;());and then in the createSlidingWindowCheckMapper I verify that each event belongs to 3 consecutive windows, for which I keep contents of last window in ValueState. In a non-failure setup this check runs fine, but it misses few windows after restore at the beginning.public class SlidingWindowCheckMapper extends RichFlatMapFunction&lt;Tuple2&lt;Integer, List&lt;Event&gt;&gt;, String&gt; { private static final long serialVersionUID = -744070793650644485L; /** This value state tracks previously seen events with the number of windows they appeared in. */ private transient ValueState&lt;List&lt;Tuple2&lt;Event, Integer&gt;&gt;&gt; previousWindow; private final int slideFactor; SlidingWindowCheckMapper(int slideFactor) { this.slideFactor = slideFactor; } @Override public void open(Configuration parameters) throws Exception { ValueStateDescriptor&lt;List&lt;Tuple2&lt;Event, Integer&gt;&gt;&gt; previousWindowDescriptor = new ValueStateDescriptor&lt;&gt;("previousWindow", new ListTypeInfo&lt;&gt;(new TupleTypeInfo&lt;&gt;(TypeInformation.of(Event.class), BasicTypeInfo.INT_TYPE_INFO))); previousWindow = getRuntimeContext().getState(previousWindowDescriptor); } @Override public void flatMap(Tuple2&lt;Integer, List&lt;Event&gt;&gt; value, Collector&lt;String&gt; out) throws Exception { List&lt;Tuple2&lt;Event, Integer&gt;&gt; previousWindowValues = Optional.ofNullable(previousWindow.value()).orElseGet( Collections::emptyList); List&lt;Event&gt; newValues = value.f1; newValues.stream().reduce(new BinaryOperator&lt;Event&gt;() { @Override public Event apply(Event event, Event event2) { if (event2.getSequenceNumber() - 1 != event.getSequenceNumber()) { out.collect("Alert: events in window out ouf order!"); } return event2; } }); List&lt;Tuple2&lt;Event, Integer&gt;&gt; newWindow = new ArrayList&lt;&gt;(); for (Tuple2&lt;Event, Integer&gt; windowValue : previousWindowValues) { if (!newValues.contains(windowValue.f0)) { out.collect(String.format("Alert: event %s did not belong to %d consecutive windows. Event seen so far %d times.Current window: %s", windowValue.f0, slideFactor, windowValue.f1, value.f1)); } else { newValues.remove(windowValue.f0); if (windowValue.f1 + 1 != slideFactor) { newWindow.add(Tuple2.of(windowValue.f0, windowValue.f1 + 1)); } } } newValues.forEach(e -&gt; newWindow.add(Tuple2.of(e, 1))); previousWindow.update(newWindow); }}</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="10811" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hcatalog modules needs scala suffix</summary>
      <description>The hcatalog connector has a compile dependency on flink-hadoop-compatibility which has a scala suffix, and thus also requires a suffix.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10812" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc-plugin fails for flink-e2e-test-utils</summary>
      <description>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:jar (attach-javadocs) on project flink-e2e-test-utils: MavenReportException: Error while creating archive:[ERROR] Exit code: 1 - javadoc: error - No public or protected classes found to document.[ERROR] [ERROR] Command line was: /usr/local/asfpackages/java/jdk1.8.0_191/jre/../bin/javadoc @options @packages[ERROR] [ERROR] Refer to the generated Javadoc files in '/home/jenkins/jenkins-slave/workspace/flink-snapshot-deployment/flink-end-to-end-tests/flink-e2e-test-utils/target/apidocs' dir.[ERROR] -&gt; [Help 1]</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-e2e-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10814" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka examples modules need scala suffix</summary>
      <description>The kafka examples need scala suffixes just like flink-examples-batch and flink-examples-streaming.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming-kafka.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-base.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.11.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.Kafka010Example.scala</file>
      <file type="M">flink-examples.flink-examples-streaming-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10822" opendate="2018-11-8 00:00:00" fixdate="2018-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable MetricQueryService interval</summary>
      <description>The MetricQueryService is used for transmitting metrics from TaskManagers to the JobManager, in order to expose them via REST and by extension the WebUI.By default the JM will poll metrics at most every 10 seconds. This has an adverse effect on the duration of our end-to-end tests, which for example query metrics via the REST API to determine whether the cluster has started. If during the first poll no TM is available it will take another 10 second for updated information to be available.By making this interval configurable we could this reduce the test duration. Additionally this could serve as a switch to disable the MetricQueryService.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.MutableIOMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10823" opendate="2018-11-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing scala suffixes</summary>
      <description>The jdbc connector and jmx/prometheus reporter have provided dependencies to scala-infected modules (streaming-java/runtime) and thus also require a scala sufix.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-jmx.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10863" opendate="2018-11-13 00:00:00" fixdate="2018-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assign uids to all operators</summary>
      <description>We should assign uids to operators in the test so that we can also properly test removing operators.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="10866" opendate="2018-11-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queryable state can prevent cluster from starting</summary>
      <description>The KvStateServerImpl can currently prevent the TaskExecutor from starting. Currently, the QS server starts per default on port 9067. If this port is not free, then it fails and stops the whole initialization of the TaskExecutor. I think the QS server should not stop the TaskExecutor from starting.We should at least change the default port to 0 to avoid port conflicts. However, this will break all setups which don't explicitly set the QS port because now it either needs to be setup or extracted from the logs.Additionally, we should think about whether a QS server startup failure should lead to a TaskExecutor failure or simply be logged. Both approaches have pros and cons. Currently, a failing QS server will also affect users which don't want to use QS. If we tolerate failures in the QS server, then a user who wants to use QS might run into problems with state not being reachable.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">docs..includes.generated.queryable.state.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10869" opendate="2018-11-13 00:00:00" fixdate="2018-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update S3 testing settings</summary>
      <description>Currently S3 tests go against a bucket hosted by 'data Artisans'.As part of reworking the AWS permission setup, we need to adapt the credentials and buckets for these tests.Future tests should refer to the following environment variables for S3 tests: `IT_CASE_S3_BUCKET` `IT_CASE_S3_ACCESS_KEY` `IT_CASE_S3_SECRET_KEY`</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.s3.S3Credentials.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.presto.s3.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.hadoop.s3a.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10872" opendate="2018-11-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend SQL client end-to-end to test KafkaTableSink for kafka connector 0.11</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10885" opendate="2018-11-14 00:00:00" fixdate="2018-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry E2E test failed on Travis</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/454943551Waiting for schema registry...[2018-11-14 12:20:59,394] ERROR Server died unexpectedly: (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain:51)org.apache.kafka.common.config.ConfigException: No supported Kafka endpoints are configured. Either kafkastore.bootstrap.servers must have at least one endpoint matching kafkastore.security.protocol or broker endpoints loaded from ZooKeeper must have at least one endpoint matching kafkastore.security.protocol. at io.confluent.kafka.schemaregistry.storage.KafkaStore.endpointsToBootstrapServers(KafkaStore.java:313) at io.confluent.kafka.schemaregistry.storage.KafkaStore.&lt;init&gt;(KafkaStore.java:130) at io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.&lt;init&gt;(KafkaSchemaRegistry.java:144) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:53) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:37) at io.confluent.rest.Application.createServer(Application.java:149) at io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain.main(SchemaRegistryMain.java:43)</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10900" opendate="2018-11-15 00:00:00" fixdate="2018-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark Kafka 2.0 connector as beta feature</summary>
      <description>Given the test problems with the Kafka 2.0 connector we should mark this connector as a beta feature until we have fully understood why so many tests deadlock.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="10911" opendate="2018-11-16 00:00:00" fixdate="2018-9-16 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Enable flink-scala-shell with Scala 2.12</summary>
      <description>Flink's flink-scala-shell module is not working with Scala 2.12. Therefore, it is currently excluded from the Scala 2.12 builds.</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10941" opendate="2018-11-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slots prematurely released which still contain unconsumed data</summary>
      <description>Our case is: Flink 1.5 batch mode, 32 parallelism to read data source and 4 parallelism to write data sink. The read task worked perfectly with 32 TMs. However when the job was executing the write task, since only 4 TMs were needed, other 28 TMs were released. This caused RemoteTransportException in the write task: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager ’the_previous_TM_used_by_read_task'. This might indicate that the remote task manager was lost. at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelInactive(PartitionRequestClientHandler.java:133) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) ... After skimming YarnFlinkResourceManager related code, it seems to me that Flink is releasing TMs when they’re idle, regardless of whether working TMs need them. Put in another way, Flink seems to prematurely release slots which contain unconsumed data and, thus, eventually release a TM which then fails a consuming task.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="10944" opendate="2018-11-20 00:00:00" fixdate="2018-2-20 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>EventTimeWindowCheckpointingITCase.testTumblingTimeWindow failed on Travis</summary>
      <description>The EventTimeWindowCheckpointingITCase.testTumblingTimeWindow failed on Travis: https://api.travis-ci.org/v3/job/457177641/log.txthttps://travis-ci.org/apache/flink/builds/457177636?utm_source=github_status&amp;utm_medium=notification</description>
      <version>1.7.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="10958" opendate="2018-11-21 00:00:00" fixdate="2018-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add overload support for user defined function</summary>
      <description>Currently overload is not supported in user defined function and given the following UDFclass Func21 extends ScalarFunction { def eval(p: People): String = { p.name } def eval(p: Student): String = { "student#" + p.name }}class People(val name: String)class Student(name: String) extends People(name)class GraduatedStudent(name: String) extends Student(name)Queries such as the following will compile failed with error msg "Found multiple 'eval' methods which match the signature." val udf = new Func21val table = ...table.select(udf(new GraduatedStudent("test"))) That's because overload is not supported in user defined function currently. I think it will make sense to support overload following the java language specification in section 15.2. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10964" opendate="2018-11-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql-client throws exception when paging through finished batch query</summary>
      <description>When paging through a batch query in state 'Finished' the sql client throws the following exception: org.apache.flink.table.client.gateway.SqlExecutionException: Could not find a result with result identifier '0c7dce30d287fdd13b934fbefe5a38d1'. </description>
      <version>None</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliChangelogResultView.java</file>
    </fixedFiles>
  </bug>
  <bug id="10992" opendate="2018-11-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug id="10997" opendate="2018-11-23 00:00:00" fixdate="2018-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro-confluent-registry does not bundle any dependency</summary>
      <description>The flink-avro-confluent-registry is not bundling any dependencies, yet defines a relocation for the transitive jackson dependency pulled in by kafka-schema-registry-client.It is like that the registry-client should be included in the jar.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11003" opendate="2018-11-26 00:00:00" fixdate="2018-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document of Java Lambda Expressions has a mistake</summary>
      <description>Documentation of Java Lambda Expressions has a mistake which may cause confusion.In the last code block, it presents some way to solve type missing problem.In 15 line:public static class MyTuple2Mapper extends MapFunction&lt;Integer, Integer&gt; {    @Override    public Tuple2&lt;Integer, Integer&gt; map(Integer i){         return Tuple2.of(i, i);     }}The second generic type in MapFunction should be Tuple2&lt;Integer, Integer&gt;</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug id="11005" opendate="2018-11-26 00:00:00" fixdate="2018-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define flink-sql-client uber-jar dependencies via artifactSet</summary>
      <description>The module flink-sql-client defines the content of its uber jar via filtering files from the set of all dependencies. I think this is not ideal because it misses for example the NOTICE files from down stream dependencies. A solution could be to define an &lt;artifactSet&gt;&lt;includes&gt;&lt;include&gt;&lt;/include&gt;&lt;/includes&gt;&lt;/artifactSet&gt; and exclude files via the filter.</description>
      <version>1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11008" opendate="2018-11-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Speed up RocksDB upload file procedure</summary>
      <description>As FLINK-10461  did, we could speed up upload checkpoint files by using multi-thread.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDataTransferTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDbStateDataTransfer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.AsyncSnapshotCallableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AsyncSnapshotCallable.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="11015" opendate="2018-11-27 00:00:00" fixdate="2018-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated code for format-specific Kafka table connectors</summary>
      <description>This is to make the "Disentangle flink-connector-kafka from flink-table and flink-json" (FLINK-9461) plan more smoothly. For more details, please refer to the discussion of FLINK-9461.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceBuilderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBaseTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JSONDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011JsonTableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11017" opendate="2018-11-28 00:00:00" fixdate="2018-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution</summary>
      <description>If a time interval was specified with YEAR TO MONTH resolution like e.g.:SELECT * FROM MytableGROUP BY TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11023" opendate="2018-11-28 00:00:00" fixdate="2018-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update LICENSE and NOTICE files for flink-connectors</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE files for flink-connectors.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1104" opendate="2014-9-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate Tuple1 from flink-streaming-connectors</summary>
      <description>The streaming connectors contains unnecessary usage of Tuple1. E.g. in KafkaSource:@Overridepublic Tuple1&lt;String&gt; deserialize(byte[] msg) { String s = new String(msg); if(s.equals("q")){ closeWithoutSend(); } return new Tuple1&lt;String&gt;(s);}Please adjust the underlying interfaces and update the documentation accordingly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.GenSequenceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTopology.java</file>
      <file type="M">docs.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="1105" opendate="2014-9-22 00:00:00" fixdate="2014-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for locally sorted output</summary>
      <description>This feature will make it possible to sort the output which is sent to an OutputFormat to obtain a locally sorted result.This feature was available in the "old" Java API and has not be ported to the new Java API yet. Hence optimizer and runtime should already have support for this feature. However, the API and job generation part is missing.It is also a subfeature of FLINK-598 which will provide also globally sorted results.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.programming.guide.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.GenericDataSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="11051" opendate="2018-12-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Bounded(Group Window) FlatAggregate operator to streaming Table API</summary>
      <description>Add FlatAggregate operator to streaming group window Table API as described in FLIP-29.The usage:tab.window(Tumble/Session/Slide... as 'w)   .groupBy('w, 'k1, 'k2)   .flatAggregate(tableAggregate('a))   .select('w.rowtime, 'k1, 'k2, 'col1, 'col2)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupTableAggProcessFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamTableAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.tableImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.WindowGroupedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.FlatAggregateTable.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="11068" opendate="2018-12-4 00:00:00" fixdate="2018-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert the API classes *Table, *Window to interfaces</summary>
      <description>A more detailed description can be found in FLIP-32.This includes: Table, GroupedTable, WindowedTable, WindowGroupedTable, OverWindowedTable, Window, OverWindowWe can keep the "Table" Scala implementation in a planner module until it has been converted to Java.We can add a method to the planner later to give us a concrete instance. This is one possibility to have a smooth transition period instead of changing all classes at once.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.TemporalTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectBatchTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="11074" opendate="2018-12-5 00:00:00" fixdate="2018-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the harness test to make it possible test with state backend</summary>
      <description>Currently, the harness test can only test without state backend. If you use a DataView in the accumulator of the aggregate function, the DataView is a java object and held in heap, not replaced with StateMapView/StateListView which values are actually held in the state backend. We should improve the harness test to make it possible to test with state backend. Otherwise, issues such as FLINK-10674 could have never been found. With this harness test available, we could test the built-in aggregate functions which use the DataView more fine grained.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CollectAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11079" opendate="2018-12-5 00:00:00" fixdate="2018-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip deployment for flnk-storm-examples</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE for flink-storm-examples. This project creates several fat example jars that are deployed to maven central.Alternatively we could about dropping these examples.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializerSnapshot.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CollectionSerializerConfigSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug id="11080" opendate="2018-12-5 00:00:00" fixdate="2018-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define flink-connector-elasticsearch6 uber-jar dependencies via artifactSet</summary>
      <description>Similar to FLINK-11005.</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.1,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11085" opendate="2018-12-6 00:00:00" fixdate="2018-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoClassDefFoundError in presto-s3 filesystem</summary>
      <description>A user has reporter an issue on the ML where using the presto-s3 filesystem fails with an exception due to a missing class. The missing class is indeed filtered out in the shade-plugin configuration.java.lang.NoClassDefFoundError: org/apache/flink/fs/s3presto/shaded/com/facebook/presto/hadoop/HadoopFileStatus at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.directory(PrestoS3FileSystem.java:446) at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.delete(PrestoS3FileSystem.java:423) at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.delete(HadoopFileSystem.java:147) at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:80) at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:250) at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:219) at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:844) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:756) at org.apache.flink.runtime.jobmaster.JobMaster.lambda$acknowledgeCheckpoint$8(JobMaster.java:680) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)</description>
      <version>1.7.0</version>
      <fixedVersion>1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11088" opendate="2018-12-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow pre-install Kerberos authentication keytab discovery on YARN</summary>
      <description>Currently flink-yarn assumes keytab is shipped as application master environment local resource on client side and will be distributed to all the TMs. This does not work for YARN proxy user mode &amp;#91;1&amp;#93; since proxy user or super user might not have access to actual users' keytab, but can request delegation tokens on users' behalf. Based on the type of security options for long-living YARN service&amp;#91;2&amp;#93;, we propose to have the keytab file path discovery configurable depending on the launch mode of the YARN client. Reference: &amp;#91;1&amp;#93; https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html&amp;#91;2&amp;#93; https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html#Securing_Long-lived_YARN_Services</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityConfiguration.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtilsTest.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTaskExecutorRunnerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="11089" opendate="2018-12-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log removal of filecache directory</summary>
      <description>When taskmanager exit or shutdown,the filecache directory named "flink-dist-cache*" will be removed,but there is not any log about this action.So I think we should log it for user to check it easy when there are some bugs.You can see IOManager.java logs the removed messages when taskmanager shutdown, filecache can do the same things.  </description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="1110" opendate="2014-9-22 00:00:00" fixdate="2014-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a serial collection-based execution mode</summary>
      <description>Summary of mailing list thread that the issue is based uponSince Flink is a layered system, programs written against the APIs can be executed in a variety of ways. In this case, we just run the functions single-threaded directly on the Java collections, instead of firing up a memory management, IPC, parallel workers, data movement, etc. That gives programs a minimal execution footprint (like in the Java8 streams API) for small data. The idea is to enable users to use the same program in all sorts of different contexts.The collection execution should sit below the common API, so both Java and Scala API can use it.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.SingleInputOperator.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputUdfOperator.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.translation.ReduceTranslationTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.translation.DeltaIterationTranslationTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.translation.AggregateTranslationTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.functions.SemanticPropertiesTranslationTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.compiler.util.CompilerTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.udf.RuntimeUDFContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.ReduceTranslationTests.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DeltaIterationTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.AggregateTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.MultipleInvokationsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.GenericDataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.GenericDataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.CollectionDataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.JavaPlan.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.OperatorTranslation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.OperatorUtilTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.Plan.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.OperatorUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Operator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GenericDataSourceBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GenericDataSinkBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.FileDataSourceBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.plan.ChannelTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.FeedbackPropertiesMatchTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.CompilerTestBase.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.CachedMatchStrategyCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.RecordModelPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.JavaApiPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.DataSourceNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.DataSinkNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.contextcheck.ContextChecker.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.RecordAPITestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.TypeComparable.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.PartitionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.FilterITCase.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.PartitionITCase.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializer.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.base.JoinOperatorBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionWithBroadcastVariableTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionAccumulatorsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Union.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.FlatMapOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.CopyingListCollector.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.IterationRuntimeUDFContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.ListCollector.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.BulkIterationBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.CollectionExecutor.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.JavaProgramTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.ConnectedComponentsWithParametrizableAggregatorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.ConnectedComponentsWithParametrizableConvergenceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.DeltaPageRankITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.FailingTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CoGroupOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.FlatMapOperatorCollectionExecutionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.GenericDataSourceBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.CollectionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CollectorMapOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.MapPartitionOperatorBase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.ReduceITCase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.DeltaIterationBase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.CollectionExecutionIterationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.JoinOperatorBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericPairComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimePairComparatorFactory.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericPairComparatorTest.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.GenericPairComparatorTest.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.ReduceOperatorBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.FlatMapOperatorCollectionTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.JoinOperatorBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.MapOperatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.PartitionMapOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.CoGroupOperatorCollectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.JoinOperatorBaseTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.common.operators.base.ReduceOperatorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.AbstractRichFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericTypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CrossOperatorBase.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpBinaryUdfOp.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpUnaryUdfOp.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.FileDataSinkBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.FilterOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.MapOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.DualInputOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11101" opendate="2018-12-7 00:00:00" fixdate="2018-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use enforcer-plugin to ban openjdk dependency in Presto S3 FileSystem module</summary>
      <description>The presto-s3-fs module defines he following exclusion in the shade-plugin:&lt;excludes&gt; &lt;exclude&gt;org.openjdk.jol&lt;/exclude&gt;&lt;/excludes&gt;This exclusion has no effect on the resulting artifact. We could think about removing it.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1111" opendate="2014-9-22 00:00:00" fixdate="2014-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Basic and Array Type Information into "flink-core" Project</summary>
      <description>They are logically part of the common ground under the Java/Scala API.Tests against the common API are currently impossible, because no type information is in the common project.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.IterationWithChainingNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.danglingpagerank.CompensatableDanglingPageRank.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.ConnectedComponentsNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.broadcastvars.KMeansIterativeNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.broadcastvars.BroadcastVarsNepheleITCase.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.runtime.TupleSerializerTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.operators.translation.ReduceTranslationTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.io.CollectionInputFormatTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.functions.SemanticPropertiesTranslationTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.unfinishedKeyPairOperation.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.TypeUtils.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.crossDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvOutputFormat.java</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvInputFormat.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.KeyGroupedIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.RecordOutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeCoGroupIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MergeIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllGroupReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CachedMatchTaskTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.TypeExtractorInputFormatsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeInformationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.TypeInfoParserTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.AggregateOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.ReduceTranslationTests.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CollectionInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropertiesProjectionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SelectByFunctionsTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.WritableTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ValueTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeInfoParser.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordSerializerFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordPairComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordPairComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordComparatorFactory.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.record.RecordComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.GenericArraySerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.CopyableValueComparator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ResultTypeQueryable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.RecordTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PrimitiveArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoField.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.InputTypeConfigurable.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.CompositeType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.BasicTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.BasicArrayTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.AtomicType.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.GenericDataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.FileDataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanUnwrappingReduceGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanRightUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanLeftUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanFilterOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingJoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.PlanBothUnwrappingCoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ProjectOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Operator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapPartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.IterativeDataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.FlatMapOperator.java</file>
      <file type="M">flink-java8-tests.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.KeySelectorTest.java</file>
      <file type="M">flink-java8-tests.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.ReduceITCase.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.java.io.AvroInputFormat.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.java.io.AvroInputFormatTypeExtractionTest.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.example.JDBCExample.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexCentricIteration.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.BatchedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SplitDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.CoStreamTask.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.InputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamcomponent.OutputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.serialization.TypeSerializerWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockInvokable.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.SinkJoiner.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.JavaApiPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.postpass.RecordModelPostPass.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpBinaryUdfOp.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.util.NoOpUnaryUdfOp.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.FeedbackPropertiesMatchTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.plan.ChannelTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RichGroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.BulkIterationBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.BinaryOperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Operator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.OperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.UnaryOperatorInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.NothingTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.TypeInformation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.InvalidTypesException.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.SemanticPropUtil.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.LocalCollectionOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.BulkIterationResultSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSource.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIteration.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DeltaIterationResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="1112" opendate="2014-9-22 00:00:00" fixdate="2014-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GroupSorting with KeySelectors</summary>
      <description>Group sorting is currently only supported for field-index keys and not for KeySelectors.This feature was requested.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.CustomPartitioningGroupingKeySelectorTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.GroupingKeySelectorTranslationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11120" opendate="2018-12-10 00:00:00" fixdate="2018-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIMESTAMPADD function handles TIME incorrectly</summary>
      <description>The error occur when timestampadd(MINUTE, 1, time '01:00:00') is executed:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Longat org.apache.calcite.rex.RexBuilder.clean(RexBuilder.java:1520)at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1318)at org.apache.flink.table.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:135)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:620)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:540)at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:288)I think it should meet the following conditions:expressionExpect the resulttimestampadd(MINUTE, -1, time '00:00:00')23:59:00timestampadd(MINUTE, 1, time '00:00:00')00:01:00timestampadd(MINUTE, 1, time '23:59:59')00:00:59timestampadd(SECOND, 1, time '23:59:59')00:00:00timestampadd(HOUR, 1, time '23:59:59')00:59:59This problem seems to be a bug in calcite. I have submitted isuse to calcite. The following is the link.CALCITE-2699</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11125" opendate="2018-12-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused imports</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockingShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BloomFilterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTablePerformanceComparison.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelTest.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.ValueTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.MissingTypeInfoTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11126" opendate="2018-12-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter out AMRMToken in the TaskManager credentials</summary>
      <description>Currently, Flink JobManager propagates its storage tokens to TaskManager to meet the requirement of YARN log aggregation (see FLINK-6376). But in this way the AMRMToken is also included in the TaskManager credentials, which could be potentially insecure. We should filter out AMRMToken before setting the tokens to TaskManager's container launch context.</description>
      <version>1.6.2,1.6.4,1.7.0,1.7.2,1.8.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11134" opendate="2018-12-12 00:00:00" fixdate="2018-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid REST API request should not log the full exception in Flink logs</summary>
      <description>2018-12-11 17:52:19,207 ERROR org.apache.flink.runtime.rest.handler.job.JobDetailsHandler - Exception occurred in REST handler.org.apache.flink.runtime.rest.NotFoundException: Job 15d06690e88d309aa1bdbb6ce7c6dcd1 not foundat org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:90)at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)at org.apache.flink.runtime.rest.handler.legacy.ExecutionGraphCache.lambda$getExecutionGraph$0(ExecutionGraphCache.java:133)at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:772)at akka.dispatch.OnComplete.internal(Future.scala:258)at akka.dispatch.OnComplete.internal(Future.scala:256)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:534)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:20)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:18)at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (15d06690e88d309aa1bdbb6ce7c6dcd1)at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGatewayFuture(Dispatcher.java:766)at org.apache.flink.runtime.dispatcher.Dispatcher.requestJob(Dispatcher.java:485)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:247)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:162)at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)at akka.actor.Actor$class.aroundReceive(Actor.scala:502)at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)at akka.actor.ActorCell.invoke(ActorCell.scala:495)at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)at akka.dispatch.Mailbox.run(Mailbox.scala:224)at akka.dispatch.Mailbox.exec(Mailbox.scala:234)... 4 moreAbove stack trace gets logged when an exception occurs in the REST API due to a bad user request.In this case, a user had a browser tab open on Flink Web UI with a job details page. While this page is open, browser polls the REST API every couple seconds to get the status of the job. However, the existing job was replaced by another job (with a different ID) while this page continues to poll the API with an outdated job ID. This should return a HTTP response code of 400 but it shouldn't log the exception stack trace in the Flink logs. This can cause the logs to get polluted real quick if someone has a Flink Web UI tab open somewhere with the outdated job ID.</description>
      <version>1.7.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="11136" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1114" opendate="2014-9-22 00:00:00" fixdate="2014-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalaStyle prevents projects from being built individually</summary>
      <description>Maven project building and verification fails, unless all are built together.Message:Failed to execute goal org.scalastyle:scalastyle-maven-plugin:0.5.0:check (default) on project flink-spargel: Failed during scalastyle execution: Unable to find configuration file at location tools/maven/scalastyle-config.xml</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.graph.TransitiveClosureNaive.java</file>
    </fixedFiles>
  </bug>
  <bug id="11142" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undefined behavior in the conversion from DataStream/DataSet to Table</summary>
      <description>When we try to convert DataStream/DataSet to Table. There are two ways of adding schema information, ByName or ByPosition.This feature first proposed in this pr.In ByPosition mode, the current code does not check if the number of fields less than its in  DataStream/DataSet. This may cause undefined behavior, e.g. make a projection in ByPosition mode.We can either fix it by adding some checking or regard this as a feature and just improve the doc to clarify it. In my opinion, the latter way seems better.twalthr Could you take a look at it when you free?</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="11159" opendate="2018-12-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow configuration whether to fall back to savepoints for restore</summary>
      <description>Ever since FLINK-3397, upon failure, Flink would restart from the latest checkpoint/savepoint which ever is more recent. With the introduction of local recovery and the knowledge that a RocksDB checkpoint restore would just copy the files, it may be time to re-consider / making this configurable:In certain situations, it may be faster to restore from the latest checkpoint only (even if there is a more recent savepoint) and reprocess the data between. On the downside, though, that may not be correct because that might break side effects if the savepoint was the latest one, e.g. consider this chain: chk1 -&gt; chk2 -&gt; sp … restore chk2 -&gt; …. Then all side effects between chk2 -&gt; sp would be reproduced.Making this configurable will allow the user to set whatever he needs / can to get the lowest recovery time in Flink.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="11165" opendate="2018-12-14 00:00:00" fixdate="2018-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log entire TaskManagerLocation instead of just the hostname</summary>
      <description>Currently there is not a straight forward way to find in which TM a task locates in, especially when the task has failed.We can find on which machine the task locates in by checking the JM log, sample as below:"Deploying Flat Map (31/40) (attempt #0) to z05c19399"But there can be multiple TMs on the machine and we need to check them one by one. So I'd suggest we add the full task manager location representation in the deploying log to support quick locating tasks. The task manager location contains resourceId. The resourceId is the containerId when job runs in yarn/mesos, or a unique AbstractID in standalone mode which can be easily identified at Flink web UI.  </description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="1118" opendate="2014-9-23 00:00:00" fixdate="2014-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Java Crash Log Files from License Header Check</summary>
      <description>I from time to time my builds fail with JVM crashes.If I try to run the build a second time, it fails due to the Header check on the Java crash log filles hs_err_pidxxxx.log.It would be nice to exclude these files from being checked as they are not part of the project ressources.</description>
      <version>None</version>
      <fixedVersion>0.7.0-incubating</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11183" opendate="2018-12-17 00:00:00" fixdate="2018-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Heap/NonHeap memory metrics broken</summary>
      <description>Hello, After upgrading from Flink 1.5.3 to Flink 1.7.0 graphite metrics for memory started reporting wrong numbers. All the jobs are reporting the same memory used. Off heap memory, direct memory, mapped memory is almost the same across all our jobs, for heap memory used all jobs report memory used ~ 30mb? Is that even correct, we've had jobs that die sporadically and it appears they are OOM-ing, but we can't really see it and verify it because of the metrics.Was there any major change to the metrics collection in flink-graphite-1.7.0?  You can see from the screenshot that it appears all our jobs are using same memory. </description>
      <version>1.7.0</version>
      <fixedVersion>1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.util.MetricUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
    </fixedFiles>
  </bug>
  <bug id="11189" opendate="2018-12-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove documentation for deprecated readSequenceFile method</summary>
      <description>The function readSequenceFile is no longer in class ExecutionEnvironment since FLINK-4315.But on the website example, the readSequenceFile function is still called from this class.https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/Currently, this method is in flink-hadoop-compatibility, so the documents should be revised.</description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="11191" opendate="2018-12-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception in code generation when ambiguous columns in MATCH_RECOGNIZE</summary>
      <description>Query:SELECT *FROM TickerMATCH_RECOGNIZE ( PARTITION BY symbol, price ORDER BY proctime MEASURES A.symbol AS symbol, A.price AS price PATTERN (A) DEFINE A AS symbol = 'a') AS Tthrows a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. I've also created a calcite ticket to fix it on calcite's side: CALCITE-2747</description>
      <version>1.7.0,1.8.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11197" opendate="2018-12-19 00:00:00" fixdate="2018-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve migration test comments about how to generate snapshots</summary>
      <description>We should generate snapshots base on the release branch instead of the master. It would better to add these notice in the comments.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.LegacyStatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11207" opendate="2018-12-20 00:00:00" fixdate="2018-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="11217" opendate="2018-12-25 00:00:00" fixdate="2018-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add back-to-top buttons</summary>
      <description></description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.ops.deployment.hadoop.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.stream.state.schema.evolution.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.dev.stream.operators.joining.md</file>
      <file type="M">docs.dev.stream.experimental.md</file>
      <file type="M">docs.dev.libs.ml.index.md</file>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug id="11232" opendate="2018-12-29 00:00:00" fixdate="2018-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Start Time of sub-task on web dashboard</summary>
      <description></description>
      <version>1.5.5,1.6.2,1.6.3,1.7.0,1.7.1</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="11280" opendate="2019-1-8 00:00:00" fixdate="2019-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDBSerializedCompositeKeyBuilder&amp;#39;s key serializer is not being reconfigured properly</summary>
      <description>This can be confirmed by failing tests:https://travis-ci.org/apache/flink/jobs/476533588The failing test is StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration, which was a new test added in FLINK-11073.This test started to fail when the changes were rebased on top of FLINK-9702.The problem is that starting from FLINK-11073, all state serializers (including key serializer) should be wrapped within / retrieved from a StateSerializerProvider, which handles the logic of compatibility checks of state serializers and reassigning serializer references to reconfigured instances if required.The new RocksDBSerializedCompositeKeyBuilder introduced in FLINK-9702, however, holds its own final reference directly to the key serializer, instead of using a StateSerializerProvider.This change essentially makes the key serializer non-reconfigurable.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.StateBackendTestContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="11382" opendate="2019-1-17 00:00:00" fixdate="2019-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable MetricFetcher if interval is configured to 0</summary>
      <description>Follow-up for FLINK-10822 to disable the MetricFetcher completely if the interval is configured to 0.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="11404" opendate="2019-1-22 00:00:00" fixdate="2019-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Add Load More Button to Exception Page</summary>
      <description>Add a load more button in the exception history page, when there the exception count is more than 20 (the truncated flag is true), clicking the load more button the frontend will add 20 to the MAX_NUMBER_EXCEPTION_TO_REPORT request params.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.exceptions.job-exceptions.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="11405" opendate="2019-1-22 00:00:00" fixdate="2019-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add maxExceptions query parameter</summary>
      <description>Now failover just show limit size task failover latest time(ps:According to the current flink code ).If  exceptions are more then 20, user couldn't see all.Update the rest api for supporting user could define exception's size to show when query it. Such as by /jobs/:jobid/exceptions?maxExceptions=&amp;#91;maxExceptions&amp;#93;</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="11469" opendate="2019-1-30 00:00:00" fixdate="2019-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="11535" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client jar does not contain table-api-java</summary>
      <description>The SQL Client jar does not package flink-table-api-java.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11544" opendate="2019-2-7 00:00:00" fixdate="2019-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to manually set job ID for job submissions via REST API</summary>
      <description>Add an option to specify the job ID during job submissions via the REST API. </description>
      <version>1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanRequestBody.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11626" opendate="2019-2-15 00:00:00" fixdate="2019-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 6.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5340" opendate="2016-12-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric exposing jobs uptimes</summary>
      <description>I would like the job manager to expose a metric indicating how long each job has been up. This way I can grab this number and measure the health of my job.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="8871" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Checkpoint cancellation is not propagated to stop checkpointing threads on the task manager</summary>
      <description>Flink currently lacks any form of feedback mechanism from the job manager / checkpoint coordinator to the tasks when it comes to failing a checkpoint. This means that running snapshots on the tasks are also not stopped even if their owning checkpoint is already cancelled. Two examples for cases where this applies are checkpoint timeouts and local checkpoint failures on a task together with a configuration that does not fail tasks on checkpoint failure. Notice that those running snapshots do no longer account for the maximum number of parallel checkpoints, because their owning checkpoint is considered as cancelled.Not stopping the task's snapshot thread can lead to a problematic situation where the next checkpoints already started, while the abandoned checkpoint thread from a previous checkpoint is still lingering around running. This scenario can potentially cascade: many parallel checkpoints will slow down checkpointing and make timeouts even more likely. A possible solution is introducing a cancelCheckpoint method  as counterpart to the triggerCheckpoint method in the task manager gateway, which is invoked by the checkpoint coordinator as part of cancelling the checkpoint.</description>
      <version>1.3.2,1.4.1,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointingCustomKvStateProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointedStreamingProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.FailingSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.CancellingIntegerSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.AccumulatingIntegerSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpointedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CoStreamCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ContinuousFileProcessingCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.ExceptionallyDoneFuture.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.common.AcknowledgeOnCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.FailureMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
    </fixedFiles>
  </bug>
  <bug id="9885" opendate="2018-7-18 00:00:00" fixdate="2018-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Elasticsearch 6.x connector</summary>
      <description>We have decided to try and merge the pending Elasticsearch 6.x PRs. This should also come with an end-to-end test that covers this.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9886" opendate="2018-7-18 00:00:00" fixdate="2018-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build SQL jars with every build</summary>
      <description>Currently, the shaded fat jars for SQL are only built in the -Prelease profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing META-INF entry and proper shading. We should build them with every release. If a build should happen quicker one can use the -Pfast profile.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9909" opendate="2018-7-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove cancellation of input futures from ConjunctFutures</summary>
      <description>With FLINK-8749, we introduced that a ConjunctFutures cancels all of its input futures if it is cancelled. This has, however, some unpleasant side effects since a all of the cancelled future's completing callbacks won't be called. Since this can lead to subtle bugs like in FLINK-9908, I would propose to remove this feature and require the user to do the cancellation of input futures explicitly.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FutureUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="9935" opendate="2018-7-24 00:00:00" fixdate="2018-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Table API: grouping by window and attribute causes java.lang.ClassCastException:</summary>
      <description> Grouping by window AND some other attribute(s) seems broken. Test case attached:class BatchStatisticsIntegrationTest extends FlatSpec with Matchers { trait BatchContext { implicit lazy val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment implicit val tableEnv: BatchTableEnvironment = TableEnvironment.getTableEnvironment(env) val data = Seq( (1532424567000L, "id1", "location1"), (1532424567000L, "id2", "location1"), (1532424567000L, "id3", "location1"), (1532424568000L, "id1", "location2"), (1532424568000L, "id2", "location3") ) val rawDataSet: DataSet[(Long, String, String)] = env.fromCollection(data) val table: Table = tableEnv.fromDataSet(rawDataSet, 'rowtime, 'id, 'location) } it should "be possible to run Table API queries with grouping by tumble window and column(s) on batch data" in new BatchContext { val results = table .window(Tumble over 1.second on 'rowtime as 'w) .groupBy('w, 'location) .select( 'w.start.cast(Types.LONG), 'w.end.cast(Types.LONG), 'location, 'id.count ) .toDataSet[(Long, Long, String, Long)] .collect() results should contain theSameElementsAs Seq( (1532424567000L, 1532424568000L, "location1", 3L), (1532424568000L, 1532424569000L, "location2", 1L), (1532424568000L, 1532424569000L, "location3", 1L) ) }}It seems like during execution time, the 'rowtime attribute replaces 'location and that causes ClassCastException.[info] Cause: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String[info] at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:28)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:160)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:46)[info] at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)[info] at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:88)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.sendToTarget(RecordWriter.java:129)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:105)[info] at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)[info] at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)[info] at org.apache.flink.api.java.operators.translation.RichCombineToGroupCombineWrapper.combine(RichCombineToGroupCombineWrapper.java:52)Here is some debug information that I was able to get. So, field serializers don't match the type of Row fields:this.instance = {Row@68451} "1532424567000,(3),1532424567000" fields = {Object[3]@68461} 0 = {Long@68462} 1532424567000 1 = {CountAccumulator@68463} "(3)" 2 = {Long@68462} 1532424567000this.serializer = {RowSerializer@68452} fieldSerializers = {TypeSerializer[3]@68455} 0 = {StringSerializer@68458} 1 = {TupleSerializer@68459} 2 = {LongSerializer@68460} arity = 3 nullMask = {boolean[3]@68457}  </description>
      <version>1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9946" opendate="2018-7-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E test archetype version is hard-coded</summary>
      <description>mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-${TEST_TYPE} \ -DarchetypeVersion=1.6-SNAPSHOT \ -DgroupId=org.apache.flink.quickstart \ -DartifactId=${ARTIFACT_ID} \ -Dversion=${ARTIFACT_VERSION} \ -Dpackage=org.apache.flink.quickstart \ -DinteractiveMode=false</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9947" opendate="2018-7-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document unified table sources/sinks/formats</summary>
      <description>The recent unification of table sources/sinks/formats needs documentation. I propose a new page that explains the built-in sources, sinks, and formats as well as a page for customization of public interfaces.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.table.descriptors.Json.java</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="9951" opendate="2018-7-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update scm developerConnection</summary>
      <description>The developer connection must be updated to point to the update remote.</description>
      <version>1.3.3,1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
