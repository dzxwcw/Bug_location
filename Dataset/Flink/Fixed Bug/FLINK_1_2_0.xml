<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="4268" opendate="2016-7-27 00:00:00" fixdate="2016-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a parsers for BigDecimal/BigInteger</summary>
      <description>Since BigDecimal and BigInteger are basic types now. It would be great if we also parse those.FLINK-628 did this a long time ago. This feature should be reintroduced.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FieldParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="4450" opendate="2016-8-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update storm version to 1.0.0</summary>
      <description>The storm package path was changed in new versionstorm old version package:backtype.storm.*storm new version pachage:org.apache.storm.*shall we update flink/flink-storm code to new storm version?</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.StormTupleTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SetupOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.FlinkTopologyContextTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestSink.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummySpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummyBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.SpoutOutputCollectorObserverTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.NullTerminatingSpoutTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.FiniteTestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkTopologyTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.StormTuple.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SetupOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.MergedInputsBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.FlinkTopologyContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormConfig.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SpoutOutputCollectorObserver.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.NullTerminatingSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.FiniteSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.TwoFlinkStreamsMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.StormFlinkStreamMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopology.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkSubmitter.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.operators.ExclamationBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.join.SingleJoinExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.print.PrintSampleStream.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.RandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.VerifyAndEnrichBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractBoltSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractLineSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.BoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.BoltPrintSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FiniteFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.InMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.OutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.TupleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojo.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNames.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounterByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizer.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizerByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocalByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBoltTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitSpoutTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamBoltLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamSpoutLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.FiniteRandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MergerBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MetaDataSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.TaskIdBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.VerifyMetaDataBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormFieldsGroupingITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormMetaDataITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormUnionITCase.java</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="4488" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent cluster shutdown after job execution for non-detached jobs</summary>
      <description>In per-job mode, the Yarn cluster currently shuts down after the first interactively executed job. Users may want to execute multiple jobs in one Jar. I would suggest to use this mechanism only for jobs which run detached. For interactive jobs, shutdown of the cluster is additionally handled by the CLI which should be sufficient to ensure cluster shutdown. Cluster shutdown could only become a problem in case of a network partition to the cluster or outage of the CLI.</description>
      <version>1.1.1,1.2.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4571" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable little parallelism in Gelly drivers</summary>
      <description>Several Gelly library implementations support a configurable "little parallelism" which is important when scaling to large data sets. These algorithms include operators at the beginning and end which process data on the order of the original DataSet, as well as middle operators that exchange 100s or 1000s more data. The "little parallelism" should be configurable in the appropriate Gelly drivers in the flink-gelly-examples module.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.AdamicAdar.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.link.analysis.HITS.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.directed.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.directed.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.TranslateVertexValues.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.TranslateGraphIds.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.TranslateEdgeValues.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.simple.undirected.Simplify.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.simple.directed.Simplify.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.filter.undirected.MaximumDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.VertexDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeTargetDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeSourceDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeDegreePair.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexOutDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexInDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexDegrees.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeTargetDegrees.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeSourceDegrees.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeDegreesPair.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  <bug id="4580" opendate="2016-9-5 00:00:00" fixdate="2016-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check that the RpcEndpoint supports the specified RpcGateway</summary>
      <description>When calling RpcService.connect the user specifies the type of the RpcGateway. At the moment, it is not checked whether the RpcEndpoint actually supports the specified RpcGateway.I think it would be good to add a runtime check that the corresponding RpcEndpoint supports the specified RpcGateway. If not, then we can let the connect method fail fast.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4582" opendate="2016-9-6 00:00:00" fixdate="2016-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow FlinkKinesisConsumer to adapt for AWS DynamoDB Streams</summary>
      <description>AWS DynamoDB is a NoSQL database service that has a CDC-like (change data capture) feature called DynamoDB Streams (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html), which is a stream feed of item-level table activities.The DynamoDB Streams shard abstraction follows that of Kinesis Streams with only a slight difference in resharding behaviours, so it is possible to build on the internals of our Flink Kinesis Consumer for an exactly-once DynamoDB Streams source.I propose an API something like this:DataStream dynamoItemsCdc = FlinkKinesisConsumer.asDynamoDBStream(tableNames, schema, config)The feature adds more connectivity to popular AWS services for Flink, and combining what Flink has for exactly-once semantics, out-of-core state backends, and queryable state with CDC can have very strong use cases. For this feature there should only be an extra dependency to the AWS Java SDK for DynamoDB, which has Apache License 2.0.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4604" opendate="2016-9-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for standard deviation/variance</summary>
      <description>Calcite's AggregateReduceFunctionsRule can convert SQL AVG, STDDEV_POP, STDDEV_SAMP, VAR_POP, VAR_SAMP to sum/count functions. We should add, test and document this rule. If we also want to add this aggregates to Table API is up for discussion.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.LogicalPlanFormatUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4643" opendate="2016-9-20 00:00:00" fixdate="2016-10-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Average Clustering Coefficient</summary>
      <description>Gelly has Global Clustering Coefficient and Local Clustering Coefficient. This adds Average Clustering Coefficient. The distinction is discussed in http://jponnela.com/web_documents/twomode.pdf (pdf page 2, document page 32).</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.AsmTestBase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  <bug id="4651" opendate="2016-9-21 00:00:00" fixdate="2016-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-register processing time timers at the WindowOperator upon recovery.</summary>
      <description>Currently the WindowOperator checkpoints the processing time timers, but upon recovery it does not re-registers them with the TimeServiceProvider. To actually reprocess them it relies on another element that will come and register a new timer for a future point in time. Although this is a realistic assumption in long running jobs, we can remove this assumption by re-registering the restored timers with the TimeServiceProvider in the open() method of the WindowOperator.</description>
      <version>None</version>
      <fixedVersion>1.1.5,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4652" opendate="2016-9-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Don&amp;#39;t pass credentials explicitly to AmazonClient - use credentials provider instead</summary>
      <description>By using the credentials explicitly we are responsible for checking and refreshing credentials before they expire. If no refreshment is done we will encounter AmazonServiceException: 'The security token included in the request is expired'. To utilize automatic refreshment of credentials pass the AWSCredentialsProvider direclty to AmazonClient by removing the getCredentials() call.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4654" opendate="2016-9-21 00:00:00" fixdate="2016-9-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>clean up docs</summary>
      <description>There are some minor but distracting glitches in the docs &amp;#8211; typos, awkward phrases, broken links.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">docs.dev.state.backends.md</file>
      <file type="M">docs.dev.state.md</file>
      <file type="M">docs.dev.libs.cep.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4656" opendate="2016-9-21 00:00:00" fixdate="2016-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port existing code to use Flink&amp;#39;s future abstraction</summary>
      <description>Port existing code to use Flink's future abstraction</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingGatewayBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcCompletenessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.TestRegistrationGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorToResourceManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.impl.FlinkFuture.java</file>
    </fixedFiles>
  </bug>
  <bug id="4660" opendate="2016-9-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopFileSystem (with S3A) may leak connections, which cause job to stuck in a restarting loop</summary>
      <description>Flink job with checkpoints enabled and configured to use S3A file system backend, sometimes experiences checkpointing failure due to S3 consistency issue. This behavior is also reported by other people and documented in https://issues.apache.org/jira/browse/FLINK-4218.This problem gets magnified by current HadoopFileSystem implementation, which can potentially leak S3 client connections, and eventually get into a restarting loop with “Timeout waiting for a connection from pool” exception thrown from aws client.I looked at the code, seems HadoopFileSystem.java never invoke close() method on fs object upon failure, but the FileSystem may be re-initialized every time the job gets restarted.A few evidence I observed:1. When I set the connection pool limit to 128, and below commands shows 128 connections are stuck in CLOSE_WAIT state. 2. task manager logs indicates that state backend file system consistently getting initialized upon job restarting.3. Log indicates there is NPE during cleanning up of stream task which was caused by “Timeout waiting for connection from pool” exception when trying to create a directory in S3 bucket.2016-09-02 08:17:50,886 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask - Error during cleanup of stream taskjava.lang.NullPointerExceptionat org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.cleanup(OneInputStreamTask.java:73)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:323)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:589)at java.lang.Thread.run(Thread.java:745)4.It appears StreamTask from invoking checkpointing operation, to handling failure, there is no logic associated with closing Hadoop File System object (which internally includes S3 aws client object), which resides in HadoopFileSystem.java.</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.CoProcessOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedStreams.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.ProcessOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="4662" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Calcite version up to 1.9</summary>
      <description>Calcite just released the 1.9 version. We should adopt it also in the Table API especially for FLINK-4294.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4664" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add translator to NullValue</summary>
      <description>Existing translators convert from LongValue (the output label type of graph generators) to IntValue, StringValue, and an offset LongValue. Translators can also be used to convert vertex or edge values. This translator will be appropriate for translating these vertex or edge values to NullValue when the values are not used in an algorithm.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.translate.TranslateTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.translate.LongValueToUnsignedIntValueTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.translate.LongValueToSignedIntValueTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.LongValueToUnsignedIntValue.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.LongValueToStringValue.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.LongValueToSignedIntValue.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.translate.LongValueAddOffset.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.HITS.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.ClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.driver.GraphMetrics.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4665" opendate="2016-9-23 00:00:00" fixdate="2016-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove boxing/unboxing to parse a primitive</summary>
      <description>I found the following issues with boxing/unboxing and Integer1. Current code doing boxing/unboxing to parse a primitive - It is more efficient to just call the static parseXXX method.2. boxing/unboxing to do type cast3. new Integer instead of valueOf - Using new Integer(int) is guaranteed to always result in a new object whereas Integer.valueOf(int) allows caching of values to be done by the compiler.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerExamplesTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.ParameterTool.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.summarize.aggregation.FloatSummaryAggregator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.NestedMethodAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4667" opendate="2016-9-23 00:00:00" fixdate="2016-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn Session CLI not listening on correct ZK namespace when HA is enabled to use ZooKeeper backend</summary>
      <description>In Yarn mode, when Flink is configured for HA using ZooKeeper backend, the leader election listener does not provide correct JM/leader info and will timeout since the listener is waiting on default ZK namespace instead of the application specific (Application ID)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-yarn-session.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4668" opendate="2016-9-23 00:00:00" fixdate="2016-9-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fix positive random int generation</summary>
      <description>According to java specMath.abs(Integer.MIN_VALUE) == Integer.MIN_VALUESo, Math.abs(rnd.nextInt()) might return negative valueTo generate positive random int value we can use rnd.nextInt(Integer.MAX_VALUE)Integer.MAX_VALUE will be excluded btw</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="4671" opendate="2016-9-23 00:00:00" fixdate="2016-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API can not be built</summary>
      <description>Running mvn clean verify in flink-table results in a build failure.[ERROR] Failed to execute goal on project flink-table_2.10: Could not resolve dependencies for project org.apache.flink:flink-table_2.10:jar:1.2-SNAPSHOT: Failure to find org.apache.directory.jdbm:apacheds-jdbm1:bundle:2.0.0-M2 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1]However, the master can be built successfully.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4689" opendate="2016-9-27 00:00:00" fixdate="2016-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a simple slot provider for the new job manager</summary>
      <description>In flip-6 branch, we need to adjust existing scheduling model. In the first step, we should introduce a simple / naive slot provider which just ignore all the sharing or location constraint, to make whole thing work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="4693" opendate="2016-9-27 00:00:00" fixdate="2016-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add session group-windows for batch tables</summary>
      <description>Add Session group-windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4700" opendate="2016-9-27 00:00:00" fixdate="2016-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the TimeProvider test</summary>
      <description>Currently the TimeProvider test fails due to a race condition. This task aims at fixing it.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimeProviderTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4708" opendate="2016-9-29 00:00:00" fixdate="2016-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scope Mini Kerberos Cluster dependencies as test dependencies</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4717" opendate="2016-9-29 00:00:00" fixdate="2016-1-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Naive version of atomic stop signal with savepoint</summary>
      <description>As a first step towards atomic stopping with savepoints we should implement a cancel command which prior to cancelling takes a savepoint. Additionally, it should turn off the periodic checkpointing so that there won't be checkpoints executed between the savepoint and the cancel command.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointDeclineReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendListCancelTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CancelOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.setup.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="4728" opendate="2016-10-3 00:00:00" fixdate="2016-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace reference equality with object equality</summary>
      <description>Some cases of testing Integer equality using == rather than Integer.equals(Integer), and some additional cleanup.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.SemanticPropertiesAPIToPlanTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.PartitioningReusageTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.dataproperties.GlobalPropertiesMatchingTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.dag.GroupCombineNodeTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.AdditionalOperatorsTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.PlanNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.WorksetIterationNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.BulkIterationNode.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sca.UdfAnalyzerExamplesTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.SortPartitionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ReduceOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.DistinctTranslationTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.TaggedValue.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FunctionAnnotation.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.RecordTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.CopyableValueTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT1Test.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.EitherSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.EitherTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.SqlDateComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.DoubleSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BigDecSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BigDecComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.OuterJoinOperatorBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.FileInputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.EnumerateNestedFilesTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.DelimitedInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.FieldList.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Ordering.java</file>
    </fixedFiles>
  </bug>
  <bug id="4730" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce CheckpointMetaData</summary>
      <description>Currently, the meta data for each checkpoint consists of up to 5 long values which are passed through several functions. When adding/removing meta data we would have to change function signatures in many places. Furthermore, this is prone to errors when the order of arguments is changed accidentally. We should introduce a CheckpointMetaData which encapsulates this checkpoint meta data.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4732" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven junction plugin security threat</summary>
      <description>We use the Maven Junction plugin http://pyx4j.com/pyx4j-maven-plugins/maven-junction-plugin/introduction.html to create a symbolic link to the build directory. On Windows, the plugin downloads an executable from the author's homepage which may be modified by an attacker. The plugin has not been updated since 2007 and the maintainer has not shown interest to fix the issue.I propose to remove the plugin while this security threat persists.</description>
      <version>None</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4734" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Remove use of Tuple setField for fixed position</summary>
      <description>Use tuple.f0 = value; rather than tuple.setField(value, 0);. Can the latter be optimized by the JVM?</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.ScatterGatherIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.pregel.VertexCentricIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.pregel.MessageCombiner.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.pregel.ComputeFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
    </fixedFiles>
  </bug>
  <bug id="4735" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate some job execution related akka messages to rpc calls</summary>
      <description>This includes the following operations about job execution:1. checkpointing2. kvstate3. savepoint4. classloading props</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="4740" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade testing libraries</summary>
      <description>JUnit 4.12 was released 4 Dec 2014. Flink is currently using JUnit 4.11 from 14 Nov 2012.PowerMock reports "org.powermock.reflect.exceptions.FieldNotFoundException: Field 'fTestClass' was not found in class org.junit.internal.runners.MethodValidator." https://github.com/jayway/powermock/issues/551This is fixed in PowerMock 1.6.1+ (currently using 1.5.5, latest is 1.6.5): https://raw.githubusercontent.com/jayway/powermock/master/changelog.txtThen Mockito causes "java.lang.NoSuchMethodError: org.mockito.mock.MockCreationSettings.getSerializableMode()Lorg/mockito/mock/SerializableMode;".This is fixed by upgrading Mockito from 1.9.5 to the latest 1.10.19.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriverTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4748" opendate="2016-10-5 00:00:00" fixdate="2016-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix shutdown of automatic watermark context</summary>
      <description>The AutomaticWatermarkContext (for ingestion time) does not shut down its timers properly.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
    </fixedFiles>
  </bug>
  <bug id="4749" opendate="2016-10-5 00:00:00" fixdate="2016-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant processing time timers and futures in the window operator</summary>
      <description>The window operator maintains redundant sets of processing time timers and futures, which make cleanup of processing time timers hard and inefficient.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4750" opendate="2016-10-5 00:00:00" fixdate="2016-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure that active processing time triggers complete before closing operators</summary>
      <description>To avoid that processing time triggers access closed/disposed operators, active triggers should be waited for when closing the operators.This is only relevant for operations on finite streams.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.NoOpTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="4762" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use plural in time interval units</summary>
      <description>During the creation of FLIP-11 we decided to rename the time interval units. From minute to minutes and so on in Java and Scala Table API.12.minutes + 2.hours reads better.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4787" opendate="2016-10-10 00:00:00" fixdate="2016-10-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add REST API call for cancel-with-savepoints</summary>
      <description>As a follow up to FLINK-4717, expose the cancel-with-savepoint command via the REST API./jobs/:jobid/cancel-with-savepoint//jobs/:jobid/cancel-with-savepoint/:targetDirectoryThe first command goes to the default savepoint directory, the second one uses the given target directory.The calls need to be async, as triggering a savepoint can take some time. For this, the handlers return a 201 (Accepted) response with the location of the status, e.g. /jobs/:jobid/cancel-with-savepoint/in-progress/:id.The user has to check that location until the final savepoint path is returned.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobStoppingHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobManagerConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarActionHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarAccessDeniedHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractExecutionGraphRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4799" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-add build-target symlink to project root</summary>
      <description>We have previously removed the plugin which created the 'build-target' link to the build target directory. See FLINK-4732. At least one user has requested to re-add the link.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4801" opendate="2016-10-11 00:00:00" fixdate="2016-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input type inference is faulty with custom Tuples and RichFunctions</summary>
      <description>This issue has been discussed on the ML:http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Type-problem-in-RichFlatMapFunction-when-using-GenericArray-type-td13929.htmlThis returns the wrong type: public static class Foo&lt;K&gt; extends Tuple2&lt;K[], K&gt; { public Foo() { } public Foo(K[] value0, K value1) { super(value0, value1); } } DataSource&lt;Foo&lt;T&gt;&gt; fooDataSource = env.fromElements(foo); DataSet&lt;Foo&lt;T&gt;&gt; ds = fooDataSource.join(fooDataSource) .where(field).equalTo(field) .with(new RichFlatJoinFunction&lt;Foo&lt;T&gt;, Foo&lt;T&gt;, Foo&lt;T&gt;&gt;() { @Override public void join(Foo&lt;T&gt; first, Foo&lt;T&gt; second, Collector&lt;Foo&lt;T&gt;&gt; out) throws Exception { out.collect(first); } });</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.TypeExtractorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
    </fixedFiles>
  </bug>
  <bug id="4813" opendate="2016-10-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Having flink-test-utils as a dependency outside Flink fails the build</summary>
      <description>The flink-test-utils depend on hadoop-minikdc, which has a dependency, which is only resolvable, if the maven-bundle-plugin is loaded.This is the error message[ERROR] Failed to execute goal on project quickstart-1.2-tests: Could not resolve dependencies for project com.dataartisans:quickstart-1.2-tests:jar:1.0-SNAPSHOT: Failure to find org.apache.directory.jdbm:apacheds-jdbm1:bundle:2.0.0-M2 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1]flink-parent loads that plugin, so all "internal" dependencies to the test utils can resolve the plugin.Right now, users have to use the maven bundle plugin to use our test utils externally.By making the hadoop minikdc dependency optional, we can probably resolve the issues. Then, only users who want to use the security-related tools in the test utils need to manually add the hadoop minikdc dependency + the plugin.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4820" opendate="2016-10-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slf4j / log4j version upgrade to support dynamic change of log levels --&gt; Make logging framework exchangeable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.log4j-travis.properties</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime-web.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MavenForkNumberPrefixLayout.java</file>
    </fixedFiles>
  </bug>
  <bug id="4821" opendate="2016-10-13 00:00:00" fixdate="2016-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement rescalable non-partitioned state for Kinesis Connector</summary>
      <description>FLINK-4379 added the rescalable non-partitioned state feature, along with the implementation for the Kafka connector.The AWS Kinesis connector will benefit from the feature and should implement it too. This ticket tracks progress for this.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4825" opendate="2016-10-13 00:00:00" fixdate="2016-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a RexExecutor that uses Flink&amp;#39;s code generation</summary>
      <description>The added ReduceExpressionRule leads to inconsistent behavior. Because some parts of an expression are evalutated using Flink's code generation and some parts use Calcite's code generation.A very easy example: boolean expressions casted to string are represented as "TRUE/FALSE" using Calcite and "true/false" using Flink.I propose to implement the RexExecutor interface and forward the calls to Flink's code generation. Additional improvements in order to be more standard compliant could be solved in new Jira issues.I will disable the rule and the corresponding tests till this issue is fixed.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.MapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.io.ValuesInputFormat.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.FlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.FlatJoinRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.Compiler.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkRelBuilder.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4826" opendate="2016-10-13 00:00:00" fixdate="2016-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add keytab based kerberos support for Mesos environment</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4850" opendate="2016-10-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkML - SVM predict Operation for Vector and not LaveledVector</summary>
      <description>It seems that evaluate operation is defined for Vector and not LabeledVector.It impacts QuickStart guide for FlinkML when using SVM.We need to update the documentation as follows:val astroTest:DataSet&amp;#91;(Vector,Double)&amp;#93; = MLUtils .readLibSVM(env, "src/main/resources/svmguide1.t") .map(l =&gt; (l.vector, l.label))val predictionPairs = svm.evaluate(astroTest)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="4876" opendate="2016-10-21 00:00:00" fixdate="2016-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="4877" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactorings around FLINK-3674 (User Function Timers)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecord.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestTimeProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileMonitoringTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.CollectingOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.NoOpTimerService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.WindowingTestHarness.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.ExtractTimestampsOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StreamTaskTimerITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.Triggerable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="4892" opendate="2016-10-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot TimerService using Key-Grouped State</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TimelyFlatMapTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.TimelyCoFlatMapTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.HeapInternalTimerServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4894" opendate="2016-10-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t block on buffer request after broadcastEvent</summary>
      <description>After broadcasting an event (like the checkpoint barrier), the record writer might block on a buffer request although that buffer will only be needed on the next write on that channel.Instead of assuming that each serializer has a buffer set, we can change the logic in the writer to request the buffer when it requires one.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4900" opendate="2016-10-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Docker image support</summary>
      <description>Support the use of a docker image, with both the unified containerizer and the Docker containerizer.Use a configuration setting to explicitly configure which image and containerizer to use.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4937" opendate="2016-10-26 00:00:00" fixdate="2016-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add incremental group window aggregation for streaming Table API</summary>
      <description>Group-window aggregates for streaming tables are currently not done in an incremental fashion. This means that the window collects all records and performs the aggregation when the window is closed instead of eagerly updating a partial aggregate for every added record. Since records are buffered, non-incremental aggregation requires more storage space than incremental aggregation.The DataStream API which is used under the hood of the streaming Table API features incremental aggregation using a ReduceFunction.We should add support for incremental aggregation in group-windows.This is a follow-up task of FLINK-4691.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4940" opendate="2016-10-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for broadcast state</summary>
      <description>As mentioned in https://cwiki.apache.org/confluence/display/FLINK/FLIP-17+Side+Inputs+for+DataStream+API we need broadcast state to support job patterns where one (or several) inputs are broadcast to all operator instances and where we keep state that that is mutated only based on input from broadcast inputs. This special restriction ensures that the broadcast state is the same on all parallel operator instances when checkpointing (except when using at-least-once mode). We therefore only have to checkpoint the state of one arbitrary instance, for example instance 0.For the different types of side inputs we need different types of state, luckily, the side input types align with these state types we currently have for keyed state: ValueState ListState MapStateWe can therefore reuse keyed state backends for our purposes but need to put a more restricting API in front of it: mutation of broadcast state must only be allowed when actually processing broadcast input. If we don't have this check users can (by mistake) modify broadcast state. This would lead to incorrect results which are very hard to notice, much less debug.With the way the Flink state API works (users can get a State in open() and work with state by calling methods on that) we have to add special wrapping state classes that only allow modification of state when processing a broadcast element.For the API, I propose to add a new interface `InternalStateAccessor`:/** * Interface for accessing persistent state. */@PublicEvolvingpublic interface InternalStateAccessor { &lt;N, S extends State&gt; S state( N namespace, TypeSerializer&lt;N&gt; namespaceSerializer, StateDescriptor&lt;S, ?&gt; stateDescriptor)}this is the same as `KeyedStateBackend.getPartitionedState()` but allows us to abstract away the special nature of broadcast state. This is also meant as an external interface and is not to be exposed to user functions. Only operators should deal with this.AbstractStreamOperator would get a new method `getBroadcastStateAccessor()` that returns an implementation of this interface. The implementation would have a KeyedStateBackend but wrap the state in special wrappers that only allow modification when processing broadcast elements (as mentioned above). On the lower implementation levels, we have to add a new entry for our state to `OperatorSnapshotResult`. For example:private RunnableFuture&lt;KeyGroupsStateHandle&gt; broadcastStateManagedFuture;Also the CheckpointCoordinator and savepoint/checkpoint serialisation logic will have to be adapted to support this new kind of state. With the ongoing changes in supporting incremental snapshotting and other new features for `KeyedStateBackend` this should be coordinated with StephanEwen and/or stefanrichter83@gmail.com and/or xiaogang.shi. We also have to be very careful about maintaining compatibility with savepoints from older versions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendSerializationProxy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.RoundRobinOperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorStateRepartitioner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.OperatorStateStore.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4945" opendate="2016-10-27 00:00:00" fixdate="2016-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaConsumer logs wrong warning about confirmation for unknown checkpoint</summary>
      <description>Checkpoints are currently not registered in all cases. While the code still behaves correctly this leads to misleading warnings.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4946" opendate="2016-10-27 00:00:00" fixdate="2016-11-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Load jar files from subdirectories of lib</summary>
      <description>Users can more easily track Flink jars with transitive dependencies when copied into subdirectories of lib. This is the arrangement of opt for FLINK-4861.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4949" opendate="2016-10-27 00:00:00" fixdate="2016-3-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Refactor Gelly driver inputs</summary>
      <description>The Gelly drivers started as simple wrappers around library algorithms but have grown to handle a matrix of input sources while often running multiple algorithms and analytics with custom parameterization.This ticket will refactor the sourcing of the input graph into separate classes for CSV files and RMat which will simplify the inclusion of new data sources.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.Usage.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">docs.dev.libs.gelly.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="4955" opendate="2016-10-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Translations Tests for KeyedStream.flatMap(TimelyFlatMapFunction)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4959" opendate="2016-10-28 00:00:00" fixdate="2016-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write Documentation for ProcessFunction</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug id="4997" opendate="2016-11-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extending Window Function Metadata</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP-2+Extending+Window+Function+Metadata</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowReduceITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowFunctionITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowFoldITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.WindowFoldITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.windowing.functions.InternalWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.FoldApplyWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="4998" opendate="2016-11-2 00:00:00" fixdate="2016-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager fails when num task slots &gt; Yarn vcores</summary>
      <description>The ResourceManager fails to acquire containers when the users configures the number of task slots to be greater than the maximum number of virtual cores of the Yarn cluster.We should check during deployment that the task slots are not configured to be larger than the virtual cores.2016-11-02 14:39:01,948 ERROR org.apache.flink.yarn.YarnFlinkResourceManager - FATAL ERROR IN YARN APPLICATION MASTER: Connection to YARN Resource Manager failedorg.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested virtual cores &lt; 0, or requested virtual cores &gt; max configured, requestedVirtualCores=3, maxVirtualCores=1</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5004" opendate="2016-11-3 00:00:00" fixdate="2016-11-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add task manager option to disable queryable state server</summary>
      <description>Add a task manager option to disable starting up the queryable state server. By default it is enabled. The testing cluster should disable it.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.NetworkEnvironmentConfiguration.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="5007" opendate="2016-11-3 00:00:00" fixdate="2016-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain externalized checkpoint on suspension</summary>
      <description>Externalized checkpoints are cleaned up when the job is suspended. Suspensions happen on graceful shut down (non-HA) or loss of leadership (HA).In case of HA, the checkpoint store does not clean up any checkpoints as they might be recovered by a new leader. The only way to stop a HA job is to actually cancel it. Therefore the configured clean up behaviour doesn't matter.In case of non-HA, suspensions happen because of graceful shut down (for example stopping a YARN session). In this case I would treat the clean up behaviour similar to cancelling the job.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION =&gt; delete on suspensionExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION =&gt; retain on suspension</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointPropertiesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
    </fixedFiles>
  </bug>
  <bug id="5027" opendate="2016-11-7 00:00:00" fixdate="2016-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileSource finishes successfully with a wrong path</summary>
      <description>When reading from a non existing File or a wrong file path, a Flink streaming program finishes successfully even though it should fail with a appropriate error message. This can be misleading since the user is not notified about the wrong file path. I think the default behaviour should be that we fail a file streaming source if the file path is wrong.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5033" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP operators don&amp;#39;t properly advance time</summary>
      <description>The CEP operator don't properly advance time in case of watermarks. Arriving watermarks should trigger the triggering of timeouts and pruning of event sequences. However, this works only if an element has arrived in the meantime. This is a bug and the time should also be advanced in case that no element arrived before the watermark.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPBasePatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="5041" opendate="2016-11-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement savepoint backwards compatibility 1.1 -&gt; 1.2</summary>
      <description>This issue tracks the implementation of backwards compatibility between Flink 1.1 and 1.2 releases.This task subsumes: Converting old savepoints to new savepoints, including a conversion of state handles to their new replacement. Converting keyed state from old backend implementations to their new counterparts. Converting operator and function state for all changed operators. Ensure backwards compatibility of the hashes used to generate JobVertexIds.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.MigrationV0ToV1Test.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.Savepoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.state.MigrationStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.state.MigrationKeyGroupStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.runtime.checkpoint.savepoint.SavepointV0Serializer.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerLike.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointSerializers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.InstantiationUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayOutputStreamWithPos.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5047" opendate="2016-11-10 00:00:00" fixdate="2016-1-10 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add sliding group-windows for batch tables</summary>
      <description>Add Slide group-windows for batch tables as described in FLIP-11.There are two ways to implement sliding windows for batch:1. replicate the output in order to assign keys for overlapping windows. This is probably the more straight-forward implementation and supports any aggregation function but blows up the data volume.2. if the aggregation functions are combinable / pre-aggregatable, we can also find the largest tumbling window size from which the sliding windows can be assembled. This is basically the technique used to express sliding windows with plain SQL (GROUP BY + OVER clauses). For a sliding window Slide(10 minutes, 2 minutes) this would mean to first compute aggregates of non-overlapping (tumbling) 2 minute windows and assembling consecutively 5 of these into a sliding window (could be done in a MapPartition with sorted input). The implementation could be done as an optimizer rule to split the sliding aggregate into a tumbling aggregate and a SQL WINDOW operator. Maybe it makes sense to implement the WINDOW clause first and reuse this for sliding windows.3. There is also a third, hybrid solution: Doing the pre-aggregation on the largest non-overlapping windows (as in 2) and replicating these results and processing those as in the 1) approach. The benefits of this is that it a) is based on the implementation that supports non-combinable aggregates (which is required in any case) and b) that it does not require the implementation of the SQL WINDOW operator. Internally, this can be implemented again as an optimizer rule that translates the SlidingWindow into a pre-aggregating TublingWindow and a final SlidingWindow (with replication).see FLINK-4692 for more discussion</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5071" opendate="2016-11-15 00:00:00" fixdate="2016-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN: yarn.containers.vcores config not respected when checking for vcores</summary>
      <description>The YarnClient validates whether the number of task slots is less then the max vcores settings of yarn but seems to ignore the yarn.containers.vcores flink config which should be used instead of the slots.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5075" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer incorrectly determines shards as newly discovered when tested against Kinesalite</summary>
      <description>A user reported that when our Kinesis connector is used against Kinesalite (https://github.com/mhart/kinesalite), we're incorrectly determining already found shards as newly discovered:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Subtask-keeps-on-discovering-new-Kinesis-shard-when-using-Kinesalite-td10133.htmlI suspect the problem to be the mock Kinesis API implementations of Kinesalite doesn't completely match with the official AWS Kinesis behaviour.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="5076" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutting down TM when shutting down new mini cluster</summary>
      <description>Currently we don't shut down task manager when shutting down mini cluster. It will cause mini cluster can not exit normally.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="5084" opendate="2016-11-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Java Table API integration tests by unit tests</summary>
      <description>The Java Table API is a wrapper on top of the Scala Table API. Instead of operating directly with Expressions like the Scala API, the Java API accepts a String parameter which is parsed into Expressions.We could therefore replace the Java Table API ITCases by tests that check that the parsing step produces a valid logical plan.This could be done by creating two Table objects for an identical query once with the Scala Expression API and one with the Java String API and comparing the logical plans of both Table objects. Basically something like the following:val ds1 = CollectionDataSets.getSmall3TupleDataSet(env).toTable(tEnv, 'a, 'b, 'c)val ds2 = CollectionDataSets.get5TupleDataSet(env).toTable(tEnv, 'd, 'e, 'f, 'g, 'h)val joinT1 = ds1.join(ds2).where('b === 'e).select('c, 'g)val joinT2 = ds1.join(ds2).where("b = e").select("c, g")val lPlan1 = joinT1.logicalPlanval lPlan2 = joinT2.logicalPlanAssert.assertEquals("Logical Plans do not match", lPlan1, lPlan2)</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.ExplainTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5109" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Content-Encoding Header in REST API responses</summary>
      <description>On REST API calls the Flink runtime responds with the header Content-Encoding, containing the value "utf-8". According to the HTTP/1.1 standard this header is invalid. ( https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5 ) Possible acceptable values are: gzip, compress, deflate. Or it should be omitted.The invalid header may cause malfunction in projects building against Flink.The invalid header may be present in earlier versions aswell.Proposed solution: Remove lines from the project, where CONTENT_ENCODING header is set to "utf-8". (I could do this in a PR.)Possible solution but may need further knowledge and skills than mine: Introduce content-encoding. Doing so may need some configuration beacuse then Flink would have to encode the responses properly (even paying attention to the request's Accept-Encoding headers).</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5118" opendate="2016-11-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent records sent/received metrics</summary>
      <description>In 1.2-SNAPSHOT running a large scale job you see that the counts for send/received records are inconsistent, e.g. in a simple word count job we see more received records/bytes than we see sent. This is a regression from 1.1 where everything works as expected.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5119" opendate="2016-11-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5128" opendate="2016-11-22 00:00:00" fixdate="2016-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get Kafka partitions in FlinkKafkaProducer only if a partitioner is set</summary>
      <description>The fetched partitions list is only used when calling open(...) for a user supplied custom partitioner in FlinkKafkaProducer.Therefore, we can actually only fetch the partition list if the user used a partitioner (right now we always do the partition fetching).</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5145" opendate="2016-11-23 00:00:00" fixdate="2016-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebInterface to aggressive in pulling metrics</summary>
      <description>The WebInterface currently fetches the values for every available metric in a single request, regardless of whether they are selected or not. For larger jobs with a high parallelism this can put put a big burden on connections.I propose to limit the polling to metrics that are actually selected.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5146" opendate="2016-11-23 00:00:00" fixdate="2016-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improved resource cleanup in RocksDB keyed state backend</summary>
      <description>Currently, the resources such as taken snapshots or iterators are not always cleaned up in the RocksDB state backend. In particular, not starting the runnable future will leave taken snapshots unreleased.We should improve the releases of all resources allocated through the RocksDB JNI bridge.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.OneShotLatch.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.async.AsyncStoppableTaskWithCallback.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.async.AsyncDoneCallback.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.async.AbstractAsyncIOCallable.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5150" opendate="2016-11-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI metric-related resource leak</summary>
      <description>The WebUI maintains a list of selected metrics for all jobs and vertices. When a metric is selected in the metric screen it is added to this list, and removed if it is unselected.The contents of this list are stored in the browser's localStorage. This allows a user to setup a metric screen, move to another page, and return to the original screen completely intact.However, if the metrics are never unselected by the user they will remain in this list. They will also still be in this list if the WebUI can't even display the corresponding job page anymore, if for example the history size limit was exceeded. They will even survive a browser restart, since they are not stored in a session-based storage.Furthermore, the WebUI still tries to update these metricsd, adding additional overhead to the WebBackend and potentially network.In other words, if you ever checked out metrics tab for some job, chances are that the next time you start the WebInterface it will still try to update the metrics for it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5159" opendate="2016-11-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve perfomance of inner joins with a single row input</summary>
      <description>All inner joins (including a cross join) can be implemented as a MapFunction if one of their inputs is a single row. This row can be passed to a MapFunction as a BroadcastSet.This approach is going to be more lightweight than the other current strategies.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5173" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade RocksDB dependency</summary>
      <description>The current RocksDB version has some bugs which have been observed to cause data corruption in at least one case.Newer RocksDB versions also support Microsoft Windows.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5187" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create analog of Row in core</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug id="5188" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust all the imports referencing types.Row and move RowCsvInputFormat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.ProjectableTableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sources.CsvTableSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sinks.CsvTableSink.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateReduceFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowSerializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.Aggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5192" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide better log config templates</summary>
      <description>Our current log config template is very generic and invites users to always set the root logger to DEBUG if they want to get more details. Since Flink depends on libraries like Akka and it's common to run Flink with other systems like Hadoop or Kafka, this also increases the log levels for those systems.I would propose to split the default logger configuration up and provide a separate debugging template. Furthermore, some noisy loggers might turned off by default, for example HadoopFileSystem.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5196" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log InputChannelDescriptor</summary>
      <description>Logging the InputChannelDescriptors is very noisy and usually infeasible to parse for larger setups with all-to-all connections.In a log of a larger scale Flink job this lead to a 11 fold reduction in file size (175 to 15 MBs).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5199" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging of submitted job graph actions in HA case</summary>
      <description>Include the involved paths (ZK and FS) when logging and make sure they happen for each operation (put, get, delete).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.SubmittedJobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="5211" opendate="2016-11-30 00:00:00" fixdate="2016-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include an example configuration for all reporters</summary>
      <description>We should extend the reporter documentation to include an example configuration for every reporter.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="5223" opendate="2016-12-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation of UDTF in Table API &amp; SQL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5292" opendate="2016-12-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the operators for 1.1-&gt;1.2 backwards compatible.</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.runtime.checkpoint.savepoint.SavepointV0Serializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamCheckpointedOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5293" opendate="2016-12-8 00:00:00" fixdate="2016-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the Kafka consumer backwards compatible.</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5303" opendate="2016-12-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CUBE/ROLLUP/GROUPING SETS operator in SQL</summary>
      <description>Add support for such operators as CUBE, ROLLUP and GROUPING SETS in SQL.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5304" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Change method name from crossApply to join in Table API</summary>
      <description>Currently, the UDTF in Table API is used with crossApply, but is used with JOIN in SQL. UDTF is something similar to Table, so it make sense to use .join("udtf(c) as (s)") in Table API too, and join is more familiar to users than crossApply.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.functions.TableFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5307" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log configuration for every reporter</summary>
      <description>Mandatory properties, like host names or ports, generally lead to exceptions if they are wrongly configured. Optional properties however revert to default-values in case these properties are not configured at all.Logging all configured properties will make it easier to find typos/missing optional properties in the configuration.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-ganglia.src.main.java.org.apache.flink.metrics.ganglia.GangliaReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="5310" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the RocksDB JNI library loading</summary>
      <description>Currently, the RocksDB JNI library is automatically and implicitly loaded by RocksDB upon initialization. If the loading fails, there is little information about why the loading failed.We should explicitly load the JNI library with retries and log better error information.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5311" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write user documentation for BipartiteGraph</summary>
      <description>We need to add user documentation. The progress on BipartiteGraph can be tracked in the following JIRA:https://issues.apache.org/jira/browse/FLINK-2254</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.gelly.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5318" opendate="2016-12-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the Rolling sink backwards compatible.</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.RollingSink.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="5328" opendate="2016-12-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConnectedComponentsITCase testJobWithoutObjectReuse fails</summary>
      <description>I've seen this fail a couple of times now: ConnectedComponentsITCase#testJobWithoutObjectReuse.Job execution failed.org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply$mcV$sp(JobManager.scala:900) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:843) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:843) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.io.IOException: Stream Closed at java.io.FileInputStream.readBytes(Native Method) at java.io.FileInputStream.read(FileInputStream.java:272) at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:72) at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:59) at org.apache.flink.api.common.io.DelimitedInputFormat.fillBuffer(DelimitedInputFormat.java:662) at org.apache.flink.api.common.io.DelimitedInputFormat.readLine(DelimitedInputFormat.java:556) at org.apache.flink.api.common.io.DelimitedInputFormat.nextRecord(DelimitedInputFormat.java:522) at org.apache.flink.api.java.io.CsvInputFormat.nextRecord(CsvInputFormat.java:78) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:166) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:654) at java.lang.Thread.run(Thread.java:745)Complete logs are attached.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="5350" opendate="2016-12-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t overwrite existing Jaas config property</summary>
      <description>If an existing Jaas configuration has been specified via the property java.security.auth.login.config, it should be used instead of overwriting the property.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.SecurityUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.HadoopSecurityContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5357" opendate="2016-12-16 00:00:00" fixdate="2016-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordCountTable fails</summary>
      <description>The execution of org.apache.flink.table.examples.java.WordCountTable fails:Exception in thread "main" org.apache.flink.table.api.TableException: POJO does not define field name: TMP_0 at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:85) at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:81) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.flink.table.typeutils.TypeConverter$.determineReturnType(TypeConverter.scala:81) at org.apache.flink.table.plan.nodes.dataset.DataSetCalc.translateToPlan(DataSetCalc.scala:110) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:305) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:289) at org.apache.flink.table.api.java.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:146) at org.apache.flink.table.examples.java.WordCountTable.main(WordCountTable.java:56) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5365" opendate="2016-12-19 00:00:00" fixdate="2016-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos AppMaster/TaskManager should obey sigterm</summary>
      <description>The AppMaster and TaskManager are ignoring the sigterm sent by Marathon/Mesos. The reason is simply that the shell scripts used to start them don't pass the signal to java.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5366" opendate="2016-12-19 00:00:00" fixdate="2016-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Savepoint Backwards Compatibility</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.UserFunctionStateJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5377" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve savepoint docs</summary>
      <description>The savepoint docs are very detailed and focus on the internals. They should better convey what users have to take care of.The following questions should be answered:What happens if I add a new operator that requires state to my flow?What happens if I delete an operator that has state to my flow?What happens if I reorder stateful operators in my flow?What happens if I add or delete or reorder operators that have no state in my flow?Should I apply .uid to all operators in my flow?Should I apply .uid to only the operators that have state?</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
      <file type="M">docs.fig.savepoints-program.ids.png</file>
      <file type="M">docs.fig.savepoints-overview.png</file>
    </fixedFiles>
  </bug>
  <bug id="5378" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5380" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of outgoing records not reported in web interface</summary>
      <description>The web frontend does not report any outgoing records in the web frontend.The amount of data in MB is reported correctly.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5381" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrolling in some web interface pages doesn&amp;#39;t work (taskmanager details, jobmanager config)</summary>
      <description>It seems that scrolling in the web interface doesn't work anymore on some pages in the 1.2 release branch.Example pages: When you click the "JobManager" tab The TaskManager logs page</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
    </fixedFiles>
  </bug>
  <bug id="5408" opendate="2017-1-4 00:00:00" fixdate="2017-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB initialization can fail with an UnsatisfiedLinkError in the presence of multiple classloaders</summary>
      <description>When the RocksDB is loaded from different ClassLoaders (for example because it is in the user code jar, or loaded dynamically in tests) it may fail with an "java.lang.UnsatisfiedLinkError: Native Library /path/to/temp/dir/librocksdbjni-linux64.so already loaded in another classloader.Apparently the JVM can handle multiple instances of the same JNI library being loaded in different class loaders, but not when coming from the same file path.This affects only version 1.2 onward, because from there we extract the JNI library into Flink's temp folders to make sure that it gets cleaned up for example by YARN when the application finishes. When giving a parent directory, RocksDB does not add a unique number sequence to the temp file name.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5417" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the wrong config file name</summary>
      <description>As the config file name is conf/flink-conf.yaml, the usage "conf/flink-config.yaml" in document is wrong and easy to confuse user. We should correct them.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.fig.slots.parallelism.svg</file>
    </fixedFiles>
  </bug>
  <bug id="5423" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement Stochastic Outlier Selection</summary>
      <description>I've implemented the Stochastic Outlier Selection (SOS) algorithm by Jeroen Jansen.http://jeroenjanssens.com/2013/11/24/stochastic-outlier-selection.htmlIntegrated as much as possible with the components from the machine learning library.The algorithm itself has been compared to four other algorithms and it it shows that SOS has a higher performance on most of these real-world datasets.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5424" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Restart Strategy Logging</summary>
      <description>I'll be submitting a PR which includes some minor improvements to logging related to restart strategies.Specifically, I added a toString so that the log contains better info about failure-rate restart strategy, and I added an explanation in the log when the restart strategy is responsible for preventing job restart (currently, there's no indication that the restart strategy had anything to do with it).</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.FailureRateRestartStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="5434" opendate="2017-1-10 00:00:00" fixdate="2017-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unsupported project() transformation from Scala DataStream docs</summary>
      <description>The Scala DataStream does not have a project() transformation, yet the docs include it as a supported operation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5447" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync documentation of built-in functions for Table API with SQL</summary>
      <description>I will split up the documentation for the built-in functions similar to the SQL structure.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5452" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make table unit tests pass under cluster mode</summary>
      <description>Currently if we change the test execution mode to TestExecutionMode.CLUSTER in TableProgramsTestBase, some cases will fail. Need to figure out whether it's the case design problem or there are some bugs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5454" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation about how to tune Checkpointing for large state</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="5455" opendate="2017-1-11 00:00:00" fixdate="2017-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation how to upgrade jobs and Flink framework versions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug id="5458" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Add documentation how to migrate from Flink 1.1. to Flink 1.2</summary>
      <description>Docs should go to docs/dev/migration.md</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5465" opendate="2017-1-11 00:00:00" fixdate="2017-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB fails with segfault while calling AbstractRocksDBState.clear()</summary>
      <description>I'm using Flink 699f4b0.## A fatal error has been detected by the Java Runtime Environment:## SIGSEGV (0xb) at pc=0x00007f91a0d49b78, pid=26662, tid=140263356024576## JRE version: Java(TM) SE Runtime Environment (7.0_67-b01) (build 1.7.0_67-b01)# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode linux-amd64 compressed oops)# Problematic frame:# C [librocksdbjni-linux64.so+0x1aeb78] rocksdb::GetColumnFamilyID(rocksdb::ColumnFamilyHandle*)+0x8## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## An error report file with more information is saved as:# /yarn/nm/usercache/robert/appcache/application_1484132267957_0007/container_1484132267957_0007_01_000010/hs_err_pid26662.logCompiled method (nm) 1869778 903 n org.rocksdb.RocksDB::remove (native) total in heap [0x00007f91b40b9dd0,0x00007f91b40ba150] = 896 relocation [0x00007f91b40b9ef0,0x00007f91b40b9f48] = 88 main code [0x00007f91b40b9f60,0x00007f91b40ba150] = 496## If you would like to submit a bug report, please visit:# http://bugreport.sun.com/bugreport/crash.jsp# The crash happened outside the Java Virtual Machine in native code.# See problematic frame for where to report the bug.#</description>
      <version>1.2.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
    </fixedFiles>
  </bug>
  <bug id="5467" opendate="2017-1-12 00:00:00" fixdate="2017-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stateless chained tasks set legacy operator state</summary>
      <description>I discovered this while trying to rescale a job with a Kafka source with a chained stateless operator.Looking into it, it turns out that this fails, because the checkpointed state contains legacy operator state for the chained operator although it is state less./cc aljoscha You mentioned that this might be a possible duplicate?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5473" opendate="2017-1-12 00:00:00" fixdate="2017-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>setMaxParallelism() higher than 1 is possible on non-parallel operators</summary>
      <description>While trying out Flink 1.2, I found out that you can set a maxParallelism higher than 1 on a non-parallel operator.I think we should have the same semantics as the setParallelism() method.Also, when setting a global maxParallelism in the execution environment, it will be set as a default value for the non-parallel operator.When restoring a savepoint from 1.1, you have to set the maxParallelism to the parallelism of the 1.1 job. Non-parallel operators will then also get the maxPar set to this value, leading to an error on restore.So currently, users restoring from 1.1 to 1.2 have to manually set the maxParallelism to 1 for all non-parallel operators.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupRangeAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5474" opendate="2017-1-12 00:00:00" fixdate="2017-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend DC/OS documentation</summary>
      <description>We could extend the DC/OS documentation a little bit to include information about how to submit a job (where to find the connection information) and that one has to install the DC/OS cli in order to add the development universe.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug id="5481" opendate="2017-1-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify Row creation</summary>
      <description>When we use ExecutionEnvironment#fromElements(X... data) it takes first element of data to define a type. If first Row in collection has wrong number of fields (there are nulls) TypeExtractor returns not RowTypeInfo, but GenericType&lt;Row&gt;</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.Types.java</file>
    </fixedFiles>
  </bug>
  <bug id="5484" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kryo serialization changed between 1.1 and 1.2</summary>
      <description>I think the way that Kryo serializes data changed between 1.1 and 1.2.I have a generic Object that is serialized as part of a 1.1 savepoint that I cannot resume from with 1.2:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427) at org.apache.flink.client.program.StandaloneClusterClient.submitJob(StandaloneClusterClient.java:101) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:400) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:68) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1486) at com.dataartisans.DidKryoChange.main(DidKryoChange.java:74) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:528) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:419) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:339) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:831) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:256) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1073) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1120) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1117) at org.apache.flink.runtime.security.HadoopSecurityContext$1.run(HadoopSecurityContext.java:43) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:40) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1117)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:900) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:843) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:843) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.IllegalStateException: Could not initialize keyed state backend. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:286) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:199) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeOperators(StreamTask.java:649) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:636) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:654) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: com.esotericsoftware.kryo.KryoException: Unable to find class: f at com.twitter.chill.java.UnmodifiableJavaCollectionSerializer.read(UnmodifiableJavaCollectionSerializer.java:62) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:232) at org.apache.flink.migration.runtime.state.memory.AbstractMemStateSnapshot.deserialize(AbstractMemStateSnapshot.java:88) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restoreHeapState(HeapKeyedStateBackend.java:448) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restoreOldSavepointKeyedState(HeapKeyedStateBackend.java:406) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restore(HeapKeyedStateBackend.java:240) at org.apache.flink.streaming.runtime.tasks.StreamTask.createKeyedStateBackend(StreamTask.java:784) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:277) ... 6 moreCaused by: com.esotericsoftware.kryo.KryoException: Unable to find class: f at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138) at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) at com.twitter.chill.java.UnmodifiableJavaCollectionSerializer.read(UnmodifiableJavaCollectionSerializer.java:59) ... 14 moreCaused by: java.lang.ClassNotFoundException: f at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 18 moreRunning the same program with 1.2 and triggering and resuming a savepoint works.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5494" opendate="2017-1-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Mesos documentation</summary>
      <description>Flink's Mesos documentation could benefit from more details how to set things up and which parameters to use.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.mesos.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="5496" opendate="2017-1-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when using Mesos HA mode</summary>
      <description>When using the Mesos' HA mode, one cannot start the Mesos appmaster, because the following class cast exception occurs:java.lang.ClassCastException: org.apache.flink.shaded.org.apache.curator.framework.imps.CuratorFrameworkImpl cannot be cast to org.apache.flink.mesos.shaded.org.apache.curator.framework.CuratorFramework at org.apache.flink.mesos.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:38) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.createWorkerStore(MesosApplicationMasterRunner.java:510) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.runPrivileged(MesosApplicationMasterRunner.java:320) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:178) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:175) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:29) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.run(MesosApplicationMasterRunner.java:175) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.main(MesosApplicationMasterRunner.java:135)It seems as if the flink-mesos module relocates the curator dependency in another namespace than flink-runtime. Not sure why this is done.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5497" opendate="2017-1-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove duplicated tests</summary>
      <description>Now we have test which run the same code 4 times, every run 17+ seconds.Need do small refactoring and remove duplicated code.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5502" opendate="2017-1-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about migrating functions from 1.1 to 1.2</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
    </fixedFiles>
  </bug>
  <bug id="5507" opendate="2017-1-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove queryable list state sink</summary>
      <description>The queryable state "sink" using ListState (".asQueryableState(&lt;name&gt;, ListStateDescriptor)") stores all incoming data forever and is never cleaned. Eventually, it will pile up too much memory and is thus of limited use.We should remove it from the API.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCase.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="5508" opendate="2017-1-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Mesos dynamic class loading</summary>
      <description>Mesos uses dynamic class loading in order to load the ZooKeeperStateHandleStore and the CuratorFramework class. This can be replaced by a compile time dependency.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="5512" opendate="2017-1-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ documentation should inform that exactly-once holds for RMQSource only when parallelism is 1</summary>
      <description>See here for the reasoning: FLINK-2624. We should add an informative warning about this limitation in the docs.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug id="5517" opendate="2017-1-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version to 1.3.0</summary>
      <description>In the thread 'Help using HBase with Flink 1.1.4', Giuliano reported seeing:java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.&lt;init&gt;()V from class org.apache.hadoop.hbase.zookeeper.MetaTableLocatorThe above has been solved by HBASE-14963hbase 1.3.0 is being released.We should upgrade hbase dependency to 1.3.0</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5524" opendate="2017-1-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support early out for code generated conjunctive conditions</summary>
      <description>Currently, all nested conditions for a conjunctive predicate are evaluated before the conjunction is checked.A condition like (v1 == v2) &amp;&amp; (v3 &lt; 5) would be compiled intoboolean res1;if (v1 == v2) { res1 = true;} else { res1 = false;}boolean res2;if (v3 &lt; 5) { res2 = true;} else { res2 = false;}boolean res3;if (res1 &amp;&amp; res2) { res3 = true;} else { res3 = false;}if (res3) { // emit something}It would be better to leave the generated code as early as possible, e.g., with a return instead of res1 = false. The code generator needs a bit of context information for that.</description>
      <version>1.1.4,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5531" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSl code block formatting is broken</summary>
      <description>Most code blocks on the ssl page aren't rendered properly and are simply shown as text.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="5555" opendate="2017-1-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about debugging watermarks</summary>
      <description>This was a frequent question on the mailing list.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5562" opendate="2017-1-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Driver fixes</summary>
      <description>Improve parametrization and output formatting.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.AnalyticHelper.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.AbstractGraphAnalytic.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.HITS.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.GraphMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.Graph500.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  <bug id="5563" opendate="2017-1-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add density to vertex metrics</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.metric.undirected.VertexMetricsTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.metric.directed.VertexMetricsTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.AsmTestBase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.metric.undirected.VertexMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.metric.directed.VertexMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="5568" opendate="2017-1-19 00:00:00" fixdate="2017-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interface for catalog, and provide an in-memory implementation, and integrate with calcite schema</summary>
      <description>The TableEnvironment now provides a mechanism to register temporary table. It registers the temp table to calcite catalog, so SQL and TableAPI queries can access to those temp tables. Now DatasetTable, DataStreamTable and TableSourceTable can be registered to TableEnvironment as temporary tables.This issue wants to provides a mechanism to connect external catalogs such as HCatalog to the TableEnvironment, so SQL and TableAPI queries could access to tables in the external catalogs without register those tables to TableEnvironment beforehand.First, we should point out that there are two kinds of catalog in Flink actually. The first one is external catalog as we mentioned before, it provides CRUD operations to databases/tables.The second one is calcite catalog, it defines namespace that can be accessed in Calcite queries. It depends on Calcite Schema/Table abstraction. SqlValidator and SqlConverter depends on the calcite catalog to fetch the tables in SQL or TableAPI.So we need to do the following things:1. introduce interface for external catalog, maybe provide an in-memory implementation first for test and develop environment.2. introduce a mechanism to connect external catalog with Calcite catalog so the tables/databases in external catalog can be accessed in Calcite catalog. Including convert databases of externalCatalog to Calcite sub-schemas, convert tables in a database of externalCatalog to Calcite tables (only support TableSourceTable).3. register external catalog to TableEnvironment.Here is the design mode of ExternalCatalogTable. identifier TableIdentifier dbName and tableName of table tableType String type of external catalog table, e.g csv, hbase, kafka schema DataSchema schema of table data, including column names and column types partitionColumnNames List&lt;String&gt; names of partition column properties Map&lt;String, String&gt; properties of external catalog table stats TableStats statistics of external catalog table comment String create time longThere is still a detail problem need to be take into consideration, that is , how to convert ExternalCatalogTable to TableSourceTable. The question is equals to convert ExternalCatalogTable to TableSource because we could easily get TableSourceTable from TableSource.Because different TableSource often contains different fields to initiate an instance. E.g. CsvTableSource needs path, fieldName, fieldTypes, fieldDelim, rowDelim and so on to create a new instance , KafkaTableSource needs configuration and tableName to create a new instance. So it's not a good idea to let Flink framework be responsible for translate ExternalCatalogTable to different kind of TableSourceTable. Here is one solution. Let TableSource specify a converter.1. provide an Annatition named ExternalCatalogCompatible. The TableSource with the annotation means it is compatible with external catalog, that is, it could be converted to or from ExternalCatalogTable. This annotation specifies the tabletype and converter of the tableSource. For example, for CsvTableSource, it specifies the tableType is csv and converter class is CsvTableSourceConverter.@ExternalCatalogCompatible(tableType = "csv", converter = classOf[CsvTableSourceConverter])class CsvTableSource(...) {...}2. Scan all TableSources with the ExternalCatalogCompatible annotation, save the tableType and converter in a Map3. When need to convert ExternalCatalogTable to TableSource , get the converter based on tableType. and let converter do convert</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.CommonTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.exceptions.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5577" opendate="2017-1-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Each time application is submitted to yarn, application id increases by two</summary>
      <description>I tested to run a long-running cluster and single job on yarn, in both cases, each time the application id would increase by two.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClientV2.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5580" opendate="2017-1-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberos keytabs not working for YARN deployment mode</summary>
      <description>Setup: Kerberos security using keytabs, Flink session on YARN deployment (in standalone, it works fine without problems).I’m getting these error messages in the YARN node managers, causing the TaskManager containers to fail to start properly:org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:tzulitai (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:&amp;#91;TOKEN, KERBEROS&amp;#93;The security configuration for Hadoop has been set to "kerberos", to the "auto: SIMPLE" seems very strange. It also seems as if credential tokens has not been properly set for the ContainerLaunchContext s, which may be an issue causing this.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="5582" opendate="2017-1-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a general distributive aggregate function</summary>
      <description>The DataStream API currently has two aggregation functions that can be used on windows and in state, both of which have limitations: ReduceFunction only supports one type as the type that is added and aggregated/returned. FoldFunction Supports different types to add and return, but is not distributive, i.e. it cannot be used for hierarchical aggregation, for example to split the aggregation into to pre- and final-aggregation.I suggest to add a generic and powerful aggregation function that supports: Different types to add, accumulate, and return The ability to merge partial aggregated by merging the accumulated type.The proposed interface is below. This type of interface is found in many APIs, like that of various databases, and also in Apache Beam: The accumulator is the state of the running aggregate Accumulators can be merged Values are added to the accumulator Getting the result from the accumulator perform an optional finalizing operationpublic interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function { ACC createAccumulator(); void add(IN value, ACC accumulator); OUT getResult(ACC accumulator); ACC merge(ACC a, ACC b);}Example use:public class AverageAccumulator { long count; long sum;}// implementation of a simple averagepublic class Average implements AggregateFunction&lt;Integer, AverageAccumulator, Double&gt; { public AverageAccumulator createAccumulator() { return new AverageAccumulator(); } public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) { a.count += b.count; a.sum += b.sum; return a; } public void add(Integer value, AverageAccumulator acc) { acc.sum += value; acc.count++; } public Double getResult(AverageAccumulator acc) { return acc.sum / (double) acc.count; }}// implementation of a weighted average// this reuses the same accumulator type as the aggregate function for 'average'public class WeightedAverage implements AggregateFunction&lt;Datum, AverageAccumulator, Double&gt; { public AverageAccumulator createAccumulator() { return new AverageAccumulator(); } public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) { a.count += b.count; a.sum += b.sum; return a; } public void add(Datum value, AverageAccumulator acc) { acc.count += value.getWeight(); acc.sum += value.getValue(); } public Double getResult(AverageAccumulator acc) { return acc.sum / (double) acc.count; }}</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPos.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.CloseableRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5613" opendate="2017-1-23 00:00:00" fixdate="2017-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryableState: requesting a non-existing key in RocksDBStateBackend is not consistent with the MemoryStateBackend and FsStateBackend</summary>
      <description>Querying for a non-existing key for a state that has a default value set currently results in the default value being returned in the RocksDBStateBackend only. MemoryStateBackend or FsStateBackend will return null which results in an UnknownKeyOrNamespace exception.Default values are now deprecated and will be removed eventually so we should not introduce them into the new queryable state API and thus adapt the RocksDBStateBackend accordingly.This refers to FLINK-5527 but solves the initial issues the opposite way.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCase.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
    </fixedFiles>
  </bug>
  <bug id="5617" opendate="2017-1-23 00:00:00" fixdate="2017-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check new public APIs in 1.2 release</summary>
      <description>Before releasing Flink 1.2.0, I would like to quickly review which new public methods we are supporting in future releases.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="5639" opendate="2017-1-25 00:00:00" fixdate="2017-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify License implications of RabbitMQ Connector</summary>
      <description>The RabbitMQ Connector has a Maven dependency under MPL 1.1.The ASF legal FAQ (https://www.apache.org/legal/resolved) classifies that as "may be included in binary form within an Apache product if the inclusion is appropriately labeled".Because we neither include sources nor binaries (but only define a Maven dependency) it is probably not relevant to Flink. But to be on the safe side for the project and users, we should add a notice as blow to the docs and the project:# License of the Rabbit MQ ConnectorFlink's RabbitMQ connector defines a Maven dependency on the"RabbitMQ AMQP Java Client", licensed under theMozilla Public License v1.1 (MPL 1.1).Flink itself neither reuses source code from the "RabbitMQ AMQP Java Client"nor packages binaries from the "RabbitMQ AMQP Java Client".Users that create and publish derivative work based on Flink'sRabbitMQ connector (thereby re-distributing the "RabbitMQ AMQP Java Client")must be aware that this may be subject to conditions declaredin the Mozilla Public License v1.1 (MPL 1.1).</description>
      <version>1.1.4,1.2.0</version>
      <fixedVersion>1.1.5,1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug id="5640" opendate="2017-1-25 00:00:00" fixdate="2017-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>configure the explicit Unit Test file suffix</summary>
      <description>There are four types of Unit Test file: *ITCase.java, *Test.java, *ITSuite.scala, *Suite.scalaFile name ending with "IT.java" is integration test. File name ending with "Test.java" is unit test.It's clear for Surefire plugin of default-test execution to declare that "Test." is Java Unit Test.The test file statistics below: Suite total: 10 ITCase total: 378 Test total: 1008 ITSuite total: 14</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5644" opendate="2017-1-25 00:00:00" fixdate="2017-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task#lastCheckpointSize metric broken</summary>
      <description>The lastCheckpointSIze metric was broken when we introduced the key-groups. I couldn't find an easy way to fix the metric, as such i propose to remove it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5659" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileBaseUtils#deleteFileOrDirectory not thread-safe on Windows</summary>
      <description>The FileBaseUtils#deleteFileOrDirectory is not thread-safe on Windows.First you will run into AccessDeniedExceptions since one thread tried to delete a file while another thread was already doing that, for which the file has to be opened.Once you resolve those exceptions (by catching them double checking whether the file still exists), you run into DirectoryNotEmptyExceptions since there is some wacky timing/visibility issue when deleting files concurrently.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.spotbugs-exclude.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.LambdaUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.function.ThrowingConsumer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5666" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Blob files are not cleaned up from ZK storage directory</summary>
      <description>When running a job with HA in an standalone cluster, the blob files are not cleaned up from the ZooKeeper storage directory.:zkStorageDir/blob/cache/blob_:randNicoK Have you seen such a behaviour while refactoring the blob server?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5667" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possible state data loss when task fails while checkpointing</summary>
      <description>It is possible that Flink loses state data when a Task fails while a checkpoint is being drawn. The scenario is the following:Flink has finished the synchronous checkpointing part and starts the asynchronous part by creating and submitting a AsyncCheckpointRunnable to an Executor. This runnable is also registered at the closeable registry. If the Task now fails before the AsyncCheckpointRunnable has completed, it will be closed due to being registered in the closeable registry. The closing operation will discard all state handles and cancel all runnable state futures. However, it will not stop the runnable from sending an acknowledge message to the CheckpointCoordinator.If this message completes the pending checkpoint, then this checkpoint will be transformed into a CompletedCheckpoint which is faulty (some of the data has already been deleted). Depending on Flink's configuration, this will discard older completed checkpoints and thus we will have state data loss.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2017-1-27 00:00:00" fixdate="2017-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local RocksDB directories not cleaned up</summary>
      <description>After cancelling a job with a RocksDB backend all files are properly cleaned up, but the parent directories still exist and are empty:859546fec3dac36bb9fcc8cbdd4e291e+- StreamFlatMap_3_0+- StreamFlatMap_3_3+- StreamFlatMap_3_4+- StreamFlatMap_3_5+- StreamFlatMap_3_6The number of empty folders varies between runs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5672" opendate="2017-1-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Job fails with java.lang.IllegalArgumentException: port out of range:-1</summary>
      <description>I started the JobManager with start-local.sh and started another TaskManager with taskmanager.sh start. My job is a Table API job with a orderBy (range partitioning with parallelism 2).The job fails with the following exception:java.lang.IllegalArgumentException: port out of range:-1 at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143) at java.net.InetSocketAddress.&lt;init&gt;(InetSocketAddress.java:188) at org.apache.flink.runtime.io.network.ConnectionID.&lt;init&gt;(ConnectionID.java:47) at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:124) at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:627) at org.apache.flink.runtime.executiongraph.Execution.deployToSlot(Execution.java:358) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:284) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:279) at org.apache.flink.runtime.concurrent.impl.FlinkFuture$5.onComplete(FlinkFuture.java:259) at akka.dispatch.OnComplete.internal(Future.scala:248) at akka.dispatch.OnComplete.internal(Future.scala:245) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:175) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:172) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at org.apache.flink.runtime.concurrent.Executors$DirectExecutor.execute(Executors.java:56) at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:122) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40) at scala.concurrent.impl.Promise$KeptPromise.onComplete(Promise.scala:333) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handleAsync(FlinkFuture.java:256) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handle(FlinkFuture.java:270) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:279) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:479) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:525) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:521) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:95) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5721" opendate="2017-2-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FoldingState to State Documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="5722" opendate="2017-2-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DISTINCT as dedicated operator</summary>
      <description>DISTINCT is currently implemented for batch Table API / SQL as an aggregate which groups on all fields. Grouped aggregates are implemented as GroupReduce with sort-based combiner.This operator can be more efficiently implemented by using ReduceFunction and hinting a HashCombine strategy. The same ReduceFunction can be used for all DISTINCT operations and can be assigned with appropriate forward field annotations.We would need a custom conversion rule which translates distinct aggregations (grouping on all fields and returning all fields) into a custom DataSetRelNode.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5723" opendate="2017-2-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use "Used" instead of "Initial" to make taskmanager tag more readable</summary>
      <description>Now in JobManager web fronted, the used memory of task managers is presented as "Initial" in table header, which actually means "memory used", from codes.I'd like change it to be more readable, even it is trivial one.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
    </fixedFiles>
  </bug>
  <bug id="5728" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaProducer should flush on checkpoint by default</summary>
      <description>As discussed in FLINK-5702, it might be a good idea to let the FlinkKafkaProducer flush on checkpoints by default. Currently, it is disabled by default.It's a very simple change, but we should think about whether or not we want to break user behaviour, or have proper usage migration.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="5731" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split up CI builds</summary>
      <description>Test builds regularly time out because we are hitting the Travis 50 min limit. Previously, we worked around this by splitting up the tests into groups. I think we have to split them further.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="5747" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eager Scheduling should deploy all Tasks together</summary>
      <description>Currently, eager scheduling immediately triggers the scheduling for all vertices and their subtasks in topological order. This has two problems: This works only, as long as resource acquisition is "synchronous". With dynamic resource acquisition in FLIP-6, the resources are returned as Futures which may complete out of order. This results in out-of-order (not in topological order) scheduling of tasks which does not work for streaming. Deploying some tasks that depend on other tasks before it is clear that the other tasks have resources as well leads to situations where many deploy/recovery cycles happen before enough resources are available to get the job running fully.For eager scheduling, we should allocate all resources in one chunk and then deploy once we know that all are available.As a follow-up, the same should be done per pipelined component in lazy batch scheduling as well. That way we get lazy scheduling across blocking boundaries, and bulk (gang) scheduling in pipelined subgroups.This also does not apply for efforts of fine grained recovery, where individual tasks request replacement resources.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.Executors.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ExceptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5750" opendate="2017-2-9 00:00:00" fixdate="2017-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5756" opendate="2017-2-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When there are many values under the same key in ListState, RocksDBStateBackend performances poor</summary>
      <description>When using RocksDB as the StateBackend, if there are many values under the same key in ListState, the windowState.get() operator performances very poor. I also the the RocksDB using version 4.11.2, the performance is also very poor. The problem is likely to related to RocksDB itself's get() operator after using merge(). The problem may influences the window operation's performance when the size is very large using ListState. I try to merge 50000 values under the same key in RocksDB, It costs 120 seconds to execute get() operation.///////////////////////////////////////////////////////////////////////////////The flink's code is as follows: class SEventSource extends RichSourceFunction [SEvent] { private var count = 0L private val alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWZYX0987654321" override def run(sourceContext: SourceContext[SEvent]): Unit = { while (true) { for (i &lt;- 0 until 5000) { sourceContext.collect(SEvent(1, "hello-"+count, alphabet,1)) count += 1L } Thread.sleep(1000) } }}env.addSource(new SEventSource) .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[SEvent] { override def getCurrentWatermark: Watermark = { new Watermark(System.currentTimeMillis()) } override def extractTimestamp(t: SEvent, l: Long): Long = { System.currentTimeMillis() } }) .keyBy(0) .window(SlidingEventTimeWindows.of(Time.seconds(20), Time.seconds(2))) .apply(new WindowStatistic) .map(x =&gt; (System.currentTimeMillis(), x)) .print()////////////////////////////////////The RocksDB Test code: val stringAppendOperator = new StringAppendOperator val options = new Options() options.setCompactionStyle(CompactionStyle.LEVEL) .setCompressionType(CompressionType.SNAPPY_COMPRESSION) .setLevelCompactionDynamicLevelBytes(true) .setIncreaseParallelism(4) .setUseFsync(true) .setMaxOpenFiles(-1) .setCreateIfMissing(true) .setMergeOperator(stringAppendOperator) val write_options = new WriteOptions write_options.setSync(false) val rocksDB = RocksDB.open(options, "/******/Data/") val key = "key" val value = "abcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ7890654321" val beginmerge = System.currentTimeMillis() for(i &lt;- 0 to 50000) { rocksDB.merge(key.getBytes(), ("s"+ i + value).getBytes()) //rocksDB.put(key.getBytes, value.getBytes) } println("finish") val begin = System.currentTimeMillis() rocksDB.get(key.getBytes) val end = System.currentTimeMillis() println("merge cost:" + (begin - beginmerge)) println("Time consuming:" + (end - begin)) }}</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5767" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>New aggregate function interface and built-in aggregate functions</summary>
      <description>Add a new aggregate function interface. This includes implementing the aggregate interface, migrating the existing aggregation functions to this interface, and adding the unit tests for these functions.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5768" opendate="2017-2-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Apply new aggregation functions for datastream and dataset tables</summary>
      <description>Apply new aggregation functions for datastream and dataset tablesThis includes:1. Change the implementation of the DataStream aggregation runtime code to use new aggregation functions and aggregate dataStream API.2. DataStream will be always running in incremental mode, as explained in 06/Feb/2017 in FLINK5564.2. Change the implementation of the Dataset aggregation runtime code to use new aggregation functions.3. Clean up unused class and method.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateReduceFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5772" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Instability with embedded Elasticsearch node in ElasticsearchSink test</summary>
      <description>This was seen in: https://api.travis-ci.org/jobs/199988755/log.txt?deansi=truetestDeprecatedIndexRequestBuilderVariant(org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase) Time elapsed: 60.227 sec &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:915) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:858) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:858) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: An error occured in ElasticsearchSink. at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:234) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:208) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:38) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:63) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:263) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:667) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: ProcessClusterEventTimeoutException[failed to process cluster event (acquire index lock) within 1m] at org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.extractFailureCauseFromBulkItemResponse(Elasticsearch1ApiCallBridge.java:117) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase$1.afterBulk(ElasticsearchSinkBase.java:169) at org.elasticsearch.action.bulk.BulkProcessor.execute(BulkProcessor.java:316) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded(BulkProcessor.java:299) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:281) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:264) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:260) at org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.add(BulkProcessorIndexer.java:41) at org.apache.flink.streaming.connectors.elasticsearch.IndexRequestBuilderWrapperFunction.process(IndexRequestBuilderWrapperFunction.java:39) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:210) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:38) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:63) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:263) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:667) at java.lang.Thread.run(Thread.java:745)The embedded elasticsearch node returned a ProcessClusterEventTimeoutException and failed the test. We should add retries in the ES tests for these kind of instabilities.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5794" opendate="2017-2-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the documentation about “UDF/UDTF" support have parameters constructor.</summary>
      <description>Depends on FLINK-5792 .</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5795" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve UDF&amp;UDTF to support constructor with parameter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5800" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure that the CheckpointStreamFactory is instantiated once per operator only</summary>
      <description>Previously, the CheckpointSteamFactory was created once per checkpoint, and its repeated initialization logic (like ensuring existence of the parent directory) caused unnecessary load on some file systems at very large scale.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamCheckpointedOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5803" opendate="2017-2-15 00:00:00" fixdate="2017-3-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [partitioned] processing time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5654) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5804" opendate="2017-2-15 00:00:00" fixdate="2017-3-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [non-partitioned] processing time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. Since no PARTITION BY clause is specified, the execution will be single threaded. The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5654) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5805" opendate="2017-2-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve docs for ProcessFunction</summary>
      <description>The documentation for ProcessFunction could be a bit clearer.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="5807" opendate="2017-2-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improved wording for doc home page</summary>
      <description>The Migration Guide section of the home page of the documentation has some awkwardness.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5808" opendate="2017-2-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Missing verification for setParallelism and setMaxParallelism</summary>
      <description>When setParallelism() is called we don't verify that it is &lt;= than max parallelism. Likewise, when setMaxParallelism() is called we don't check that the new value doesn't clash with a previously set parallelism.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupRangeAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.AbstractEventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.FoldApplyWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.FoldApplyProcessWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="5814" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist creates wrong symlink when not used with cleaned before</summary>
      <description>If &lt;flink-dir&gt;/build-target already exists, 'mvn package' for flink-dist will create a symbolic link inside &lt;flink-dir&gt;/build-target instead of replacing that symlink. This is due to the behaviour of ln &amp;#45;sf for target links that point to directories and may be solved by adding the --no-dereference parameter.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5822" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Checkpoint Coordinator aware of State Backend</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.BlockingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackendFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5828" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BlobServer create cache dir has concurrency safety problem</summary>
      <description>Caused by: java.lang.RuntimeException: org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Could not upload the jar files to the job manager. at FlinkJob_20170217_161058_000004.bind(FlinkJob_20170217_161058_000004.java:45) at com.aliyun.kepler.rc.query.schedule.FlinkQueryJob.call(FlinkQueryJob.java:53) at com.aliyun.kepler.rc.query.schedule.FlinkQueryJob.call(FlinkQueryJob.java:13) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.AbstractExecutorService$2.run(AbstractExecutorService.java:120) ... 3 common frames omittedCaused by: org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Could not upload the jar files to the job manager. at com.aliyun.kepler.rc.flink.client.Client.runBlocking(Client.java:178) at org.apache.flink.api.java.ClientEnvironment.execute(ClientEnvironment.java:169) at org.apache.flink.api.java.ClientEnvironment.execute(ClientEnvironment.java:225) at FlinkJob_20170217_161058_000004.bind(FlinkJob_20170217_161058_000004.java:42) ... 7 common frames omittedCaused by: org.apache.flink.runtime.client.JobSubmissionException: Could not upload the jar files to the job manager. at org.apache.flink.runtime.client.JobClientActor$2.call(JobClientActor.java:359) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:94) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) ... 3 common frames omittedCaused by: java.io.IOException: Could not retrieve the JobManager's blob port. at org.apache.flink.runtime.blob.BlobClient.uploadJarFiles(BlobClient.java:706) at org.apache.flink.runtime.jobgraph.JobGraph.uploadUserJars(JobGraph.java:556) at org.apache.flink.runtime.client.JobClientActor$2.call(JobClientActor.java:357) ... 7 common frames omittedCaused by: java.io.IOException: PUT operation failed: Server side error: Could not create cache directory '/home/kepler/kepler3012/data/work/blobs/blobStore-c3566cb2-b3d6-40ae-bdcf-594a81c8881b/cache'. at org.apache.flink.runtime.blob.BlobClient.putInputStream(BlobClient.java:476) at org.apache.flink.runtime.blob.BlobClient.put(BlobClient.java:338) at org.apache.flink.runtime.blob.BlobClient.uploadJarFiles(BlobClient.java:730) at org.apache.flink.runtime.blob.BlobClient.uploadJarFiles(BlobClient.java:701) ... 9 common frames omitted</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5829" opendate="2017-2-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Bump Calcite version to 1.12 once available</summary>
      <description>Once Calcite 1.12 is release we should update to remove some copied classes.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5852" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JSON generation code into static methods</summary>
      <description>In order to implement the HistoryServer we need a way to generate the JSON responses independent of the REST API. As such i suggest to move the main parts of the generation code for job-specific handlers into static methods.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IOMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecution.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ArchivedExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="5881" opendate="2017-2-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalarFunction(UDF) should support variable types and variable arguments</summary>
      <description>As a sub-task of FLINK-5826. We would like to support the ScalarFunction first and make the review a little bit easier.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedScalarFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5882" opendate="2017-2-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableFunction (UDTF) should support variable types and variable arguments</summary>
      <description>It's the second approach of FLINK-5826.We would like to make table functions (UDTF) of Flink support variable arguments.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5897" opendate="2017-2-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Untie Checkpoint Externalization from FileSystems</summary>
      <description>Currently, externalizing checkpoint metadata and storing savepoints depends strictly on FileSystems.Since state backends are more general, storing and cleaning up checkpoints with state backend hooks requires to untie savepoints and externalized checkpoints from filesystems.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ExceptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5899" opendate="2017-2-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug in EventTimeTumblingWindow for non-partialMerge aggregate</summary>
      <description>The row length used to initialize DataSetTumbleTimeWindowAggReduceGroupFunction was not set properly. (I think this is introduced by mistake when merging the code).We currently lack the built-in non-partial-merge Aggregates. Therefore this has not been captured by the unit test. Reproduce step:1. set the "supportPartial" to false for SumAggregate2. Then both testAllEventTimeTumblingWindowOverTime and testEventTimeTumblingGroupWindowOverTime will fail.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5903" opendate="2017-2-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>taskmanager.numberOfTaskSlots and yarn.containers.vcores did not work well in YARN mode</summary>
      <description>Now Flink did not respect taskmanager.numberOfTaskSlots and yarn.containers.vcores in flink-conf.yaml, but only -s parameter in CLI.Details is that taskmanager.numberOfTaskSlots is not working in anyway andyarn.containers.vcores is only used in requesting container(TM) resources but not aware to TM, which means TM will always think it has default(1) Slots if -s is not configured.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5904" opendate="2017-2-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jobmanager.heap.mb and taskmanager.heap.mb not work in YARN mode</summary>
      <description>I set taskmanager.heap.mb to 5120 and jobmanager.heap.mb to 2048, and run ./yarn-session.sh -n 3, but the YARN only allocates 4GB for this application.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="5906" opendate="2017-2-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support to register UDAGG in Table and SQL API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5928" opendate="2017-2-27 00:00:00" fixdate="2017-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoints overwritting each other</summary>
      <description>I noticed that PR #3346 accidentally broke externalized checkpoints by using a fixed meta data file name. We should restore the old behaviour with creating random files and double check why no test caught this.This will likely superseded by upcoming changes from StephanEwen to use metadata streams on the JobManager.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="5941" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let handlers take part in job archiving</summary>
      <description>The key idea behind the HistoryServer is to pre-compute all JSON responses which the WebFrontend could request and store them as files in a directory structure resembling the REST-API.For this require a mechanism to generate the responses and their corresponding REST URL.FLINK-5852 made it easier to re-use the JSON generation code, while FLINK-5870 made handlers aware of the REST URLs that they are registered one.The aim of this JIRA is to extend job-related handlers, building on the above JIRAs, enabling them to generate a number of (Path, Json) pairs for a given ExecutionGraph, containing all responses that they could generate for the given graph and their respective REST URL..</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedJobGenerationUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsSubtaskDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5949" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink on YARN checks for Kerberos credentials for non-Kerberos authentication methods</summary>
      <description>Reported in ML: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-Yarn-and-MapR-Kerberos-issue-td11996.htmlThe problem is that the Flink on YARN client incorrectly assumes UserGroupInformation.isSecurityEnabled() returns true only for Kerberos authentication modes, whereas it actually returns true for other kinds of authentications too.We could make use of UserGroupInformation.getAuthenticationMethod() to check for KERBEROS only.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5954" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Always assign names to the window in the Stream SQL API</summary>
      <description>CALCITE-1603 and CALCITE-1615 brings in supports for TUMBLE, HOP, SESSION grouped windows, as well as the corresponding auxiliary functions that allow uses to query the start and the end of the windows (e.g., TUMBLE_START() and TUMBLE_END() see http://calcite.apache.org/docs/stream.html for more details).The goal of this jira is to add support for these auxiliary functions in Flink. Flink already has runtime supports for them, as these functions are essential mapped to the WindowStart and WindowEnd classes.To implement this feature in transformation, the transformation needs to recognize these functions and map them to the WindowStart and WindowEnd classes.The problem is that both classes can only refer to the windows using alias. Therefore this jira proposes to assign a unique name for each window to enable the transformation.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5955" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging a list of buffered records will have problem when ObjectReuse is turned on</summary>
      <description>Turn on ObjectReuse in MultipleProgramsTestBase:TestEnvironment clusterEnv = new TestEnvironment(cluster, 4, true);Then the tests "testEventTimeSessionGroupWindow", "testEventTimeSessionGroupWindow", and "testEventTimeTumblingGroupWindowOverTime" will fail.The reason is that we have buffered iterated records for group-merge. I think we should change the Agg merge to pair-merge, and later add group-merge when needed (in the future we should add rules to select either pair-merge or group-merge, but for now all built-in aggregates should work fine with pair-merge).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5956" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add retract method into the aggregateFunction</summary>
      <description>Retraction method is help for processing updated message. It will also very helpful for window Aggregation. This PR will first add retraction methods into the aggregateFunctions, such that on-going over window Aggregation can get benefit from it.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5972" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow shrinking merging windows</summary>
      <description>A misbehaving MergingWindowAssigner can cause a merge that results in a window that is smaller than the span of all the merged windows. This, in itself is not problematic. It becomes problematic when the end timestamp of a window that was not late before merging is now earlier than the watermark (the timestamp is smaller than the watermark).There are two choices: immediately process the window drop the windowprocessing the window will lead to late data downstream.The current behaviour is to silently drop the window but that logic has a bug: we only remove the dropped window from the MergingWindowSet but we don't properly clean up state and timers that the window still (possibly) has. We should fix this bug in the process of resolving this issue.We should either just fix the bug and still silently drop windows or add a check and throw an exception when the end timestamp falls below the watermark.</description>
      <version>1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5976" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring duplicate Tokenizer in flink-test</summary>
      <description>There are some duplicate code like this in flink-test, I think refactor this will be better. ```public final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { // normalize and split the line String[] tokens = value.toLowerCase().split("W+"); // emit the pairs for (String token : tokens) { if (token.length() &gt; 0) { out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } }}```</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.outputformat.TextOutputFormatITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.outputformat.CsvOutputFormatITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapred.WordCountMapredITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapreduce.WordCountMapreduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5985" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink treats every task as stateful (making topology changes impossible)</summary>
      <description>It seems that Flink treats every Task as stateful so changing the topology is not possible without setting uid on every single operator.If the topology has an iteration this is virtually impossible (or at least gets super hacky)</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotResult.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.Snapshotable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ManagedInitializationContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DoneFuture.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5987" opendate="2017-3-7 00:00:00" fixdate="2017-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper dependency to 3.4.10</summary>
      <description>zookeeper 3.4.8 has been released.Among the fixes the following are desirable:ZOOKEEPER-706 large numbers of watches can cause session re-establishment to fail ZOOKEEPER-1797 PurgeTxnLog may delete data logs during rollThis issue upgrades zookeeper dependency to 3.4.8</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5990" opendate="2017-3-8 00:00:00" fixdate="2017-3-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [partitioned] event time OVER ROWS BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER ROWS aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is required The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates event time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5803) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5998" opendate="2017-3-8 00:00:00" fixdate="2017-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Un-fat Hadoop from Flink fat jar</summary>
      <description>As a first step towards FLINK-2268, I would suggest to put all hadoop dependencies into a jar separate from Flink's fat jar.This would allow users to put a custom Hadoop jar in there, or even deploy Flink without a Hadoop fat jar at all in environments where Hadoop is provided (EMR).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6004" opendate="2017-3-9 00:00:00" fixdate="2017-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow FlinkKinesisConsumer to skip corrupted messages</summary>
      <description>It is quite clear from the fix of FLINK-3679 that in reality, users might encounter corrupted messages from Kafka / Kinesis / generally external sources when deserializing them.The consumers should support simply skipping those messages, by letting the deserialization schema return null, and checking null values within the consumer.This has been done for the Kafka consumer already. This ticket tracks the improvement for the Kinesis consumer.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestSourceContext.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="6011" opendate="2017-3-9 00:00:00" fixdate="2017-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support TUMBLE, HOP, SESSION window in streaming SQL</summary>
      <description>CALCITE-1603 and CALCITE-1615 introduces the support of the TUMBLE / HOP / SESSION windows in the parser.This jira tracks the efforts of adding the corresponding supports on the planners / optimizers in Flink.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="6012" opendate="2017-3-9 00:00:00" fixdate="2017-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support WindowStart / WindowEnd functions in streaming SQL</summary>
      <description>This jira proposes to add support for TUMBLE_START() / TUMBLE_END() / HOP_START() / HOP_END() / SESSUIB_START() / SESSION_END() in the planner in Flink.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="6027" opendate="2017-3-13 00:00:00" fixdate="2017-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore the exception thrown by the subsuming of old completed checkpoints</summary>
      <description>When a checkpoint is added into the CompletedCheckpointStore via the method addCheckpoint(), the oldest checkpoints will be removed from the store if the number of stored checkpoints exceeds the given limit. The subsuming of old checkpoints may fail and make addCheckpoint() throw exceptions which are caught by CheckpointCoordinator. Finally, the states in the new checkpoint will be deleted by CheckpointCoordinator. Because the new checkpoint is still in the store, we may recover the job from the new checkpoint. But the recovery will fail as the states of the checkpoint are all deleted.We should ignore the exceptions thrown by the subsuming of old checkpoints because we can always recover from the new checkpoint when successfully adding it into the store. The ignorance may produce some dirty data, but it's acceptable because they can be cleaned with the cleanup hook introduced in the near future.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6038" opendate="2017-3-14 00:00:00" fixdate="2017-6-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add deep links to Apache Bahir Flink streaming connector documentations</summary>
      <description>Recently, the Bahir documentation for Flink streaming connectors in Bahir was added to Bahir's website: BAHIR-90.We should add deep links to the individual Bahir connector dos under /dev/connectors/overview, instead of just shallow links to the source README.md s in the community ecosystem page.</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="6056" opendate="2017-3-15 00:00:00" fixdate="2017-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>apache-rat exclude flink directory in tools</summary>
      <description>The flink* directory in the tools is temporary cloned when build distribution.So when build the Flink project, we should exclude the flink* directory.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6059" opendate="2017-3-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject DataSet&lt;Row&gt; and DataStream&lt;Row&gt; without RowTypeInformation</summary>
      <description>It is not possible to automatically extract proper type information for Row because it is not typed with generics and holds values in an Object[].Consequently is handled as GenericType&lt;Row&gt; unless a RowTypeInfo is explicitly specified.This can lead to unexpected behavior when converting a DataSet&lt;Row&gt; or DataStream&lt;Row&gt; into a Table. If the data set or data stream has a GenericType&lt;Row&gt;, the rows are treated as atomic type and converted into a single field.I think we should reject input types of GenericType&lt;Row&gt; when converting data sets and data streams and request a proper RowTypeInfo.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6075" opendate="2017-3-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>ORDER BY *time ASC</summary>
      <description>These will be split in 3 separated JIRA issues. However, the design is the same only the processing function differs in terms of the output. Hence, the design is the same for all of them.Time target: Proc Time*SQL targeted query examples:*Sort exampleQ1)` SELECT a FROM stream1 GROUP BY HOP(proctime, INTERVAL '1' HOUR, INTERVAL '3' HOUR) ORDER BY b` Comment: window is defined using GROUP BYComment: ASC or DESC keywords can be placed to mark the ordering typeLimit exampleQ2) `SELECT a FROM stream1 WHERE rowtime BETWEEN current_timestamp - INTERVAL '1' HOUR AND current_timestamp ORDER BY b LIMIT 10`Comment: window is defined using time ranges in the WHERE clauseComment: window is row triggeredTop exampleQ3) `SELECT sum(a) OVER (ORDER BY proctime RANGE INTERVAL '1' HOUR PRECEDING LIMIT 10) FROM stream1` Comment: limit over the contents of the sliding windowGeneral Comments:-All these SQL clauses are supported only over windows (bounded collections of data). -Each of the 3 operators will be supported with each of the types of expressing the windows. *Description*The 3 operations (limit, top and sort) are similar in behavior as they all require a sorted collection of the data on which the logic will be applied (i.e., select a subset of the items or the entire sorted set). These functions would make sense in the streaming context only in the context of a window. Without defining a window the functions could never emit as the sort operation would never trigger. If an SQL query will be provided without limits an error will be thrown (`SELECT a FROM stream1 TOP 10` -&gt; ERROR). Although not targeted by this JIRA, in the case of working based on event time order, the retraction mechanisms of windows and the lateness mechanisms can be used to deal with out of order events and retraction/updates of results.*Functionality example*We exemplify with the query below for all the 3 types of operators (sorting, limit and top). Rowtime indicates when the HOP window will trigger – which can be observed in the fact that outputs are generated only at those moments. The HOP windows will trigger at every hour (fixed hour) and each event will contribute/ be duplicated for 2 consecutive hour intervals. Proctime indicates the processing time when a new event arrives in the system. Events are of the type (a,b) with the ordering being applied on the b field.`SELECT a FROM stream1 HOP(proctime, INTERVAL '1' HOUR, INTERVAL '2' HOUR) ORDER BY b (LIMIT 2/ TOP 2 / &amp;#91;ASC/DESC&amp;#93; `)Rowtime Proctime Stream1 Limit 2 Top 2 Sort &amp;#91;ASC&amp;#93; 10:00:00 (aaa, 11) 10:05:00 (aab, 7) 10-11 11:00:00 aab,aaa aab,aaa aab,aaa 11:03:00 (aac,21) 11-12 12:00:00 aab,aaa aab,aaa aab,aaa,aac 12:10:00 (abb,12) 12:15:00 (abb,12) 12-13 13:00:00 abb,abb abb,abb abb,abb,aac...*Implementation option*Considering that the SQL operators will be associated with window boundaries, the functionality will be implemented within the logic of the window as follows. Window assigner – selected based on the type of window used in SQL (TUMBLING, SLIDING…) Evictor/ Trigger – time or count evictor based on the definition of the window boundaries Apply – window function that sorts data and selects the output to trigger (based on LIMIT/TOP parameters). All data will be sorted at once and result outputted when the window is triggeredAn alternative implementation can be to use a fold window function to sort the elements as they arrive, one at a time followed by a flatMap to filter the number of outputs. *General logic of Join*```inputDataStream.window(new &amp;#91;Slide/Tumble&amp;#93;&amp;#91;Time/Count&amp;#93;Window())//.trigger(new &amp;#91;Time/Count&amp;#93;Trigger()) – use default//.evictor(new &amp;#91;Time/Count&amp;#93;Evictor()) – use default .apply(SortAndFilter());```------------JIRA will contain ORDER BY *time ASC OFFSET FETCHORDER BY *time DESC OFFSET FETCHORDER BY * OFFSET FETCH</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="6107" opendate="2017-3-18 00:00:00" fixdate="2017-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add custom checkstyle for flink-streaming-java</summary>
      <description>There was some consensus on the ML (https://lists.apache.org/thread.html/94c8c5186b315c58c3f8aaf536501b99e8b92ee97b0034dee295ff6a@%3Cdev.flink.apache.org%3E) that we want to have a more uniform code style. We should start module-by-module and by introducing increasingly stricter rules. We have to be aware of the PR situation and ensure that we have minimal breakage for contributors.This issue aims at adding a custom checkstyle.xml for flink-streaming-java that is based on our current checkstyle.xml but adds these checks for Javadocs:&lt;!--JAVADOC CHECKS--&gt;&lt;!-- Checks for Javadoc comments. --&gt;&lt;!-- See http://checkstyle.sf.net/config_javadoc.html --&gt;&lt;module name="JavadocMethod"&gt; &lt;property name="scope" value="protected"/&gt; &lt;property name="severity" value="error"/&gt; &lt;property name="allowMissingJavadoc" value="true"/&gt; &lt;property name="allowMissingParamTags" value="true"/&gt; &lt;property name="allowMissingReturnTag" value="true"/&gt; &lt;property name="allowMissingThrowsTags" value="true"/&gt; &lt;property name="allowThrowsTagsForSubclasses" value="true"/&gt; &lt;property name="allowUndeclaredRTE" value="true"/&gt;&lt;/module&gt;&lt;!-- Check that paragraph tags are used correctly in Javadoc. --&gt;&lt;module name="JavadocParagraph"/&gt;&lt;module name="JavadocType"&gt; &lt;property name="scope" value="protected"/&gt; &lt;property name="severity" value="error"/&gt; &lt;property name="allowMissingParamTags" value="true"/&gt;&lt;/module&gt;&lt;module name="JavadocStyle"&gt; &lt;property name="severity" value="error"/&gt; &lt;property name="checkHtml" value="true"/&gt;&lt;/module&gt;This checks: Every type has a type-level Javadoc Proper use of &lt;p&gt; in Javadocs First sentence must end with a proper punctuation mark Proper use (including closing) of HTML tags</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.RichSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.test.resources.log4j.properties</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.HDFSCopyToLocal.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.HDFSCopyFromLocal.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.functions.StreamingFunctionUtils.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorStateHandles.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueProcessAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalAggregateProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalAggregateProcessAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.CheckpointCommitter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SideOutputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OnWatermarkCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalWatermarkCallbackService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.AsyncCollectionResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphUserHashHasher.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphHasher.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.AllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsText.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.collector.selector.CopyingDirectedOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.migration.streaming.api.graph.StreamGraphHasherV1.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Triggerable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TypeInformationSerializationSchemaTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.keys.ArrayKeySelectorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.CollectingSourceContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractDeserializationSchemaTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecordTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RebalancePartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.KeyMapTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.KeyMapPutTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.KeyMapPutIfAbsentTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AllWindowTranslationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.TestEvent.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.SpilledBufferOrEventSequenceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BufferSpillerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferAlignmentLimitTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.graph.TranslationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.windowing.deltafunction.EuclideanDistanceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.windowing.deltafunction.CosineDistanceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.TypeFillTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.streamtask.MockRecordWriter.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamMapTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamGroupedReduceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamGroupedFoldTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateDescriptorPassingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SlotAllocationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.PrintSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.ListSourceContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.IngestionTimeExtractorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.FromElementsFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.AscendingTimestampExtractorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.SimpleStringSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.SerializationSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.DeserializationSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.AbstractDeserializationSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.keys.KeySelectorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTaskException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamElement.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ShufflePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.RebalancePartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ForwardPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.package-info.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPunctuatedWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.package-info.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.ExtractTimestampsOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.TriggerResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.Trigger.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.TumblingAlignedProcessingTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.SlidingAlignedProcessingTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TwoInputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OneInputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ChainingStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.WindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.InternalProcessApplyWindowContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.DeltaFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.TimestampAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.StatefulSequenceSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteSinkFunctionByMillis.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsCsv.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.PrintSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.DiscardingSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.ProcessFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.IngestionTimeExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.co.CoFlatMapFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.AsyncFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SplitStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.collector.selector.OutputSelector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.CheckpointingMode.java</file>
      <file type="M">docs.internals.ide.setup.md</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.NoOpIntMap.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.EvenOddOutputSelector.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskStoppingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.TimeWindowTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamProjectTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimerException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.package-info.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsynchronousException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatus.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecord.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.LatencyMarker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.StreamPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.GlobalPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.ConfigurableStreamPartitioner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.TimestampedValue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamingReader.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BlockingQueueBroker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.windows.Window.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.windows.TimeWindow.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.windows.GlobalWindow.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.PurgingTrigger.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.EventTimeTrigger.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.DeltaTrigger.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.time.Time.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.evictors.Evictor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.evictors.DeltaEvictor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.WindowAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.ProcessingTimeSessionWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.MergingWindowAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.GlobalWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.BaseAlignedWindowAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.watermark.Watermark.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.UnionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SplitTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SelectTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.PartitionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.FeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CoFeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.TimeCharacteristic.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamProject.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamMap.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedReduce.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedFold.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamFlatMap.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamFilter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.KeyedProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamMap.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamFlatMap.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueueEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.Emitter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphHasherV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.RichAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceIterableWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceIterableAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceApplyWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceApplyProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceApplyProcessAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ReduceApplyAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ProcessAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.PassThroughWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.PassThroughAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldApplyWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldApplyProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldApplyProcessAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.FoldApplyAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.FieldsFromTuple.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.FieldsFromArray.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.FieldFromTuple.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.FieldFromArray.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.Extractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.ConcatenatedExtract.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.extractor.ArrayFromTuple.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.ExtractionAwareDeltaFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.EuclideanDistance.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.delta.CosineDistance.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.AggregateApplyWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.AggregateApplyAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.TimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MultipleIdsMessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromSplittableIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FileReadFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormat.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.SocketClientSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.IterativeStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.aggregation.SumAggregator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamEdge.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.evictors.CountEvictor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.evictors.TimeEvictor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.KeyMap.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessorFactory.java</file>
      <file type="M">tools.maven.strict-checkstyle.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.migration.streaming.runtime.streamrecord.MultiplexingStreamRecordSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.migration.streaming.runtime.streamrecord.StreamRecordSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.Checkpointed.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedAsynchronously.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedRestoring.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.ListCheckpointed.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.collector.selector.DirectedOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AsyncDataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.CoGroupedStreams.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedStreams.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.JoinedStreams.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.StreamProjection.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.aggregation.AggregationFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.aggregation.ComparableAggregator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.aggregation.Comparator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.aggregation.SumFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.AscendingTimestampExtractor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.collector.AsyncCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.RichAsyncFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.co.CoProcessFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="6117" opendate="2017-3-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;zookeeper.sasl.disable&amp;#39; not takes effet when starting CuratorFramework</summary>
      <description>The value of 'zookeeper.sasl.disable' not used in the right way when starting CuratorFramework.Here are all the settings relevant to high-availability in my flink-conf.yaml: high-availability: zookeeper high-availability.zookeeper.quorum: localhost:2181 high-availability.zookeeper.storageDir: hdfs:///flink/ha/Obviously, no explicit value is set for 'zookeeper.sasl.disable' so default value of 'true'(ConfigConstants.DEFAULT_ZOOKEEPER_SASL_DISABLE) would be applied. But when FlinkYarnSessionCli &amp; FlinkApplicationMasterRunner start,both logs show that they attempt connecting to zookeeper in 'SASL' mode.logs are like this:2017-03-18 23:53:10,498 INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=localhost:2181 sessionTimeout=60000 watcher=org.apache.flink.shaded.org.apache.curator.ConnectionState@5949eba82017-03-18 23:53:10,498 INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=localhost:2181 sessionTimeout=60000 watcher=org.apache.flink.shaded.org.apache.curator.ConnectionState@5949eba82017-03-18 23:53:10,522 WARN org.apache.zookeeper.ClientCnxn - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-3047036396963510842.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.2017-03-18 23:53:10,522 WARN org.apache.zookeeper.ClientCnxn - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-3047036396963510842.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.2017-03-18 23:53:10,530 INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/127.0.0.1:21812017-03-18 23:53:10,530 INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/127.0.0.1:21812017-03-18 23:53:10,534 ERROR org.apache.flink.shaded.org.apache.curator.ConnectionState - Authentication failed</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.ZooKeeperModule.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="6149" opendate="2017-3-22 00:00:00" fixdate="2017-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add additional flink logical relation nodes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.cost.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.TableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushFilterIntoTableSourceScanRuleBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushProjectIntoTableSourceScanRuleBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.BatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetDistinctRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetIntersectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetMinusRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSingleRowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSortRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.PushFilterIntoBatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.PushProjectIntoBatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamOverAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.PushFilterIntoStreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.PushProjectIntoStreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="6157" opendate="2017-3-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make TypeInformation fully serializable</summary>
      <description>TypeInformation is already declared serializable, however, there are no tests that verify that all classes are really serializable.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TryTypeInfoTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TraversableTypeInfoTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.OptionTypeInfoTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.EnumValueTypeInfoTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.EitherTypeInfoTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfoTest.scala</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.PrimitiveArrayTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.NothingTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.BasicArrayTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.ValueTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.TupleTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.RowTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.PojoTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.MissingTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.GenericTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.EnumTypeInfoTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.EitherTypeInfoTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.typeutils.WritableTypeInfoTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-connectors.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6177" opendate="2017-3-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for "Distributed Cache" in streaming applications</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.distributedCache.DistributedCacheTest.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="6181" opendate="2017-3-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper scripts use invalid regex</summary>
      <description>This issue has been reported by a user: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/unable-to-add-more-servers-in-zookeeper-quorum-peers-in-flink-1-2-td12321.html</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-zookeeper-quorum.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-zookeeper-quorum.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6203" opendate="2017-3-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet Transformations</summary>
      <description>the example of GroupReduce on sorted groups can't remove duplicate Strings in a DataSet.need to add "prev=t"such as:val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =&gt; var prev: (Int, String) = null for (t &lt;- in) { if (prev == null || prev != t) out.collect(t) prev=t // this line is missing in the example } }</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="6207" opendate="2017-3-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate type serializers for async snapshots of CopyOnWriteStateTable</summary>
      <description>TypeSerializer are used for copy-on-write and the parallel snapshots in the CopyOnWriteStateTable. For stateful serializers, this can lead to race conditions. Snapshots need to duplicate the serializers before using them.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug id="6211" opendate="2017-3-29 00:00:00" fixdate="2017-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validation error in Kinesis Consumer when using AT_TIMESTAMP as start position</summary>
      <description>private static void validateOptionalDateProperty(Properties config, String key, String message) { if (config.containsKey(key)) { try { initTimestampDateFormat.parse(config.getProperty(key)); — double value = Double.parseDouble(config.getProperty(key)); — if (value &lt; 0) { throw new NumberFormatException(); } } catch (ParseException | NumberFormatException e){ throw new IllegalArgumentException(message); } } }}This validation function will always fail regardless of either string format or double type.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6212" opendate="2017-3-29 00:00:00" fixdate="2017-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing reference to flink-avro dependency</summary>
      <description>In the Connectors page of the Batch (DataSet API) there is a section called "Avro support in Flink"This section mentions the use of certain classes that are part of the flink-avro dependency but this fact is mentioned nowhere. This explanation should be added as well as an xml snippet with the maven dependency as in other parts of the documentation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="6213" opendate="2017-3-29 00:00:00" fixdate="2017-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When number of failed containers exceeds maximum failed containers and application is stopped, the AM container will be released 10 minutes later</summary>
      <description>When number of failed containers exceeds maximum failed containers and application is stopped, the AM container will be released 10 minutes later. I checked yarn log and found out after invoking unregisterApplicationMaster, the AM container is not released. After 10 minutes, the release is triggered by RM ping check timeout.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="6236" opendate="2017-3-31 00:00:00" fixdate="2017-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoint page needs to include web console possibility</summary>
      <description>Starting Flink 1.2.0 it is also possible to point to the savepoint when starting a job. However, the page only mention the CLI only.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug id="6237" opendate="2017-4-1 00:00:00" fixdate="2017-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support RAND and RAND_INTEGER on Table API &amp; SQL</summary>
      <description>support RAND and RAND_INTEGER with and without seed on on Table API &amp; SQL.like: RAND(&amp;#91;seed&amp;#93;), RAND_INTEGER(&amp;#91;seed, &amp;#93; bound)</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.random.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.RandCallGen.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="6326" opendate="2017-4-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add ProjectMergeRule at logical optimization stage</summary>
      <description>add ProjectMergeRule to merge projections. Some SQLs can not push projection into scan without this rule. e.g.table1: id: int, name: stringtable2: id: string, score: double, first: string, last: stringSELECT a.id, b.score FROM (SELECT id FROM table1 WHERE id &gt; 10) a LEFT OUTER JOIN (SELECT * FROM table2) b ON CAST(a.id AS VARCHAR) = b.id== Optimized Logical Plan without ProjectMergeRule ==DataSetCalc(select=[id, score]) DataSetJoin(where=[=(id0, id1)], join=[id, id0, id1, score, first, last], joinType=[LeftOuterJoin]) DataSetCalc(select=[id, CAST(id) AS id0], where=[&gt;(id, 10)]) BatchTableSourceScan(table=[[table1]], fields=[id]) BatchTableSourceScan(table=[[table2]], fields=[id, score, first, last])== Optimized Logical Plan with ProjectMergeRule ==DataSetCalc(select=[id, score]) DataSetJoin(where=[=(id0, id1)], join=[id, id0, id1, score], joinType=[LeftOuterJoin]) DataSetCalc(select=[id, CAST(id) AS id0], where=[&gt;(id, 10)]) BatchTableSourceScan(table=[[table1]], fields=[id]) BatchTableSourceScan(table=[[table2]], fields=[id, score])</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6330" opendate="2017-4-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Docker documentation</summary>
      <description>The "Docker" page in the docs exists but is blank.Add something useful here, including references to the official images that should exist once 1.2.1 is released, and add a brief "Kubernetes" page as well, referencing the helm chart.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.2,1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6336" opendate="2017-4-20 00:00:00" fixdate="2017-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Placement Constraints for Mesos</summary>
      <description>Fenzo supports placement constraints for tasks, and operators expose agent attributes to frameworks in the form of attributes about the agent offer.It would be extremely helpful in our multi-tenant cluster to be able to make use of this facility.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.setup.mesos.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="6386" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing bracket in &amp;#39;Compiler Limitation&amp;#39; section</summary>
      <description>"This means that types such as `Tuple2&lt;String,Integer` or `Collector&lt;String&gt;` declared as..."should be "This means that types such as `Tuple2&lt;String,Integer&gt;` or `Collector&lt;String&gt;` declared as..."</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java8.md</file>
    </fixedFiles>
  </bug>
  <bug id="6415" opendate="2017-4-28 00:00:00" fixdate="2017-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure core Flink artifacts have no specific logger dependency</summary>
      <description>Flink's code is written against slf4jTo make sure users can use their custom logging framework we need to have no direct compile-scope dependency in any core a dependency in flink-dist that is not in the fat jar an explicit dependency in examples (to see logs when running in the IDE) an explicit test dependency (for logs of test execution)All except point (1) are already fixed.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6541" opendate="2017-5-11 00:00:00" fixdate="2017-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jar upload directory not created</summary>
      <description>Steps to reproduce: setup configuration property: jobmanager.web.tmpdir = /mnt/flink/web this directory should not exist Run flink job manager. in logs:2017-05-11 12:07:58,397 ERROR org.apache.flink.runtime.webmonitor.WebMonitorUtils - WebServer could not be created [main]java.io.IOException: Jar upload directory /mnt/flink/web/flink-web-3f2733c3-6f4c-4311-b617-1e93d9535421 cannot be created or is not writable.Expected: create parent directories if they do not exit. i.e. use "uploadDir.mkdirs()" instead of "uploadDir.mkdir()"Note: BlobServer create parent directories (See BlobUtils storageDir.mkdirs())</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6593" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Bug in ProctimeAttribute or RowtimeAttribute with CodeGenerator</summary>
      <description>the ProctimeAttribute or RowtimeAttribute should not be take into codegenerator</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6615" opendate="2017-5-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>tmp directory not cleaned up on shutdown</summary>
      <description>Steps to reproduce:1) Stop task manager gracefully (kill -6 )2) In the logs:2017-05-17 13:35:50,147 INFO org.apache.zookeeper.ClientCnxn - EventThread shut down [main-EventThread]2017-05-17 13:35:50,200 ERROR org.apache.flink.runtime.io.disk.iomanager.IOManager - IOManager failed to properly clean up temp file directory: /mnt/flink/tmp/flink-io-66f1d0ec-8976-41bf-9575-f80b181b0e47 [flink-akka.actor.default-dispatcher-2]java.nio.file.DirectoryNotEmptyException: /mnt/flink/tmp/flink-io-66f1d0ec-8976-41bf-9575-f80b181b0e47 at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at org.apache.flink.util.FileUtils.deleteDirectory(FileUtils.java:154) at org.apache.flink.runtime.io.disk.iomanager.IOManager.shutdown(IOManager.java:109) at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.shutdown(IOManagerAsync.java:185) at org.apache.flink.runtime.taskmanager.TaskManager.postStop(TaskManager.scala:241) at akka.actor.Actor$class.aroundPostStop(Actor.scala:477)Expected: on shutdown delete non-empty directory anyway.Notes: after process terminated, I've checked "/mnt/flink/tmp/flink-io-66f1d0ec-8976-41bf-9575-f80b181b0e47" directory and didn't find anything there. So it looks like timing issue.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FileUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7004" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Travis Trusty image</summary>
      <description>As shown in this PR https://github.com/apache/flink/pull/4167 switching to the Trusty image on Travis seems to stabilize the build times.We should switch for 1.2, 1.3 and 1.4.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.2.2,1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="8863" opendate="2018-3-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add user-defined function support in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned in FLIP-24. It should be possible to declare user-defined functions in the SQL client. For now, we limit the registration to classes that implement ScalarFunction, TableFunction, AggregateFunction. Functions that are implemented in SQL are not part of this issue. I would suggest to introduce a functions top-level property. The declaration could look similar to: functions: - name: testFunction from: class &lt;-- optional, default: class class: org.my.MyScalarFunction constructor: &lt;-- optional, needed for certain types of functions - 42.0 - class: org.my.Class &lt;-- possibility to create objects via properties constructor: - 1 - true - false - "whatever" - type: INT value: 1</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.PrimitiveTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.FunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.CsvTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.ClassTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.service.FunctionService.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.PrimitiveTypeValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.PrimitiveType.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.HierarchyDescriptorValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.HierarchyDescriptor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.FunctionValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.FunctionDescriptor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.Descriptor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.ClassTypeValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.ClassType.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-udf.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.UserDefinedFunctions.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.UserDefinedFunction.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Source.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.DescriptorProperties.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
    </fixedFiles>
  </bug>
  <bug id="8864" opendate="2018-3-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CLI query history in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned in FLIP-24.It would be great to have the possibility of persisting the CLI's query history. Such that queries can be reused when the CLI Client is started again. Also a search feature as it is offered by terminals would be good.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="8865" opendate="2018-3-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CLI query code completion in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned in FLIP-24.Calcite already offers a code completion functionality. It would be great if we could expose this feature also through the SQL CLI Client.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlMultiLineParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCompleter.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
