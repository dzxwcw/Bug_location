<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="21384" opendate="2021-2-16 00:00:00" fixdate="2021-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically copy maven dependencies to clipboard on click</summary>
      <description>Flink has a number of optional dependencies users may need to copy into their pom files to use, such as connectors and formats. The docs should automatically copy the maven dependency to the users' clipboard when clicked.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.static.js.flink.js</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.layouts.shortcodes.sql.download.table.html</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
    </fixedFiles>
  </bug>
  <bug id="24960" opendate="2021-11-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>Nov 18 22:37:08 ================================================================================Nov 18 22:37:08 Test testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase) is running.Nov 18 22:37:08 --------------------------------------------------------------------------------Nov 18 22:37:25 22:37:25,470 [ main] INFO org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase [] - Extracted hostname:port: 5718b812c7ab:38622Nov 18 22:52:36 ==============================================================================Nov 18 22:52:36 Process produced no output for 900 seconds.Nov 18 22:52:36 ============================================================================== https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&amp;view=logs&amp;j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&amp;t=cc452273-9efa-565d-9db8-ef62a38a0c10&amp;l=36395</description>
      <version>1.14.3,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="25280" opendate="2021-12-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaPartitionSplitReaderTest failed on azure due to Offsets out of range with no configured reset policy for partitions</summary>
      <description>2021-12-13T03:30:12.8392593Z Dec 13 03:30:12 [ERROR] Tests run: 6, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 85.344 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest2021-12-13T03:30:12.8394604Z Dec 13 03:30:12 [ERROR] testNumBytesInCounter Time elapsed: 0.24 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8396218Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}2021-12-13T03:30:12.8397052Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8397697Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8398394Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)2021-12-13T03:30:12.8399306Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8399924Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8400610Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8401385Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testNumBytesInCounter(KafkaPartitionSplitReaderTest.java:153)2021-12-13T03:30:12.8402174Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8402911Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8403818Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8404452Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8405028Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8405740Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8406749Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8407886Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8408845Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8409507Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2021-12-13T03:30:12.8410219Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8411081Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8411785Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8412740Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8413553Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8414293Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8415078Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8415977Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8417383Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8418339Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8419209Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8419942Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8420716Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8421423Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8422192Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8423061Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8423740Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8424514Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8425224Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8425912Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8426672Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8427251Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8427947Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8428848Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8429554Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8430394Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8431046Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8431704Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8432455Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8433310Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8433983Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8434685Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8435379Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8436164Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8436853Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8437658Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8438321Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8439127Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8439833Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8440559Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8441222Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8442108Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8443108Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8443845Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8444481Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8445164Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8445835Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8446527Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8447150Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8447920Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8448754Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8449532Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8450305Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8450976Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8451567Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8451989Z Dec 13 03:30:12 2021-12-13T03:30:12.8452504Z Dec 13 03:30:12 [ERROR] testHandleSplitChangesAndFetch Time elapsed: 0.037 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8454253Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-2=2}2021-12-13T03:30:12.8455008Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8455818Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8456428Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1282)2021-12-13T03:30:12.8457043Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8457626Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8458300Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8459238Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.assignSplitsAndFetchUntilFinish(KafkaPartitionSplitReaderTest.java:261)2021-12-13T03:30:12.8460115Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testHandleSplitChangesAndFetch(KafkaPartitionSplitReaderTest.java:105)2021-12-13T03:30:12.8460760Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8461310Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8461937Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8462742Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8463452Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8464077Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8464794Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8465636Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8466298Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8466990Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2021-12-13T03:30:12.8467722Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8468454Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8469192Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8469941Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8470649Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8471493Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8472238Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8473107Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8473827Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8474551Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8475262Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8476109Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8477234Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8478320Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8479437Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8480706Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8481725Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8482890Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8483628Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8484318Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8484989Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8485557Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8486228Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8487030Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8487731Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8488431Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8489079Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8489731Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8490576Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8491262Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8491917Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8492550Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8493404Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8494341Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8495043Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8495729Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8496390Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8497036Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8497730Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8498411Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8499075Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8499820Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8500699Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8501427Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8502155Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8502853Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8503533Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8504175Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8504790Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8505463Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8506155Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8506837Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8507495Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8508098Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8508684Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8509121Z Dec 13 03:30:12 2021-12-13T03:30:12.8509555Z Dec 13 03:30:12 [ERROR] testPendingRecordsGauge{String}[1] Time elapsed: 0.037 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8510837Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}2021-12-13T03:30:12.8511603Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8512314Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8513023Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)2021-12-13T03:30:12.8513726Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8514320Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8514990Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8515785Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testPendingRecordsGauge(KafkaPartitionSplitReaderTest.java:195)2021-12-13T03:30:12.8516439Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8516982Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8517599Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8518168Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8518689Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8519286Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8519965Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8520724Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8521346Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8522067Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)2021-12-13T03:30:12.8522856Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8523569Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8524281Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8525034Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8525759Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8526477Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8527146Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8527788Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8528479Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8529197Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8529907Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8530615Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8531290Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8532087Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8532981Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8533690Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8534344Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8535079Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8535920Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8536611Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8537256Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8538004Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8538773Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)2021-12-13T03:30:12.8539606Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)2021-12-13T03:30:12.8540337Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)2021-12-13T03:30:12.8541043Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)2021-12-13T03:30:12.8541698Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2021-12-13T03:30:12.8542374Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8543038Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)2021-12-13T03:30:12.8543597Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8544175Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2021-12-13T03:30:12.8544752Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)2021-12-13T03:30:12.8545336Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8545909Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8546483Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8547063Z Dec 13 03:30:12 at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)2021-12-13T03:30:12.8547657Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)2021-12-13T03:30:12.8548221Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2021-12-13T03:30:12.8548796Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8549382Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8549941Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8550510Z Dec 13 03:30:12 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2021-12-13T03:30:12.8551089Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2021-12-13T03:30:12.8551742Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2021-12-13T03:30:12.8552355Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2021-12-13T03:30:12.8553007Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2021-12-13T03:30:12.8553600Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2021-12-13T03:30:12.8554173Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2021-12-13T03:30:12.8554746Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2021-12-13T03:30:12.8555332Z Dec 13 03:30:12 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2021-12-13T03:30:12.8555910Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2021-12-13T03:30:12.8556492Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2021-12-13T03:30:12.8557062Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2021-12-13T03:30:12.8557661Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2021-12-13T03:30:12.8558246Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2021-12-13T03:30:12.8558915Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2021-12-13T03:30:12.8559554Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)2021-12-13T03:30:12.8560251Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)2021-12-13T03:30:12.8560959Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8561662Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8562420Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8563140Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8563795Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8564500Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8565183Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8565852Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8566632Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8567724Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8568691Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8569384Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8570088Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8570756Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8571405Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8572436Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8573210Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8573873Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8574445Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8575106Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8575902Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8576603Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8577308Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8577953Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8578602Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8579383Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8580063Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8580724Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8581456Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8582313Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8583105Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8583737Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8584367Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8585019Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8585660Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8586287Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8586947Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8587623Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8588313Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8588963Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8589570Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8590154Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8590666Z Dec 13 03:30:12 testHandleSplitChangesAndFetchtestNumBytesInCountertestPendingRecordsGaugefrom KafkaPartitionSplitReaderTest failed with this issue</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="25472" opendate="2021-12-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Log4j 2.17.1</summary>
      <description>We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.</description>
      <version>1.12.8,1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25699" opendate="2022-1-19 00:00:00" fixdate="2022-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HashMap for MAP value constructors</summary>
      <description>Currently, the usage of maps is inconsistent. It is not ensured that duplicate keys get eliminated. For CAST and output conversion this is solved. However, we should have a similar implementation in MAP value constructor like in CAST.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25785" opendate="2022-1-24 00:00:00" fixdate="2022-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.210</summary>
      <description>Two security vulnerabilities in H2 Console (CVE-2022-23221 and possible DNS rebinding attack) are fixed in 2.0.120. Flink is currently on 2.0.206 since https://issues.apache.org/jira/browse/FLINK-25576Note: Flink is using this dependency only for testing, so it's not directly impacted by the CVE. We just want to be good citizens and update our dependencies</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25786" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust the generation of subpartition data storage order for sort-shuffle from random shuffle to random shift</summary>
      <description>Currently, for sort-shuffle the generation of subpartition data storage order  is random shuffle. However, if there is no enough resources to run the downstream consumer tasks in parallel, the performance can be influenced because of the random disk IO caused by the random subpartition data storage order. This ticket aims to improve this scenario.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="25796" opendate="2022-1-25 00:00:00" fixdate="2022-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid record copy for result partition of sort-shuffle if there are enough buffers for better performance</summary>
      <description>Currently, for result partition of sort-shuffle, there is extra record copy overhead Introduced by clustering records by subpartition index. For small records, this overhead can cause even 20% performance regression. This ticket aims to solve the problem.In fact, the hash-based implementation is a nature way to achieve the goal of sorting records by partition index. However, it incurs some serious weaknesses. For example, when there is no enough buffers or there is data skew, it can waste buffers and influence compression efficiency which can cause performance regression.This ticket tries to solve the issue by dynamically switching between the two implementations, that is, if there are enough buffers, the hash-based implementation will be used and if there is no enough buffers, the sort-based implementation will be used.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="25805" opendate="2022-1-25 00:00:00" fixdate="2022-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use compact DataType serialization for default classes instead of internal ones</summary>
      <description>It is more likely that default conversion classes spam the plan than internal classes. In most cases when internal classes are used, they usually also use logical type instead of data type. So it should be safer to skip default conversion classes. This also reduces the plan size for serializing `ResolvedSchema`.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="25856" opendate="2022-1-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix use of UserDefinedType in from_elements</summary>
      <description>If we define a new UserDefinedType, and use it in `from_elements`, it will failed.class VectorUDT(UserDefinedType): @classmethod def sql_type(cls): return DataTypes.ROW( [ DataTypes.FIELD("type", DataTypes.TINYINT()), DataTypes.FIELD("size", DataTypes.INT()), DataTypes.FIELD("indices", DataTypes.ARRAY(DataTypes.INT())), DataTypes.FIELD("values", DataTypes.ARRAY(DataTypes.DOUBLE())), ] ) @classmethod def module(cls): return "pyflink.ml.core.linalg" def serialize(self, obj): if isinstance(obj, SparseVector): indices = [int(i) for i in obj._indices] values = [float(v) for v in obj._values] return 0, obj.size(), indices, values elif isinstance(obj, DenseVector): values = [float(v) for v in obj._values] return 1, None, None, values else: raise TypeError("Cannot serialize %r of type %r".format(obj, type(obj)))self.t_env.from_elements([ (Vectors.dense([1, 2, 3, 4]), 0., 1.), (Vectors.dense([2, 2, 3, 4]), 0., 2.), (Vectors.dense([3, 2, 3, 4]), 0., 3.), (Vectors.dense([4, 2, 3, 4]), 0., 4.), (Vectors.dense([5, 2, 3, 4]), 0., 5.), (Vectors.dense([11, 2, 3, 4]), 1., 1.), (Vectors.dense([12, 2, 3, 4]), 1., 2.), (Vectors.dense([13, 2, 3, 4]), 1., 3.), (Vectors.dense([14, 2, 3, 4]), 1., 4.), (Vectors.dense([15, 2, 3, 4]), 1., 5.), ], DataTypes.ROW([ DataTypes.FIELD("features", VectorUDT()), DataTypes.FIELD("label", DataTypes.DOUBLE()), DataTypes.FIELD("weight", DataTypes.DOUBLE())]))</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug id="25879" opendate="2022-1-30 00:00:00" fixdate="2022-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track used search terms in Matomo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.partials.docs.inject.menu-after.html</file>
    </fixedFiles>
  </bug>
  <bug id="25883" opendate="2022-1-30 00:00:00" fixdate="2022-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is too large</summary>
      <description>In this line, the value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is set to 3153600000. This is more than the default value of threading.TIMEOUT_MAX on Windows Python, which is 4294967. Due to this, "OverflowError: timeout value is too large" error is produced.Full traceback: File "G:\PycharmProjects\PyFlink\venv_from_scratch\lib\site-packages\apache_beam\runners\worker\data_plane.py", line 218, in run while not self._finished.wait(next_call - time.time()): File "C:\Python38\lib\threading.py", line 558, in wait signaled = self._cond.wait(timeout) File "C:\Python38\lib\threading.py", line 306, in wait gotit = waiter.acquire(True, timeout)OverflowError: timeout value is too large</description>
      <version>None</version>
      <fixedVersion>1.12.8,1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug id="25897" opendate="2022-1-31 00:00:00" fixdate="2022-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update project configuration gradle doc to 7.x version</summary>
      <description>Update the gradle build script and its doc page to 7.x</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="25917" opendate="2022-2-2 00:00:00" fixdate="2022-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Share RpcSystem aross tests</summary>
      <description>We currently create a dedicated RpcSystem for each test, meaning that we go through the whole cycle of extracting the rpc-akka jar and creating a new classloader.For testing purposes we can re-use the same RpcSystem, which will alleviate FLINK-18356 and maybe speed up some test or two.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="25926" opendate="2022-2-2 00:00:00" fixdate="2022-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.postgresql:postgresql to 42.3.3</summary>
      <description>Security vulnerability CVE-2022-21724 is fixed in 42.2.25. Flink is currently on 42.2.10.Note: Flink uses this dependency in a Provided scope only.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.psql.PostgresTypeMapper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25960" opendate="2022-2-4 00:00:00" fixdate="2022-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distribute the data read buffers more fairly among result partitions for sort-shuffle</summary>
      <description>Currently, the data read buffers for sort-shuffle are allocated in a random way and some result partitions may occupy too many buffers which leads to the starvation of other result partitions. This ticket aims to improve the scenario by not reading data for those result partitions which already occupy more than the average number of read buffers per result partition.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25961" opendate="2022-2-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transport client from Elasticsearch 6/7 connectors (tests only)</summary>
      <description>The Elasticsearch 6/7 connectors still use deprecated transport client for integration tests. This is not really necessary and brings a lot of dependencies, the HighLevelRestClient is fully sufficient and could be used as drop-in replacement.The change affects only tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25962" opendate="2022-2-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink generated Avro schemas can&amp;#39;t be parsed using Python</summary>
      <description>Flink currently generates Avro schemas as records with the top-level name "record"Unfortunately, there is some inconsistency between Avro implementations in different languages that may prevent this record from being read, notably Python, which generates the error:avro.schema.SchemaParseException: record is a reserved type name(See the comment on FLINK-18096 for the full stack trace).The Java SDK accepts this name, and there's an ongoing discussion about what the expected behaviour should be.  This should be clarified and fixed in Avro, of course.Regardless of the resolution, the best practice (which is used almost everywhere else in the Flink codebase) is to explicitly specify a top-level namespace for an Avro record.   We should use a default like: org.apache.flink.avro.generated.</description>
      <version>1.14.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroBulkFormatTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="26017" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add debug log message when marking a job result as dirty</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="26018" opendate="2022-2-8 00:00:00" fixdate="2022-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary late events when using the new KafkaSource</summary>
      <description>There is an issue with the new KafkaSource connector in Flink 1.14: when one task consumes messages from multiple topic partitions (statically created, timestamp are in order), it may start with one partition and advances watermarks before the data from other partitions come. In this case, the early messages in other partitions may unnecessarily be considered  as late ones.I discussed with renqs, it seems that the new KafkaSource only adds a partition into WatermarkMultiplexer when it receives data from that partition. In contrast, FlinkKafkaConsumer adds all known partition before it fetch any data. Attached two files: the messages in Kafka and the corresponding TM logs.</description>
      <version>1.14.3,1.14.4</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.TestingSourceOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.SourceOperatorEventTimeTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.splitreader.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26049" opendate="2022-2-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The tolerable-failed-checkpoints logic is invalid when checkpoint trigger failed</summary>
      <description>After triggerCheckpoint, if checkpoint failed, flink will execute the tolerable-failed-checkpoints logic. But if triggerCheckpoint failed, flink won't execute the tolerable-failed-checkpoints logic.How to reproduce this issue?In our online env, hdfs sre deletes the flink base dir by mistake, and flink job don't have permission to create checkpoint dir. So cause flink trigger checkpoint failed.There are some didn't meet expectations: JM just log "Failed to trigger checkpoint for job 6f09d4a15dad42b24d52c987f5471f18 since Trigger checkpoint failure" . Don't show the root cause or exception. user set tolerable-failed-checkpoints=0, but if triggerCheckpoint failed, flink won't execute the tolerable-failed-checkpoints logic.  When triggerCheckpoint failed, numberOfFailedCheckpoints is always 0 When triggerCheckpoint failed, we can't find checkpoint info in checkpoint history page.   All metrics are normal, so the next day we found out that the checkpoint failed, and the checkpoint has been failing for a day. it's not acceptable to the flink user.I have some ideas: Should tolerable-failed-checkpoints logic be executed when triggerCheckpoint fails? When triggerCheckpoint failed, should increase numberOfFailedCheckpoints? When triggerCheckpoint failed, should show checkpoint info in checkpoint history page? JM just show "Failed to trigger checkpoint", should we show detailed exception to easy find the root cause? Masters, could we do these changes? Please correct me if I'm wrong.</description>
      <version>1.13.5,1.14.3</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsCountsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsCounts.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="26099" opendate="2022-2-13 00:00:00" fixdate="2022-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table connector proctime attributes has syntax error</summary>
      <description>The example for proctime attributes has syntax error (missing comma after 3rd column) table proctime: CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN MyField4 AS PROCTIME() -- declares a proctime attribute) WITH (...)   </description>
      <version>1.14.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="26112" opendate="2022-2-14 00:00:00" fixdate="2022-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port getRestEndpoint method to the specific service type subclass</summary>
      <description>In the FLINK-20830, we introduce serval subclass to deal with the service build and query, This ticket is meant to move the related code to the proper class </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.ServiceType.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.NodePortService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.LoadBalancerService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.ClusterIPService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.cli.KubernetesSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="26160" opendate="2022-2-15 00:00:00" fixdate="2022-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Connector: stopCursor description should be changed. Connector only stop when auto discovery is disabled.</summary>
      <description>In Pulsar source connector, the stopCursor description can mislead user to believe that the source connector will exit if a BoundedStopCursor is set.  However this might not be true if auto partition discovery is enabled (the discovery loop will always run and expects new partitionSplits). We need to modify the description.</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="26164" opendate="2022-2-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document watermark alignment</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug id="26223" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making ZK-related logs available in tests</summary>
      <description>Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.We have multiple options: Introduce an extension to change the ZK log level for specific tests Lower the ZK log level again and make the logs being written to the standard log files Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs</description>
      <version>1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="26395" opendate="2022-2-28 00:00:00" fixdate="2022-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of RAND_INTEGER is wrong in SQL function documents</summary>
      <description>RAND_INTEGER will returns a integer value, but document of SQL function shows it will return a double value.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2642" opendate="2015-9-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Table API crashes when executing word count example</summary>
      <description>I tried to run the examples provided in the documentation of Flink's Table API. Unfortunately, the Scala word count example provided in the documentation doesn't work and does not give a meaningful exception.(Other examples work fine)Here my code:package org.apache.flink.examples.scalaimport org.apache.flink.api.scala._import org.apache.flink.api.scala.table._object WordCount { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment case class WC(word: String, count: Int) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable val result = expr.groupBy('word).select('word, 'count.sum as 'count).toDataSet[WC] result.print() }}Here the thrown exception:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.ExpressionSelectFunction caused an exception: null at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1368) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.openTask(ChainedMapDriver.java:47) at org.apache.flink.runtime.operators.RegularPactTask.openChainedTasks(RegularPactTask.java:1408) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:142) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:30) at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:23) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) at org.apache.flink.api.table.codegen.IndentStringContext.j(Indenter.scala:23) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:55) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:32) at org.apache.flink.api.table.codegen.ExpressionCodeGenerator.generate(ExpressionCodeGenerator.scala:66) at org.apache.flink.api.table.runtime.ExpressionSelectFunction.open(ExpressionSelectFunction.scala:46) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33) at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1366) ... 5 more</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26429" opendate="2022-3-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump karma from 6.3.11 to 6.3.14</summary>
      <description>We should bump Karma from 6.3.11 to 6.3.14. Karma prior to version 6.3.14 contains a cross-site scripting vulnerability.This doesn't directly affect Flink.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="2643" opendate="2015-9-9 00:00:00" fixdate="2015-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Travis Build Profile to Exclude Hadoop 2.0.0-alpha, Include 2.7.0</summary>
      <description>In discussion on the mailing list we reached consensus to change the Hadoop versions that we build Flink with on Travis.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-include-yarn.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="26430" opendate="2022-3-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump follow-redirects from 1.14.7 to 1.14.8</summary>
      <description>We should bump follow-redirects from 1.14.7 to 1.14.8. Version prior to 1.14.8 could expose sensitive information to an unauthorized actor.This doesn't directly affect Flink.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="26467" opendate="2022-3-3 00:00:00" fixdate="2022-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile RowDataToStringConverter lazily</summary>
      <description>Currently, we prepare for `print()` whenever `sqlQuery` is called. However, we could postpone the compilation until it is really needed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="26501" opendate="2022-3-6 00:00:00" fixdate="2022-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Scala nightly end-to-end test failed on azure due to checkponts failed and logs contains exceptions</summary>
      <description>2022-03-05T02:35:36.4040037Z Mar 05 02:35:36 2022-03-05 02:35:34,334 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1646447734295 for job b236087395260dc34648b84c2b86d6e8.2022-03-05T02:35:36.4041701Z Mar 05 02:35:36 2022-03-05 02:35:34,387 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Decline checkpoint 1 by task e8a324cae6bf452d32db6797bbbafad0 of job b236087395260dc34648b84c2b86d6e8 at 127.0.0.1:45911-0a50f5 @ localhost (dataPort=44047).2022-03-05T02:35:36.4043279Z Mar 05 02:35:36 org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4044531Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4045729Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4047172Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4049092Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4050158Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4050929Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4051776Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4052559Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4053373Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4054849Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4055685Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4056461Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4057219Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4057899Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4059666Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4061005Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4062324Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4063941Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4065009Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4066205Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4067514Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4068255Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4069019Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4069638Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4070271Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4070862Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4071453Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4072430Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4073023Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4073687Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4074596Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4075712Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4076437Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4077754Z Mar 05 02:35:36 2022-03-05 02:35:34,410 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job b236087395260dc34648b84c2b86d6e8. (0 consecutive failed attempts so far)2022-03-05T02:35:36.4078865Z Mar 05 02:35:36 org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4080161Z Mar 05 02:35:36 at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4081619Z Mar 05 02:35:36 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:988) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4083063Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4085407Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4086635Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]2022-03-05T02:35:36.4087419Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]2022-03-05T02:35:36.4088438Z Mar 05 02:35:36 at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]2022-03-05T02:35:36.4089614Z Mar 05 02:35:36 Caused by: org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4090937Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4092177Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4093430Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4094740Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4095836Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4096579Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4097766Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4098684Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4101381Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4102353Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4103218Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4104019Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4104801Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4105719Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4108356Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4110333Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4112523Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4113601Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4114790Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4116110Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4117636Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4118641Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4119307Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4120161Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4120842Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4121482Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4122113Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4122736Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4123332Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4123984Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4124749Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4125750Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4126591Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4128133Z Mar 05 02:35:36 2022-03-05 02:35:34,430 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1) (e8a324cae6bf452d32db6797bbbafad0) switched from RUNNING to FINISHED. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=cc5499f8-bdde-5157-0d76-b6528ecd808e&amp;l=18735</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="26543" opendate="2022-3-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that exceptions generated in startup are missed in Python loopback mode</summary>
      <description></description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug id="26609" opendate="2022-3-11 00:00:00" fixdate="2022-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sum operation in KeyedStream</summary>
      <description>In Java, KeyedStream has a "sum" operator, But when i using in PyFlink. that operator is not found. So i implemented it in Python Flink KeyedStream And test has passed. But i don't know if this feature is necessary in PyFlink.so i send a jira task in here, we can discuss it. If that feature is necessary, i have already implemented that feature so i can summit a PR in github. Thansks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="26618" opendate="2022-3-13 00:00:00" fixdate="2022-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove jar statement not aligned with pipleline.jars</summary>
      <description>Currently, `remove jar` statement doesn't remove the corresponding jars in pipeline.jars.</description>
      <version>1.14.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="26619" opendate="2022-3-13 00:00:00" fixdate="2022-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for the window assigners in Python DataStream API</summary>
      <description>Write the window allocator usage document of pyflink data flow API</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.windows.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="26716" opendate="2022-3-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate PostgreSQL tests to Testcontainers</summary>
      <description>Currently, PostgreSQL tests are executed based on an embedded database.  This has issues on some systems (Mac) and blocked work on FLINK-25926. It is required to migrate those tests to Testcontainers.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.factory.JdbcCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2673" opendate="2015-9-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala API does not support Option type as key</summary>
      <description>The Scala API does not support the Option type as a key. It could be useful to allow grouping on a field with this type.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.JoinITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.OptionTypeInfo.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26736" opendate="2022-3-18 00:00:00" fixdate="2022-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-avro-confluent-registry</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroRowDataSeDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.ConfluentSchemaRegistryCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.CachedSchemaCoderProviderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27246" opendate="2022-4-14 00:00:00" fixdate="2022-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB</summary>
      <description>I think this bug should get fixed in https://issues.apache.org/jira/browse/FLINK-23007Unfortunately I spotted it on Flink 1.14.3java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$9211' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:198) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.&lt;init&gt;(RegularOperatorChain.java:63) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:666) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling "HashAggregateWithKeys$9211": Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.CodeContext.write(CodeContext.java:940) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:12282) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11941) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11926) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4408) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4274) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4461) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4403) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4120) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3957) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4448) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5004) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4417) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5057) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4409) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ParenthesizedExpression.accept(Java.java:4924) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 more</description>
      <version>1.14.3,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.code.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaCodeSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.IfStatementRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.CodeRewriterTestBase.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.JavaCodeSplitter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.IfStatementRewriter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27338" opendate="2022-4-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve spliting file for Hive table with orc format</summary>
      <description>Currently, for hive source, it'll use the hdfs block size configured with key dfs.block.size in hdfs-site.xml as the max split size to split the files. The default value is usually 128M/256M depending on configuration.The strategy to split file is not reasonable for the number of splits tend to be less so that can't make good use of the parallel computing.What's more, when enable parallelism inference for hive source, it'll set the parallelism of Hive source to the num of splits when it's not bigger than max parallelism. So, it'll limit the source parallelism and could degrade the perfermance.To solve this problem, the idea is to calcuate a reasonable split size based on files's total size, block size,  default parallelism or parallelism configured by user.  The Jira is try to improve the splitting file logic for Hive table with orc format.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.PartitionMonitorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSourceDynamicFileEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceDynamicFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="27339" opendate="2022-4-21 00:00:00" fixdate="2022-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some classes don&amp;#39;t have a package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-test-utils.src.test.java.TableAssertionTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.src.main.java.FileSinkProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27418" opendate="2022-4-26 00:00:00" fixdate="2022-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL TopN result is wrong</summary>
      <description>Flink SQL TopN is executed multiple times with different results, sometimes with correct results and sometimes with incorrect results.Example:@Test public void flinkSqlJoinRetract() { EnvironmentSettings settings = EnvironmentSettings.newInstance() .useBlinkPlanner() .inStreamingMode() .build(); StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment(); streamEnv.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv, settings); tableEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(10000)); RowTypeInfo waybillTableTypeInfo = buildWaybillTableTypeInfo(); RowTypeInfo itemTableTypeInfo = buildItemTableTypeInfo(); SourceFunction&lt;Row&gt; waybillSourceFunction = buildWaybillStreamSource(waybillTableTypeInfo); SourceFunction&lt;Row&gt; itemSourceFunction = buildItemStreamSource(itemTableTypeInfo); String waybillTable = "waybill"; String itemTable = "item"; DataStreamSource&lt;Row&gt; waybillStream = streamEnv.addSource( waybillSourceFunction, waybillTable, waybillTableTypeInfo); DataStreamSource&lt;Row&gt; itemStream = streamEnv.addSource( itemSourceFunction, itemTable, itemTableTypeInfo); Expression[] waybillFields = ExpressionParser .parseExpressionList(String.join(",", waybillTableTypeInfo.getFieldNames()) + ",proctime.proctime").toArray(new Expression[0]); Expression[] itemFields = ExpressionParser .parseExpressionList( String.join(",", itemTableTypeInfo.getFieldNames()) + ",proctime.proctime") .toArray(new Expression[0]); tableEnv.createTemporaryView(waybillTable, waybillStream, waybillFields); tableEnv.createTemporaryView(itemTable, itemStream, itemFields); String sql = "select \n" + " city_id, \n" + " count(*) as cnt\n" + "from (\n" + " select id,city_id\n" + " from (\n" + " select \n" + " id,\n" + " city_id,\n" + " row_number() over(partition by id order by utime desc ) as rno \n" + " from (\n" + " select \n" + " waybill.id as id,\n" + " coalesce(item.city_id, waybill.city_id) as city_id,\n" + " waybill.utime as utime \n" + " from waybill left join item \n" + " on waybill.id = item.id \n" + " ) \n" + " )\n" + " where rno =1\n" + ")\n" + "group by city_id"; StatementSet statementSet = tableEnv.createStatementSet(); Table table = tableEnv.sqlQuery(sql); DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; rowDataStream = tableEnv.toRetractStream(table, Row.class); rowDataStream.printToErr(); try { streamEnv.execute(); } catch (Exception e) { e.printStackTrace(); } } private static RowTypeInfo buildWaybillTableTypeInfo() { TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG(), Types.LONG()}; String[] fields = new String[]{"id", "city_id", "rider_id", "utime"}; return new RowTypeInfo(types, fields); } private static RowTypeInfo buildItemTableTypeInfo() { TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG()}; String[] fields = new String[]{"id", "city_id", "utime"}; return new RowTypeInfo(types, fields); } //id,rider_id,city_id,utime private static SourceFunction&lt;Row&gt; buildWaybillStreamSource(RowTypeInfo rowTypeInfo) { return new SourceFunction&lt;Row&gt;() { private volatile boolean stopped = false; int count = 0; int[] ids = {111, 222, 333, 111}; String[] cityIds = {"A", "A", "B", "A"}; @Override public void run(SourceContext&lt;Row&gt; ctx) throws Exception { while (!stopped) { int id = ids[count % ids.length]; String cityId = cityIds[count % cityIds.length]; Row row = new Row(4); row.setField(0, id); row.setField(1, cityId); row.setField(2, (long) RandomUtils.nextInt(1000, 2000)); row.setField(3, System.currentTimeMillis()); printRow(rowTypeInfo, row); ctx.collect(row); if (++count &gt; 3) { stopped = true; } } } @Override public void cancel() { stopped = true; } }; } //id,city_id,utime private static SourceFunction&lt;Row&gt; buildItemStreamSource(RowTypeInfo rowTypeInfo) { return new SourceFunction&lt;Row&gt;() { private volatile boolean stopped = false; int count = 0; int[] ids = {111, 333}; String[] cityIds = {"C", "D"}; @Override public void run(SourceContext&lt;Row&gt; ctx) throws Exception { while (!stopped) { Thread.sleep(RandomUtils.nextInt(1000, 2000)); int id = ids[count % ids.length]; String cityId = cityIds[count % cityIds.length]; Row row = new Row(3); row.setField(0, id); row.setField(1, cityId); //row.setField(2, System.currentTimeMillis()); printRow(rowTypeInfo, row); ctx.collect(row); if (++count &gt;= 2) { stopped = true; } } } @Override public void cancel() { stopped = true; } }; } public static void printRow(RowTypeInfo rowTypeInfo, Row row) { String prefix = ""; for (int i = 0; i &lt; rowTypeInfo.getArity(); ++i) { prefix = i &gt; 0 ? "," : ""; System.out.print(prefix + rowTypeInfo.getFieldNames()[i] + ":" + row.getField(i)); } System.out.println(); }------------------------------------------------------------wrong resultright resultid:111,city_id:A,rider_id:1137,utime:1650979957702id:222,city_id:A,rider_id:1976,utime:1650979957725id:333,city_id:B,rider_id:1916,utime:1650979957725id:111,city_id:A,rider_id:1345,utime:1650979957725(true,A,1)(false,A,1)(true,A,2)(true,B,1)(false,A,2)(true,A,1)(false,A,1)(true,A,2)id:111,city_id:C,utime:null(false,A,2)(true,A,1)(true,C,1)(false,A,1)(false,C,1)(true,C,2)id:333,city_id: D,utime:null(false,B,1)(true,D,1)The final result:C,2D,1is wrong. id:111,city_id:A,rider_id:1155,utime:1650980662019id:222,city_id:A,rider_id:1875,utime:1650980662042id:333,city_id:B,rider_id:1430,utime:1650980662042id:111,city_id:A,rider_id:1308,utime:1650980662042(true,A,1)(false,A,1)(true,A,2)(true,B,1)(false,A,2)(true,A,1)(false,A,1)(true,A,2)id:111,city_id:C,utime:null(false,A,2)(true,A,1)(false,A,1)(true,A,2)(false,A,2)(true,A,1)(true,C,1)id:333,city_id: D,utime:null(false,B,1)(true,D,1)The final result:A,1C,1D,1is right.  </description>
      <version>1.12.2,1.14.3</version>
      <fixedVersion>1.15.1,1.16.0,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="27441" opendate="2022-4-28 00:00:00" fixdate="2022-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrollbar is missing for particular UI elements (Accumulators, Backpressure, Watermarks)</summary>
      <description>The angular version bump introduced a bug, where for nzScroll does not support percentage in CSS calc, so the scrollbar will be invisible. There is an easy workaround, the linked Angular discussion covers it.Angular issue: https://github.com/NG-ZORRO/ng-zorro-antd/issues/3090</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="27544" opendate="2022-5-9 00:00:00" fixdate="2022-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="27618" opendate="2022-5-16 00:00:00" fixdate="2022-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CumeDist</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.SumAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedOverWindowFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.SlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.OffsetOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.InsensitiveOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.AggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27619" opendate="2022-5-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support NTILE</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.InsensitiveOverFrame.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.ExpressionBuilder.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27763" opendate="2022-5-25 00:00:00" fixdate="2022-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove netty bundling&amp;relocation in flink-streaming-kinesis-tests</summary>
      <description>flink-streaming-kinesis-tests bundles and relocates netty (and no other dependency). We can safely remove this because no class within this module references netty, nor any other module relies on the relocated versions.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="28094" opendate="2022-6-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK to support ap-southeast-3</summary>
      <description>The AWS base module pulls AWS SDK v2.17.52 which does not support ap-southeast-3. Update to the latest version. Ensure to cover connectors (KDS/KDF/DDB) and formats (avro-glue-schema-registry and json-glue-schema-registry)</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSAsyncSinkUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="28095" opendate="2022-6-16 00:00:00" fixdate="2022-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace IOUtils dependency on oss filesystem</summary>
      <description>The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.We can make our lives a little bit easier by using the Flink IOUtils instead.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.java.org.apache.flink.fs.osshadoop.writer.OSSRecoverableFsDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="28096" opendate="2022-6-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support set variable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveDialectFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28568" opendate="2022-7-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implements a new lookup join operator (sync mode only) with state to eliminate the non determinism</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.LookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="28606" opendate="2022-7-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preserve distributed consistency of OperatorEvents from OperatorCoordinator to subtasks</summary>
      <description>This is a component of our solution to the consistency issue in the operator coordinator mechanism. In this step, we would guarantee the consistency of all communications in one direction, from OC to subtasks. This would need less workload and should unblock the implementation of the CEP coordinator in FLIP-200.Roughly, we would need to implement the following process in this step. Let the OC finish processing all the incoming OperatorEvents before the snapshot. Closes the gateway that sends operator events to its subtasks when the OC completes the snapshot. Wait until all the outgoing OperatorEvents created before the snapshot are sent and acked. Send checkpoint barriers to the Source operators. Open the corresponding gateway of a subtask when the OC learned that the subtask has completed the checkpoint.</description>
      <version>1.14.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.util.IncompleteFuturesTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.ComponentClosingUtilsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorEventDispatcherImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.FinishedOperatorChain.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.TestingOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorEventValveTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.EventReceivingTasks.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.SubtaskAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorEventValve.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.EventSender.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpointContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="28609" opendate="2022-7-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink-Pulsar connector fails on larger schemas</summary>
      <description>When a model results in a larger schema (this seems to be related to its byte array representation), the number of expected bytes to read is different than the number of actually read bytes: exception.txt. The "read" is such a case is always 1018 while the expected "byteLen" gives a greater value. For smaller schemata, the numbers are equal (less than 1018) and no issue occurs.The problem reproduction is on GitHub. There are 2 simple jobs (SimpleJob1 and SimpleJob2) using basic models for the Pulsar source definition (PulsarMessage1 and PulsarMessage2, respectively). Each of the corresponding schemata is properly serialised and deserialised, unless an effective byte array length becomes excessive (marked with "the problem begins" in model classes). The fail condition can be achieved by a number of fields (PulsarMessage1) or just longer field names (PulsarMessage2). The problem occurs on either Avro or a JSON schema set in the Pulsar source definition.</description>
      <version>1.14.3,1.14.4,1.14.5,1.15.1</version>
      <fixedVersion>1.16.0,1.14.6,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="28655" opendate="2022-7-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support show jobs statement in SqlGatewayService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28658" opendate="2022-7-24 00:00:00" fixdate="2022-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for job lifecycle statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sql-gateway.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="2933" opendate="2015-10-27 00:00:00" fixdate="2015-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink scala libraries exposed with maven should carry scala version</summary>
      <description>&amp;#91;If I put this on the wrong component, can someone please update?&amp;#93;Major versions of scala are not forward nor backwards compatible. Libraries build for 2.10 will not work with 2.11 or vice versa.In order to avoid build related problems, it is strongly recommended to append the scala version it is compatible within the artifact id. This ensures the correct version of the library is pulled in rather than deferring the problem to a future build or runtime error.For example, akka exposes the following packages for the same version:&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.10&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt;&lt;/dependency&gt;</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.change-scala-version.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-flume.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-operator-stats.pom.xml</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.pom.xml</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3310" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add back pressure statistics to web frontend</summary>
      <description>When a task is receiving data at a higher rate than it can process, the task is back pressuring preceding tasks. Currently, there is no way to tell whether this is the case or not. An indicator for back pressure is tasks being stuck in buffer requests on the network stack. This means that they have filled all their buffers with data, but the following tasks/network are not consuming them fast enough.A simple way to measure back pressure is to sample running tasks and report back pressure if they are stuck in the blocking buffers calls.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.FontAwesome.otf</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.woff2</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.woff</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.ttf</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.eot</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.directives.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
