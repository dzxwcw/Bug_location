<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="23902" opendate="2021-8-20 00:00:00" fixdate="2021-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="27384" opendate="2022-4-25 00:00:00" fixdate="2022-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In the Hive dimension table, when the data is changed on the original partition, the create_time configuration does not take effect</summary>
      <description>In the Hive dimension table, when the data is changed on the original partition, the create_time configuration does not take effect.The current table structure directory is as follows:From the above figure, we can know that when hive is a dimension table, it will load the data of dt=2021-04-22, hr=27.If a new partition arrives now, the data of the latest partition can be read smoothly. However, if the data is modified on the original partition, theoretically, the data of the modified partition is read, because of the create_time and latest configuration, but this is not the case in practice. The data that was originally loaded is still read.</description>
      <version>1.14.4,1.15.1</version>
      <fixedVersion>1.16.0,1.15.3,1.14.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28288" opendate="2022-6-29 00:00:00" fixdate="2022-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support decode and encode built-in function in the Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28463" opendate="2022-7-8 00:00:00" fixdate="2022-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL syntax support CREATE TABLE AS SELECT(CTAS)</summary>
      <description>Support CTAS(CREATE TABLE AS SELECT) syntaxCREATE TABLE [ IF NOT EXISTS ] table_name [ WITH ( table_properties ) ][ AS query_expression ]</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="28490" opendate="2022-7-11 00:00:00" fixdate="2022-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce "ANALYZE TABLE" Syntax in sql parser</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="28491" opendate="2022-7-11 00:00:00" fixdate="2022-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce APPROX_COUNT_DISTINCT aggregate function for batch sql</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstLastValueAggFunctionWithOrderTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28492" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "ANALYZE TABLE" execution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesCatalog.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.Date.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataString.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataLong.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDouble.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBoolean.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBinary.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatistics.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="28493" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document to describe "ANALYZE TABLE" syntax</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="28495" opendate="2022-7-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typos or mistakes of Flink CEP Document in the official website</summary>
      <description>1. "how you can migrate your job from an older Flink version to Flink-1.3." -&gt; "how you can migrate your job from an older Flink version to Flink-1.13."2. "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: { C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}" -&gt; "Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B}, {C A2 B}, {C A2 A3 B}, {C A2 A4 B}, {C A2 A3 A4 B}, {C A3 B}, {C A3 A4 B}, {C A4 B}"3. "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no elements mapped to the specified variable." -&gt; "For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no events mapped to the PatternName."</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.zh.docs.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="28522" opendate="2022-7-12 00:00:00" fixdate="2022-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-sequence-file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SerializableHadoopConfigurationTest.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.formats.sequencefile.SequenceStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="28577" opendate="2022-7-17 00:00:00" fixdate="2022-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>1.15.1 web ui console report error about checkpoint size</summary>
      <description>1.15.11 start-cluster2 submit job: ./bin/flink run -d ./examples/streaming/TopSpeedWindowing.jar3 trigger savepoint: ./bin/flink savepoint {{{jobId} ./sp0}}4 open web ui for job and change to checkpoint tab, nothing showed.Chrome console log shows some error:{{main.a7e97c2f60a2616e.js:1 ERROR TypeError: Cannot read properties of null (reading 'checkpointed_size')    at q (253.e9e8f2b56b4981f5.js:1:607974)    at Sl (main.a7e97c2f60a2616e.js:1:186068)    at Br (main.a7e97c2f60a2616e.js:1:184696)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at N8 (main.a7e97c2f60a2616e.js:1:185128)    at Br (main.a7e97c2f60a2616e.js:1:185153)    at B8 (main.a7e97c2f60a2616e.js:1:191872)}}   </description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="28609" opendate="2022-7-19 00:00:00" fixdate="2022-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink-Pulsar connector fails on larger schemas</summary>
      <description>When a model results in a larger schema (this seems to be related to its byte array representation), the number of expected bytes to read is different than the number of actually read bytes: exception.txt. The "read" is such a case is always 1018 while the expected "byteLen" gives a greater value. For smaller schemata, the numbers are equal (less than 1018) and no issue occurs.The problem reproduction is on GitHub. There are 2 simple jobs (SimpleJob1 and SimpleJob2) using basic models for the Pulsar source definition (PulsarMessage1 and PulsarMessage2, respectively). Each of the corresponding schemata is properly serialised and deserialised, unless an effective byte array length becomes excessive (marked with "the problem begins" in model classes). The fail condition can be achieved by a number of fields (PulsarMessage1) or just longer field names (PulsarMessage2). The problem occurs on either Avro or a JSON schema set in the Pulsar source definition.</description>
      <version>1.14.3,1.14.4,1.14.5,1.15.1</version>
      <fixedVersion>1.16.0,1.14.6,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="28617" opendate="2022-7-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SqlGatewayService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28636" opendate="2022-7-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility to test POJO compliance</summary>
      <description>Users should be encouraged to eagerly verify that their POJOs satisfy all the requirements that Flink imposes, however we provide no convenient way to test that.They currently have to resort to something like below, which isn't obvious at all:TypeSerializer&lt;Event&gt; eventSerializer = TypeInformation.of(Event.class).createSerializer(new ExecutionConfig());assertThat(eventSerializer).isInstanceOf(PojoSerializer.class);</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug id="28644" opendate="2022-7-22 00:00:00" fixdate="2022-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support collecting arbitrary number of streams</summary>
      <description>Extend the collect api to support collecting multiple streams, as outlined in FLIP-251.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.StreamCollectorExtension.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.StreamCollector.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.WritableSavepointWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.WritableSavepointITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamCollectTestITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">docs.content.docs.dev.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="28696" opendate="2022-7-26 00:00:00" fixdate="2022-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Notify all newlyAdded/Merged blocked nodes to BlocklistListener</summary>
      <description>This bug was introduced by FLINK-28660. Our newly added logic results in that blocklist listener will not be notified when there are no newly added nodes (only merge nodes) 。 </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blocklist.DefaultBlocklistHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blocklist.DefaultBlocklistHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2872" opendate="2015-10-19 00:00:00" fixdate="2015-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation for Scala part to add readFileOfPrimitives</summary>
      <description>Currently the Scala part of the ExecutionEnvironment missing readFileOfPrimitives to create Dataset from file for primitive types.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="28720" opendate="2022-7-27 00:00:00" fixdate="2022-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hive partition when flink has no data to write</summary>
      <description>When writing data to a specified partition (static partition) of a Hive table with Flink SQL, the partition should be added just like Hive/Spark regardless of whether there is data written or not.we should also ensure that Insert into and insert overwrite semantics.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionLoader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28738" opendate="2022-7-29 00:00:00" fixdate="2022-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[doc] Add a user doc about the correctness for non-deterministic updates</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="28765" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create a doc for protobuf format</summary>
      <description>After this feature has been done https://issues.apache.org/jira/browse/FLINK-18202, we should write a doc to introduce how to use the protobuf format in SQL. </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.connectors.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28778" opendate="2022-8-2 00:00:00" fixdate="2022-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bulk fetch of table and column statistics for given partitions</summary>
      <description></description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkRecomputeStatisticsProgram.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.CatalogTableStatisticsConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestCatalogFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.plan.stats.TableStats.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.plan.stats.ColumnStats.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataString.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataLong.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDouble.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataDate.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBoolean.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogColumnStatisticsDataBinary.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveStatsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.HivePartitionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="28801" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade OSS SDK Version</summary>
      <description></description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28804" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use proper stand-ins for missing metrics groups</summary>
      <description>A few classes call the open() method of schemas with a custom initialization context that either throws an exception or returns null when the metricgroup is accessed.The correct approach is to return an unregistered metrics group.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.SerializationSchemaAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.DeserializationSchemaAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28841" opendate="2022-8-5 00:00:00" fixdate="2022-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document dynamic property support for startup scripts</summary>
      <description>The support for dynamic properties in startup scripts isn't documented anywhere.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="28848" opendate="2022-8-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduces LOOKUP join hint to support delayed retry for lookup join (table alias unsupported in hint)</summary>
      <description>main part of flip234</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.hints.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.hints.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.hints.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.hints.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.JoinHintResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.ClearQueryBlockAliasResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.hints.ShuffleMergeJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.hints.ShuffleHashJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.hints.NestLoopJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.hints.JoinHintTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.batch.sql.join.hints.BroadcastJoinHintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.alias.ClearJoinHintWithInvalidPropagationShuttleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.JoinHintResolver.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.JoinStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHintStrategies.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHints.java</file>
    </fixedFiles>
  </bug>
  <bug id="28849" opendate="2022-8-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add it cases with lookup hint using new lookup function api for lookup join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="28861" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-deterministic UID generation might cause issues during restore</summary>
      <description>I want to use the savepoint mechanism to move existing jobs from one version of Flink to another, by: Stopping a job with a savepoint Creating a new job from the savepoint, on the new version.In Flink 1.15.1, it fails, even when going from 1.15.1 to 1.15.1. I get this error, meaning that it could not map the state from the previous job to the new one because of one operator:Failed to rollback to checkpoint/savepoint hdfs://hdfs-name:8020/flink-savepoints/savepoint-046708-238e921f5e78. Cannot map checkpoint/savepoint state for operator d14a399e92154660771a806b90515d4c to the new program, because the operator is not available in the new program.After investigation, the problematic operator corresponds to a ChangelogNormalize operator, that I do not explicitly create. It is generated because I use tableEnv.fromChangelogStream(stream, schema, ChangelogMode.upsert()) (the upsert mode is important, other modes do not fail). The table created is passed to an SQL query using the SQL API, which generates something like:ChangelogNormalize&amp;#91;8&amp;#93; -&gt; Calc&amp;#91;9&amp;#93; -&gt; TableToDataSteam -&gt; &amp;#91;my_sql_transformation&amp;#93; -&gt; &amp;#91;my_sink&amp;#93;In previous versions of Flink it seems this operator was always given the same uid so the state could match when starting from the savepoint. In Flink 1.15.1, I see that a different uid is generated every time. I could not find a reliable way to set that uid manually. The only way I found was by going backwards from the transformation:dataStream.getTransformation().getInputs().get(0).getInputs().get(0).getInputs().get(0).setUid("the_user_defined_id");</description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.upgrading.md</file>
      <file type="M">docs.content.zh.docs.ops.upgrading.md</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.InternalConfigOptionsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.LongAdaptiveHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.internal.CompiledPlanUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.FlinkPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeTranslator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeConfig.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.ProviderContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.StreamingSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonTestUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TransformationsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="28863" opendate="2022-8-8 00:00:00" fixdate="2022-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot result of RocksDB native savepoint should have empty shared-state</summary>
      <description>The current snapshot result of RocksDB native savepoint has non-empty shared state, which is obviously not correct as all snapshot artifacts already stay in the exclusive checkpoint scope folder.This does not bring real harmful result due to we would not register the snapshot results of RocksDB native savepoint.</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="28865" opendate="2022-8-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add updated Print sink for new interfaces</summary>
      <description>The built-in print sink still uses the old sink interfaces. Add a new implementation for the new sink interfaces.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.PrintSinkOutputWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28934" opendate="2022-8-11 00:00:00" fixdate="2022-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Source put all the splits to only one parallelism when using Exclusive subscription</summary>
      <description>The image here shows if we start a Flink application with four parallelism and four splits. All the splits would be sent to the first added reader. This is because we don't assign splits by pre-divide splits according to the size of parallelism. The readers are added to the enumerator one by one in the first bootstrap.</description>
      <version>1.14.5,1.15.1,1.16.0</version>
      <fixedVersion>1.16.0,1.14.6,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.CursorPosition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.NonSharedSplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarSerdeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.CollectIteratorAssertions.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.embedded.PulsarEmbeddedRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.f4d91193-72ba-4ce4-ad83-98f780dce581</file>
    </fixedFiles>
  </bug>
  <bug id="28960" opendate="2022-8-15 00:00:00" fixdate="2022-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar throws java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement</summary>
      <description>Unknown HK2 failure detected:MultiException stack 1 of 2java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:137) at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:124) at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.&lt;init&gt;(JaxbAnnotationIntrospector.java:116) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) at java.base/java.lang.Class.newInstance(Class.java:584)</description>
      <version>1.15.1,1.14.6</version>
      <fixedVersion>1.17.0,1.15.3,1.14.7,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28971" opendate="2022-8-15 00:00:00" fixdate="2022-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adds user documentation for the new LOOKUP hint</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.hints.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.hints.md</file>
    </fixedFiles>
  </bug>
  <bug id="28972" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods of StartCursor and StopCursor to align the Java</summary>
      <description>Add fromPublishTime in the StartCursor classAdd afterEventTime and afterPublishTime in the StopCursor class</description>
      <version>1.14.5,1.15.1,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.connectors.tests.test.pulsar.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="28978" opendate="2022-8-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector doesn&amp;#39;t work for new AWS regions</summary>
      <description>The current validation in the Kinesis connector checks that the AWS Region string specified is present in the Regions enum attached in the AWS SDK. This is not desirable because every time AWS launches a new region, we will have to update the AWS SDK shaded into the connector. We want to change it such that we validate the shape of the string, allowing for future AWS Regions.  Current list of regions:ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, ap-southeast-3, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global</description>
      <version>1.13.6,1.14.5,1.15.1</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSGeneralUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.main.java.org.apache.flink.connector.aws.util.AWSGeneralUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="28988" opendate="2022-8-16 00:00:00" fixdate="2022-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result for filter after temporal join</summary>
      <description>The following code can reproduce the case public class TemporalJoinSQLExample1 { public static void main(String[] args) throws Exception { // set up the Java DataStream API final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set up the Java Table API final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final DataStreamSource&lt;Tuple3&lt;Integer, String, Instant&gt;&gt; ds = env.fromElements( new Tuple3&lt;&gt;(0, "online", Instant.ofEpochMilli(0)), new Tuple3&lt;&gt;(0, "offline", Instant.ofEpochMilli(10)), new Tuple3&lt;&gt;(0, "online", Instant.ofEpochMilli(20))); final Table table = tableEnv.fromDataStream( ds, Schema.newBuilder() .column("f0", DataTypes.INT()) .column("f1", DataTypes.STRING()) .column("f2", DataTypes.TIMESTAMP_LTZ(3)) .watermark("f2", "f2 - INTERVAL '2' SECONDS") .build()) .as("id", "state", "ts"); tableEnv.createTemporaryView("source_table", table); final Table dedupeTable = tableEnv.sqlQuery( "SELECT * FROM (" + " SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY ts DESC) AS row_num FROM source_table" + ") WHERE row_num = 1"); tableEnv.createTemporaryView("versioned_table", dedupeTable); DataStreamSource&lt;Tuple2&lt;Integer, Instant&gt;&gt; event = env.fromElements( new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(0)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(5)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(10)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(15)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(20)), new Tuple2&lt;&gt;(0, Instant.ofEpochMilli(25))); final Table eventTable = tableEnv.fromDataStream( event, Schema.newBuilder() .column("f0", DataTypes.INT()) .column("f1", DataTypes.TIMESTAMP_LTZ(3)) .watermark("f1", "f1 - INTERVAL '2' SECONDS") .build()) .as("id", "ts"); tableEnv.createTemporaryView("event_table", eventTable); final Table result = tableEnv.sqlQuery( "SELECT * FROM event_table" + " LEFT JOIN versioned_table FOR SYSTEM_TIME AS OF event_table.ts" + " ON event_table.id = versioned_table.id"); result.execute().print(); result.filter($("state").isEqual("online")).execute().print(); }}  The result of temporal join is the following:op         id                     ts        id0                         state                    ts0             row_num+I          01970-01-01 08:00:00.000          0                        online1970-01-01 08:00:00.000                   1+I          01970-01-01 08:00:00.005          0                        online1970-01-01 08:00:00.000                   1+I          01970-01-01 08:00:00.010          0                       offline1970-01-01 08:00:00.010                   1+I          01970-01-01 08:00:00.015          0                       offline1970-01-01 08:00:00.010                   1+I          01970-01-01 08:00:00.020          0                        online1970-01-01 08:00:00.020                   1+I          01970-01-01 08:00:00.025          0                        online1970-01-01 08:00:00.020                   1 After filtering with predicate state = 'online', I expect only the two rows with state offline will be filtered out. But I got the following result:op         id                     ts        id0                         state                    ts0             row_num+I          01970-01-01 08:00:00.020          0                        online1970-01-01 08:00:00.020                   1+I          01970-01-01 08:00:00.025          0                        online1970-01-01 08:00:00.020                   1    </description>
      <version>1.15.1</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29062" opendate="2022-8-22 00:00:00" fixdate="2022-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protobuf-plugin should download protoc via maven on flink-protobuf module</summary>
      <description>&amp;#91;ERROR&amp;#93; Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run (default) on project flink-protobuf: Error extracting protoc for version 3.21.2: Unsupported platform: protoc-3.21.2-osx-x86_64.exe -&gt; &amp;#91;Help 1&amp;#93;This issue is similar to FLINK-23661 but on the different module.</description>
      <version>1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-protobuf.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29513" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka version to 3.2.3</summary>
      <description>Kafka 3.2.3 contains certain security fixes (see https://downloads.apache.org/kafka/3.2.3/RELEASE_NOTES.html). We should upgrade the dependency in Flink</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29514" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Minikdc to v3.2.4</summary>
      <description>Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
