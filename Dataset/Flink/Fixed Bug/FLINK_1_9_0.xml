<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10995" opendate="2018-11-23 00:00:00" fixdate="2018-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Copy intermediate serialization results only once for broadcast mode</summary>
      <description>The emitted records from operator would be firstly serialized into intermediate bytes array in RecordSerializer, then copy the intermediate results into target buffers for different sub partitions.  For broadcast mode, the same intermediate results would be copied as many times as the number of sub partitions, and this would affect the performance seriously in large scale jobs.We can copy to only one target buffer which would be shared by all the sub partitions to reduce the overheads. For emitting latency marker in broadcast mode, we should flush the previous shared target buffers first, and then request a new buffer for the target sub partition to send latency marker.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.FileChannelBoundedDataTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.IteratorWrappingTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionAvailabilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferConsumer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11745" opendate="2019-2-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TTL end-to-end test restores from the savepoint after the job cancelation</summary>
      <description>The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1175" opendate="2014-10-19 00:00:00" fixdate="2014-10-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for KeySelector and possibility to select more than one field for groupBy for streaming</summary>
      <description>The streaming API currently only supports grouping by one field in the tuple. To match the functionality of the batch API it is necessary to add support for the KeySelector and also the possibility to select more than one field to group by.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.FieldsPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.WindowReduceInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowGroupReduceInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedBatchReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedBatchGroupReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoGroupedWindowReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoGroupedReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoGroupedBatchReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.FieldsPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.StreamRecord.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowGroupReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedBatchReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedBatchGroupReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoGroupedWindowReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoGroupedReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoGroupedBatchReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.GroupedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.CoWindowDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.CoBatchedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.BatchedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="11791" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe how to build Flink with Hadoop in build guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="11892" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port conflict when running nightly end-to-end tests</summary>
      <description>When I do the end-to-end check according to `https://github.com/apache/flink/tree/master/flink-end-to-end-tests`. I got the follows problem:1. Executed command， and the message of console as follows:FLINK_DIR=/Users/jincheng/work/FlinkRelease/1.8/flink-1.8.0/flink-dist/target/flink-1.8.0-bin/flink-1.8.0export FLINK_DIRsh flink-end-to-end-tests/run-nightly-tests.sh......Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.Dispatcher REST endpoint is up.Job (180a3cfc35d549417e5807520d7402f9) is running.Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...2. Log info:2019-03-13 07:56:33,670 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator         - Remoting shut down.2019-03-13 07:56:33,673 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)Caused by: java.net.BindException: Could not start actor system on any port in port range 6123at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:172)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)3. environmentMacOS: 10.14.3 Java version "1.8.0_151"jinchengsunjcs-iMac:~ jincheng$ echo $0-bash</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="11896" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce stream physical nodes</summary>
      <description>This issues aims to introduce flink stream physical RelNode, such as StreamExecCalc, StreamExecExchange, StreamExecExpand etc.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  <bug id="11898" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support code generation for all Blink built-in functions and operators</summary>
      <description>Support code generation for built-in functions and operators. FLINK-11788 has supported some of the operators. This issue is aiming to complement the functions and operators supported in Flink SQL.This should inlclude: CONCAT, LIKE, SUBSTRING, UPPER, LOWER, and so on.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryStringTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.util.SegmentsUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.TypeConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.InternalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.ThreadLocalCache.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.CompileUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.util.HashUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.LazyBinaryFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BoxedWrapperRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryRowWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryGeneric.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArrayWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.RowTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ArrayTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.MathFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.LiteralTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11899" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce parquet ColumnarRow split reader</summary>
      <description>Parquet ColumnarRow split reader is introduced to read parquet data in batches.When returning each row of data, instead of actually retrieving each field, we use BaseRow's abstraction to return a Columnar Row-like view.This will greatly improve the downstream filtered scenarios, so that there is no need to access redundant fields on the filtered data.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.TestUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetTableSource.java</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11950" opendate="2019-3-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing dependencies in NOTICE file of flink-dist.</summary>
      <description>Add Missing dependencies in NOTICE file of flink-dist. </description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="11956" opendate="2019-3-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove shading from S3 filesystems build</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.flink.fs.s3presto.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">tools.travis.shade.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1197" opendate="2014-10-28 00:00:00" fixdate="2014-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Special page on website that describes issues that occur together with types</summary>
      <description>Describe topics like: issues, restrictions etc. when using Java 8 Lambdas general TypeExtractor functionalities like simple input type inferences and known issues</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.internal.job.scheduling.md</file>
    </fixedFiles>
  </bug>
  <bug id="11989" opendate="2019-3-21 00:00:00" fixdate="2019-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable metric reporter modules in jdk9 runs</summary>
      <description>The Reporter modules are currently disabled on the travis jdk9 jobs as we ran into some issues in the MetricRegistry that prevented them from suceeding.It appears that this issue no longer exists, so let's enable them again.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12029" opendate="2019-3-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Column selections</summary>
      <description>In this Jira will add column operators/operations as follows:Fine-grained column operations Column selectionSee google doc, And I also have done some prototype</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.tableImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ColumnOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolverRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.BuiltInFunctionDefinitions.java</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="12041" opendate="2019-3-28 00:00:00" fixdate="2019-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ResettableExternalBuffer to blink batch</summary>
      <description>Introduce ResettableExternalBuffer:A resettable external buffer for binary row. It stores records in memory and spill to disk when memory is not enough. When the spill is completed, the records are written to memory again. The returned iterator reads the data in write order (read spilled records first). It supports infinite length. It can open multiple Iterators. It support new iterator with beginRow.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryRowChannelInputViewIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryRow.java</file>
    </fixedFiles>
  </bug>
  <bug id="12050" opendate="2019-3-28 00:00:00" fixdate="2019-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BlockingShutdownTest fails on Java 9</summary>
      <description>21:21:28.689 [ERROR] testProcessShutdownBlocking(org.apache.flink.runtime.util.BlockingShutdownTest) Time elapsed: 0.961 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Cannot determine process ID at org.apache.flink.runtime.util.BlockingShutdownTest.testProcessShutdownBlocking(BlockingShutdownTest.java:57)21:21:28.689 [ERROR] testProcessExitsDespiteBlockingShutdownHook(org.apache.flink.runtime.util.BlockingShutdownTest) Time elapsed: 0.325 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Cannot determine process ID at org.apache.flink.runtime.util.BlockingShutdownTest.testProcessExitsDespiteBlockingShutdownHook(BlockingShutdownTest.java:98)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="12065" opendate="2019-3-29 00:00:00" fixdate="2019-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E tests fail due to illegal-access warning on Java 9</summary>
      <description>When accessing inaccessible fields via reflection a warning is printed on Java 9 like below:WARNING: An illegal reflective access operation has occurredWARNING: All illegal access operations will be denied in a future releaseWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.core.memory.HybridMemorySegment (file:/home/travis/build/zentol/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT/lib/flink-dist_2.11-1.9-SNAPSHOT.jar) to field java.nio.Buffer.addressThese are printed into the .out file of the processes, and cause e2e tests to fail since we check for empty .out files.From what I've gathered we cannot disable these warnings, so we'll have to adapt the check to ignore these.We can't just fix these accesses since they also occur in libraries (like akka's netty)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12068" opendate="2019-3-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Backtrack failover regions if intermediate results are unavailable</summary>
      <description>The batch failover strategy needs to be able to backtrack fail over regions if an intermediate result is unavailable. Either by explicitly checking whether the intermediate result partition is available or via a special exception indicating that a result partition is no longer available.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.TestFailoverTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailoverRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="12117" opendate="2019-4-5 00:00:00" fixdate="2019-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CassandraConnectorITCase fails on Java 9</summary>
      <description>From what I found cassandra never really supported Java 9, so we will likely have to disable the tests in the jdk9 profile.java.lang.ExceptionInInitializerError at org.github.jamm.MemoryMeter.measure(MemoryMeter.java:178) at org.apache.cassandra.utils.ObjectSizes.measure(ObjectSizes.java:162) at org.apache.cassandra.utils.ObjectSizes.&lt;clinit&gt;(ObjectSizes.java:39) at org.apache.cassandra.dht.RandomPartitioner.&lt;clinit&gt;(RandomPartitioner.java:47) at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:292) at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:434) at org.apache.cassandra.utils.FBUtilities.instanceOrConstruct(FBUtilities.java:450) at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:400) at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:353) at org.apache.cassandra.config.DatabaseDescriptor.&lt;clinit&gt;(DatabaseDescriptor.java:119) at org.apache.cassandra.service.StartupChecks$4.execute(StartupChecks.java:167) at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:107) at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:162) at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:416) at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase$EmbeddedCassandraService.start(CassandraConnectorITCase.java:147) at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.startCassandra(CassandraConnectorITCase.java:186) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 5 at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3116) at java.base/java.lang.String.substring(String.java:1885) at org.github.jamm.MemoryLayoutSpecification.getEffectiveMemoryLayoutSpecification(MemoryLayoutSpecification.java:190) at org.github.jamm.MemoryLayoutSpecification.&lt;clinit&gt;(MemoryLayoutSpecification.java:31) ... 34 more</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12119" opendate="2019-4-6 00:00:00" fixdate="2019-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OWASP Dependency Check</summary>
      <description>In order to obtain some visibility on the current known security vulnerabilities in Flink's dependencies. It would be useful to include the OWASP dependency check plugin &amp;#91;1&amp;#93; into our Maven build.By including it into flink-parent, we can get summary of all dependencies of all child projects by runningmvn clean org.owasp:dependency-check-maven:5.0.0-M2:aggregateWe should probably exclude some modules from the dependency-check. These could be: flink-docs flink-fs-tests flink-yarn-tests flink-contribAnything else? What about flink-python/flink-streaming-python?**In addition I propose to exclude all dependencies in the system or provided scope.At least initially, the build would never fails because of vulnerabilities. &amp;#91;1&amp;#93; https://jeremylong.github.io/DependencyCheck/dependency-check-maven/index.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12133" opendate="2019-4-9 00:00:00" fixdate="2019-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support unbounded aggregate in streaming table runtime</summary>
      <description>This ticket is aiming to support unbounded aggregate in streaming runtime. This should includes:1. GroupAggFunction: function that support unbounded aggregate without optimizations2. MiniBatchGroupAggFunction: function that support unbounded aggregate with minibatch optimization3. MiniBatchLocalGroupAggFunction &amp; MiniBatchGlobalGroupAggFunction: function that support unbounded aggregate with local combine optimization</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.MapViewTypeInfoFactory.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.over.SumAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.bundle.MapBundleOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.TypeConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.type.InternalTypes.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.rank.UpdateRankFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.rank.RetractRankFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.rank.AppendRankFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.rank.AbstractRankFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.NonBufferOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.UnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.UnboundedOverWindowFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.UnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.SlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.OffsetOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.frame.InsensitiveOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.keyselector.BinaryRowKeySelector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.bundle.MapBundleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.NamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.AggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.UpdatableRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ObjectArrayRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.Decimal.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.DistinctAggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.runtime.utils.FailingCollectionSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.functions.aggfunctions.ConcatWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.functions.aggfunctions.ConcatWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.NullSerializer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.NullAwareMapSerializer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.dataview.Order.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.ConcatWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.ConcatWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.SqlMax2ndAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.util.KeySelectorUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.dataview.DataView.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.dataview.ListView.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.dataview.MapView.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.dataview.DataViewSpec.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.dataview.DataViewUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.cost.FlinkDefaultRelMetadataProvider.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.cost.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.exec.BatchExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.exec.StreamExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.IncrementalAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.aggregation.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.ListViewSerializer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.ListViewTypeInfo.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.ListViewTypeInfoFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.MapViewSerializer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.MapViewTypeInfo.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12137" opendate="2019-4-9 00:00:00" fixdate="2019-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more proper explanation on flink streaming connectors</summary>
      <description>When I write code using flink streaming connector, IntelliJ shows alert message.I want to suggest more clear guidance message without warning message.Actually, AWS_CREDENTIALS_PROVIDER is a constant of AWSConfigConstants class.When documentation shows AWSConfigConstants class instead of ConsumerConfigConstants which is extended class of it, developers do not need to take their times to figure out warning message. </description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="12139" opendate="2019-4-9 00:00:00" fixdate="2019-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink on mesos - Parameterize disk space needed.</summary>
      <description>We are having a small issue while trying to deploy Flink on Mesos using marathon. In our set up of Mesos we are required to specify the amount of disk space we want to have for the applications we deploy there.The current default value in Flink is 0 and it's currently is not parameterizable. This means that we ask 0 disk space for our instances so Flink can't work.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosEntrypointUtils.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12165" opendate="2019-4-11 00:00:00" fixdate="2019-4-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add resolution rule that checks all unresolved expressions are resolved</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolverRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.ExpressionResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12171" opendate="2019-4-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The network buffer memory size should not be checked against the heap size on the TM side</summary>
      <description>Currently when computing the network buffer memory size on the TM side in TaskManagerService#calculateNetworkBufferMemory`(version 1.8 or 1.7) or NetworkEnvironmentConfiguration#calculateNewNetworkBufferMemory(master), the computed network buffer memory size is checked to be less than `maxJvmHeapMemory`. However, in TM side, maxJvmHeapMemory stores the maximum heap memory (namely -Xmx) . With the above process, when TM starts, -Xmx is computed in RM or in taskmanager.sh with (container memory - network buffer memory - managed memory),  thus the above checking implies that the heap memory of the TM must be larger than the network memory, which seems to be not necessary. This may cause TM to use more memory than expected. For example, for a job who has a large network throughput, uses may configure network memory to 2G. However, if users want to assign 1G to heap memory, the TM will fail to start, and user has to allocate at least 2G heap memory (in other words, 4G in total for the TM instead of 3G) to make the TM runnable. This may cause resource inefficiency. Therefore, I think the network buffer memory size also need to be checked against the total memory instead of the heap memory on the TM  side: Checks that networkBufFraction &lt; 1.0. Compute the total memory by ( jvmHeapNoNet / (1 - networkBufFraction)). Compare the network buffer memory with the total memory.This checking is also consistent with the similar one done on the RM side.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-dist.src.test.java.org.apache.flink.dist.TaskManagerHeapSizeCalculationJavaBashTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12192" opendate="2019-4-15 00:00:00" fixdate="2019-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for grouping sets and distinct aggregate</summary>
      <description>This issue aims to supports generating optimized logical plan for grouping sets and distinct aggregate. (mentioned in FLINK-12076 and FLINK-12098)for batch, query with distinct aggregate will be rewritten into two non-distinct aggregates by extended AggregateExpandDistinctAggregatesRule, the first aggregate computes the distinct key and non-distinct aggregate function, and the second aggregate computes the distinct aggregate function based on first aggregate result.  The first aggregate has grouping sets if there are more than one distinct aggregate functions on different fields.for stream, query with distinct aggregate is handled by SplitAggregateRule in FLINK-12161.query with grouping sets (or cube, rollup) will be rewritten into a regular aggregate with expand, and the expand node will duplicates the input data for each simple group. e.g.schema:MyTable: a: INT, b: BIGINT, c: VARCHAR(32), d: VARCHAR(32) Original records:+-----+-----+-----+-----+| a | b | c | d |+-----+-----+-----+-----+| 1 | 1 | c1 | d1 |+-----+-----+-----+-----+| 1 | 2 | c1 | d2 |+-----+-----+-----+-----+| 2 | 1 | c1 | d1 |+-----+-----+-----+-----+SELECT a, c, SUM(b) as b FROM MyTable GROUP BY GROUPING SETS (a, c)logical plan after expanded:LogicalCalc(expr#0..3=[{inputs}], proj#0..1=[{exprs}], b=[$t3]) LogicalAggregate(group=[{0, 2, 3}], groups=[[]], b=[SUM($1)]) LogicalExpand(projects=[{a=[$0], b=[$1], c=[null], $e=[1]}, {a=[null], b=[$1], c=[$2], $e=[2]}]) LogicalNativeTableScan(table=[[builtin, default, MyTable]])notes:'$e = 1' is equivalent to 'group by a''$e = 2' is equivalent to 'group by c'expanded records:+-----+-----+-----+-----+| a | b | c | $e |+-----+-----+-----+-----+ ---+---| 1 | 1 | null| 1 | |+-----+-----+-----+-----+ records expanded by record1| null| 1 | c1 | 2 | |+-----+-----+-----+-----+ ---+---| 1 | 2 | null| 1 | |+-----+-----+-----+-----+ records expanded by record2| null| 2 | c1 | 2 | |+-----+-----+-----+-----+ ---+---| 2 | 1 | null| 1 | |+-----+-----+-----+-----+ records expanded by record3| null| 1 | c1 | 2 | |+-----+-----+-----+-----+ ---+---</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.ExpandUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12193" opendate="2019-4-15 00:00:00" fixdate="2019-1-15 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Send TM "can be released status" with RM heartbeat</summary>
      <description>We introduced a conditional release of Task Executor in Resource Manager in FLINK-10941. At the moment RM directly asks TE every release timeout whether it can be released (all depending consumers are done). We can piggyback TE/RM heartbeats for this purpose. In this case, we do not need additional RPC call to TE gateway and could potentially release TE quicker.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12226" opendate="2019-4-17 00:00:00" fixdate="2019-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI docs about SUSPEND/TERMINATE</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="12229" opendate="2019-4-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement Lazy Scheduling Strategy</summary>
      <description>Implement a SchedulingStrategy that covers the functionality of ScheduleMode.LAZY_FROM_SOURCES, i.e., vertices are scheduled when all the input data are available.Acceptance Criteria: New strategy is tested in isolation using test implementations (i.e., without having to submit a job)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingExecutionVertex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulerOperations.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingExecutionVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="12253" opendate="2019-4-18 00:00:00" fixdate="2019-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup a class hierarchy for the new logical type system</summary>
      <description>Setup a new class hierarchy around LogicalType in table-common.The classes implement the types listed in the table of FLIP-37.The classes won't be connected to the API yet.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.NullType.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalTypeVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12254" opendate="2019-4-18 00:00:00" fixdate="2019-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the new type system through the API</summary>
      <description>Exposes the new type system through API methods.Introduces new methods, adds converters for backwards-compatibility, and deprecates old methods.Adds checks to types that are not supported by the legacy planner.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.ProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.NestedFieldsProjectableTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.DefinedFieldMapping.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.expressions.rules.VerifyNoUnresolvedExpressionsRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.lookups.FieldReferenceLookup.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowEmitStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.FieldReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LocalReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.Expression.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TypeLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSchemaValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSchemaTest.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliResultView.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.descriptors.LiteralValueValidator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.TimeIntervalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowTableAggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sinks.TableSinkBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.sources.TableSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TableSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.CollectTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.CsvTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationExpressionsUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.TableOperationTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ValueLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.expressions.ExpressionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ExpandColumnFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AliasOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12257" opendate="2019-4-18 00:00:00" fixdate="2019-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert CatalogTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs</summary>
      <description>In FLINK-11476, we created CatalogManager to hook up planner with unified catalog APIs. What's missing there is, at the very last step, convert CatalogBaseTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs, like how ExternalTableUtil.fromExternalCatalogTable() works to convert the old ExternalCatalogTable to a Calcite table</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.TestExternalTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12269" opendate="2019-4-20 00:00:00" fixdate="2019-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Temporal Table Join in blink planner</summary>
      <description>Support translate following "FOR SYSTEM_TIME AS OF" query into StreamExecTemporalTableJoin.SELECT o.amout, o.currency, r.rate, o.amount * r.rateFROM Orders AS o JOIN LatestRates FOR SYSTEM_TIME AS OF o.proctime AS r ON r.currency = o.currencyThis is an extension to current temporal join (FLINK-9738) using a standard syntax introduced in Calcite 1.19.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedHashFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedCollector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedClass.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.runtime.collector.TableFunctionCollector.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.rules.logical.FlinkJoinPushExpressionsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="12271" opendate="2019-4-20 00:00:00" fixdate="2019-1-20 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Display input field name list when throw Cannot resolve field exception</summary>
      <description>Currently, when we select a field that does not exist, an cannot solve field exception is thrown. For example,org.apache.flink.table.api.ValidationException: Cannot resolve field [_4]It would be better to also display the input field list to indicate existing fields, such as: org.apache.flink.table.api.ValidationException: Cannot resolve field [_4], input field list:[_1, _2, _3].</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ReferenceResolverRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="12312" opendate="2019-4-24 00:00:00" fixdate="2019-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily disable CLI command for rescaling</summary>
      <description>Temporarily remove support to rescale job via CLI. See this thread for more details: https://www.mail-archive.com/dev@flink.apache.org/msg25266.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RescalingBehaviour.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendModifyTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="12326" opendate="2019-4-25 00:00:00" fixdate="2019-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a basic test framework, just like the existing Java TableAPI, abstract some TestBase.</summary>
      <description>Add a basic test framework, just like the existing Java/Scala TableAPI, abstract some TestBase.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.end.to.end.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12345" opendate="2019-4-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for stream window aggregate</summary>
      <description>This issue aims to add support for generating optimized logical plan for stream window aggregate queries, e.g.SELECT COUNT(*), TUMBLE_END(rowtime, INTERVAL '15' MINUTE) + INTERVAL '1' MINUTEFROM MyTable GROUP BY TUMBLE(rowtime, INTERVAL '15' MINUTE)the above query will be optimized to following planCalc(select=[EXPR$0, +(CAST(w$end), 60000:INTERVAL MINUTE) AS EXPR$1])+- GroupWindowAggregate(window=[TumblingGroupWindow], properties=[w$start, w$end, w$rowtime], select=[COUNT(*) AS EXPR$0, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime]) +- Exchange(distribution=[single]) +- Calc(select=[rowtime]) +- DataStreamScan(table=[[_DataStreamTable_0]], fields=[a, b, c, proctime, rowtime])</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.StreamOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.StreamOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.cost.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.cost.FlinkDefaultRelMetadataProvider.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.FieldReferenceExpression.java</file>
    </fixedFiles>
  </bug>
  <bug id="12346" opendate="2019-4-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala-suffix check broken on Travis</summary>
      <description>the scala-suffix check currently does not work on travis since the maven output is not what the script expects. On travis we have timestamps in the maven output, which breaks the parsing.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.verify.scala.suffixes.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12347" opendate="2019-4-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table-runtime-blink is missing scala suffix</summary>
      <description>flink-table-runtime-blink has a dependency on flink-streaming-java and thus requires a scala suffix.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12362" opendate="2019-4-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove legacy container number config option for Flink on yarn</summary>
      <description>The -n config option for Flink on YARN has been deprecated and removed from documentation. However, there are still some legacy code exists in the codebase. We need to clean up them.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendRunWithYarnTest.java</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonShellParserTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">docs.ops.deployment.aws.zh.md</file>
      <file type="M">docs.ops.deployment.aws.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="12364" opendate="2019-4-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a CheckpointFailureManager to centralized manage checkpoint failure</summary>
      <description>This issue tracks the work of T2 section about in design document : https://docs.google.com/document/d/1ce7RtecuTxcVUJlnU44hzcO2Dwq9g4Oyd8_biy94hJc/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.IterateITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferAlignmentLimitTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.exceptions.CheckpointException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.DeclineCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.InputEndOfStreamException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineTaskNotReadyException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineTaskNotCheckpointingException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineSubsumedException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineOnCancellationBarrierException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.decline.AlignmentLimitExceededException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12365" opendate="2019-4-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add stats related catalog APIs</summary>
      <description>This is to support (table and column) stats for table/partition with related to catalog.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableWritableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogTable.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.GenericHiveMetastoreCatalogUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="12366" opendate="2019-4-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Catalog APIs to make them more consistent and coherent</summary>
      <description>Some of the APIs seem inconsistent with others in terms of exception thrown and error handling. This is to clean them up to maintain consistency and coherence.</description>
      <version>1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableWritableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.exceptions.PartitionNotExistException.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="1237" opendate="2014-11-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add custom partitioners for shuffle steps</summary>
      <description>It would be helpful to be able to specify a custom partitioner for shuffle steps. This would users allow to control how their data is distributed across the cluster. Especially in situations where the number of data records is equal to the number of slots it might be a possibility to avoid skewed data distributions due to the default hashing function.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.AggregateTranslationTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.StaticlyNestedIterationsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.IterationWithChainingNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancellingTestBase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaAggregateOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.TaskConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.ShipStrategyType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.RecordOutputEmitter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.PartitionFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputEmitter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.HistogramPartitionFunction.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.costs.CostEstimator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.BinaryUnionNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.CollectorMapNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.FilterNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.FlatMapNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.GroupReduceNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.MapNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.MapPartitionNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.PartitionNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.ReduceNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dataproperties.PartitioningProperty.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dataproperties.RequestedGlobalProperties.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.GroupReduceProperties.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.GroupReduceWithCombineProperties.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.ReduceProperties.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.SortMergeJoinDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.Channel.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.FeedbackPropertiesMatchTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.JoinOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.ReduceOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.JoinOperatorBaseTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.MapOperatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.base.PartitionMapOperatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldListTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldSetTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.AggregateOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DistinctOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Grouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.CsvInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.CsvOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.DelimitedOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.operators.ReduceOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.ChannelSelector.java</file>
    </fixedFiles>
  </bug>
  <bug id="12370" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrated Travis for Python Table API</summary>
      <description>Integrated Travis for Python Table API</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12371" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for converting (NOT) IN/ (NOT) EXISTS to SemiJoin, and generating optimized logical plan</summary>
      <description>This issue aims to convert IN/EXISTS to semi-join, and NOT IN/NOT EXISTS to anti-join.In Calcite, SemiJoin only represents semi-join, (could not represent anti-join) and requires equi join condition. Queries like `select * from left where left.a1 in (select right.a2 from right where left.b1 &gt; right.b2)` and `select * from left where not exists (select * from right)` could not be converted to Calcite SemiJoin operator.To solve the above problem, We need copy the SemiJoin class to Flink, and make the following changes:1. make SemiJoin extending from Join, not from EquiJoin. (to support non-equi join condition) 2. add isAnti field attribute to represent anti-join.Currently, there are no rules to convert (NOT) IN/ (NOT) EXISTS to SemiJoin, so we need a whole new rule (named FlinkSubQueryRemoveRule) to meet our requirement.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecWindowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdSelectivity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPopulationSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPercentageOriginalRows.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.util.JoinTypeUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12372" opendate="2019-4-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionSlotAllocator</summary>
      <description>Add and implement ExecutionSlotAllocator interfaceDesign document: https://docs.google.com/document/d/1fstkML72YBO1tGD_dmG2rwvd9bklhRVauh4FSsDDwXUAcceptance criteria ExecutionSlotAllocator interface is defined and implemented interface implementation is unit tested</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="12413" opendate="2019-5-6 00:00:00" fixdate="2019-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionFailureHandler</summary>
      <description>Implement ExecutionFailureHandler and related classes.Acceptance criteria Implementation or definitions exist for: ExecutionFailureHandler FailureHandlingResult RestartBackoffTimeStrategy (interface definition is enough) ExecutionFailureHandler is unit tested in isolation FailureHandlingResult is unit tested in isolation</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ThrowableClassifierTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.throwable.ThrowableClassifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailoverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="12414" opendate="2019-5-6 00:00:00" fixdate="2019-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionGraph to SchedulingTopology Adapter</summary>
      <description>Implement an adapter, which adapts the ExecutionGraph to the SchedulingTopology interface.Acceptance criteria The adapter always reflects an up to date view of the ExecutionGraph state</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="12431" opendate="2019-5-7 00:00:00" fixdate="2019-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port utility methods for extracting fields information from TypeInformation</summary>
      <description>We need those methods in the api-module in order to create Table out of DataSet/Stream.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12432" opendate="2019-5-7 00:00:00" fixdate="2019-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SchedulerNG Stub Implementation</summary>
      <description>Add a stub implementation of the SchedulerNG interface that can be configured as a replacement for LegacyScheduler.Acceptance Criteria Stub implementation of SchedulerNG is added (DefaultSchedulerNG) DefaultSchedulerNG can be enabled via feature toggle in flink-conf.yaml If the feature toggle is not configured, LegacyScheduler should be used a a default.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12446" opendate="2019-5-8 00:00:00" fixdate="2019-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation how to enable openSSL</summary>
      <description>Since we won't include openSSL support by default, we should describe the (two) way of enabling this for Flink.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12447" opendate="2019-5-8 00:00:00" fixdate="2019-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump required Maven version to 3.1.1 (from 3.0.3)</summary>
      <description>See https://lists.apache.org/thread.html/57dec7c338eb95247b7a05ded371f4a78420a964045ea9557d501c3f@%3Cdev.flink.apache.org%3E The frontend-maven-plugin requires at least Maven 3.1.0.I propose to bump the required Maven version to 3.1.1.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="12458" opendate="2019-5-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PartitionConnectionException for unreachable producer</summary>
      <description>If the consumer can not establish a connection to remote task executor, which might indicate the remote task executor is not reachable. We could wrap this connection exception into new proposed `PartitionConnectionException` which also extends `PartitionException`, then the job master would decide whether to restart the upstream region to re-producer partition data.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.TestingConnectionManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
    </fixedFiles>
  </bug>
  <bug id="12469" opendate="2019-5-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up catalog API on default/current DB</summary>
      <description>Currently catalog API has get/setCurrentDatabase(), which is more user session specific. In our design principal, catalog instance is agnostic to user sessions. Thus, current database concept doesn't belong there. However, a catalog should support a (configurable) default database, which would be taken as user's current database when user's session doesn't specify a current DB.This JIRA is to remove current database concept from catalog api and add default database instead.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="1247" opendate="2014-11-17 00:00:00" fixdate="2014-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Links in documentation broken</summary>
      <description>There seem to be some links in the 0.7-incubating documentation broken. For example, the Aggregator link in the Accumulators &amp; Counters section does not work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.web.client.md</file>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.examples.md</file>
    </fixedFiles>
  </bug>
  <bug id="12472" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting attemptFailuresValidityInterval of jobs on Yarn</summary>
      <description>According to the documentation of Yarn, a yarn application can set a attemptFailuresValidityInterval  to reset application attempts. "attemptFailuresValidityInterval. The default value is -1. when attemptFailuresValidityInterval in milliseconds is set to &gt; 0, the failure number will no take failures which happen out of the validityInterval into failure count. If failure count reaches to maxAppAttempts, the application will be failed." We can make use of this feature to make Flink jobs on Yarn to be more long-running.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="12473" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the interface of ML pipeline and ML lib</summary>
      <description>This Jira will introduce the major interfaces for ML pipeline and ML lib.The major interfaces and their relationship diagram is shown as below. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo] </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12485" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a tool to check the user interface of Python Table API aligns with Java Table API.</summary>
      <description>After FLINK-12407 Python Table API will align with current Java Table API. So, In the following Table API development, it's better to add a new user interface to the Java Table API will also need to be added to the Python Table API. So In this Jira, we want dd a tool to check the user interface of Python Table API aligns with Java Table API. I'm not sure if we want to force contributors to contribute to the Python Table API while developing the Java Table API. But at least we should have the ability to automatically check if Python is aligned with Java functionality. What do you think?</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
    </fixedFiles>
  </bug>
  <bug id="12487" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules to rewrite expression and merge calc</summary>
      <description>This issue aims to introduce planner rules to rewrite expression and merge calc, rules include:1. ConvertToNotInOrInRule, that converts a cascade of predicates to IN or NOT_IN,e.g. converts predicate (x = 1 OR x = 2 OR x = 3 OR x = 4) AND y = 5 to predicate x IN (1, 2, 3, 4) AND y = 5 converts predicate (x &lt;&gt; 1 AND x &lt;&gt; 2 AND x &lt;&gt; 3 AND x &lt;&gt; 4) AND y = 5 to predicate x NOT IN (1, 2, 3, 4) AND y = 52. RewriteCoalesceRule, that rewrites Coalesce to Case When3. FlinkCalcMergeRule, that is copied from Calcite CalcMergeRule, and it will simplify the merged program</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12488" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scope of MemorySegment metrics has changed</summary>
      <description>The scope of TotalMemorySegments and AvailableMemorySegments has changed since 1.8.Previously it was Status.Network, whereas now it is just Network.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12489" opendate="2019-5-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink on Mesos - Parameterize network resources.</summary>
      <description>Mesos supports network resource parameters. It would be nice if Flink can specify network resources consumption.Unfortunately, network resource is not standarized in Mesos, so fenzo name: "network" should be customized.Thus we can introduce two parameters:1. Network Bandwidth in mb.2. Name of the network resource in mesos, where "network" will be the value by default.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.Offer.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
    </fixedFiles>
  </bug>
  <bug id="12492" opendate="2019-5-12 00:00:00" fixdate="2019-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor optimize the cep operator by avoiding unnecessary copy</summary>
      <description>When use with rocksdbbackend, we can skip the copy of input events. But to make this more constraint, i add a method in the KeyedStateBackend interface to make this still work id there is added a new backend..</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CepOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12507" opendate="2019-5-14 00:00:00" fixdate="2019-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix AsyncLookupJoin doesn&amp;#39;t close all generated ResultFutures</summary>
      <description>There is a fragile test in AsyncLookupJoinITCase, that not all the udfs are closed at the end.02:40:48.787 [ERROR] Tests run: 22, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 47.098 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase02:40:48.791 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=HEAP](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase) Time elapsed: 1.266 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;0&gt; but was:&lt;2&gt; at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)02:40:48.794 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=ROCKSDB](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase) Time elapsed: 1.033 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;0&gt; but was:&lt;2&gt; at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.lookup.AsyncLookupJoinRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12511" opendate="2019-5-14 00:00:00" fixdate="2019-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make variable "comment" in all catalog metadata classes final</summary>
      <description>Because of historical reasons, currently the variable "comment" in all catalog metadata classes are not final yet, it has a default value and can be overwritten in constructor. It creates problems like overloaded constructors are built in a wrong way.We should remove the default value of "comment" and make it final that can only be assigned value upon construction.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogDatabase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogView.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogTable.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogDatabase.java</file>
    </fixedFiles>
  </bug>
  <bug id="12512" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableSourceTest#testNestedProject test failed</summary>
      <description> 20:41:59.128 [ERROR] testNestedProject(org.apache.flink.table.api.stream.table.TableSourceTest) Time elapsed: 0.047 s &lt;&lt;&lt; FAILURE!org.junit.ComparisonFailure: null expected:&lt;...deepNested.nested2.f[lag AS nestedFlag, deepNested.nested2.num AS nestedNum])StreamTableSourceScan(table=[[T]], fields=[id, deepNested, nested], source=[TestSource(read nested fields: id.*, deepNested.nested2.num, deepNested.nested2.flag], deepNested.nested1...&gt; but was:&lt;...deepNested.nested2.f[1 AS nestedFlag, deepNested.nested2.f0 AS nestedNum])StreamTableSourceScan(table=[[T]], fields=[id, deepNested, nested], source=[TestSource(read nested fields: id.*, deepNested.nested2.f1, deepNested.nested2.f0], deepNested.nested1...&gt; at org.apache.flink.table.api.stream.table.TableSourceTest.testNestedProject(TableSourceTest.scala:375)log details : https://api.travis-ci.org/v3/job/532319575/log.txt </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="12516" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Travis base image from trusty to xenial</summary>
      <description>Currently, our Travis tests are running with a trusty environment which is officially not supported by Ubuntu anymore. It also brings old system libraries which blocks unit tests for FLINK-11579.I propose to update to xenial (Ubuntu 16.04 LTS) which is still supported until 2021-04.Since Travis doesn't support oraclejdk8 on xenial, however, this also implies a switch to openJDK 8.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12517" opendate="2019-5-15 00:00:00" fixdate="2019-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run network tests with dynamically-linked openSSL</summary>
      <description>FLINK-9816 adds the ability to work with Netty's wrapper around native openSSL implementations. We should set up unit tests that verify the artifacts we provide, i.e. the dynamically-linked openSSL one in flink-shaded.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12518" opendate="2019-5-15 00:00:00" fixdate="2019-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run e2e tests with openSSL</summary>
      <description>We should modify one end-to-end test each to run with: Java-based SSL dynamically linked openSSL statically linked openSSL</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ssl.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12519" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules about semi/anti join</summary>
      <description>This issue aims to introduce planner rules about semi/anti join, rules include:1. FlinkSemiAntiJoinFilterTransposeRule that pushes semi/anti join down in a tree past a filter2. FlinkSemiAntiJoinJoinTransposeRule that pushes semi/anti join down in a tree past a non semi/anti join3. FlinkSemiAntiJoinProjectTransposeRule that push semi/anti join down in a tree past a project4. ProjectSemiAntiJoinTransposeRule that pushes a project down in a tree past a semi/anti joinplanner rules about non semi/anti join will be introduced in FLINK-12509.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1252" opendate="2014-11-18 00:00:00" fixdate="2014-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a serializer for Date</summary>
      <description>Since FLINK-610 is still an unresolved issue, I've started working on adding support for Date fields in POJOs in Flink.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.EnumTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.EnumSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.EnumComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DateComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ReduceITCase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="12524" opendate="2019-5-16 00:00:00" fixdate="2019-5-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules about rank</summary>
      <description>This issue aims to introduce planner rules about rank, rules include:1. CalcRankTransposeRule, that transposes FlinkLogicalCalc past FlinkLogicalRank to reduce rank input fields.2. RankNumberColumnRemoveRule, that emoves the output column of rank number iff there is a equality condition for the rank column.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPopulationSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12534" opendate="2019-5-16 00:00:00" fixdate="2019-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the test cost for Python API</summary>
      <description>Currently, we add the Python API Travis test for Scala 2.12 / Java 9 / Hadoop 2.4.1. due to Python API using Py4j communicate with JVM, the test for Java 9 is enough, and we can remove the test for Scala 2.12 and  Hadoop 2.4.1.  </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12572" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement HiveInputFormat to read Hive tables</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveTableConfig.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTablePartition.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12590" opendate="2019-5-22 00:00:00" fixdate="2019-1-22 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Replace http links in documentation</summary>
      <description></description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.misc.CollectionExecutionExample.java</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
      <file type="M">flink-dist.src.main.flink-bin.README.txt</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.zh.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.zh.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug id="12592" opendate="2019-5-22 00:00:00" fixdate="2019-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamTableEnvironment object has no attribute connect</summary>
      <description>The Python build module failed on Travis with the following problem: 'StreamTableEnvironment' object has no attribute 'connect'.https://api.travis-ci.org/v3/job/535684431/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug id="12604" opendate="2019-5-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register TableSource/Sink as CatalogTable</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.PathResolutionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSourceSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.BatchTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.ExternalTableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.ExternalCatalogSchema.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CalciteCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.TableOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.TableOperationDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.TableOperationCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12614" opendate="2019-5-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor YARN tests to not do assertion in @After methods</summary>
      <description>The YARN are executing assertions in the @After methods, which leads to output that it unnecessarily obfuscated. For example, it is not said explicitly which test has failed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="12627" opendate="2019-5-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to configure and use catalogs in SQL CLI</summary>
      <description>Ticket of its corresponding Chinese version is FLINK-12894.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="12635" opendate="2019-5-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API stability test does not cover jar upload</summary>
      <description>The stability test for the REST API currently resides in flink-runtime, but the jar upload is handled via an extension in runtime-web. Since this extension cannot be loaded in flink-runtime it isn't covered by the test.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutine.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityCheckResult.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.Compatibility.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12636" opendate="2019-5-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API stability test does not fail on compatible modifications</summary>
      <description>The stability test does not fail properly if the API was modified in a compatible way. The test should still fail until the snapshot was regenerated.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">flink-runtime.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12639" opendate="2019-5-28 00:00:00" fixdate="2019-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>FLIP-42: Rework Documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.setup.instructions.zh.md</file>
      <file type="M">docs.tutorials.setup.instructions.md</file>
      <file type="M">docs.tutorials.local.setup.zh.md</file>
      <file type="M">docs.tutorials.local.setup.md</file>
      <file type="M">docs.tutorials.index.zh.md</file>
      <file type="M">docs.tutorials.index.md</file>
      <file type="M">docs.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.tutorials.flink.on.windows.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.tutorials.api.tutorials.zh.md</file>
      <file type="M">docs.tutorials.api.tutorials.md</file>
      <file type="M">docs.examples.index.zh.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="12641" opendate="2019-5-28 00:00:00" fixdate="2019-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release all partitions on job termination</summary>
      <description>Introduce general logic on the JobManager to release partitions that are no longer required.In the first version, partitions will be kept until the consumer has finished, similarly to the current behavior.Further optimizations, potentially involving failover-regions, can be tackled in a follow-up.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="12643" opendate="2019-5-28 00:00:00" fixdate="2019-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionGraph to FailoverTopology Adapter</summary>
      <description>Implement an adapter, which adapts the ExecutionGraph and its sub-components(ExecutionVertex, ExecutionEdge) to the FailoverTopology interfaces.Core components: 1. DefaultFailoverTopology (reflects ExecutionGraph)2. DefaultFailoverVertex (reflects ExecutionVertex)3. DefaultFailoverEdge (reflects ExecutionEdge) </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12644" opendate="2019-5-28 00:00:00" fixdate="2019-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup Java 9 cron jobs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12649" opendate="2019-5-28 00:00:00" fixdate="2019-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a shim layer to support multiple versions of HMS</summary>
      <description>We need a shim layer of HMS client to talk to different versions of HMS</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12658" opendate="2019-5-28 00:00:00" fixdate="2019-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Flink with Hive GenericUDF</summary>
      <description>https://hive.apache.org/javadocs/r3.1.1/api/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="12659" opendate="2019-5-28 00:00:00" fixdate="2019-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Flink with Hive GenericUDTF</summary>
      <description>https://hive.apache.org/javadocs/r3.1.1/api/org/apache/hadoop/hive/ql/udf/generic/GenericUDTF.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveScalarFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12664" opendate="2019-5-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement TableSink to write Hive tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12678" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractCatalog to manage the common catalog name and default database name for catalogs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="12679" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;default-database&amp;#39; config for catalog entries in SQL CLI yaml file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12683" opendate="2019-5-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Provide task manager&amp;#39;s location information for checkpoint coordinator specific log messages</summary>
      <description>Currently, the AcknowledgeCheckpoint does not contain the task manager's location information. When a task's snapshot task sends an ack message to the coordinator, we can only log this message:Received late message for now expired checkpoint attempt 6035 from ccd88d08bf82245f3466c9480fb5687a of job 775ef8ff0159b071da7804925bbd362f.Sometimes we need to get this sub task's location information to do the further debug work, e.g. stack trace dump. But, without the location information, It will not help to quickly locate the problem. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12685" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports UNNEST query in blink planner</summary>
      <description>this issue aim to support queries with UNNEST keyword, which relate to nested fields.for example: table name: MyTableschema: a: int, b int, c array&amp;#91;int&amp;#93;sql:SELECT a, b, s FROM MyTable, UNNEST(MyTable.c) AS A (s)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkDecorrelateProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12689" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist is missing flink-azure-fs-hadoop dependency</summary>
      <description>Build fails when building with:mvn clean install -pl flink-dist -am -DskipTests -Dfast [INFO] flink-scala-shell .................................. SUCCESS [ 10.989 s][INFO] flink-dist ......................................... FAILURE [ 26.068 s][INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 13:24 min[INFO] Finished at: 2019-05-31T09:55:22+02:00[INFO] Final Memory: 313M/1834M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (opt) on project flink-dist_2.11: Failed to create assembly: Error adding file to archive: /Users/gyao/Documents/work/code/github/flink/flink-dist/../flink-filesystems/flink-azure-fs-hadoop/target/flink-azure-fs-hadoop-1.9-SNAPSHOT.jar -&gt; [Help 1]Azure FS dependency should be added to flink-dist with provided scope.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12705" opendate="2019-6-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow user to specify the Hive version in use</summary>
      <description>Follow up of FLINK-12649</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogValidator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12709" opendate="2019-6-3 00:00:00" fixdate="2019-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement RestartBackoffTimeStrategyFactoryLoader</summary>
      <description>We need to implement a RestartBackoffTimeStrategyFactoryLoader to instantiate RestartBackoffTimeStrategyFactory.In order to be backwards compatible, the loader is responsible for converting RestartStrategy configurations(https://ci.apache.org/projects/flink/flink-docs-stable/dev/restart_strategies.html）and RestartStrategyConfiguration to latest RestartBackoffTimeStrategy configurations.The converted configurations will be used to create RestartBackoffTimeStrategy.Factory via RestartBackoffTimeStrategy#createFactory(Configuration). </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.NoOrFixedIfCheckpointingEnabledRestartStrategyFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FixedDelayRestartBackoffTimeStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="12710" opendate="2019-6-3 00:00:00" fixdate="2019-4-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Unify built-in and user-defined functions in the API modules</summary>
      <description>Currently, there are three completely different stacks of functions: Table API builtins, SQL builtins, and user-defined types.Both the Blink and the legacy planner define a separate list of functions and implementations with different type system and type checking logic.The long-term goal of this issue is to unify all 6 different stacks into a common one. This includes better support for type inference which relates to FLINK-12251.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.ColumnFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ColumnOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.VerifyNoUnresolvedExpressionsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.expressions.ExpressionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TypeLiteralExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ExpressionVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.ExpressionDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.CallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.KeywordParseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.expressions.rules.VerifyNoUnresolvedExpressionsRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.composite.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.SortOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AliasOperationUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ReferenceResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.OverWindowResolverRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.FlattenCallRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ExpandColumnFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.OperationExpressionsUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LookupCallResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.OverWindowPartitionedOrdered.java</file>
    </fixedFiles>
  </bug>
  <bug id="12713" opendate="2019-6-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deprecate descriptor, validator, and factory of ExternalCatalog</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ExternalCatalogFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.ExternalCatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.ExternalCatalogDescriptor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12736" opendate="2019-6-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager may release TM with allocated slots</summary>
      <description>The ResourceManager looks out for TaskManagers that have not had any slots allocated on them for a while, as these could be released to safe resources. If such a TM is found the RM checks via an RPC call whether the TM still holds any partitions. If no partition is held then the TM is released.However, in the RPC callback no check is made whether the TM is actually still idle. In the meantime a slot could've been allocated on the TM.</description>
      <version>1.7.2,1.8.1,1.9.0</version>
      <fixedVersion>1.7.3,1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12742" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add insert into partition grammar as hive dialect</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sqlexec.SqlExecutableStatement.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.SqlProperty.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlRowType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlMapType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlArrayType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.ExtendedSqlType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12743" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce unbounded streaming anti/semi join operator</summary>
      <description>This operator is responsible for unbounded streaming semi/anti join, and will be optimized in following cases:1. If the join keys (with equality condition) are also primary key, we will have a more efficient state layout2. If the inputs have primary keys, but join keys are not primary key, we can also come up with an efficient state layout3. Inputs don't have primary keys, this will go to default implementation</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.NullAwareJoinHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12745" opendate="2019-6-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sparse and dense vector class, and dense matrix class with basic operations.</summary>
      <description>There are basic vector and matrix library for the machine-learning algorithms.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12746" opendate="2019-6-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Getting Started - DataStream Example Walkthrough</summary>
      <description>The planned structure for the new Getting Started Guide is Flink Overview (~ two pages) Project Setup Java Scala Python Quickstarts Example Walkthrough - Table API / SQL Example Walkthrough - DataStream API Docker Playgrounds Flink Cluster Playground Flink Interactive SQL Playground In this ticket we should add "Project Setup" and "Quickstarts -&gt; Example Walkthrough - DataStream API", which covers everything what we have today. This will replace the current "Tutorials" and "Examples" section, which can be removed as part of this ticket as well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.tutorials.datastream.api.md</file>
      <file type="M">docs.redirects.example.quickstart.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.getting-started.walkthroughs.index.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.index.md</file>
      <file type="M">docs.getting-started.tutorials.index.zh.md</file>
      <file type="M">docs.getting-started.tutorials.index.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.getting-started.index.md</file>
      <file type="M">docs.getting-started.examples.index.zh.md</file>
      <file type="M">docs.getting-started.examples.index.md</file>
      <file type="M">docs.getting-started.docker-playgrounds.index.zh.md</file>
      <file type="M">docs.getting-started.docker-playgrounds.index.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.md</file>
      <file type="M">flink-walkthroughs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.walkthroughs.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="12747" opendate="2019-6-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting Started - Table API Example Walkthrough</summary>
      <description>The planned structure for the new Getting Started Guide is Flink Overview (~ two pages) Project Setup Java Scala Python Quickstarts Example Walkthrough - Table API / SQL Example Walkthrough - DataStream API Docker Playgrounds Flink Cluster Playground Flink Interactive SQL Playground This tickets adds the Example Walkthrough for the Table API, which should follow the same structure as the DataStream Example (FLINK-12746), which needs to be completed first.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.getting-started.tutorials.index.zh.md</file>
      <file type="M">docs.getting-started.tutorials.index.md</file>
      <file type="M">docs.getting-started.examples.index.zh.md</file>
      <file type="M">docs.getting-started.examples.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="12756" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate HiveCatalog from TypeInformation-based old type system to DataType-based new type system</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12758" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink-ml-lib module</summary>
      <description>The Jira introduces a new module "flink-ml-lib" under flink-ml-parent.The flink-ml-lib is planned in the roadmap in FLIP-39, as the code base of library implementations of FlinkML. This Jira only aims to create the module, and algorithms will be added in separate Jira in the future. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo]</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12763" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail job immediately if tasks’ resource needs can not be satisfied.</summary>
      <description></description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.StandaloneResourceManagerWithUUIDFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionJobVertexBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerFailUnfulfillableTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12765" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bookkeeping of available resources of allocated slots in SlotPool.</summary>
      <description>In this version, a task will always requests slot with its own resource need. If the resource need is less than the default slot resource, it will always be allocated to a default sized slot.  The extra resources in the slot leaves chances for other tasks within the same slot sharing group to fit in. To take these chance, SlotPool will maintain available resources of each allocated slot. Available resource of an allocated slot should always be the total resource of the slot minus resources of tasks already assigned onto the slot. In this way, the SlotPool would be able to determine whether another task can fit into the slot. If a task cannot fit into the slot, for slot sharing group the SlotPool should request another slot from the ResourceManager, and for colocation group it should fail the job.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotSelectionStrategyTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.LocationPreferenceSlotSelectionStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PreviousAllocationSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.LocationPreferenceSlotSelectionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolCoLocationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12766" opendate="2019-6-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Dynamically allocate TaskExecutor’s managed memory to slots.</summary>
      <description>This step is a temporal workaround for release 1.9 to meet the basic usability requirements of batch functions from Blink.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.ResourceSpecTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.resources.Resource.java</file>
    </fixedFiles>
  </bug>
  <bug id="12767" opendate="2019-6-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user defined connectors/format</summary>
      <description>Currently, only built-in connectors such as FileSystem/Kafka/ES are supported and only built-in formats such as OldCSV/JSON/Avro/CSV/ are supported. We should also provide a convenient way for the connectors/formats that are not built-in supported.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12772" opendate="2019-6-6 00:00:00" fixdate="2019-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support mapping ARRAY, MAP, ROW (STRUCT) between Flink and Hive in HiveCatalog</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12774" opendate="2019-6-7 00:00:00" fixdate="2019-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pin version of build-helper-maven plugin</summary>
      <description>We currently use two different version of the build-helper-maven-plugin, and for a short time recently had a third. I propose to pin the version in the root plugin management.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12775" opendate="2019-6-7 00:00:00" fixdate="2019-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 7.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="12778" opendate="2019-6-7 00:00:00" fixdate="2019-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix deriveTableAggRowType bug for non-composite types</summary>
      <description>Currently, we call aggCalls.get(0).`type`.getFieldList.foreach(builder.add) when derive row type for table aggregate. However, for types which are not composite types, the field list would be null. Table Aggregate should, of course, support non-composite types.To solve the problem, we should judge whether types are structured. This is because a composite type will be converted to a RelDataType which contains field list and is structured.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12795" opendate="2019-6-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Extracted creation &amp; configuration of FrameworkConfig &amp; RelBuilder to separate class in blink planner</summary>
      <description>just as commit (e682395a) in flink planner, do similar things in blink planner</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12803" opendate="2019-6-11 00:00:00" fixdate="2019-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the package name for python API</summary>
      <description>Currently the package name of flink APIs should cantians the language name, such as: flink-java -&gt; org.apache.flink.api.java flink-scala -&gt; org.apache.flink.api.scalaSo I think we should follow the pattern of API package name and correct the current python API package name for `flink-python`. But for long-term goal, the flink API package name should be: org.apache.flink.api.common. org.apache.flink.api.common.python. org.apache.flink.api.datastream. org.apache.flink.api.datastream.scala. org.apache.flink.api.datastream.python. org.apache.flink.api.table. org.apache.flink.api.table.scala. org.apache.flink.api.table.python. So, in this JIRA, we should correct the package name from `org.apache.flink.python` -------&gt; `org.apache.flink.api.table.python`What do you think?</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.python.PythonTableUtils.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonDriverTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.client.PythonDriverOptionsParserFactoryTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonGatewayServer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonDriverOptionsParserFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonDriverOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.client.PythonDriver.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.bridge.PythonBridgeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.bridge.pickle.ByteArrayConstructor.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.bridge.pickle.ArrayConstructor.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="12804" opendate="2019-6-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce mailbox-based ExecutorService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.TestMailboxExecutor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorServiceImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.SuspendedMailboxDefaultAction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorServiceImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutor.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskSelectiveReadingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxSender.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxReceiver.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.Mailbox.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12806" opendate="2019-6-11 00:00:00" fixdate="2019-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove beta feature remark from the Universal Kafka connector</summary>
      <description>I think we can remove this remark from the docs as in the last half year there were no issues reported that would say otherwise.The remark about universal connector being a beta feature was introduced in: https://issues.apache.org/jira/browse/FLINK-10900</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="12812" opendate="2019-6-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set resource profiles for task slots</summary>
      <description></description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12824" opendate="2019-6-13 00:00:00" fixdate="2019-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>set parallelism for stream SQL</summary>
      <description>Parallelism setting has been developped for batch SQL,  the calculation method can also apply to stream SQL.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.SortOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.BatchFinalParallelismSetter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.BatchParallelismProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.BatchShuffleStageParallelismCalculator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.NodeResourceConfig.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.ShuffleStage.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.ShuffleStageGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.NodeResource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.BatchFinalParallelismSetterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.BatchShuffleStageParallelismCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.plan.nodes.resource.batch.parallelism.ShuffleStageGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.plan.nodes.resource.MockNodeTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.nodes.resource.BatchExecResourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.nodes.resource.BatchExecResourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12828" opendate="2019-6-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support -f option with a sql script file as input</summary>
      <description>We expect user to run a script file directly on the command line. Something like: sql-client embedded -f myscript.sql, which will execute the given file without entering interactive mode</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TerminalStreamsResource.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlMultiLineParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="12834" opendate="2019-6-13 00:00:00" fixdate="2019-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CharType and BinaryType in blink runner</summary>
      <description>1.Now we use LogicalType VarcharType to support calcite char type.2.Subsequent TableApi also generates LogicalType's CharType.We need real support CharType in internal code gen and computation.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.ClassLogicalTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.AbstractHeapVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.TypeGetterSetters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryWriter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.PrintCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.BinaryStringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankLikeAggFunctionBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="12844" opendate="2019-6-14 00:00:00" fixdate="2019-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use default conversion class LocalDate/LocalTime/LocalDateTime for DateType/TimeType/TimestampType in blink</summary>
      <description>Now we still use java.sql.Timestamp to be default conversion class for TimestampType.We should use design of new Type System to use java.time.Local***.NOTE, This may affect user behaviour:eg: UDF with eval(Object o), now will pass a java.time.Local*** instead of java.sql.**.Compatibility method: use DataType.bridgedTo(java.sql.**.class);</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.window.grouping.HeapWindowsGroupingTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseRowTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.LogicalTypeDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.ClassLogicalTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Types.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.CurrentTimePointCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ArrayTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.RowTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.common.AggregateReduceGroupingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.WindowJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1285" opendate="2014-11-26 00:00:00" fixdate="2014-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make execution mode configurable</summary>
      <description>As discussed in a PR: https://github.com/apache/incubator-flink/pull/227#discussion_r20788430.The goal would be to make the execution mode configurable in order to easily configure closure cleaning, custom serializers, object reuse etc.Configuration could be done either via 1) setters or 2) a configuration object.I vote for 2).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CollectorMapDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.resettable.BlockResettableIterator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ReduceITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.DependencyConnectedComponentsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.CoGroupConnectedComponentsITCase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.JavaProgramTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.KeyGroupedIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.KeyGroupedIteratorImmutableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.SortMergeCoGroupIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MassiveStringValueSortingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MapTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.TestTaskContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CachedMatchTaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.MutableToRegularIteratorWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.KeyGroupedIteratorImmutable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.KeyGroupedIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.SortMergeCoGroupIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.MergeMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.FixedLengthRecordSorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.PactTaskContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MatchDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapPartitionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.InMemoryPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.BuildSecondReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.BuildSecondHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.BuildFirstReOpenableHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.BuildFirstHashMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.FlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CrossDriver.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.Plan.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MutableObjectIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.broadcast.BroadcastVariableMaterialization.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.io.HashPartitionIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.SynchronousChainedCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.CompactingHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.MergeIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NonReusingMergeMatchIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.ReaderIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.plugable.DeserializationDelegate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EmptyMutableObjectIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.KeyGroupedMutableObjectIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.NonReusingKeyGroupedIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.NonReusingMutableToRegularIteratorWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.RegularToMutableObjectIterator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.InstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MassiveStringSortingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MergeIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeMatchIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MutableObjectIteratorWrapper.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.RandomIntPairGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TestData.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.types.StringPair.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UniformIntPairGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UniformRecordGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UniformStringPairGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.NonReusingKeyGroupedIteratorTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.CollectionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideMatchDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllGroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12850" opendate="2019-6-14 00:00:00" fixdate="2019-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce TypeInfo for LocalDate/LocalTime/LocalDateTime</summary>
      <description>Now in the new type system of table, the default class of time type is LocalDate and so on.There are some situations that need to be converted to TypeInformation, such as toDataStream, so we need to provide TypeInformation support such as LocalDate.Introduce LocalTimeTypeInfoIntroduce LocalDateSerializer, LocalTimeSerializer, LocalDateTimeSerializerIntroduce LocalDateComparator, LocalTimeComparator, LocalDateTimeComparator</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.Types.java</file>
    </fixedFiles>
  </bug>
  <bug id="12855" opendate="2019-6-15 00:00:00" fixdate="2019-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stagger TumblingProcessingTimeWindow processing to distribute workload</summary>
      <description>Flink natively triggers all panes belonging to same window at the same time. In other words, all panes are aligned and their triggers all fire simultaneously, causing the thundering herd effect.This new feature provides the option that panes could be staggered across partitioned streams, so that their workloads are distributed.Attachment: proof of concept working</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.TumblingProcessingTimeWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows.java</file>
    </fixedFiles>
  </bug>
  <bug id="12856" opendate="2019-6-15 00:00:00" fixdate="2019-6-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rule to push projection into TableSource</summary>
      <description>This issue aims to support push projection into ProjectableTableSource or NestedFieldsProjectableTableSource to reduce output fields of a TableSource</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12858" opendate="2019-6-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential distributed deadlock in case of synchronous savepoint failure</summary>
      <description>Current implementation of stop-with-savepoint (FLINK-11458) would lock the thread (on syncSavepointLatch) that carries StreamTask.performCheckpoint(). For non-source tasks, this thread is implied to be the task's main thread (stop-with-savepoint deliberately stops any activity in the task's main thread).Unlocking happens either when the task is cancelled or when the corresponding checkpoint is acknowledged.It's possible, that other downstream tasks of the same Flink job "soft" fail the checkpoint/savepoint due to various reasons (for example, due to max buffered bytes BarrierBuffer.checkSizeLimit(). In such case, the checkpoint abortion would be notified to JM . But it looks like, the checkpoint coordinator would handle such abortion as usual and assume that the Flink job continues running.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
    </fixedFiles>
  </bug>
  <bug id="12864" opendate="2019-6-16 00:00:00" fixdate="2019-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improves the performance of Python Table API test cases</summary>
      <description>Most of the Python Table API test cases can be unit test instead of integration test. This can shorten the test time of Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.completeness.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.set.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.schema.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.distinct.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug id="12867" opendate="2019-6-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add insert overwrite grammar as HIVE dialect</summary>
      <description>Support grammar like: insert overwrite tbl1 partition(a=1) select a from tbl2;This overwrite can use whole table or single partition as effective scope. </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="1287" opendate="2014-11-26 00:00:00" fixdate="2014-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve File Input Split assignment</summary>
      <description>While running some DFS read-intensive benchmarks, I found that the assignment of input splits is not optimal. In particular in cases where the numWorker != numDataNodes and when the replication factor is low (in my case it was 1).In the particular example, the input had 40960 splits, of which 4694 were read remotely. Spark did only 2056 remote reads for the same dataset.With the replication factor increased to 2, Flink did only 290 remote reads. So usually, users shouldn't be affected by this issue.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.io.LocatableSplitAssignerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.LocatableInputSplitAssigner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12876" opendate="2019-6-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an adapter of region failover NG for legacy scheduler</summary>
      <description>We need an adapter to adapt flip1.RestartPipelinedRegionStrategy for legacy scheduler, so that the legacy scheduler can support fine grained recovery.The failover recovery should respect scheduleMode(EAGER/LAZY_FROM_SOURCES) to avoid more tasks get re-scheduled than that before failover.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.PipelinedFailoverRegionBuildingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategyBuildingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.FailoverStrategyLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="12877" opendate="2019-6-18 00:00:00" fixdate="2019-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify catalog database implementations</summary>
      <description>per discussion in https://issues.apache.org/jira/browse/FLINK-12841</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogDatabase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericCatalogDatabase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogDatabase.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogDatabase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12883" opendate="2019-6-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add elaborate partition release logic</summary>
      <description>Partitions should be released when all it's consuming pipelined regions are done.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="12888" opendate="2019-6-18 00:00:00" fixdate="2019-6-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rule to push filter into TableSource</summary>
      <description>This issue aims to support push filter into FilterableTableSource to reduce output records of a TableSource</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.MetadataTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkContextImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.PlannerContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="12889" opendate="2019-6-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job keeps in FAILING state</summary>
      <description>There is a topology of 3 operator, such as, source, parser, and persist. Occasionally, 5 subtasks of the source encounters exception and turns to failed, at the same time, one subtask of the parser runs into exception and turns to failed too. The jobmaster gets a message of the parser's failed. The jobmaster then try to cancel all the subtask, most of the subtasks of the three operator turns to canceled except the 5 subtasks of the source, because the state of the 5 ones is already FAILED before jobmaster try to cancel it. Then the jobmaster can not reach a final state but keeps in  Failing state meanwhile the subtask of the source kees in canceling state.  The job run on a flink 1.7 cluster on yarn, and there is only one tm with 10 slots. The attached files contains a jm log , tm log and the ui picture. The exception timestamp is about 2019-06-16 13:42:28.</description>
      <version>1.7.2,1.8.1,1.9.0</version>
      <fixedVersion>1.7.3,1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12902" opendate="2019-6-19 00:00:00" fixdate="2019-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup flink-yarn-tests dependencies</summary>
      <description>The dependency situation in this module is just insane; you have a few unshaded hadoop dependencies from `flink-yarn-tests`, some shaded hadoop dependencies from `flink-shaded-yarn-tests` and yet another set of shaded hadoop dependencies from `flink-shaded-hadoop`. All of these are available on the test classpath, resulting in what is effectively an incomprehensible classpath.This is highlighted in the shading patterns in `flink-shaded-yarn-tests`, which absolutely had to be in sync with `flink-shaded-hadoop` as otherwise you ended up with strange `VerifyErrors`.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.licenses.LICENSE.asm</file>
      <file type="M">flink-shaded-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12905" opendate="2019-6-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert CatalogView to org.apache.calcite.schema.Table so that planner can use unified catalog APIs</summary>
      <description>Similar to FLINK-12257 we should convert Flink's views to Calcite's views.The tricky part is that we have to pass around the SqlParser somehow.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="12916" opendate="2019-6-20 00:00:00" fixdate="2019-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KeyedComplexChainTest.testMigrationAndRestore failed on Travis</summary>
      <description>The test case KeyedComplexChainTest.testMigrationAndRestore failed on Travis because a Task received the cancellation from one of its inputsCaused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task received cancellation from one of its inputs at org.apache.flink.streaming.runtime.io.BarrierBuffer.notifyAbortOnCancellationBarrier(BarrierBuffer.java:428) at org.apache.flink.streaming.runtime.io.BarrierBuffer.processCancellationBarrier(BarrierBuffer.java:327) at org.apache.flink.streaming.runtime.io.BarrierBuffer.pollNext(BarrierBuffer.java:208) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:102) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:47) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:128) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.performDefaultAction(OneInputStreamTask.java:101) at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:268) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:376) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:676) ... 1 morehttps://api.travis-ci.org/v3/job/548181384/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="12917" opendate="2019-6-20 00:00:00" fixdate="2019-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support complex type of array, map, struct for Hive functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSimpleUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDTF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.DeferredObjectAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="12933" opendate="2019-6-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support "use catalog" and "use database" in SQL CLI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="12934" opendate="2019-6-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hadoop dependencies as &amp;#39;flink-shaded-hadoop-2-uber&amp;#39; for flink-connector-hive to connect to remote hive metastore service</summary>
      <description>Now, for all hadoop related dependencies, we just use the single jar of flink-shaded-hadoop-2-uber, instead of depending on individual hadoop jars. It's much more convenient for users to use now.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12937" opendate="2019-6-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce join reorder planner rules in blink planner</summary>
      <description>This issue aims to let blink planner support join reorder. LoptOptimizeJoinRule in Calcite could meet our requirement for now, so we could use directly this rule in blink planner. JoinToMultiJoinRule , ProjectMultiJoinMergeRule and FilterMultiJoinMergeRule should be also introduced to support LoptOptimizeJoinRule.additionally, we add a new rule named RewriteMultiJoinConditionRule which could apply transitive closure on `MultiJoin` for equi-join predicates to create more optimization possibilities.by default, join reorder is disabled, unless sql.optimizer.join-reorder.enabled is set as true.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.FlinkJoinPushExpressionsRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.FlinkAggregateInnerJoinTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="12951" opendate="2019-6-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add logic to bridge DDL to table source(sink)</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.DatabaseCalciteSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlExecutableStatements.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CatalogManagerCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CatalogCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="12954" opendate="2019-6-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports create(drop) view grammar for sql parser</summary>
      <description>Add watermark to create table; Also add create view grammar</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="12955" opendate="2019-6-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HBase LookupableTableSource and TableFactory</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSource.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseRowInputFormat.java</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12957" opendate="2019-6-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix thrift and protobuf dependency examples in documentation</summary>
      <description>The examples in the docs are not up-to-date anymore and should be updated.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.custom.serializers.md</file>
    </fixedFiles>
  </bug>
  <bug id="12959" opendate="2019-6-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use BoundedInput and InputSelectable in blink</summary>
      <description>Now BoundedInput and InputSelectable are ready in runtime. Blink planner should use it instead of invoking endInput in close.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.sort.StreamSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.StreamSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.SortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.SortLimitOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.HashJoinType.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12966" opendate="2019-6-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>finalize package name of Hive table source/sink</summary>
      <description>It's been brought by ykt836 and Jeff Zhang that the package name of `org.apache.flink.batch.connector` may not be proper.I think @zjuwangg named it this way because most connector packages are named as org.apache.flink.streaming.connectors.xxx and he is just following the convention. However, as we are forwarding to streaming-batch unification, we probably don't need "streaming/batch" in the package names any more, coz, like file source/sink, hive source/sink can (doesn't mean we necessarily will) be made as streaming in the future. I'm thinking of just org.apache.flink.connectors.hive. What do you think?cc ykt836 xuefuz lirui</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveRunnerShimV4.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveRunnerShimV3.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveRunnerShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.FlinkStandaloneHiveServerContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.FlinkStandaloneHiveRunner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTablePartition.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableInputSplit.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.FlinkHiveException.java</file>
    </fixedFiles>
  </bug>
  <bug id="12968" opendate="2019-6-25 00:00:00" fixdate="2019-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a casting utility</summary>
      <description>In order to make the new type system fully usable, we need to define logical casting rules that include both unsafe and safe casting, type widening etc. similar to the existing class: org.apache.flink.table.typeutils.TypeCoercion</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.YearMonthIntervalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DayTimeIntervalType.java</file>
    </fixedFiles>
  </bug>
  <bug id="1297" opendate="2014-12-2 00:00:00" fixdate="2014-9-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for tracking statistics of intermediate results</summary>
      <description>One of the major problems related to the optimizer at the moment is the lack of proper statistics.With the introduction of staged execution, it is possible to instrument the runtime code with a statistics facility that collects the required information for optimizing the next execution stage.I would therefore like to contribute code that can be used to gather basic statistics for the (intermediate) result of dataflows (e.g. min, max, count, count distinct) and make them available to the job manager.Before I start, I would like to hear some feedback form the other users.In particular, to handle skew (e.g. on grouping) it might be good to have some sort of detailed sketch about the key distribution of an intermediate result. I am not sure whether a simple histogram is the most effective way to go. Maybe somebody would propose another lightweight sketch that provides better accuracy.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12971" opendate="2019-6-25 00:00:00" fixdate="2019-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the constraint that lookup join needs a primary key or index key</summary>
      <description>Currently, we add a constraint in dimension table lookup join that the lookup fields must be a primary key or index key. This is not a logic constraint but a performance constraint. Because if there are no indexes on the lookup key, the lookup performance will be poor. We will remove this constraint because not every table have a primary key or indexes (e.g. Hive tables). It's the user's responsibility if the lookup fields are not keys and get a bad performance. In this case, users should add indexes on these fields. In the future, we can also propagate out these tuning information before SQL is executed.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.InMemoryLookupableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.LookupJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sources.TableIndex.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sources.DefinedPrimaryKey.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sources.DefinedIndexes.java</file>
    </fixedFiles>
  </bug>
  <bug id="12981" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore NaN values in histogram&amp;#39;s percentile implementation</summary>
      <description>Histogram metrics use "long" values and therefore, there is no Double.NaN in DescriptiveStatistics' data and there is no need to cleanse it while working with it.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="12983" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace descriptive histogram&amp;#39;s storage back-end</summary>
      <description>DescriptiveStatistics relies on their ResizableDoubleArray for storing double values for their histograms. However, this is constantly resizing an internal array and seems to have quite some overhead.Additionally, we're not using SynchronizedDescriptiveStatistics which, according to its docs, we should. Currently, we seem to be somewhat safe because ResizableDoubleArray has some synchronized parts but these are scheduled to go away with commons.math version 4.Internal tests with the current implementation, one based on a linear array of twice the histogram size (and moving values back to the start once the window reaches the end), and one using a circular array (wrapping around with flexible start position) has shown these numbers using the optimised code from FLINK-10236, FLINK-12981, and FLINK-12982: only adding values to the histogramBenchmark Mode Cnt Score Error UnitsHistogramBenchmarks.dropwizardHistogramAdd thrpt 30 47985.359 ± 25.847 ops/msHistogramBenchmarks.descriptiveHistogramAdd thrpt 30 70158.792 ± 276.858 ops/ms--- with FLINK-10236, FLINK-12981, and FLINK-12982 ---HistogramBenchmarks.descriptiveHistogramAdd thrpt 30 75303.040 ± 475.355 ops/msHistogramBenchmarks.descrHistogramCircularAdd thrpt 30 200906.902 ± 384.483 ops/msHistogramBenchmarks.descrHistogramLinearAdd thrpt 30 189788.728 ± 233.283 ops/ms after adding each value, also retrieving a common set of metrics:Benchmark Mode Cnt Score Error UnitsHistogramBenchmarks.dropwizardHistogram thrpt 30 400.274 ± 4.930 ops/msHistogramBenchmarks.descriptiveHistogram thrpt 30 124.533 ± 1.060 ops/ms--- with FLINK-10236, FLINK-12981, and FLINK-12982 ---HistogramBenchmarks.descriptiveHistogram thrpt 30 251.895 ± 1.809 ops/msHistogramBenchmarks.descrHistogramCircular thrpt 30 301.068 ± 2.077 ops/msHistogramBenchmarks.descrHistogramLinear thrpt 30 234.050 ± 5.485 ops/ms</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="12984" opendate="2019-6-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only call Histogram#getStatistics() once per set of retrieved statistics</summary>
      <description>In some occasions, Histogram#getStatistics() was called multiple times to retrieve different statistics. However, at least the Dropwizard implementation has some constant overhead per call and we should maybe rather interpret this method as returning a point-in-time snapshot of the histogram in order to get consistent values when querying them.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.AbstractPrometheusReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12990" opendate="2019-6-26 00:00:00" fixdate="2019-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type doesn&amp;#39;t consider the local TimeZone</summary>
      <description>Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="12995" opendate="2019-6-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hive-1.2.1 build to Travis</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="12996" opendate="2019-6-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add predefined type validators, strategies, and transformations</summary>
      <description> FLINK-12924 introduced new interfaces for performing type inference. We need a set of predefined type validators, strategies, and transformations implemented with the new interfaces in order to represent the old type checking logic. Those interfaces can be inspired by Calcite's org.apache.calcite.sql.type.ReturnTypes, org.apache.calcite.sql.type.OperandTypes, and org.apache.calcite.sql.type.SqlTypeTransforms.</description>
      <version>1.9.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.ArgumentCount.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.CallContextBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.MissingTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeValidatorsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.VaryingSequenceTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.SingleInputTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.PassingTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.LiteralTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.ExplicitTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.CompositeTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.validators.AnyTypeValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.ConstantArgumentCount.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInference.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeValidators.java</file>
    </fixedFiles>
  </bug>
  <bug id="12997" opendate="2019-6-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release partitions for reset vertices</summary>
      <description>If a vertex with is reset (e.g., due to a global failover) we also have to issue release calls to the TM to release said partition.This case is not handled by the partition release strategy as that one deals with releases that could be due to job progress (i.e., a partition no longer being required because we have downstream partitions that we could back on instead in case of failures).</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="12998" opendate="2019-6-26 00:00:00" fixdate="2019-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Plugins mechanism</summary>
      <description>Plugins mechanism must be documented before the release. We should write down: benefits how to use existing FileSystem plugins how to implement an own custom FileSystem plugin potential issues of relaying on Thread.currentThread().getContextClassLoader()  (currently it's set only for FileSystemFactory class loading and FileSystemFactory#create() method call - if a FileSystem is accessing getContextClassLoader during runtime (write/reading) it will not work properly as a plugin.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  <bug id="13001" opendate="2019-6-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ExecutionGraphBuilder for testing</summary>
      <description>Creating an ExecutionGraph for testing is quite a hassle since we have no builder for doing so easily.Quite a few classes contain utility methods accepting a subset of the required arguments, which are painful to extend.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13003" opendate="2019-6-26 00:00:00" fixdate="2019-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Temporal TableFunction Join in processing time and event time</summary>
      <description>This is a feature port from flink-planner to blink-planner to support temporal TableFunction join (or called versioned joins).</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.util.BaseRowUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13006" opendate="2019-6-27 00:00:00" fixdate="2019-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove GenericUDTFReplicateRows from GenericUDTFTest because Hive 1.2.1 doesn&amp;#39;t have it</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13012" opendate="2019-6-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle default partition name of Hive table</summary>
      <description>When a partition value is null or empty string, Hive assigns a default partition name. We need to be able to handle it.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="13017" opendate="2019-6-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken and irreproducible dockerized docs build</summary>
      <description>The build tools around docs/docker seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:bash: /etc/bash_completion.d/git-prompt.sh: No such file or directorybash: __git_ps1: command not found/usr/bin/env: 'ruby.ruby2.5': No such file or directorybash: __git_ps1: command not foundReason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get builds which are independent from the host system (making them reproducible) not have the commands in the container affect the host</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13029" opendate="2019-6-28 00:00:00" fixdate="2019-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove expressionBridge from QueryOperations factories</summary>
      <description>Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilderImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.OperationTreeBuilderFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="13037" opendate="2019-7-1 00:00:00" fixdate="2019-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Concepts -&gt; Glossary" page into Chinese</summary>
      <description>Translate Glossary page into Chinese: https://ci.apache.org/projects/flink/flink-docs-master/concepts/glossary.htmlThe markdown file is located in docs/concepts/glossary.md.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.glossary.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13040" opendate="2019-7-1 00:00:00" fixdate="2019-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve blink planner configurations</summary>
      <description>1. improve planner and runtime config of blink </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.sort.BufferedKVExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.sort.BinaryExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.watermarkassigner.WatermarkAssignerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.watermarkassigner.MiniBatchedWatermarkAssignerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.BufferedKVExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.sort.BinaryExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithMiniBatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.SortLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.SemiJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.ScalarQueryITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.OuterJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinWithoutKeyITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCaseHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinConditionTypeCoerceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.InnerJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.HashDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.DistinctAggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateJoinTransposeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.ModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.join.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.TwoStageAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.JoinDeriveNullFilterRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.nodes.resource.ExecNodeResourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.common.JoinReorderTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SortTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SortLimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SortMergeSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.agg.SortAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.agg.HashAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.nodes.resource.ExecNodeResourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.MiniBatchIntervalInferRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.IncrementalAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.NodeResourceConfig.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.util.OperatorType.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistribution.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.reuse.SubplanReuser.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.JoinDeriveNullFilterRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecJoinRuleBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13048" opendate="2019-7-1 00:00:00" fixdate="2019-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support decimal in Flink&amp;#39;s integration with Hive user defined functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="13049" opendate="2019-7-2 00:00:00" fixdate="2019-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port planner expressions to blink-planner from flink-planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.windowProperties.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerResolvedFieldReference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.SortWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13066" opendate="2019-7-2 00:00:00" fixdate="2019-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>append hive-site.xml to path of Hive conf dir</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13067" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links to contributing docs</summary>
      <description>As contributing links change on https://github.com/apache/flink-web, all links to contributing related docs have become broken. We need to fix these broken links.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.filesystems.md</file>
      <file type="M">docs.redirects.example.quickstart.md</file>
      <file type="M">docs.internals.components.zh.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.libs.gelly.index.zh.md</file>
      <file type="M">docs.dev.libs.gelly.index.md</file>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug id="13074" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PartitionableTableSink doesn&amp;#39;t work in flink&amp;blink planner</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CatalogSinkModifyOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13076" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Calcite dependency to 1.20.0 in blink planner</summary>
      <description>Bump Calcite dependency to 1.20.0 in flink-table-planner-blink module.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.subquery.SubQuerySemiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.subquery.SubqueryCorrelateVariablesValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.subquery.SubQueryAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ReplaceMinusWithAntiJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.ReplaceIntersectWithSemiJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkLogicalRelFactories.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.rules.logical.SubQueryDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.rules.logical.FlinkFilterJoinRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.rel.core.JoinRelType.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.rel.core.Join.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13082" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support MatchRecognize in blink planner</summary>
      <description>Support MATCH RECOGNIZE in blink planner. This will port the functionality of MATCH RECOGNIZE in flink-planner to blink-planner.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13087" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add group window Aggregate operator to Table API</summary>
      <description>Add Group Window Aggregate operator to Table API as described in Google doc.The usage:val res = tab .window(Tumble over 15.minute on 'rowtime as 'w) .groupBy('w, 'a) // leave out groupBy-clause to define global aggregates .agg(fun: AggregateFunction) // output has columns 'a, 'b, 'c .select('a, 'c, 'w.start)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.GroupWindowTableAggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.WindowGroupedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="13090" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test Hive connector with hive runner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13094" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an easy way to read timers using the State Processor API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimerSerializer.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.input.KeyedStateInputFormatTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.functions.KeyedStateReaderFunction.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TestInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="13095" opendate="2019-7-4 00:00:00" fixdate="2019-10-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Provide an easy way to read / bootstrap window state using the State Processor API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OneInputOperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.KeyedOperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWindowReaderITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.WindowReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.functions.WindowReaderFunction.java</file>
      <file type="M">docs.dev.libs.state.processor.api.zh.md</file>
      <file type="M">docs.dev.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="13107" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Copy TableApi IT and UT to Blink planner</summary>
      <description>The issue aims to copy the testcases in the following packages from flink-planner and original blink to Blink-planner:1. org.apache.flink.table.api.batch.table2. org.apache.flink.table.api.stream.table3. org.apache.flink.table.runtime.batch.table4. org.apache.flink.table.runtime.stream.table</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.util.FieldInfoUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TypedFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregationITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13121" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set batch/streaming properties to StreamGraph in blink table planner</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.ExecutionConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.catalog.PathResolutionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.nodes.resource.NodeResourceConfig.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.ExecutorBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13123" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation</summary>
      <description>Currently, the REST API and CLI around stopping and cancelling jobs are not aligned in terms of terminology and the differences between cancel and job are not as clear as they could be.I would like to make the following changes to the CLI: add deprecation warning for cancel -s command and redirect users to stop rename -s of stop command to -p for savepoint location. Emphasize that this is optional, as a savepoint is taken in any caseI would like to make the following changes to the REST API: Rename stop-with-savepoint to stop Rename "endOfEventTime" to "drain" in accordance with the CLI</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.TerminationModeQueryParameter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.StopOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="13125" opendate="2019-7-5 00:00:00" fixdate="2019-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note to PubSub connector documentation about beta status</summary>
      <description>As part of the review of FLINK-9311, we decided to add a note to the documentation page that the connector is considered beta by the community.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.pubsub.md</file>
    </fixedFiles>
  </bug>
  <bug id="13134" opendate="2019-7-7 00:00:00" fixdate="2019-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>override default hadoop version from 2.4.1 to 2.7.5 in flink-connector-hive</summary>
      <description>Hive 2.3.4 relies on Hadoop 2.7.2 or later version. The default hadoop version in Flink globally is 2.4.1 which misses some classes in 2.7.2 and thus doesn't meet the requirement.Found this bug when running tests locally after merging FLINK-12934. Not sure why Travis CI succeeded for FLINK-12934 though, maybe because the build machine has all versioned hadoops cached locally.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13136" opendate="2019-7-8 00:00:00" fixdate="2019-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation error about stop job with restful api</summary>
      <description>Currently, https://ci.apache.org/projects/flink/flink-docs-master/monitoring/rest_api.html#jobs-jobid-1 does not support "stop" mode. If users use stop mode will throw an exception:The "stop" command has been removed. Please use "stop-with-savepoint" instead.IMO, We should remove it. </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.TerminationModeQueryParameter.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="13137" opendate="2019-7-8 00:00:00" fixdate="2019-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy python docs</summary>
      <description> Batch (DataSet API)/Transformations and Batch (DataSet API)/Zipping Elements have legacy flink python related documentation</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.zip.elements.guide.zh.md</file>
      <file type="M">docs.dev.batch.zip.elements.guide.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.zh.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="13155" opendate="2019-7-8 00:00:00" fixdate="2019-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails on Travis</summary>
      <description>The SQL Client end-to-end test which executes test-scripts/test_sql_client.sh fails on Travis with non-empty out files.https://api.travis-ci.org/v3/job/554991859/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13165" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Complete requested slots in request order</summary>
      <description>When executing batch jobs with fewer slots than requested we should make sure that the slot requests are being completed in the order in which they were enqueued into the SlotPool. Otherwise we might risk that a consumer task gets deployed before a producer causing a resource deadlock situation.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMapTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="13166" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support batch slot requests</summary>
      <description>In order to support the execution of batch jobs with fewer slots than requested we need to introduce a special slot request notion which does not register an eager timeout. Moreover, this slot request should not react to failure signals from the ResourceManager and only time out if there is not available or allocated slot which can fulfill the requested ResourceProfile.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolInteractionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13170" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Planner should get table factory from catalog when creating sink for CatalogTable</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13173" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only run openSSL tests if desired</summary>
      <description>Rename flink.tests.force-openssl to flink.tests.with-openssl and only run openSSL-based unit tests if this is set. This way, we avoid systems where the bundled dynamic libraries do not work. Travis seems to run fine and will have this property set.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13175" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLINK-12951 breaks SQL CLI&amp;#39;s ExecutionContextTest</summary>
      <description>https://github.com/apache/flink/pull/8844 breaks SQL CLI's ExecutionContextTestErrors from it's CI in https://travis-ci.com/flink-ci/flink/jobs/21437096614:23:25.985 [ERROR] Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.518 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.client.gateway.local.ExecutionContextTest14:23:25.985 [ERROR] testDatabases(org.apache.flink.table.client.gateway.local.ExecutionContextTest) Time elapsed: 10.807 s &lt;&lt;&lt; ERROR!org.apache.flink.table.client.gateway.SqlExecutionException: Could not create environment instance. at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testDatabases(ExecutionContextTest.java:128)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalid view 'TestView1' with query:SELECT scalarUDF(IntegerField1) FROM default_catalog.default_database.TableNumber1Cause: SQL validation failed. From line 1, column 38 to line 1, column 82: Object 'TableNumber1' not found within 'default_catalog.default_database' at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testDatabases(ExecutionContextTest.java:128)14:23:25.985 [ERROR] testCatalogs(org.apache.flink.table.client.gateway.local.ExecutionContextTest) Time elapsed: 5.295 s &lt;&lt;&lt; ERROR!org.apache.flink.table.client.gateway.SqlExecutionException: Could not create environment instance. at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:87)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalid view 'TestView1' with query:SELECT scalarUDF(IntegerField1) FROM default_catalog.default_database.TableNumber1Cause: SQL validation failed. From line 1, column 38 to line 1, column 82: Object 'TableNumber1' not found within 'default_catalog.default_database' at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:87)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13185" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Bump Calcite dependency to 1.20.0 in sql parser &amp; flink planner</summary>
      <description>blink planner had upgraded calcite version to 1.20.0 (before version is 1.19.0), and blink planner will support DDL in FLINK-1.9 which depends on flink-sql-parser. so calcite version in flink-sql-parser should also be upgrade to 1.20.0.walterddr, FLINK-11935 will not be fixed in this issue, because supporting DDL in blink planner is blocked by this.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.RetractionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.BasicOperatorTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilderFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13195" opendate="2019-7-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add create table support for SqlClient</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="13197" opendate="2019-7-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hive view row type mismatch when expanding in planner</summary>
      <description>One goal of HiveCatalog and hive integration is to enable Flink-Hive interoperability, that is Flink should understand existing Hive meta-objects, and Hive meta-objects created thru Flink should be understood by Hive.Taking an example of a Hive view v1 in HiveCatalog and database hc.db. Unlike an equivalent Flink view whose full path in expanded query should be hc.db.v1, the Hive view's full path in the expanded query should be db.v1 such that Hive can understand it, no matter it's created by Hive or Flink.lirui can you help to ensure that Flink can also query Hive's view in both Flink planner and Blink planner?cc xuefuz</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.ViewExpansionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.SqlCatalogViewTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13198" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce TimeUtils with parsing method</summary>
      <description>We need a parse function to parse strings like "123ms", "321s", "12min" and such, so that users can write configurations like "cache.ttl = 1min" and the user input can be changed into a Java Duration.This pull request introduce TimeUtils with parseDuration method to meet the above needs.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.DescriptorProperties.java</file>
    </fixedFiles>
  </bug>
  <bug id="13208" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Notice file for upgrading calcite to 1.20</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="13210" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector test should dependent on blink planner instead of legacy planner</summary>
      <description>Blink planner has more support and more functions, and some ITCase will not be able to measure it without relying on Blink-planner in test.And now, the table env is unified, I think we can use unified table env to it cases.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13211" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add drop table support for flink planner</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13214" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector is missing jdk.tools exclusion for Java 9</summary>
      <description>[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.9-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path C:\Dev\Java\9/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13216" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AggregateITCase.testNestedGroupByAgg fails on Travis</summary>
      <description>The AggregateITCase.testNestedGroupByAgg fails on Travis withAggregateITCase.testNestedGroupByAgg:472 expected:&lt;List(1,1,1,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)&gt; but was:&lt;List(1,1,7,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)&gt;https://api.travis-ci.org/v3/job/557214216/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="1322" opendate="2014-12-12 00:00:00" fixdate="2014-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not respect WriteMode set by configuration</summary>
      <description>The Scala API does not have output methods which do not take a WriteMode parameter. As default value the NO_OVERWRITE is set. Consequently, a possible global WriteMode set in the configuration is always overwritten. The Java API behaves differently, if no WriteMode is provided. We should sync both APIs to guarantee consistent behaviour.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13220" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add create/drop table support for blink planner</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.DropTableOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13221" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AdaptedRestartPipelinedRegionStrategyNGFailoverTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13222" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for AdaptedRestartPipelinedRegionStrategyNG</summary>
      <description>It should be documented that if jobmanager.execution.failover-strategy is set to region, the new pipelined region failover strategy (AdaptedRestartPipelinedRegionStrategyNG) will be used. Acceptance Criteria config values region and full are documented to be decided: config values region-legacy and individual remain undocumented</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs.dev.restart.strategies.zh.md</file>
      <file type="M">docs.dev.restart.strategies.md</file>
    </fixedFiles>
  </bug>
  <bug id="13226" opendate="2019-7-11 00:00:00" fixdate="2019-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaProducerExactlyOnceITCase.testMultipleSinkOperators fails on Travis</summary>
      <description>The KafkaProducerExactlyOnceITCase.testMultipleSinkOperators fails on Travis with not producing output for 300 s.https://api.travis-ci.org/v3/job/557290235/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13227" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Asynchronous I/O for External Data Access" page into Chinese</summary>
      <description>The page url is https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/asyncio.htmlThe markdown file is located in flink/docs/dev/stream/operators/asyncio.md</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.asyncio.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13228" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopRecoverableWriterTest.testCommitAfterNormalClose fails on Travis</summary>
      <description>HadoopRecoverableWriterTest.testCommitAfterNormalClose failed on Travis withHadoopRecoverableWriterTest.testCommitAfterNormalClose Â» IO The stream is closedhttps://api.travis-ci.org/v3/job/557293706/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.AbstractRecoverableWriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13229" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExpressionReducer with udf bug in blink</summary>
      <description>When a udf is reduced by ExpressionReducer, there will be a code gen exception.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13237" opendate="2019-7-12 00:00:00" fixdate="2019-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add expression table api test to blink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.RexNodeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.typeutils.TypeInfoCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.comparison.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.RowTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.CompositeAccessValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.MathFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.LiteralTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13243" opendate="2019-7-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AbstractOperatorRestartTestBase fails on Windows</summary>
      <description>The regular expressions contain new-lines, resulting in no matches being found.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="13248" opendate="2019-7-12 00:00:00" fixdate="2019-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance mailbox executor with yield-to-downstream functionality</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorServiceImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxSender.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxReceiver.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.Mailbox.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorServiceImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="1325" opendate="2014-12-15 00:00:00" fixdate="2014-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a closure cleaner for Java</summary>
      <description>The Java API could really need a simple closure cleaner.All functions that are implemented as anonymous subclasses hold a reference to the enclosing class, unless they are implemented as part of a static method.That reference (called this$0) causes serialization to fail, as it draws non serializable classes into the function, even in cases where the function makes no access to the enclosing data.It is possible to manually set this reference to null, using reflection, or using a debugger. Then the serialization succeeds.I suggest to add a closure cleaner that uses an ASM visitor over the function's code to see if there is any access to the this$0 field. In case there is non, the field should be set to null.The problem can be reproduced with the simple program below:public class Test { public void runProgram() throws Exception { ExecutionEnvironment env = ExecutionEnvironment .getExecutionEnvironment(); env.generateSequence(1, 10) .map(new MapFunction&lt;Long, Long&gt;() { public Long map(Long value) { return value * 2; } }) .print(); env.execute(); } public static void main(String[] args) throws Exception { new Test().runProgram(); }}</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.GroupedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.CoWindowDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.CoBatchedDataStream.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13250" opendate="2019-7-12 00:00:00" fixdate="2019-4-12 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Blink Planner should assign concrete resources to all nodes</summary>
      <description>Currently some nodes retain a resource spec of "DEFAULT"/ "UNKNOWN" as the result of merging the pre-existing "DEFAULT" with the actual profile.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13255" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip hive connector test for JDK9 profile</summary>
      <description>The Hive binary we depend upon has issues when running with JDK9, e.g. it assumes application class loader is URL class loader, which is no longer true in JDK9.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13256" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Periodical checkpointing is stopped after failovers</summary>
      <description>In this case, we observed that the job initially is triggering periodical checkpoints as expected.But after 2 region failovers, no checkpoint is triggered any more, even after all the tasks are RUNNING again.A sample log is attached along with the related topology desc pic.This case may not be reproduced every time. </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.FailoverRegion.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13257" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink runner should avoid stream operator implementing BoundedOneInput</summary>
      <description>According to https://issues.apache.org/jira/browse/FLINK-11879 , BoundedOneInput should not coexist with checkpoint, so we can not use BoundedOneInput in streaming mode.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpandCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13262" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new Table &amp; SQL API type system</summary>
      <description>Add documentation for the new Table &amp; SQL API type system including information about supported data types per planner and current limitations.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TimeType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.MultisetType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LocalZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.FloatType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DayTimeIntervalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.ArrayType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.DataType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.DataTypes.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13263" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>supports explain DAG plan in flink-python</summary>
      <description>update existing `explain` to support explain DAG plan in flink-python</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.exceptions.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="13273" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow switching planners in SQL Client</summary>
      <description>Once FLINK-13267 is resolved, we can also enable switching planners in the SQL Client via a execution property. Even though this is kind of a new feature, it had to be postponed after the feature-freeze as the relocation of FLINK-13267 would have created many merge conflicts otherwise.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectBatchTableSink.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.ExecutorBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13274" opendate="2019-7-15 00:00:00" fixdate="2019-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor HiveTableSource Test using HiveRunner Test Frame</summary>
      <description>Since we have already imported hiveRunner as our hive test environment, it's time to refactor HiveTableSource using hiveRunner to keep consistent.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13284" opendate="2019-7-16 00:00:00" fixdate="2019-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct some builtin functions&amp;#39; return type inference in Blink planner</summary>
      <description>Several builtin functions in Blink planner such as DATE_FORMATdeclares ReturnTypes.cascade(ReturnTypes.explicit(SqlTypeName.VARCHAR), SqlTypeTransforms.TO_NULLABLE)which should be ReturnTypes.cascade(ReturnTypes.explicit(SqlTypeName.VARCHAR), SqlTypeTransforms.FORCE_NULLABLE)instead.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13289" opendate="2019-7-16 00:00:00" fixdate="2019-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink-planner should setKeyFields to upsert table sink</summary>
      <description>Hi , in flink-jdbc connector module, I change the Flink planner to Blink planner to test all test case,because we want to use Blank planner in our program. When I test the JDBCUpsertTableSinkITCase class , the method testUpsert throw the exception:java.lang.UnsupportedOperationException: JDBCUpsertTableSink can not support I saw the src code,in Flink planner , the StreamPlanner set the JDBCUpsertTableSink' keyFields,but in Blink planner , I didn't find anywhere to set JDBCUpsertTableSink' keyFields,so JDBCUpsertTableSink keyFields is null, when execute JDBCUpsertTableSink newFormat(),it thrown the exception.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13308" opendate="2019-7-17 00:00:00" fixdate="2019-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python releases 2 jars</summary>
      <description>flink-python uses a classifier to differentiate itseld from the old python API. turns out thsi doesn't work since it still tries to release a normal unshaded flink-python jar.We should drop the classifier, and either stick to flink-python or rename it as proposed in FLINK-12776.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-shell.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-container.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="13310" opendate="2019-7-17 00:00:00" fixdate="2019-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove shade-plugin configuration in hive-connector</summary>
      <description>The hive connector has a shade plugin configuration but isn't doing anything interesting. We may as well remove it.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13312" opendate="2019-7-17 00:00:00" fixdate="2019-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move tests for data type mappings between Flink and Hive into its own test class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13314" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner</summary>
      <description>Correct resultType of the following PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner:Minus/plus/Div/Mul/Ceil/Floor/Round </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="13315" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port wmstrategies to api-java-bridge</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13322" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix serializer snapshot recovery in BaseArray and BaseMap serializers.</summary>
      <description>In BaseArray and BaseMap serializers, their element (or key/value) serializers are not stored in the config snapshot. When restoring the BaseArray/BaseMap serializers from the snapshots, their element/key/value serializers might be incorrect. This situation will happen when user uses his custom kryo serializers as element/key/value serializers.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.typeutils.BaseMapSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.typeutils.BaseArraySerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryArrayTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BaseRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseMapSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.InternalSerializers.java</file>
    </fixedFiles>
  </bug>
  <bug id="13323" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for complex data formats</summary>
      <description>There are currently no tests guarding some complex data formats, for example nested row, generic array and generic map.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.util.SegmentsUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DecimalTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryRowTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13325" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test case for FLINK-13249</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="1333" opendate="2014-12-16 00:00:00" fixdate="2014-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getter/Setter recognition for POJO fields with generics is not working</summary>
      <description>Fields likeprivate List&lt;Contributors&gt; contributors;Are not recognized correctly, even if they have getters and setters.Workaround: make them public.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13339" opendate="2019-7-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an implementation of pipeline&amp;#39;s api</summary>
      <description>Add an implement PipelineStage, Estimator, Transformer, Model. Add MLSession to hold the execution environment and others session shared variable. Add AlgoOperator for the implementation of algorithms. Add BatchOperator and StreamOperator based on AlgoOperator Add TableSourceBatchOp and TableSourceStreamOp</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13341" opendate="2019-7-20 00:00:00" fixdate="2019-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connector Sinks should implement consumeDataStream instead of emitDataStream</summary>
      <description>Some streamTableSink#consumeDataStream(DataStream) don't be implemented as returning the sink transformation DataStreamSink when using blink planner.which will throw the following errors:Exception in thread "main" org.apache.flink.table.api.TableException: The StreamTableSink#consumeDataStream(DataStream) must be implemented and return the sink transformation DataStreamSink. However, org.apache.flink.streaming.connectors.kafka.Kafka010TableSink doesn't implement this method. at org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123) at org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50) at org.apache.flink.table.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:60) at org.apache.flink.table.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50) at org.apache.flink.table.planner.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:71) at org.apache.flink.table.planner.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:70) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.StreamPlanner.translateToPlan(StreamPlanner.scala:70) at org.apache.flink.table.planner.PlannerBase.translate(PlannerBase.scala:155) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:446) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:355) at org.apache.flink.table.examples.java.StreamSQLLookupJoinExample.main(StreamSQLLookupJoinExample.java:139)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCAppendTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchUpsertTableSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraAppendTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="13345" opendate="2019-7-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump jstack output for Flink JVMs after Jepsen Tests</summary>
      <description>Dump the output of jstack -l &lt;pid&gt; for all Flink JVMs after each Jepsen test. This is helpful for debugging deadlocks.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="13353" opendate="2019-7-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2 args constructor in REPLACE expression</summary>
      <description>Replace definition in stringExpression.scala has another constructor with 2 arguments.According to source code, the args' meaning are str, begin. And it call other constructor with 3 args adding the 3rd arg which is the length of str.But its expectTypes is (String, String, String), but actually is (String, int, int).So I think the 2 args defined constructor means search and replacement is "" default, not begin and length of str. </description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13354" opendate="2019-7-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to use blink planner</summary>
      <description>Add documentation for how to use different planner “Overview”: add pom dependency “Concepts &amp; Common API”: add description about how to use different planner in code</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="13355" opendate="2019-7-22 00:00:00" fixdate="2019-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Temporal Table Join in blink planner</summary>
      <description>Add documentation for Temporal Table Join in blink planner “Streaming Concepts / Temporal Tables”: introduce concepts of temporal table in blink planner and the difference and sameness to flink planner temporal table “Joins in Continuous Queries”: how to use temporal join in bink planner “SQL”: join with temporal table in SQL</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="13356" opendate="2019-7-22 00:00:00" fixdate="2019-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for TopN and Deduplication in blink planner</summary>
      <description>Add documentation for TopN in blink planner“SQL”: how to write TopN in SQL and some tips</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="13359" opendate="2019-7-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DDL introduction</summary>
      <description>Add documentation for DDL introduction “Concepts &amp; Common API”: Add a section to describe how to execute DDL on TableEnvironment. “SQL Client”: Add a section and example in SQL CLI page too?</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="13369" opendate="2019-7-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recursive closure cleaner ends up with stackOverflow in case of circular dependency</summary>
      <description></description>
      <version>1.8.1,1.9.0</version>
      <fixedVersion>1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.ClosureCleanerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13374" opendate="2019-7-23 00:00:00" fixdate="2019-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala compiler causes StackOverflowError</summary>
      <description>Here is a instance:https://api.travis-ci.org/v3/job/562043336/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13384" opendate="2019-7-23 00:00:00" fixdate="2019-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The back pressure monitoring does not work for StreamSources</summary>
      <description>I think it is caused by: FLINK-12483. The reason is that the BackPressureStatsTrackerImpl samples only the main thread. FLINK-12483 introduced a separate thread for executing the sources in the mailbox model. It is similar to other old bug that concerned only Kafka source: FLINK-3456</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImplITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskStackTraceSampleableTaskAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug id="13386" opendate="2019-7-23 00:00:00" fixdate="2019-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some frictions in the new default Web UI</summary>
      <description>While manually testing the new WebUI I found a few frictions. when using the UI the left panel hides unexpectedly at random moments mouse wheel does not work on the logs (taskmanager, jobmanager) pane the jobmanager configuration is not sorted different sorting of the operators (the old UI showed the sources first) the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics arrow does not touch the rectangles in Chrome (see attached screenshot)There are also some views missing in the new UI that I personally found useful in the old UI: can't see watermarks for all operators at once no numeric metrics (only graphs)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.pipe.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-duration.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
    </fixedFiles>
  </bug>
  <bug id="13388" opendate="2019-7-23 00:00:00" fixdate="2019-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update UI screenshots in the documentation to the new default Web Frontend</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-3.png</file>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-2.png</file>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-1.png</file>
      <file type="M">docs.page.img.quickstart-example.jobmanager-overview.png</file>
      <file type="M">docs.page.img.quickstart-example.jobmanager-job.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-history.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.subtasks.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.png</file>
      <file type="M">docs.fig.back.pressure.sampling.ok.png</file>
      <file type="M">docs.fig.back.pressure.sampling.in.progress.png</file>
      <file type="M">docs.fig.back.pressure.sampling.high.png</file>
    </fixedFiles>
  </bug>
  <bug id="13398" opendate="2019-7-24 00:00:00" fixdate="2019-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector doesn&amp;#39;t compile on Java 9</summary>
      <description>recent dependency exclusions re-introduced a jdk.tools dependency which cannot be resolved on Java 9.Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-connector-hive_2.11: Error creating shaded jar: Could not resolve following dependencies: [jdk.tools:jdk.tools:jar:1.7 (system)]: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.11:jar:1.10-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path /home/travis/openjdk9/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13409" opendate="2019-7-25 00:00:00" fixdate="2019-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supported java UDFs in python API</summary>
      <description>It's better to support java UDF in python API.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Table.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">docs.dev.table.udfs.zh.md</file>
      <file type="M">docs.dev.table.udfs.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="13423" opendate="2019-7-25 00:00:00" fixdate="2019-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to find function in hive 1</summary>
      <description>I hit the following error when I try to use count in sql on hive1btenv.sqlQuery("select count(1) from date_dim").toDataSet[Row].print()org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get function tpcds_text_2.COUNTat org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:127)at org.apache.flink.table.api.internal.TableEnvImpl.sqlQuery(TableEnvImpl.scala:427)... 30 elidedCaused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get function tpcds_text_2.COUNTat org.apache.flink.table.catalog.hive.HiveCatalog.getFunction(HiveCatalog.java:1033)at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:167)at org.apache.flink.table.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:74)at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73)at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1183)at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1198)at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1168)at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:925)at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:639)at org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:123)... 31 more</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
    </fixedFiles>
  </bug>
  <bug id="13429" opendate="2019-7-25 00:00:00" fixdate="2019-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails</summary>
      <description>The SQL Client test does not work on the current master and hangs when executing CEP SQL. We reproduced this on two machines.At commit 475c30cd4064a7bc2e32c963b6ca58e7623251c6 it was working.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.java</file>
    </fixedFiles>
  </bug>
  <bug id="13431" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NameNode HA configuration was not loaded when running HiveConnector on Yarn</summary>
      <description>Reading the Hive table (version 1.2.1) and write into the hdfs file launch the job from SQL Client  StackTrace: Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: idc-nn        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)        at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:668)        at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:604)        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2598)        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2632)        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2614)        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:97)        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)        at org.apache.flink.batch.connectors.hive.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:164)        at org.apache.flink.batch.connectors.hive.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:67)        at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:256)        ... 22 moreCaused by: java.net.UnknownHostException: idc-nn </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="13436" opendate="2019-7-26 00:00:00" fixdate="2019-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TPC-H queries as E2E tests</summary>
      <description>We should add the TPC-H queries as E2E tests in order to verify the blink planner.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13438" opendate="2019-7-26 00:00:00" fixdate="2019-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support date type in Hive</summary>
      <description>Similar to JDBC connectors, Hive connectors communicate with Flink framework using TableSchema, which contains DataType. As the time data read from and write to Hive connectors must be java.sql.* types and the default conversion class of our time data types are java.time.*, we have to fix Hive connector with DataTypes.DATE/TIME/TIMESTAMP support.But currently when reading tables from Hive, the table schema is created using Hive's schema, so the time types in the created schema will be sql time type not local time type. If user specifies a local time type in the table schema when creating a table in Hive, he will get a different schema when reading it out. This is undesired.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
    </fixedFiles>
  </bug>
  <bug id="13440" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test that fails job when sync savepoint is discarded.</summary>
      <description>This is a test for FLINK-12858</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13441" opendate="2019-7-26 00:00:00" fixdate="2019-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add batch sql E2E test which runs with fewer slots than parallelism</summary>
      <description>We should adapt the existing batch E2E test to use the newly introduced ScheduleMode#LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST and verify that the job runs on a cluster with fewer slots than the job's parallelism. In order to make this work, we need to set the shuffles to be blocking via ExecutionMode#BATCH. As a batch job we should use the DataSetAllroundTestProgram. Update: currently, the ScheduleMode#LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST option is set only by table planner(s) and cannot be set (configured) for general purpose (batch) job. As agreed offline, this ticket would add a new e2e test for batch sql job instead of modifying DataSetAllroundTestProgram. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13450" opendate="2019-7-28 00:00:00" fixdate="2019-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust tests to tolerate arithmetic differences between x86 and ARM</summary>
      <description>Certain arithmetic operations have different precision/rounding on ARM versus x86.Tests using floating point numbers should be changed to tolerate a certain minimal deviation.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.DataSetCost.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13453" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump shade plugin to 3.2.1</summary>
      <description>The shade plugin fails with an IllegalArgumentException when run on Java 11.</description>
      <version>None</version>
      <fixedVersion>shaded-9.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13454" opendate="2019-7-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp jaxb dependencies</summary>
      <description>The japicmp plugins fails with a ClassNotFoundExceptions with the currently defined jaxb dependencies when run on java 11.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13455" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move jdk.tools exclusions out of dependency management</summary>
      <description>Defining exclusions via dependencyManagement is a bit unreliable, since the shade-plugin ignores them during dependency resolution  if they are defined for transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13456" opendate="2019-7-29 00:00:00" fixdate="2019-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lombok to 1.16.22</summary>
      <description>Compiling the tests for flink-core fails with an ErrorDuringInitialization due to lombok,</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13464" opendate="2019-7-29 00:00:00" fixdate="2019-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump powermock to 2.0.2</summary>
      <description>We have to bump powermock to 2.0.2 to resolve this issue in the InitOutputPathTest:java.lang.IllegalStateException: Failed to transform class with name org.apache.flink.core.fs.InitOutputPathTest. Reason: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:119)    at org.powermock.core.classloader.MockClassLoader.loadMockClass(MockClassLoader.java:174)    at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:102)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)    at java.base/java.lang.Class.forName0(Native Method)    at java.base/java.lang.Class.forName(Class.java:398)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:154)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:47)    at org.powermock.tests.utils.impl.AbstractTestSuiteChunkerImpl.createTestDelegators(AbstractTestSuiteChunkerImpl.java:107)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.&lt;init&gt;(JUnit4TestSuiteChunkerImpl.java:69)    at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.&lt;init&gt;(AbstractCommonPowerMockRunner.java:36)    at org.powermock.modules.junit4.PowerMockRunner.&lt;init&gt;(PowerMockRunner.java:34)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:49)    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: javassist.CannotCompileException: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at javassist.expr.NewExpr.replace(NewExpr.java:214)    at org.powermock.core.transformers.javassist.support.PowerMockExpressionEditor.edit(PowerMockExpressionEditor.java:73)    at javassist.expr.ExprEditor.loopBody(ExprEditor.java:212)    at javassist.expr.ExprEditor.doit(ExprEditor.java:91)    at javassist.CtClassType.instrument(CtClassType.java:1431)    at org.powermock.core.transformers.javassist.InstrumentMockTransformer.transform(InstrumentMockTransformer.java:41)    at org.powermock.core.transformers.javassist.AbstractJavaAssistMockTransformer.transform(AbstractJavaAssistMockTransformer.java:40)    at org.powermock.core.transformers.support.DefaultMockTransformerChain.transform(DefaultMockTransformerChain.java:43)    at org.powermock.core.classloader.MockClassLoader.transformClass(MockClassLoader.java:184)    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:102)    ... 27 moreCaused by: compile error: the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at javassist.compiler.MemberCodeGen.getAccessibleConstructor(MemberCodeGen.java:709)    at javassist.compiler.MemberCodeGen.atMethodCallCore2(MemberCodeGen.java:610)    at javassist.compiler.MemberCodeGen.atMethodCallCore(MemberCodeGen.java:589)    at javassist.expr.NewExpr$ProceedForNew.doit(NewExpr.java:237)    at javassist.compiler.JvstCodeGen.atCallExpr(JvstCodeGen.java:235)    at javassist.compiler.ast.CallExpr.accept(CallExpr.java:46)    at javassist.compiler.CodeGen.atAssignCore(CodeGen.java:877)    at javassist.compiler.CodeGen.atVariableAssign(CodeGen.java:810)    at javassist.compiler.CodeGen.atAssignExpr(CodeGen.java:764)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:332)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:351)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.CodeGen.atIfStmnt(CodeGen.java:411)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:355)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.Javac.compileStmnt(Javac.java:569)    at javassist.expr.NewExpr.replace(NewExpr.java:208)    ... 36 more</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13465" opendate="2019-7-29 00:00:00" fixdate="2019-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump javassist to 3.24.0-GA</summary>
      <description>Bump javassist to resolve this is issue in InitOutputPathTest:java.lang.ClassFormatError: Nest-host class_info_index 47 has bad constant type in class file org/apache/flink/core/fs/InitOutputPathTest$SyncedFileSystem    at java.base/java.lang.ClassLoader.defineClass1(Native Method)    at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1016)    at org.powermock.core.classloader.MockClassLoader.defineClass(MockClassLoader.java:180)    at org.powermock.core.classloader.MockClassLoader.loadMockClass(MockClassLoader.java:176)    at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:102)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)    at java.base/java.lang.Class.getDeclaredMethods0(Native Method)    at java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3167)    at java.base/java.lang.Class.privateGetPublicMethods(Class.java:3192)    at java.base/java.lang.Class.getMethods(Class.java:1905)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.getTestMethods(PowerMockJUnit44RunnerDelegateImpl.java:109)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit44RunnerDelegateImpl.java:85)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit47RunnerDelegateImpl.java:42)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit49RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit49RunnerDelegateImpl.java:25)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:165)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:47)    at org.powermock.tests.utils.impl.AbstractTestSuiteChunkerImpl.createTestDelegators(AbstractTestSuiteChunkerImpl.java:107)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.&lt;init&gt;(JUnit4TestSuiteChunkerImpl.java:69)    at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.&lt;init&gt;(AbstractCommonPowerMockRunner.java:36)    at org.powermock.modules.junit4.PowerMockRunner.&lt;init&gt;(PowerMockRunner.java:34)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:49)    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13494" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner changes source parallelism which causes stream SQL e2e test fails</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.nodes.resource.ExecNodeResourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.resource.ExecNodeResourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testGetStatsFromCatalog.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.FinalParallelismSetterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.resource.MockNodeTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.NodeResource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.NodeResourceUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.FinalParallelismSetter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ParallelismProcessor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStage.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.resource.parallelism.ShuffleStageParallelismCalculator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13495" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink-planner should support decimal precision to table source</summary>
      <description>Now there is an exception when use DataTypes.DECIMAL(5, 2) to table source when use blink-planner.Some conversions between DataType and TypeInfo loose precision information.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.MemoryTableSourceSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13498" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce Kafka producer startup time by aborting transactions in parallel</summary>
      <description>When a Flink job with a Kafka producer starts up without previous state, it currently starts 5 * kafkaPoolSize number of Kafka producers (per sink instance) to abort potentially existing transactions from a first run without a completed snapshot.Apparently, this is quite slow and it is also done sequentially. Until there is a better way of aborting these transactions with Kafka, we could do this in parallel quite easily and at least make use of lingering CPU resources.</description>
      <version>1.8.1,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
    </fixedFiles>
  </bug>
  <bug id="13499" opendate="2019-7-30 00:00:00" fixdate="2019-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on MapR artifact repository</summary>
      <description>The MapR artifact repository causes some problems. It does not reliably offer a secure (https://) access.We should change the MapR FS connector to work based on reflection and avoid a hard dependency on any of the MapR vendor-specific artifacts. That should allow us to get rid of the dependency without regressing on the support for the file system.</description>
      <version>1.9.0</version>
      <fixedVersion>1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.travis.nightly.sh</file>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactory.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFileSystem.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1350" opendate="2015-1-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add blocking intermediate result partitions</summary>
      <description>The current state of runtime support for intermediate results (see https://github.com/apache/incubator-flink/pull/254 and FLINK-986) only supports pipelined intermediate results (with back pressure), which are consumed as they are being produced.The next variant we need to support are blocking intermediate results (without back pressure), which are fully produced before being consumed. This is for example desirable in situations, where we currently may run into deadlocks when running pipelined.I will start working on this on top of my pending pull request.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferPool.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.testjar.KMeansForTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.exampleJavaPrograms.WordCountITCase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.JavaProgramTestBase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.StreamRecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.StreamingAbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.AtomicDisposableReferenceCounterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.DiscardingRecycler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerProcessReapingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestTaskEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.MockSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.MockProducer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.MockNotificationListener.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.MockInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.MockConsumer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.LargeRecordsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.queue.PipelinedPartitionQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferPoolFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.IteratorWrappingMockSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.AbstractReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.SeekableFileChannelInputViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerPerformanceBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerAsyncTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannelsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.FileChannelStreamsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.FileChannelStreamsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.ChannelViewsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.NetworkEnvironmentConfiguration.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.TaskManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.AtomicDisposableReferenceCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.TempBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableHashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.HashPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.IntermediateResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.IntermediateDataSet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationHeadPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.io.SerializedUpdateBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.TaskEventDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.RemoteAddress.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.queue.PipelinedPartitionQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.queue.IntermediateResultPartitionQueueIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.queue.IntermediateResultPartitionQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.queue.IllegalQueueIteratorRequestException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.IntermediateResultPartitionProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.IntermediateResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.LocalConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.ConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferPoolOwner.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.AbstractJobVertex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphConstructionTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.OptimizerNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.Channel.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plan.PlanNode.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.PipelineBreakerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.AbstractID.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.Tuple.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.PartialPartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.PartitionConsumerDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.PartitionDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.PartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionEdge.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.ChannelReaderInputViewIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelOutputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AbstractFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousBlockReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousBlockWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousBlockWriterWithCallback.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelWriterWithCallback.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelReaderInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.ChannelWriterOutputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.FileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.HeaderlessChannelReaderInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IOManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.QueuingCallback.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.SynchronousFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.SeekableFileChannelInputView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.SpillingBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BufferWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13501" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes a few issues in documentation for Hive integration</summary>
      <description>Going thru existing Hive doc I found the following issues that should be addressed:1. Section "Hive Integration" should come after "SQL client" (at the same level).2. In Catalog section, there are headers named "Hive Catalog". Also, some information is duplicated with that in "Hive Integration"3. "Data Type Mapping" is Hive specific and should probably move to "Hive integration"</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13504" opendate="2019-7-31 00:00:00" fixdate="2019-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoSuchFieldError when executing DDL via tEnv.sqlUpdate in application project</summary>
      <description>When we create a quickstart project to try flink 1.9/1.10, a NoSuchFieldError is thrown.The dependencies (the flink 1.0 is installed locally for commit 70fe6aa747ad021bbb8dd8cdc0beecc863f010be, flink 1.9 has the same problem): &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;The program code:package com.github.wuchong;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;public class DDLTest { public static void main(String[] args) { EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.sqlUpdate("CREATE TABLE MyTable (\n" + " a int, \n" + " b bigint, \n" + " c varchar \n" + ")\n comment 'table comment'\n" + "partitioned by (b)\n" + "with (\n" + " connector = 'csv', \n" + " csv.path = '/tmp/path'\n" + ")"); }}The exception:Exception in thread "main" java.lang.NoSuchFieldError: names at org.apache.flink.sql.parser.ddl.SqlCreateTable.fullTableName(SqlCreateTable.java:326) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:140) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:86) at org.apache.flink.table.planner.StreamPlanner.parse(StreamPlanner.scala:115) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at com.github.wuchong.DDLTest.main(DDLTest.java:29)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13518" opendate="2019-7-31 00:00:00" fixdate="2019-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive tests</summary>
      <description>Hive straight up doesn't support Java 11 (or anything above Java 8 really), so we might as well disable all tests on Java 11.15:49:57.131 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.208 s - in org.apache.flink.batch.connectors.hive.HiveTableFactoryTestSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.15/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.15:50:11.610 [INFO] Running org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.625 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.627 [ERROR] org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest Time elapsed: 0.007 s &lt;&lt;&lt; ERROR!java.lang.IllegalStateException: Failed to create HiveServer :Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.RuntimeException: Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13523" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct arithmetic function&amp;#39;s semantic for Blink planner</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.MiniBatchGroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ReturnTypeInference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.agg.TestLongAvgFunc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13528" opendate="2019-8-1 00:00:00" fixdate="2019-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka E2E tests fail on Java 11</summary>
      <description>The kafka E2E tests fail on Java 11 with a timeout. Since kafka added support for Java 11 in 2.1.0 we may have to just disable them.[2019-08-15 07:21:47,491] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)java.net.ConnectException: Connection refused at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)Exception in thread "main" org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server 'localhost:2181' with timeout of 30000 ms at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:1233) at org.I0Itec.zkclient.ZkClient.&lt;init&gt;(ZkClient.java:157) at org.I0Itec.zkclient.ZkClient.&lt;init&gt;(ZkClient.java:131) at kafka.utils.ZkUtils$.createZkClientAndConnection(ZkUtils.scala:79) at kafka.utils.ZkUtils$.apply(ZkUtils.scala:61) at kafka.admin.TopicCommand$.main(TopicCommand.scala:53) at kafka.admin.TopicCommand.main(TopicCommand.scala)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09SecuredRunITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetention08ITCase.java</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13529" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct agg function&amp;#39;s semantic for Blink planner</summary>
      <description></description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlFirstLastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlIncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13531" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not print log and call &amp;#39;release&amp;#39; if no requests should be evicted from the shared slot</summary>
      <description>After adding the logic to bookkeeping the resource used in the shared slots, the resource requests will be recorded inside the MultiTaskSlot and when the underlying slot is allocated, all the resource requests will be checked if there is over-subscription, if so, some requests will be failed.In the current implementation, the code does not check the amount to fail before printing the over-allocated debug log and tries to fail them. This should not cause actual errors, but it will  Print a debug log saying some requests will be failed even if no one to fail. If the total number of requests is 0 (This is possible if there already AllocatedSlot before the first request), the release method will be called. Although it will do nothing with the current implementation (the slot is still being created and not added to any other data structure), it may cause error if the release logic changes in the future.To fix this issue, we should add a explicit check on the number of requests to fail. </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13545" opendate="2019-8-2 00:00:00" fixdate="2019-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin</summary>
      <description>run tpcds 14.a on blink planner, an exception will thrownjava.lang.ArrayIndexOutOfBoundsException: 84 at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564) at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80) at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284) at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)the reason is JoinToMultiJoinRule should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by SemiJoin which is not matched JoinToMultiJoinRule.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13547" opendate="2019-8-2 00:00:00" fixdate="2019-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct string function&amp;#39;s semantic for Blink planner</summary>
      <description>Currently many string builtin functions in blink planner follow hive/spark semantics, which should keep compatible with old planner. And some non-standard functions(Blink planner intros) should be removed. concat/concat_ws function (null treatment) substring function (follow calcite/flink) from_base64 should return string not binary intro truncate function to blink planner uuid should be no-argument (remove the one-argument version) length/jsonvalue/keyvalue/substr (non-standard function should be removed) md5/sha1/sha2/sha224/sha256/sha384/sha512(remove the two-arguments version) ascii (operand type should beSqlTypeFamily.CHARACTER)</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryStringTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryStringUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13550" opendate="2019-8-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for CPU FlameGraphs in web UI</summary>
      <description>For a better insight into a running job, it would be useful to have ability to render a CPU flame graph for a particular job vertex.Flink already has a stack-trace sampling mechanism in-place, so it should be straightforward to implement.This should be done by implementing a new endpoint in REST API, which would sample the stack-trace the same way as current BackPressureTracker does, only with a different sampling rate and length of sampling.Here is a little demo of the feature.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.tsconfig.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.index.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.share.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.public-api.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoSamplesRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.JobVertexStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexFlameGraphHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.TaskThreadInfoResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.rest.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="13558" opendate="2019-8-2 00:00:00" fixdate="2019-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include table examples in flink-dist</summary>
      <description>We want to treat the table api as first-class API. We already included in the lib directory flink.We should also include some examples of the table api in the distribution.Before that we should strip all the dependency and just include the classes from example module.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13563" opendate="2019-8-3 00:00:00" fixdate="2019-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TumblingGroupWindow should implement toString method</summary>
      <description>@Test def testAllEventTimeTumblingGroupWindowOverTime(): Unit = { val util = streamTestUtil() val table = util.addDataStream[(Long, Int, String)]( "T1", 'long, 'int, 'string, 'rowtime.rowtime) val windowedTable = table .window(Tumble over 5.millis on 'rowtime as 'w) .groupBy('w) .select('int.count) util.verifyPlan(windowedTable) }currently, it's physical plan is HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])we know nothing about the TumblingGroupWindow except its name. the expected plan isHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.logical.groupWindows.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13564" opendate="2019-8-3 00:00:00" fixdate="2019-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</summary>
      <description>just as FLINK-11017, blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13567" opendate="2019-8-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed on Travis</summary>
      <description>The Avro Confluent Schema Registry nightly end-to-end test failed on Travis with[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directoryhttps://api.travis-ci.org/v3/job/567273939/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13568" opendate="2019-8-4 00:00:00" fixdate="2019-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL create table doesn&amp;#39;t allow STRING data type</summary>
      <description>Creating a table with "string" data type fails with tableEnv.sqlUpdate().</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlColumnType.java</file>
    </fixedFiles>
  </bug>
  <bug id="13579" opendate="2019-8-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed launching standalone cluster due to improper configured irrelevant config options for active mode.</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ResourceManagerUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13587" opendate="2019-8-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some transformation names are not set in blink planner</summary>
      <description>Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses "LookupJoin" directly which loses a lot of informatoion.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ProjectPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkPruneEmptyRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLimit0RemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcPruneAggregateCallRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testGetStatsFromCatalog.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.RelDisplayNameWriterImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.Expand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13591" opendate="2019-8-6 00:00:00" fixdate="2019-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Completed Job List&amp;#39; in Flink web doesn&amp;#39;t display right when job name is very long</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="13594" opendate="2019-8-6 00:00:00" fixdate="2019-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the &amp;#39;from_element&amp;#39; method of flink python api to apply to blink planner</summary>
      <description>The initial implementation of python 'from_element' method is based on DataStream API and DataSet API, which is not available when the TableEnvironment's planner is Blink. To fix this problem, we need to reimplement the python from_element method based on InputFormatTableSource and InputFormat.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="13596" opendate="2019-8-6 00:00:00" fixdate="2019-10-6 01:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Add two utils for Table transformations.</summary>
      <description>For the input data type is Table, but during the ML training process, we need use DataSet and DataStream type. Thus, we need the type transformation utils in the ML algorithm implementations. Add the transformation between DataSet and Table Add the transformation between DataStream and Table Add unit test cases.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.DataStreamConversionUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.DataSetConversionUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.DataStreamConversionUtil.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.DataSetConversionUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13630" opendate="2019-8-7 00:00:00" fixdate="2019-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamTableEnvironment#toAppendStream without QueryConfig overrides values set on TableConfig</summary>
      <description>In a sequence like this:tEnv.getConfig().setIdleStateRetentionTime(minRetention, maxRetention);Table table = tEnv.fromDataStream(elements);tEnv.toAppendStream(table, Row.class);the call to toAppendStream will override the value set in tEnv.getConfig.setIdleStateRetentionTime. I think we should change that behavior, as we want to drop the version with explicit QueryConfig and recommend the described setup.Thanks hequn8128 for pointing it out.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13643" opendate="2019-8-7 00:00:00" fixdate="2019-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the workaround for users with a different minor Hive version</summary>
      <description>We officially support two Hive versions. However, we can tell user how to work around the limitation if their Hive version is only minorly differently.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13645" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error in code-gen when using blink planner in scala shell</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13663" opendate="2019-8-9 00:00:00" fixdate="2019-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test for modern Kafka failed on Travis</summary>
      <description>The SQL Client end-to-end test for modern Kafka failed on Travis because it could not download https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz.Maybe we could add a similar retry logic as with the Kinesis end-to-end test FLINK-13599.https://api.travis-ci.org/v3/job/569262834/log.txthttps://api.travis-ci.org/v3/job/569262828/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13667" opendate="2019-8-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Add the utility class for the Table</summary>
      <description>Add the utility class for the Table the operations on column name the operations on column type</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.TableUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.TableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1367" opendate="2015-1-7 00:00:00" fixdate="2015-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add field aggregations to Streaming Scala api</summary>
      <description>Field aggregations are missing from the streaming scala api for case classes.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13699" opendate="2019-8-13 00:00:00" fixdate="2019-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TableFactory doesn&amp;#39;t work with DDL when containing TIMESTAMP/DATE/TIME types</summary>
      <description>Currently, in blink planner, we will convert DDL to TableSchema with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME. However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).A simple solution is encode DataTypes.TIMESTAMP() as "TIMESTAMP" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.util.HBaseTestBase.java</file>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.util.HBaseTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TypeStringUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TypeStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13704" opendate="2019-8-13 00:00:00" fixdate="2019-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-H end-to-end test (Blink planner) fails on Travis</summary>
      <description>The TPC-H end-to-end test fails on Travis with the following problem:Running query #22...Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:129) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)[FAIL] Test script contains errors.Checking for errors...org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL update statement. at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:539) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:432) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:367) at org.apache.flink.table.client.cli.CliClient.callInsertInto(CliClient.java:496) at org.apache.flink.table.client.cli.CliClient.lambda$submitUpdate$0(CliClient.java:231) at java.util.Optional.map(Optional.java:215) at org.apache.flink.table.client.cli.CliClient.submitUpdate(CliClient.java:228) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:127) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 13, column 10 to line 13, column 30: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:125) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:82) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:154) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:89) at org.apache.flink.table.planner.delegation.PlannerBase.parse(PlannerBase.scala:130) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.sqlUpdate(StreamTableEnvironmentImpl.java:299) at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$applyUpdate$12(LocalExecutor.java:531) at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216) at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:529) ... 9 moreCaused by: org.apache.calcite.runtime.CalciteContextException: From line 13, column 10 to line 13, column 30: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:809) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4807) at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1762) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:273) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:215) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5566) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5553) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1680) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1665) at org.apache.calcite.sql.type.InferTypes.lambda$static$0(InferTypes.java:46) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1854) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862) at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereOrOn(SqlValidatorImpl.java:4006) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:3998) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3368) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3111) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3093) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3365) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:932) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:639) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:122) ... 18 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature substr(&lt;CHARACTER&gt;, &lt;NUMERIC&gt;, &lt;NUMERIC&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 51 morehttps://api.travis-ci.org/v3/job/570757857/log.txthttps://api.travis-ci.org/v3/job/570757863/log.txthttps://api.travis-ci.org/v3/job/570757869/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13708" opendate="2019-8-14 00:00:00" fixdate="2019-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transformations should be cleared because a table environment could execute multiple job</summary>
      <description>currently, if a table environment execute more than one sql jobs, the following job contains transformations about the previous job. the reason is the transformations is not cleared after execution</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.BatchExecutorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.StreamExecutor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.ExecutorBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.BatchExecutor.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug id="13711" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive array values not properly displayed in SQL CLI</summary>
      <description>Array values are displayed like: [Ljava.lang.Integer;@632~ [Ljava.lang.Integer;@6de~</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13712" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 1.9.0 release notes to documentation</summary>
      <description>Similar to https://ci.apache.org/projects/flink/flink-docs-release-1.9/release-notes/flink-1.8.html, we need a release note page for 1.9.0.This will be linked by the announcement blog post on the Apache Flink website.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13715" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Program-related english documentation.</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.packaging.md</file>
    </fixedFiles>
  </bug>
  <bug id="13716" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Program-related chinese documentation</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.packaging.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13718" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable HBase tests</summary>
      <description>The HBase tests are categorically failing on Java 11. Given that HBase itself does not support Java 11 at this point we should just disable these tests for the time being.HBaseConnectorITCase.activateHBaseCluster:81-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader HBaseLookupFunctionITCase.activateHBaseCluster:95-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader HBaseSinkITCase.activateHBaseCluster:91-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13737" opendate="2019-8-15 00:00:00" fixdate="2019-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist should add provided dependency on flink-examples-table</summary>
      <description>In FLINK-13558 we changed the `flink-dist/bin.xml` to also include flink-examples-table in the binary distribution. The flink-dist module though does not depend on the flink-examples-table.If only the flink-dist module is built with its dependencies (this happens in the release scripts). The table examples are not built and thus not included in the distribution</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13742" opendate="2019-8-16 00:00:00" fixdate="2019-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix code generation when aggregation contains both distinct aggregate with and without filter</summary>
      <description>The following test will fail when the aggregation contains COUNT(DISTINCT c) and COUNT(DISTINCT c) filter .... @Test def testDistinctWithMultiFilter(): Unit = { val sqlQuery = "SELECT b, " + " SUM(DISTINCT (a * 3)), " + " COUNT(DISTINCT SUBSTRING(c FROM 1 FOR 2))," + " COUNT(DISTINCT c)," + " COUNT(DISTINCT c) filter (where MOD(a, 3) = 0)," + " COUNT(DISTINCT c) filter (where MOD(a, 3) = 1) " + "FROM MyTable " + "GROUP BY b" val t = failingDataSource(StreamTestData.get3TupleData).toTable(tEnv).as('a, 'b, 'c) tEnv.registerTable("MyTable", t) val result = tEnv.sqlQuery(sqlQuery).toRetractStream[Row] val sink = new TestingRetractSink result.addSink(sink) env.execute() val expected = List( "1,3,1,1,0,1", "2,15,1,2,1,0", "3,45,3,3,1,1", "4,102,1,4,1,2", "5,195,1,5,2,1", "6,333,1,6,2,2") assertEquals(expected.sorted, sink.getRetractResults.sorted) }</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13745" opendate="2019-8-16 00:00:00" fixdate="2019-3-16 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13746" opendate="2019-8-16 00:00:00" fixdate="2019-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch (v2.3.5) sink end-to-end test fails on Travis</summary>
      <description>The Elasticsearch (v2.3.5) sink end-to-end test fails on Travis because it logs contain the following line:INFO org.elasticsearch.plugins - [Terror] modules [], plugins [], sites []Due to this, the error check is triggered.https://api.travis-ci.org/v3/job/572255901/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13747" opendate="2019-8-16 00:00:00" fixdate="2019-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some TODOs in Hive connector</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13760" opendate="2019-8-18 00:00:00" fixdate="2019-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hardcode Scala version dependency in hive connector</summary>
      <description>FLINK-13688 introduced a flink-test-utils dependency in flink-connector-hive. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13769" opendate="2019-8-19 00:00:00" fixdate="2019-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchFineGrainedRecoveryITCase.testProgram failed on Travis</summary>
      <description>BatchFineGrainedRecoveryITCase.testProgram failed on Travis.23:14:26.860 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 50.007 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase23:14:26.868 [ERROR] testProgram(org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase) Time elapsed: 49.469 s &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.testProgram(BatchFineGrainedRecoveryITCase.java:225)Caused by: java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@localhost:39333/user/taskmanager_3#-344551647]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@localhost:39333/user/taskmanager_3#-344551647]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.https://travis-ci.org/apache/flink/jobs/573523669</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13770" opendate="2019-8-19 00:00:00" fixdate="2019-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty to 4.1.39.Final</summary>
      <description>I quickly went through all the changelogs for Netty 4.1.32 (which wecurrently use) to the latest Netty 4.1.39.Final. Below, you will find alist of bug fixes and performance improvements that may affect us. Nicechanges we could benefit from, also for the Java &gt; 8 efforts. The mostimportant ones fixing leaks etc are #8921, #9167, #9274, #9394, and thevarious CompositeByteBuf fixes. The rest are mostly performanceimprovements.Since we are still early in the dev cycle for Flink 1.10, it would benice to update now and verify that the new version works correctly.Netty 4.1.33.Final- Fix ClassCastException and native crash when using kqueue transport(#8665)- Provide a way to cache the internal nioBuffer of the PooledByteBufferto reduce GC (#8603)Netty 4.1.34.Final- Do not use GetPrimitiveArrayCritical(...) due multiple not-fixed bugsrelated to GCLocker (#8921)- Correctly monkey-patch id also in whe os / arch is used within libraryname (#8913)- Further reduce ensureAccessible() overhead (#8895)- Support using an Executor to offload blocking / long-running taskswhen processing TLS / SSL via the SslHandler (#8847)- Minimize memory footprint for AbstractChannelHandlerContext forhandlers that execute in the EventExecutor (#8786)- Fix three bugs in CompositeByteBuf (#8773)Netty 4.1.35.Final- Fix possible ByteBuf leak when CompositeByteBuf is resized (#8946)- Correctly produce ssl alert when certificate validation fails on theclient-side when using native SSL implementation (#8949)Netty 4.1.37.Final- Don't filter out TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (#9274)- Try to mark child channel writable again once the parent channelbecomes writable (#9254)- Properly debounce wakeups (#9191)- Don't read from timerfd and eventfd on each EventLoop tick (#9192)- Correctly detect that KeyManagerFactory is not supported when usingOpenSSL 1.1.0+ (#9170)- Fix possible unsafe sharing of internal NIO buffer in CompositeByteBuf(#9169)- KQueueEventLoop won't unregister active channels reusing a filedescriptor (#9149)- Prefer direct io buffers if direct buffers pooled (#9167)Netty 4.1.38.Final- Prevent ByteToMessageDecoder from overreading when !isAutoRead (#9252)- Correctly take length of ByteBufInputStream into account forreadLine() / readByte() (#9310)- availableSharedCapacity will be slowly exhausted (#9394)</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
    </fixedFiles>
  </bug>
  <bug id="13805" opendate="2019-8-20 00:00:00" fixdate="2019-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bad Error Message when TaskManager is lost</summary>
      <description>When a TaskManager is lost, the job reports as the failure causeorg.apache.flink.util.FlinkException: The assigned slot 6d0e469d55a2630871f43ad0f89c786c_0 was removed.That is a pretty bad error message, as a user I don't know what that means. Sounds like it could simply refer to internal book keeping, maybe some rebalancing or so.You need to know a lot about Flink to understand that this means actually "TaskManager failure".</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13814" opendate="2019-8-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTableSink should strip quotes from partition values</summary>
      <description></description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13817" opendate="2019-8-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose whether web submissions are enabled</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.DashboardConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="13818" opendate="2019-8-22 00:00:00" fixdate="2019-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check whether web submission are enabled</summary>
      <description>The WebUI should preemptively check whether web-submissions are enabled (via FLINK-13817), and adjust the web-submission page accordingly.</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="13825" opendate="2019-8-23 00:00:00" fixdate="2019-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The original plugins dir is not restored after e2e test run</summary>
      <description>Previously, the result of Flink distribution build didn't contain plugins dir.Instead, for some e2e tests, the directory was created (and removed) by a test's setup steps.FLINK-12868 has added a pre-created plugins dir into the Flink distribution build, but without adjusting the e2e tests. As the result, after some e2e tests run, the original directory is removed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13826" opendate="2019-8-23 00:00:00" fixdate="2019-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT OVERWRITE for Hive connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CatalogSinkModifyOperation.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13831" opendate="2019-8-23 00:00:00" fixdate="2019-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Free Slots / All Slots display error</summary>
      <description>Free Slots / All Slots display error</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="13841" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Hive version support to all 1.2 and 2.3 versions</summary>
      <description>This is to support all 1.2 (1.2.0, 1.2.1, 1.2.2) and 2.3 (2.3.0-5) versions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="13868" opendate="2019-8-27 00:00:00" fixdate="2019-9-27 01:00:00" resolution="Done">
    <buginformation>
      <summary>Job vertex add taskmanager id in rest api</summary>
      <description>In web, user want to see subtask run in which taskmanager. But now there is no taskmanager's id, user have to judge it by host and port. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="13872" opendate="2019-8-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate Operations Playground to Chinese</summary>
      <description>The Operations Playground is a quick and convenient way to learn about Flink's operational features (job submission, failure recovery, job updates, scaling, metrics).We should translate it to Chinese as well.</description>
      <version>1.9.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="13873" opendate="2019-8-27 00:00:00" fixdate="2019-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose RocksDB column_family as variable</summary>
      <description>When we use influxdb reporter report rocksdb metric data，influxdb reporter call {#getLogicalScope()} get the measurements,and call {#getAllVariables()} get tags. and we need column family as tags for group by in influxSQL。so It need call addgroup("column_family", columnFamilyName) build metric group</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitor.java</file>
      <file type="M">docs..includes.generated.rocks.db.native.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="13877" opendate="2019-8-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.1.0 and 2.1.1</summary>
      <description>This is to support Hive 2.1 versions (2.1.0 and 2.1.1).</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13896" opendate="2019-8-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala 2.11 maven compile should target Java 1.8</summary>
      <description>When setting TableEnvironment in scala as follwing: // we can repoduce this problem by put following code in // org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest@Testdef testCreateEnvironment(): Unit = { val settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build(); val tEnv = TableEnvironment.create(settings);}Then mvn test would fail with an error message like: error: Static methods in interface require -target:JVM-1.8 We can fix this bug by adding:&lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-target:jvm-1.8&lt;/arg&gt; &lt;/args&gt;&lt;/configuration&gt; to scala-maven-plugin config   </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13903" opendate="2019-8-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.3.6</summary>
      <description>Hive 2.3.6 is released a few days ago. We can trivially support this version as well, as we have already provided support for previous 2.3.x releases.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="13930" opendate="2019-8-30 00:00:00" fixdate="2019-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.x</summary>
      <description>Including 3.1.0, 3.1.1, and 3.1.2.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveServerContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSimpleUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDTF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13931" opendate="2019-8-30 00:00:00" fixdate="2019-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.0.x</summary>
      <description>Including 2.0.0 and 2.0.1.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13941" opendate="2019-9-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent data-loss by not cleaning up small part files from S3.</summary>
      <description></description>
      <version>1.8.0,1.8.1,1.9.0</version>
      <fixedVersion>1.8.2,1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="13942" opendate="2019-9-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Overview page for Getting Started section</summary>
      <description>The Getting Started section provide different types of tutorials that target users with different interests and backgrounds.We should add a brief overview page that describes the different tutorials such that users easily find the material that they need to get started with Flink.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="13943" opendate="2019-9-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide api to convert flink table to java List (blink planner)</summary>
      <description>It would be nice to convert flink table to java List so that I can do other data manipulation in client side after execution flink job. For flink planner, I can convert flink table to DataSet and use DataSet#collect, but for blink planner, there's no such api.EDIT from FLINK-14807:Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. Other apis such as Table#head, Table#print is also helpful.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13953" opendate="2019-9-4 00:00:00" fixdate="2019-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Facilitate enabling new Scheduler in MiniCluster Tests</summary>
      <description>Currently, tests using the MiniCluster use the legacy scheduler by default. Once the new scheduler is implemented, we should run tests with the new scheduler enabled. However, it is not expected that all tests will pass immediately. Therefore, it should be possible to enable the new scheduler for a subset of tests. In the first step the tests should be able to run manually against new scheduler.Acceptance Criteria A junit test category AlsoRunWithSchedulerNG can be used to mark MiniCluster tests. A new maven profile scheduler-ng will be enabled to support running AlsoRunWithSchedulerNG annotated tests with the new scheduler.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="1396" opendate="2015-1-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hadoop input formats directly to the user API.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopIOFormatsITCase.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopInputFormatTest.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputFormatTest.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyReporter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyProgressable.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.utils.HadoopUtils.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.example.HadoopMapredCompatWordCount.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.utils.HadoopUtils.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.example.WordCount.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.hadoop.compatibility.md</file>
      <file type="M">docs.css.main.main.css</file>
    </fixedFiles>
  </bug>
  <bug id="13973" opendate="2019-9-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14004" opendate="2019-9-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define SourceReaderOperator to verify the integration with StreamOneInputProcessor</summary>
      <description>We already refactored the task input and output in runtime stack for considering the requirements of FLIP-27. In order to further verify that the new source could work well with the unified StreamOneInputProcessor in mailbox model, we define the SourceReaderOperator as task input and implement a unit test for passing through the whole process.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceReaderStreamTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14005" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.2.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14006" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java UDFs in Python API</summary>
      <description>Currently, user can not find out the doc for  how to using Java UDFs in Python API. So we should add the detail doc. In https://ci.apache.org/projects/flink/flink-docs-master/dev/table/udfs.html only describe how to using the APIs, but do not mention how to add the JARs to the class path.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="14007" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java user-defined source/sink in Python API</summary>
      <description>Currently, user can not find out the doc for how to using Java user-defined source/sink in Python API. So we should add the detail doc. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="14010" opendate="2019-9-9 00:00:00" fixdate="2019-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher &amp; JobManagers don&amp;#39;t give up leadership when AM is shut down</summary>
      <description>In YARN deployment scenario, YARN RM possibly launches a new AM for the job even if the previous AM does not terminated, for example, when AMRM heartbeat timeout. This is a common case that RM will send a shutdown request to the previous AM and expect the AM shutdown properly.However, currently in YARNResourceManager, we handle this request in onShutdownRequest which simply close the YARNResourceManager but not Dispatcher and JobManagers. Thus, Dispatcher and JobManager launched in new AM cannot be granted leadership properly. Visually,on previous AM: Dispatcher leader, JM leaderson new AM: ResourceManager leadersince on client side or in per-job mode, JobManager address and port are configured as the new AM, the whole cluster goes into an unrecoverable inconsistent status: client all queries the dispatcher on new AM who is now the leader. Briefly, Dispatcher and JobManagers on previous AM do not give up their leadership properly.</description>
      <version>1.7.2,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14033" opendate="2019-9-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed caches are not registered in Yarn Per Job Cluster Mode</summary>
      <description>CacheFiles in StreamExecutionEnvironment is not used in Job Submission in the Yarn per job cluster mode. Compare to the job submission in session cluster mode that will upload distributed cache files onto http server in application master, we should get the cache files in job graph and register them into blob store in YarnJobClusterEntrypoint.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="14049" opendate="2019-9-11 00:00:00" fixdate="2019-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update error message for failed partition updates to include task name</summary>
      <description>The error message for failed partition updates does not include the task name.That makes it useless during debugging.Adding the task name is a simple addition that make this error message much more helpful.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="14100" opendate="2019-9-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce OracleDialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="14101" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SqlServerDialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="14107" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer record emitter deadlock under event time alignment</summary>
      <description>When the emitter reaches the max timestamp for the current queue, it stops emitting and waits for the max timestamp to advance. Since it simultaneously selects the next queue as the new "minimum" queue, it may deadlock if the previous min queue represents the new global lower bound after the max timestamp advanced. This occurs very infrequently and we were finally able to reproduce.</description>
      <version>1.8.2,1.9.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.RecordEmitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14121" opendate="2019-9-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress to 1.20</summary>
      <description>See https://commons.apache.org/proper/commons-compress/security-reports.html</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="14150" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary __pycache__ directories appears in pyflink.zip</summary>
      <description>It seems we are packaging _pycache_ directories into pyflink.zip. These directories contain bytecode cache files that are automatically generated by python3. We should remove them from the python source code folder before packaging.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14156" opendate="2019-9-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute/run processing timer triggers taking into account operator level mailbox loops</summary>
      <description>With FLINK-12481, the timer triggers are executed by the mailbox thread and passed to the mailbox with the maximum priority.In case of operators that use mailbox.yield() (introduced in FLINK-13248), current approach may execute timer triggers that belong to an upstream operator. Such timer trigger, may potentially call processElement|Watermark() which eventually would come back to the current operator. This situation may be similar to FLINK-13063.To avoid this, the proposal is to set mailbox letters priorities of timer triggers with the priority of the operator that the trigger belongs to.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskOperatorTimerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14176" opendate="2019-9-24 00:00:00" fixdate="2019-10-24 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>add taskmanager link in vertex‘s page of taskmanager</summary>
      <description>Add taskmanager's link in vertex's page of taskmanager, so user could go to taskmanegr's page.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="14179" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the description of &amp;#39;SHOW FUNCTIONS&amp;#39; in SQL Client</summary>
      <description>Currently 'SHOW FUNCTIONS' lists not only user-defined functions, but also system-defined ones, the description 'Shows all registered user-defined functions.' not correctly depicts this functionality. I think we can change the description to 'Shows all system-defined and user-defined functions.'  </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
    </fixedFiles>
  </bug>
  <bug id="14198" opendate="2019-9-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add type options to all flink python API doc</summary>
      <description>Currently ":type:" and ":rtype:" options in python docstrings have been fully supported in sphinx(https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html) and PyCharm(https://www.jetbrains.com/help/pycharm/using-docstrings-to-specify-types.html). Sphinx will generate python API documents with type annotations if these options exist in function docstrings. PyCharm also collects the type information in docstrings to detect potential coding mistakes and provide autocomplete support during python program development. There are already few interfaces in python API that have these options. We should add these options to the rest. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
    </fixedFiles>
  </bug>
  <bug id="1420" opendate="2015-1-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small cleanup on code after 0.8 release</summary>
      <description>This issue track PR https://github.com/apache/flink/pull/302 for master and 0.8.1 release</description>
      <version>None</version>
      <fixedVersion>0.8.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.CaseClassComparatorTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.misc.MassiveCaseClassSortingITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.functions.ClosureCleanerITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.TraversableSerializer.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassSerializer.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassComparator.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.RecoveryITCase.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.client.JobClient.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.MapOperator.java</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.relational.TPCHQuery3.scala</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.relational.TPCHQuery10.scala</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.graph.DeltaPageRank.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.InputSplitSource.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.InputSplit.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.Client.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.windowing.Time.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.windowing.Delta.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamJoinOperator.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamCrossOperator.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14202" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution plan for Python Calc when there is a condition</summary>
      <description>As discussed in https://github.com/apache/flink/pull/9748:"For the filter, we calculate these condition UDFs together with other UDFs and do the filter later. I think we can optimize it a bit, i.e., calculate the conditions first and then check whether to call the other UDFs. This can be easily achieved in the SplitRule."</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonScalarFunctionSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonScalarFunctionSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="14208" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize Python UDFs with parameters of constant values</summary>
      <description>We need support Python UDFs with parameters of constant values. It should be noticed that the constant parameters are not needed to be transferred between the Java operator and the Python worker.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  <bug id="14212" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDFs without arguments</summary>
      <description>We should support Python UDFs without arguments </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="14218" opendate="2019-9-26 00:00:00" fixdate="2019-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support precise function reference</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.CallExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.UnresolvedCallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.QualifyBuiltInFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.LookupCallResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionLookup.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalogUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug id="14219" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support ambiguous function reference</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14241" opendate="2019-9-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ARM installation step for flink e2e container test</summary>
      <description>Flink e2e container test runs under docker, docker-compose and kubernets. But the way Flink using to install the package doesn't work on ARM. docker-compose doesn't have ARM release bin file minikube doesn't support ARM.So we should add the new step for ARM arch.  install docker-compose by `pip install` use kubeadm instead of minikube</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-container.docker.build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14270" opendate="2019-9-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>new web ui should display more than 4 metrics</summary>
      <description>The old web UI can display at least 9 metrics at once, and this can be valuable.The new interface is limited to 4 metrics, which is not enough.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.resize.resize.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="14273" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message when signature validation of UDAF is failed</summary>
      <description>When UDAF parameters are inconsistent with the definition of accumulate method, all arguments to the accumulate method are listed in the error. But the first argument of accumulate is accumulator, users don't have to care when using SQL.For example:INSERT INTO Orders SELECT name, USERUDAF(id, name) FROM Orders GROUP BY TUMBLE(rowTime, interval '10' second ), id, nameUSERUDAF is a User-Defined Aggregate Functions, and accumulate is defined as follow:public void accumulate(Long acc, String a) {……}At present, error is as follows:Caused by: org.apache.flink.table.api.ValidationException: Given parameters of function do not match any signature. Actual: (java.lang.Integer, java.lang.String) Expected: (java.lang.Integer, java.lang.String)This error will mislead users, and the expected errors are as follows :Caused by: org.apache.flink.table.api.ValidationException: Given parameters of function do not match any signature. Actual: (java.lang.Integer, java.lang.String) Expected: (java.lang.String)</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14301" opendate="2019-9-30 00:00:00" fixdate="2019-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add documentation for functions categories and new function resolution orders</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.table.functions.builtinFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.builtinFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug id="1433" opendate="2015-1-22 00:00:00" fixdate="2015-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HADOOP_CLASSPATH to start scripts</summary>
      <description>With the Hadoop file system wrapper, its important to have access to the hadoop filesystem classes.The HADOOP_CLASSPATH seems to be a standard environment variable used by Hadoop for such libraries.Deployments like Google Compute Cloud set this variable containing the "Google Cloud Storage Hadoop Wrapper". So if users want to use the Cloud Storage in an non-yarn environment, we need to address this issue.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.webclient.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">docs.example.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="14337" opendate="2019-10-7 00:00:00" fixdate="2019-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistoryServer does not handle NPE on corruped archives properly</summary>
      <description>The HistoryServerTest.testHistoryServerIntegration failed on Travis with[ERROR] testHistoryServerIntegration[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest) Time elapsed: 10.667 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;3&gt; but was:&lt;2&gt;https://api.travis-ci.org/v3/job/594533358/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.history.FsJobArchivist.java</file>
    </fixedFiles>
  </bug>
  <bug id="14356" opendate="2019-10-9 00:00:00" fixdate="2019-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce "raw" format to (de)serialize message to a single field</summary>
      <description>I want to use flink sql to write kafka messages directly to hdfs. The serialization and deserialization of messages are not involved in the middle.  The bytes of the message directly convert the first field of Row.  However, the current RowSerializationSchema does not support the conversion of bytes to VARBINARY. Can we add some special RowSerializationSchema and RowDerializationSchema ? ========================================================================Copied from FLINK-9963:Sometimes it might be useful to just read or write a single value into Kafka or other connectors. We should add a single-value SerializationSchemaFactory and single-value DeserializationSchemaFactory, the types below and their array types shall be considered.byte, short, int, long, float, double, stringFor the numeric types, we might want to specify the endian format.A string type single-value format will be added with this issue for future reference.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.SingleValueRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.SingleValueFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.MockRowDataTypeInfo.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueRowDataSerialization.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueRowDataDeserialization.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueFormatFactory.java</file>
      <file type="M">docs.dev.table.connectors.formats.singleValue.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.singleValue.md</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="1436" opendate="2015-1-22 00:00:00" fixdate="2015-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Command-line interface verbose option &amp; error reporting</summary>
      <description>Let me run just a basic Flink job and add the verbose flag. It's a general option, so let me add it as a first parameter:&gt; ./flink -v run ../examples/flink-java-examples-0.8.0-WordCount.jar hdfs:///input hdfs:///output9Invalid action!./flink &lt;ACTION&gt; &amp;#91;GENERAL_OPTIONS&amp;#93; &amp;#91;ARGUMENTS&amp;#93; general options: h,-help Show the help for the CLI Frontend. v,-verbose Print more detailed error messages.Action "run" compiles and runs a program. Syntax: run &amp;#91;OPTIONS&amp;#93; &lt;jar-file&gt; &lt;arguments&gt; "run" action arguments: c,-class &lt;classname&gt; Class with the program entry point ("main" method or "getPlan()" method. Only needed if the JAR file does not specify the class in its manifest. m,-jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. p,-parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration.Action "info" displays information about a program. "info" action arguments: c,-class &lt;classname&gt; Class with the program entry point ("main" method or "getPlan()" method. Only needed if the JAR file does not specify the class in its manifest. e,-executionplan Show optimized execution plan of the program (JSON) m,-jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. p,-parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration.Action "list" lists running and finished programs. "list" action arguments: m,-jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. r,-running Show running programs and their JobIDs s,-scheduled Show scheduled prorgrams and their JobIDsAction "cancel" cancels a running program. "cancel" action arguments: i,-jobid &lt;jobID&gt; JobID of program to cancel m,-jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration.What just happened? This results in a lot of output which is usually generated if you use the --help option on command-line tools. If your terminal window is large enough, then you will see a tiny message:"Please specify an action". I did specify an action. Strange. If you read the help messages carefully you see, that "general options" belong to the action.&gt; ./flink run -v ../examples/flink-java-examples-0.8.0-WordCount.jar hdfs:///input hdfs:///output9For the sake of mitigating user frustration, let us also accept -v as the first argument. It may seem trivial for the day-to-day Flink user but makes a difference for a novice.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendListCancelTest.java</file>
      <file type="M">docs.cli.md</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendPackageProgramTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendInfoTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14380" opendate="2019-10-11 00:00:00" fixdate="2019-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type Extractor POJO setter check does not allow for Immutable Case Class</summary>
      <description>When deciding if a class conforms to POJO using the type extractor Flink checks that the class implements a setter and getter method. For the setter method Flink makes the assertion that the return type is `Void`. This is an issue if using a case class as often the return type of a case class setter is a copy of the objects class. Consider the following case class:case class SomeClass(x: Int) { x_=(newX: Int): SomeClass = { this.copy(x = newX) }}This class will be identified as not being valid POJO although getter (generated) and setter methods are provided because the return type of the setter is not void. This issue discourages immutabilaty and makes the usage of case classes not possible without falling back to Kryo Serializer.The issue is located in https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/java/typeutils/TypeExtractor.java on line 1806. Here is a permalink to the line https://github.com/apache/flink/blob/80b27a150026b7b5cb707bd9fa3e17f565bb8112/flink-core/src/main/java/org/apache/flink/api/java/typeutils/TypeExtractor.java#L1806A copy of the if check is hereif((methodNameLow.equals("set"+fieldNameLow) || methodNameLow.equals(fieldNameLow+"_$eq")) &amp;&amp; m.getParameterTypes().length == 1 &amp;&amp; // one parameter of the field's type (m.getGenericParameterTypes()[0].equals( fieldType ) || (fieldTypeWrapper != null &amp;&amp; m.getParameterTypes()[0].equals( fieldTypeWrapper )) || (fieldTypeGeneric != null &amp;&amp; m.getGenericParameterTypes()[0].equals(fieldTypeGeneric) ) )&amp;&amp; // return type is void. m.getReturnType().equals(Void.TYPE) ) { hasSetter = true; } }I believe the m.getReturnType().equals(Void.TYPE)should be modified to m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz)This will allow for case class setters which return copies of the object enabling to use case classes. This allows us to maintain immutability without being forced to fall back to the Kryo Serializer.</description>
      <version>1.8.2,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedStreams.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedStreams.java</file>
    </fixedFiles>
  </bug>
  <bug id="14397" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to run Hive UDTF with array arguments</summary>
      <description>Tried to call org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2 (in hive-contrib) with query: "select x,y from foo, lateral table(hiveudtf(arr)) as T(x,y)". Failed with exception:java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Ljava.lang.Integer;</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
    </fixedFiles>
  </bug>
  <bug id="14408" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In OldPlanner, UDF open method can not be invoke when SQL is optimized</summary>
      <description>For now, UDF open method can not be invoked when SQL is optimized. For example, a SQL as follow:SELECT MyUdf(1) as constantValue FROM MyTableMyUdf.open can not be invoked in OldPlanner.So, we can construct a constantFunctionContext or a constantRuntimeContext and invoke it just like BlinkPlanner.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14413" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade-plugin ApacheNoticeResourceTransformer uses platform-dependent encoding</summary>
      <description>Some NOTICE files contain quotes that, at least on my system, result in some encoding errors when generating the binary licensing. One example can be found here; the closing quotes would be replaced with a question mark.This is due to the ApacheNoticeResourceTransformer using the platform encoding.</description>
      <version>shaded-8.0,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>shaded-9.0,1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14416" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Module interface and ModuleManager</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="14417" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop CoreModule to provide Flink built-in functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14418" opendate="2019-10-16 00:00:00" fixdate="2019-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HiveModule to provide Hive built-in functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
    </fixedFiles>
  </bug>
  <bug id="14419" opendate="2019-10-16 00:00:00" fixdate="2019-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ModuleFactory, ModuleDescriptor, ModuleValidator for factory discovery service</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.ModuleConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14422" opendate="2019-10-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add metric for network memory</summary>
      <description>This issue refers to Step 2 in the implementation proposal of FLIP-102</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.NettyShuffleMetricFactory.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerMetricsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="14445" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build failed when making sdist</summary>
      <description>From the description of error-log from building python module in travis, it seems invocation failed for sdist-make and then the phase of building python module exited.The instance log: https://api.travis-ci.com/v3/job/246710918/log.txt</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="14459" opendate="2019-10-19 00:00:00" fixdate="2019-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build hangs</summary>
      <description>The build of python module hangs when installing conda. See travis log: https://api.travis-ci.org/v3/job/599704570/log.txtCan't reproduce it neither on my local mac nor on my repo with travis.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14468" opendate="2019-10-20 00:00:00" fixdate="2019-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kubernetes docs</summary>
      <description>Two minor improvements to documented Kubernetes resource definitions: avoid referencing deprecated extensions/v1beta1/Deployment run unprivileged</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="14504" opendate="2019-10-23 00:00:00" fixdate="2019-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add partition management REST API endpoints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.DataSetMetaInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="14516" opendate="2019-10-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove non credit based network code</summary>
      <description>After a survey on the dev mailing list the feedback was that old code path is not used and no longer needed. Based on that we should be safe to drop it and make credit based flow control the only option (currently it's the default).</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.SpillingCheckpointBarrierAlignerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractBufferStorage.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferOrEventSequence.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferStorage.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CachedBufferStorage.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.EmptyBufferStorage.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.LinkedBufferStorage.java</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.InputBufferPoolUsageGauge.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.NettyShuffleMetricFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.SequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyServerLowAndHighWatermarkTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputGateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BufferSpillerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerAlignmentLimitTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.SpilledBufferOrEventSequenceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="1452" opendate="2015-1-26 00:00:00" fixdate="2015-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "flink-contrib" maven module and README.md with the rules</summary>
      <description>I'll also create a JIRA component</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordOutputFormat.java</file>
      <file type="M">flink-addons.pom.xml</file>
      <file type="M">flink-addons.flink-tachyon.src.test.resources.tachyonHadoopConf.xml</file>
      <file type="M">flink-addons.flink-tachyon.src.test.resources.log4j.properties</file>
      <file type="M">flink-addons.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-addons.flink-tachyon.src.test.java.org.apache.flink.tachyon.HDFSTest.java</file>
      <file type="M">flink-addons.flink-tachyon.pom.xml</file>
      <file type="M">flink-addons.flink-streaming.pom.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.windowing.Time.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.windowing.Delta.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.TemporalOperator.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamJoinOperator.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamCrossOperator.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.SplitDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.ConnectedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.java.org.apache.flink.api.streaming.scala.ScalaStreamingAggregator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.pom.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowJoin.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.socket.SocketTextStreamWordCount.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.wordcount.WordCount.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.wordcount.PojoExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowingExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.TimeWindowingExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.SlidingExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.MultiplePoliciesExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.windowing.DeltaExtractExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.twitter.util.TwitterStreamData.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.socket.SocketTextStreamWordCount.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.ml.IncrementalLearningSkeleton.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.join.WindowJoin.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.iteration.IterateExample.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockRecordWriterFactory.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCollector.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoContext.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.state.OperatorStateTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.state.MapStateTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.ShufflePartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.GlobalPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.ForwardPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.FieldsPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.DistributePartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.partitioner.BroadcastPartitionerTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.TumblingEvictionPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.TimeTriggerPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.TimeEvictionPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.PunctuationPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.DeltaPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.CountTriggerPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.policy.CountEvictionPolicyTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.FieldsFromTupleTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.FieldsFromArrayTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.FieldFromTupleTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.FieldFromArrayTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.ConcatinatedExtractTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.extractor.ArrayFromTupleTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.deltafunction.EuclideanDistanceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.deltafunction.CosineDistanceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.WindowCrossJoinTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamvertex.StreamVertexTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamvertex.MockRecordWriter.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamrecord.UIDTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.SourceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.PrintTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.OutputSplitterTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.IterateTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.WindowInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.StreamReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.ProjectTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.MapTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.FlatMapTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.FilterTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoWindowTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CounterInvokableTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoStreamReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoMapTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoGroupedReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.CoFlatMapTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.StreamCollectorTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.OutputSelectorTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.DirectedOutputTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.keys.KeySelectorUtil.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.ClusterUtil.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.SimpleState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.PartitionableState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.OperatorState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.NullableCircularBuffer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.MapState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.GraphState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.CircularFifoList.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.checkpoint.StateCheckpoint.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.state.checkpoint.MapCheckpoint.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.StreamPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.ShufflePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.GlobalPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.FieldsPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.DistributePartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.partitioner.BroadcastPartitioner.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.StreamRecordWriter.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.CoRecordReader.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.CoReaderIterator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.io.BlockingQueueBroker.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TumblingEvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TriggerPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeTriggerPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeEvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.PunctuationPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.EvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.DeltaPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountTriggerPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountEvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CloneableTriggerPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CloneableEvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.ActiveTriggerPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.ActiveTriggerCallback.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.ActiveEvictionPolicyWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.ActiveEvictionPolicy.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.ActiveCloneableEvictionPolicyWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.WindowingHelper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.TimestampWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.Timestamp.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.Time.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.SystemTimestamp.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.Delta.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.Count.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.FieldsFromTuple.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.FieldsFromArray.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.FieldFromTuple.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.FieldFromArray.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.Extractor.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.ConcatinatedExtract.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.extractor.ArrayFromTuple.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.deltafunction.ExtractionAwareDeltaFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.deltafunction.EuclideanDistance.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.deltafunction.DeltaFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.deltafunction.CosineDistance.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamVertexException.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamVertex.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamTaskContext.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamIterationTail.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamIterationHead.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamingRuntimeContext.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.OutputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.InputHandler.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.CoStreamVertex.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.UID.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.StreamRecordSerializer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamrecord.StreamRecord.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamGraph.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.StreamInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.SourceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.SinkInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.WindowGroupReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.StreamReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.ProjectInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.MapInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedWindowInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.FlatMapInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.FilterInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoWindowInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoMapInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoGroupedReduceInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.co.CoFlatMapInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.CounterInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.BatchIterator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.ChainableInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.SourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.RichSourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.RichParallelSourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.ParallelSourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.GenSequenceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.GenericSourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.FromElementsFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.FileSourceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.FileReadFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.FileMonitoringFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.WriteSinkFunctionByMillis.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.WriteSinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.WriteFormatAsText.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.WriteFormatAsCsv.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.WriteFormat.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.SinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.RichSinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.PrintSinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.FileSinkFunctionByMillis.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.sink.FileSinkFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.RichCoWindowFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.RichCoReduceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.RichCoMapFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.RichCoFlatMapFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.JoinWindowFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CrossWindowFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CoWindowFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CoReduceFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CoMapFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.co.CoFlatMapFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.SumFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.SumAggregator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.Comparator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.ComparableAggregator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.AggregationFunction.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.TemporalWindow.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.TemporalOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.StreamJoinOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.StreamCrossOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.StreamProjection.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SplitDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.IterativeDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.GroupedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedDataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.StreamOutput.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.OutputSelector.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.DirectedCollectorWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.CollectorWrapper.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.pom.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.json.JSONParserTest2.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.json.JSONParserTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.db.DBStateTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.SimpleStringSchema.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.SerializationSchema.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.RawSchema.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.DeserializationSchema.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterTopology.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterStreaming.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQTopology.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTopology.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaSink.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.json.JSONParser.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.json.JSONParseFlatMap.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeTopology.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSink.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.RedisState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.MemcachedState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.LevelDBState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.DefaultDBSerializer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.DBStateWithIterator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.DBStateIterator.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.DBState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.DBSerializer.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.db.CustomSerializationDBState.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.ConnectorSource.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.pom.xml</file>
      <file type="M">flink-addons.flink-spargel.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-spargel.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-spargel.src.test.java.org.apache.flink.test.spargel.SpargelConnectedComponentsITCase.java</file>
      <file type="M">flink-addons.flink-spargel.src.test.java.org.apache.flink.spargel.java.SpargelTranslationTest.java</file>
      <file type="M">flink-addons.flink-spargel.src.test.java.org.apache.flink.spargel.java.SpargelCompilerTest.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexUpdateFunction.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.VertexCentricIteration.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.VertexUpdateFunction.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.SpargelIteration.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.MessagingFunction.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.MessageIterator.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.record.Edge.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.OutgoingEdge.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.MessagingFunction.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.MessageIterator.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelPageRankCountingVertices.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelPageRank.java</file>
      <file type="M">flink-addons.flink-spargel.src.main.java.org.apache.flink.spargel.java.examples.SpargelConnectedComponents.java</file>
      <file type="M">flink-addons.flink-spargel.pom.xml</file>
      <file type="M">flink-addons.flink-jdbc.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-jdbc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.DevNullLogStream.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.example.JDBCExample.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-addons.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.example.JDBCExample.java</file>
      <file type="M">flink-addons.flink-jdbc.pom.xml</file>
      <file type="M">flink-addons.flink-hbase.src.test.resources.log4j.properties</file>
      <file type="M">flink-addons.flink-hbase.src.test.resources.hbase-site.xml</file>
      <file type="M">flink-addons.flink-hbase.src.test.java.org.apache.flink.addons.hbase.example.HBaseReadExample.java</file>
      <file type="M">flink-addons.flink-hbase.src.main.java.org.apache.flink.addons.hbase.TableInputSplit.java</file>
      <file type="M">flink-addons.flink-hbase.src.main.java.org.apache.flink.addons.hbase.TableInputFormat.java</file>
      <file type="M">flink-addons.flink-hbase.pom.xml</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.wrapper.HadoopTupleUnwrappingIteratorTest.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.record.HadoopRecordInputOutputITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopTestData.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopReduceFunctionITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopReduceCombineFunctionITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopMapredITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopMapFunctionITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopIOFormatsITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopInputFormatTest.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputOutputITCase.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputFormatTest.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopTupleUnwrappingIterator.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopOutputCollector.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyReporter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyProgressable.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.utils.HadoopUtils.java</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.internal.add.operator.md</file>
      <file type="M">docs.internal.general.arch.md</file>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">flink-addons.flink-avro.pom.xml</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.avro.DataInputDecoder.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.avro.DataOutputEncoder.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.avro.FSDataInputStreamWrapper.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.io.avro.example.AvroTypeExample.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.io.avro.example.User.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.java.io.AvroInputFormat.java</file>
      <file type="M">flink-addons.flink-avro.src.main.java.org.apache.flink.api.java.io.AvroOutputFormat.java</file>
      <file type="M">flink-addons.flink-avro.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.AvroExternalJarProgramITCase.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.AvroOutputFormatTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.EncoderDecoderTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.testjar.AvroExternalJarProgram.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.io.avro.AvroRecordInputFormatTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.io.avro.generated.Colors.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.io.avro.generated.User.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.java.io.AvroInputFormatTypeExtractionTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-addons.flink-avro.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-addons.flink-avro.src.test.resources.logback-test.xml</file>
      <file type="M">flink-addons.flink-avro.src.test.resources.testdata.avro</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.example.WordCount.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopOutputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.utils.HadoopUtils.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.example.HadoopMapredCompatWordCount.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopOutputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultFlinkTypeConverter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultHadoopTypeConverter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.FlinkTypeConverter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopTypeConverter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableComparableWrapper.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapper.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapperConverter.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCount.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCountWithOutputFormat.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSink.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSource.java</file>
      <file type="M">flink-addons.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14522" opendate="2019-10-24 00:00:00" fixdate="2019-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust GC Cleaner for unsafe memory and Java 11</summary>
      <description>sun.misc.Cleaner is not available since Java 9.It was moved to jdk.internal.ref.Cleaner of java.base module (Open JDK-8148117), but another new public API  was introduced to achieve the same behaviour (java.lang.ref.Cleaner);A popular solution is use reflection to look up for the location of the Cleaner class depending on running JVM version.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemoryUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14543" opendate="2019-10-28 00:00:00" fixdate="2019-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partition for temporary table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.DescriptorProperties.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.descriptors.ConnectTableDescriptor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="14724" opendate="2019-11-12 00:00:00" fixdate="2019-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join condition could be simplified in logical phase</summary>
      <description>currently the plan of tpcds q38.sql contains NestedLoopJoin, because it's join condition is CAST(AND(IS NOT DISTINCT FROM($2, $3), IS NOT DISTINCT FROM($1, $4), IS NOT DISTINCT FROM($0, $5))):BOOLEAN, and planner can't find equal join keys from the condition by Join#analyzeCondition.SimplifyJoinConditionRule could solve this.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14727" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update doc of supported Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="14728" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add reminder for users of potential thread safety issues of hive built-in function</summary>
      <description>remind users of https://issues.apache.org/jira/browse/HIVE-16183</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="14876" opendate="2019-11-20 00:00:00" fixdate="2019-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Putting xercesImpl related classes into alwaysParentFirstLoaderPatterns</summary>
      <description>As mentioned in the [9683|https://github.com/apache/flink/pull/9683]  , There is a issue when running jobs when including flink-shaded-hadoop-2 package while changing the `hadoop.version` to our cluster hadoop version.I reproducing this case, the following exception was reported:javax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be createdjavax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:311) at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267) at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:120) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2412) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2375) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2285) at org.apache.hadoop.conf.Configuration.get(Configuration.java:892) at org.apache.hadoop.mapred.JobConf.checkAndWarnDeprecation(JobConf.java:2010) at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:449) at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:186) at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:167) at org.apache.flink.hadoopcompatibility.scala.HadoopInputs$.readHadoopFile(HadoopInputs.scala:127) at com.github.ehiggs.spark.terasort.FlinkTeraSort$.main(FlinkTeraSort.scala:76) at com.github.ehiggs.spark.terasort.FlinkTeraSort.main(FlinkTeraSort.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:586) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:448) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746) at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1009) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1082) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1082)Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308) ... 30 moreCaused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Provider org.apache.xerces.jaxp.DocumentBuilderFactoryImpl not a subtype at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:294) at java.security.AccessController.doPrivileged(Native Method) at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289) ... 30 moreI enabled `-vebose:class` for client, and find out that the `DocumentBuilderFactoryImpl` class was loaded twice. The first time it was loaded is because the flink app starts and initialize the configuration and print something, the second time it was loaded is because the calling of `HadoopInputs.readHadoopFile` in the user code.[Loaded javax.xml.parsers.DocumentBuilderFactory from /opt/soft/openjdk8u202-b08/jre/lib/rt.jar][Loaded org.apache.xerces.jaxp.DocumentBuilderFactoryImpl from file:/home/liupengcheng/git/infra-client/bin/packages/common-infra_client-pack-zjyprc-hadoop/bin/packages/zjyprc-hadoop-flink1.9-hadoop-pack-2.6.0-mdh2.6.0.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar][Loaded javax.xml.parsers.DocumentBuilderFactory from file:/home/liupengcheng/git/infra-client/bin/packages/common-infra_client-pack-zjyprc-hadoop/bin/packages/flink-1.9.0-mdh1.9.0.0-SNAPSHOT/../benchmark-test-1.1-SNAPSHOT-shaded.jar][Loaded org.apache.xerces.jaxp.DocumentBuilderFactoryImpl from file:/home/liupengcheng/git/infra-client/bin/packages/common-infra_client-pack-zjyprc-hadoop/bin/packages/flink-1.9.0-mdh1.9.0.0-SNAPSHOT/../benchmark-test-1.1-SNAPSHOT-shaded.jar]val dataSet = env.createInput(HadoopInputs.readHadoopFile( new TeraInputFormat, classOf[Array[Byte]], classOf[Array[Byte]], inputFile)) .partitionCustom(new FlinkTeraSortPartitioner(new TeraSortPartitioner(partitions)), 0) .sortPartition(0, Order.ASCENDING)when calling `HadoopInputs.readhadoopFile`, it will instantiate `jobConf` and call it's `get` method, thus will finally causing the loading of `DocumentBuilderFactoryImpl`.After loading `DocumentBuilderFactoryImpl`, it will try to check whether it's subtype of `DocumentBuilderFactory`. Here, it' will report error due to that the `DocumentBuilderFacotory` is loaded from `HADOOP_CLASSPATH`, not the user code.public static DocumentBuilderFactory newInstance() { return FactoryFinder.find( /* The default property name according to the JAXP spec */ DocumentBuilderFactory.class, // "javax.xml.parsers.DocumentBuilderFactory" /* The fallback implementation class name */ "com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl");}I found that  the `xercesImpl` package was introduced by the `flink-shaded-hadoop-2`, because it shaded in `xercesImpl` from hadoop-hdfs deps. Moreover, this `xcercesImpl` is included in hadoop-hdfs for all 2.6 ~ 3.0 versions, I think we can add it to the `alwaysParentFirstPatterns` to avoid this problem. cc Aljoscha Krettek</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="14878" opendate="2019-11-20 00:00:00" fixdate="2019-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support `use catalog` through sqlUpdate() method in TableEnvironment</summary>
      <description>Support `USE CATALOG catalogName` through `sqlUpdate()` method in TableEnvironment</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlUseCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14896" opendate="2019-11-21 00:00:00" fixdate="2019-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector doesn&amp;#39;t shade jackson dependency</summary>
      <description>flink-kinesis-connector depends on aws java sdk which is shaded to org.apache.flink.kinesis.shaded.com.amazonaws. However, the aws sdk has a transitive dependency to jackson wich is not shaded in the artifact. This creates problem when running flink on YARN: The aws sdk requires jackson-core v2.6 but hadoop pulls in 2.3. See here. If YARN uses the loads wrong jackson version from classpath. Jod fails with2019-11-20 17:23:11,563 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler - Unhandled exception.org.apache.flink.client.program.ProgramInvocationException: The program caused an error:     at org.apache.flink.client.program.OptimizerPlanEnvironment.getOptimizedPlan(OptimizerPlanEnvironment.java:93)    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:80)    at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:126)    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$6(JarRunHandler.java:142)    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.enable([Lcom/fasterxml/jackson/core/JsonParser$Feature;)Lcom/fasterxml/jackson/databind/ObjectMapper;    at com.amazonaws.partitions.PartitionsLoader.&lt;clinit&gt;(PartitionsLoader.java:54)    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:65)    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:53)    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)    at com.amazonaws.client.builder.AwsClientBuilder.getRegionObject(AwsClientBuilder.java:256)    at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:460)    at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)    at com.amazonaws.client.builder.AwsAsyncClientBuilder.build(AwsAsyncClientBuilder.java:80)...The flink-kinesis-connector should do as other connectors: shade jackson or use the flink-shaded-jackson core dependency</description>
      <version>1.9.0,1.16.0,1.15.2</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14899" opendate="2019-11-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query</summary>
      <description>CREATE TABLE user_log ( user_id VARCHAR, item_id VARCHAR, category_id VARCHAR, behavior VARCHAR, ts TIMESTAMP) WITH ( 'connector.type' = 'kafka', 'connector.version' = 'universal', 'connector.topic' = 'user_behavior', 'connector.startup-mode' = 'earliest-offset', 'connector.properties.0.key' = 'zookeeper.connect', 'connector.properties.0.value' = 'localhost:2181', 'connector.properties.1.key' = 'bootstrap.servers', 'connector.properties.1.value' = 'localhost:9092', 'update-mode' = 'append', 'format.type' = 'json', 'format.derive-schema' = 'true');CREATE TABLE user_dist ( dt VARCHAR, user_id VARCHAR, behavior VARCHAR) WITH ( 'connector.type' = 'jdbc', 'connector.url' = 'jdbc:mysql://localhost:3306/flink-test', 'connector.table' = 'user_behavior_dup', 'connector.username' = 'root', 'connector.password' = ‘******', 'connector.write.flush.max-rows' = '1');INSERT INTO user_distSELECT dt, user_id, behaviorFROM ( SELECT dt, user_id, behavior, ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc from user_log) )WHERE rownum = 1;Exception in thread "main" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.Iterator$class.foreach(Iterator.scala:891)at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)at scala.collection.AbstractIterable.foreach(Iterable.scala:54)at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)at scala.collection.AbstractTraversable.map(Traversable.scala:104)at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14903" opendate="2019-11-21 00:00:00" fixdate="2019-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relax structured types constraints</summary>
      <description>As mentioned in FLIP-65:In order to allow type extraction of structured types that are not registered in a catalog, we need to relax the structured type concept to "inline or anonymous structured types" that are not identified by an object identifier in a catalog but the fully qualified implementation class.In order to support case classes and immutable types, we relax the constraint of enforcing a default constructor by the alternative of having a constructor that fully assigns all fields (same parameter names and types). Because we are already using code generation, the implementation of creating instances even without a default constructor is relatively easy.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.types.LogicalTypeAssignableTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeDuplicatorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCompatibleCheckTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeParser.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeDuplicator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.UserDefinedType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.StructuredType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DistinctType.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14904" opendate="2019-11-21 00:00:00" fixdate="2019-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename ANY type to RAW type</summary>
      <description>As discussed on the mailing list we will rename the ANY type to RAW type. This should not have a big impact for users as it was recently introduced in the last release and not exposed everywhere yet:https://lists.apache.org/thread.html/4b953b6779267556754c755857e2a550d41c37cf5d1d703b63984cc3@%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseMapSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseArraySerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.types.LogicalTypeAssignableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.NestedRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.ClassLogicalTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.TypeGetterSetters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryWriter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.GenericRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCompatibleCheckTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LogicalTypeDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeParser.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeGeneralization.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TypeInformationAnyType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalTypeVisitor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LogicalTypeRoot.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LegacyTypeInformationType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.AnyType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.DataTypes.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="14905" opendate="2019-11-21 00:00:00" fixdate="2019-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify packaging process</summary>
      <description>Right now we have a Java 11 profile for customizing the build in various ways, from bumping plugins, setting compiler flags, disabling tests and even some dependencies.The dependencies bit a problematic since it implies that not all jars created by the Java 8 build process can actually run on Java 11.This creates an unnecessary burden for users who want to use Java 11 (reminder: the only supported LTS version) as they'd have to compile Flink themselves for Java 11.Releasing a second set of jars/binaries is out of the question due to the confusion and overhead this would cause, so instead we should ensure that the Java 8 jars are usable on Java 11.My proposal is to use a multi-release jar to package additional dependencies that are only loaded on Java 11.I have verified that this approach works conceptually, whether it works with the dependency we have to deal with (jaxb-api) remains to be seen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14910" opendate="2019-11-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DisableAutoGeneratedUIDs fails on keyBy</summary>
      <description>There doesn't seem to be a way to add a UID to the Partition operator created by KeyBy, causing `disableAutoGeneratedUIDs` to fail. Here's a simple test case that will reproduce the issue: @Testpublic void testFailedUID() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().disableAutoGeneratedUIDs(); DataStream&lt;String&gt; data = env.fromCollection(Arrays.asList("1", "2", "3")).uid("source-uid"); data.keyBy(i -&gt; i) .map(i -&gt; i).uid("map-uid"); env.execute();}testFailedUID(twitch.creatoranalytics.sessions.StreamingJobTest) Time elapsed: 0.008 sec &lt;&lt;&lt; ERROR!java.lang.IllegalStateException: Auto generated UIDs have been disabled but no UID or hash has been assigned to operator Partition  This passes if the keyBy is removed. </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.graph.StreamingJobGraphGeneratorNodeHashTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14912" opendate="2019-11-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>register, drop, and alter catalog functions from DDL via catalog</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="14954" opendate="2019-11-26 00:00:00" fixdate="2019-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="14967" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a utility for creating data types via reflection</summary>
      <description>As mentioned in FLIP-65, a utility will create data types from any kind of class possibly enriched with DataTypeHint annotations.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.ClassDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.ClassDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15001" opendate="2019-12-2 00:00:00" fixdate="2019-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The digest of sub-plan reuse should contain retraction traits for stream physical nodes</summary>
      <description>This bug is found in FLINK-14946:The plan for the given sql in FLINK-14946 is however, the plan after sub-plan reuse is: in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15010" opendate="2019-12-2 00:00:00" fixdate="2019-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp directories flink-netty-shuffle-* are not cleaned up</summary>
      <description>Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-&lt;uid&gt;</description>
      <version>1.9.0,1.9.1,1.9.2</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15063" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input group and output group of the task metric are reversed</summary>
      <description>In code, the input group and output group of the task metric are reversed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="15065" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB configurable options doc description error</summary>
      <description>RocksDB Configurable Options As description, rocksdb has default options:state.backend.rocksdb.compaction.level.max-size-level-base: 10MBstate.backend.rocksdb.compaction.level.target-file-size-base: 4MBstate.backend.rocksdb.writebuffer.size: 2MBBut I found the actual options is: state.backend.rocksdb.compaction.level.max-size-level-base: 256MBstate.backend.rocksdb.compaction.level.target-file-size-base: 64MBstate.backend.rocksdb.writebuffer.size: 64MBmaybe the description is wrong</description>
      <version>1.9.0</version>
      <fixedVersion>1.8.4,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs..includes.generated.rocks.db.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15259" opendate="2019-12-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveInspector.toInspectors() should convert Flink constant to Hive constant</summary>
      <description>repro test: public class HiveModuleITCase { @Test public void test() { TableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(); tEnv.unloadModule("core"); tEnv.loadModule("hive", new HiveModule("2.3.4")); tEnv.sqlQuery("select concat('an', 'bn')"); }}seems that currently HiveInspector.toInspectors() didn't convert Flink constant to Hive constant before calling hiveShim.getObjectInspectorForConstantI don't think it's a blocker</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.WritableHiveObjectConversion.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.FlinkHiveException.java</file>
    </fixedFiles>
  </bug>
  <bug id="15473" opendate="2020-1-3 00:00:00" fixdate="2020-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Linux on ppc64le to MemoryArchitecture</summary>
      <description>Similar to issue FLINK-13449Please add "ppc64le" to MemoryArchitecture.javafinal List&lt;String&gt; names64bit = Arrays.asList("amd64", "x86_64", "aarch64", "ppc64le"); </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ProcessorArchitecture.java</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t hide web frontend side pane automatically</summary>
      <description>As mentioned in FLINK-13386 the side pane hides automatically in some cases but not all cases. When I was debugging or trying the web frontend I found this behaviour a bit disconcerting. Could we disable the hiding by default? The user can still manually hide if they want to.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug id="15543" opendate="2020-1-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apache Camel not bundled but listed in flink-dist NOTICE</summary>
      <description>Apache Camel dependencies are listed in the flink-dist NOTICE, but we removed the dependency in 1.9.0 (see FLINK-12040).</description>
      <version>1.9.0</version>
      <fixedVersion>1.8.4,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="15631" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot use generic types as the result of an AggregateFunction in Blink planner</summary>
      <description>It is not possible to use a GenericTypeInfo for a result type of an AggregateFunction in a retract mode with state cleaning disabled. @Test def testGenericTypes(): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build() val tEnv = StreamTableEnvironment.create(env, setting) val t = env.fromElements(1, 2, 3).toTable(tEnv, 'a) val results = t .select(new GenericAggregateFunction()('a)) .toRetractStream[Row] val sink = new TestingRetractSink results.addSink(sink).setParallelism(1) env.execute() }class RandomClass(var i: Int)class GenericAggregateFunction extends AggregateFunction[java.lang.Integer, RandomClass] { override def getValue(accumulator: RandomClass): java.lang.Integer = accumulator.i override def createAccumulator(): RandomClass = new RandomClass(0) override def getResultType: TypeInformation[java.lang.Integer] = new GenericTypeInfo[Integer](classOf[Integer]) override def getAccumulatorType: TypeInformation[RandomClass] = new GenericTypeInfo[RandomClass]( classOf[RandomClass]) def accumulate(acc: RandomClass, value: Int): Unit = { acc.i = value } def retract(acc: RandomClass, value: Int): Unit = { acc.i = value } def resetAccumulator(acc: RandomClass): Unit = { acc.i = 0 }}The code above fails with:Caused by: java.lang.UnsupportedOperationException: BinaryGeneric cannot be compared at org.apache.flink.table.dataformat.BinaryGeneric.equals(BinaryGeneric.java:77) at GroupAggValueEqualiser$17.equalsWithoutHeader(Unknown Source) at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:177) at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:170) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)This is related to FLINK-13702</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.LazyBinaryFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15802" opendate="2020-1-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose the new type inference for table functions</summary>
      <description>This will allow to use table functions with the new type inference. It requires different changes through the stack. Support for structured type is not included here. Which means that only a ROW type can be used in the first version.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.collector.WrappingCollector.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TypedFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.FlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DeferredTypeFlinkTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.ScalarFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15804" opendate="2020-1-29 00:00:00" fixdate="2020-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the new type inference in Scala Table API scalar functions</summary>
      <description>Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used "inline". We should support them as well.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15875" opendate="2020-2-4 00:00:00" fixdate="2020-2-4 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Bump Beam to 2.19.0</summary>
      <description>Currently PyFlink depends on Beam's portability framework for Python UDF execution. The current dependent version is 2.15.0. We should bump it to 2.19.0(the latest version) as it includes several critical features/fixes, e.g.1) BEAM-7951: It allows to not serialize the window/timestamp/pane info between the Java operator and the Python worker which could definitely improve the performance a lot2) BEAM-8935: It allows to fail fast if the Python worker start up failed. Currently it takes 2 minutes to detect the failure if the Python worker is started failed. 3) BEAM-7948: It supports periodically flush the data between the Java operator and the Python worker. This feature is especially useful for streaming jobs and could improve the latency.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverOptionsParserFactory.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.runners.fnexecution.environment.ProcessManager.java</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16011" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize the within usage in Pattern</summary>
      <description>In CEP, we can use Pattern.within() to set a window in which the pattern should be matched. However, the usage of within is ambiguous and confusing to user.For example: Pattern.begin("a").within(t1).followedBy("b").within(t2) will use the minimal of t1 and t2 as the window time for the whole pattern. Pattern.begin("a").followedBy("b").within(t2) will use t2 as the window time. But Pattern.begin("a").within(t1).followedBy("b") will have no window time While Pattern.begin("a").notFollowedBy("not").within(t1).followedBy("b").within(t2) will use t2 as the window time.So I propose to normalize the usage of within() and make strict checking when compiling the pattern. For example, we can only allow within() at the end of the pattern and point it out if user set it somewhere else when compiling the pattern.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16186" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Speed up ElasticsearchITCase#testInvalidElasticsearchCluster</summary>
      <description>ElasticsearchITCase#testInvalidElasticsearchCluster runs for a significant time (30-40 seconds), as it tries to connect to a non-existent clusters.We should find a way to reduce the timeout, or implement the test in some other fashion.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16389" opendate="2020-3-2 00:00:00" fixdate="2020-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Kafka 0.10 to 0.10.2.2</summary>
      <description>Click to add description</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16524" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16611" opendate="2020-3-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datadog reporter should chunk large reports</summary>
      <description>Datadog has a maximum size for reports that it accepts.If the report exceeds this size it is simply rejected, rendering the reporter unusable.We should investigate what this size limit is, and split the report into multiple chunks.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DSeries.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DMetric.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="17062" opendate="2020-4-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the conversion from Java row type to Python row type</summary>
      <description>It iterate over the result of FieldsDataType.getFieldDataTypes when converting Java row type to Python row type. The result is non-deterministic as the result of FieldsDataType.getFieldDataTypes is of type map.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="17063" opendate="2020-4-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make null type be a possible result of a type inference</summary>
      <description>There are use cases that can benefit from making a null type a valid result of input and output type inference.Example. We can use null type in VALUES clause for a temporary placeholder that will be later inferred based on types of other rows.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17065" opendate="2020-4-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about the Python versions supported for PyFlink</summary>
      <description>We need to add notes of Python versions supported by PyFlink in doc so that users can choose the correct Python versions</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="17073" opendate="2020-4-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slow checkpoint cleanup causing OOMs</summary>
      <description>A user reported that he sees a decline in checkpoint cleanup speed when upgrading from Flink 1.7.2 to 1.10.0. The result is that a lot of cleanup tasks are waiting in the execution queue occupying memory. Ultimately, the JM process dies with an OOM.Compared to Flink 1.7.2, we introduced a dedicated ioExecutor which is used by the HighAvailabilityServices (FLINK-11851). Before, we use the AkkaRpcService thread pool which was a ForkJoinPool with a max parallelism of 64. Now it is a FixedThreadPool with as many threads as CPU cores. This change might have caused the decline in completed checkpoint discard throughput. This suspicion needs to be validated before trying to fix it!&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r390e5d775878918edca0b6c9f18de96f828c266a888e34ed30ce8494%40%3Cuser.flink.apache.org%3E</description>
      <version>1.7.3,1.8.0,1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RegionFailoverITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.CheckpointsUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SerializableRunnable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaningRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="17096" opendate="2020-4-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mini-batch group aggregation doesn&amp;#39;t expire state even if state ttl is enabled</summary>
      <description>At the moment, MiniBatch Group Agg include Local/Global doesn`t support State TTL, for streaming job, it will lead to OOM in long time running, so we need to make state data expire after ttl, the solution is that use incremental cleanup feature refer to FLINK-16581</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingSemiAntiJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchIncrementalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.StateListView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.PerKeyStateDataViewStore.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17107" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17132" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Prometheus version</summary>
      <description>My pushgateway version: 1.2.0Use the flink-metrics-prometheus module to report the metric, pushgateway successfully received the data, but Jobmanager and TaskManager will print "Failed to push metrics to PushGateway" and throw "java.io.IOException: Response code from xxx was 200".The reason is that Flink uses an incompatible low-level pushgateway client. It is recommended to upgrade to the latest version.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="17254" opendate="2020-4-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve the PyFlink documentation and examples to use SQL DDL for source/sink definition</summary>
      <description>Currently there are two ways to register a table sink/source in PyFlink table API:1) TableEnvironment.connect2) TableEnvironment.sql_updateI think it's better to provide documentation and examples on how to use 2) in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="17256" opendate="2020-4-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Suppport keyword arguments in the PyFlink Descriptor API</summary>
      <description>Keyword arguments is a very commonly used feature in Python. We should support it in the PyFlink Descriptor API to make the API more user friendly for Python users.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  <bug id="17321" opendate="2020-4-22 00:00:00" fixdate="2020-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for casting collection types.</summary>
      <description>Casts of collection types are not supported yet.E.g. query: "SELECT cast (a as ARRAY&lt;double&gt;) FROM (VALUES (array&amp;#91;3, 2, 1&amp;#93;)) AS T(a)"fails with:org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from 'ARRAY&lt;INT NOT NULL&gt; NOT NULL' to 'ARRAY&lt;DOUBLE&gt; NOT NULL'. at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.generateCast(ScalarOperatorGens.scala:1284) at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:691) at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:486) at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:52) at org.apache.calcite.rex.RexCall.accept(RexCall.java:288) at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:132) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:152) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:179) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:49) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:62) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:38) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalcBase.translateToPlan(BatchExecCalcBase.scala:42) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToTransformation(BatchExecSink.scala:131) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:97) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:49) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlan(BatchExecSink.scala:49) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:72) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:71) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:71) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)...Similar behaviour can be observed for MULTISET, MAP, ROW</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="17342" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedule savepoint if max-inflight-checkpoints limit is reached instead of forcing (in UC mode)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointPropertiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17454" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_configuration.py ConfigurationTests::test_add_all failed on travis</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=383&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=455fddbf-5921-5b71-25ac-92992ad80b28=========================== short test summary info ============================FAILED pyflink/common/tests/test_configuration.py::ConfigurationTests::test_add_all====== 1 failed, 499 passed, 19 skipped, 97 warnings in 182.59s (0:03:02) ======ERROR: InvocationError for command /__w/1/s/flink-python/.tox/py37-cython/bin/pytest --durations=0 (exited with code 1)</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
    </fixedFiles>
  </bug>
  <bug id="17456" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hive connector tests to execute DDL &amp; DML via TableEnvironment</summary>
      <description>In hive connector tests, currently we delegate DDLs and some DMLs to the hive shell. With hive parser in place, we should be able to execute those DDLs and DMLs on Flink side.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17562" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>POST /jars/:jarid/plan is not working</summary>
      <description>The handlers introduced in FLINK-11853 is not using the correct headers, and registers a second handler under the same URL.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="17564" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inflight data of incoming channel may be disordered for unaligned checkpoint</summary>
      <description>For unaligned checkpoint, when checkpointing the inflight data of incoming channel, both task thread and Netty thread may add data to the channel state writer. More specifically, the task thread will first request inflight buffers from the input channel and add the buffers to the channel state writer, and then the Netty thread will add the following up buffers (if any) to the channel state writer. The buffer adding of task thread and Netty thread is not synchronized so the Netty thread may add buffers before the task thread which leads to disorder of the data and corruption of the data stream.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnalignerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17568" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task may consume data after checkpoint barrier before performing checkpoint for unaligned checkpoint</summary>
      <description>For unaligned checkpoint, task may consume data after the checkpoint barrier before performing checkpoint which lead to consumption of duplicated data and corruption of data stream.More specifically, when the Netty thread notifies the checkpoint barrier for the first time and enqueue a checkpointing task in the mailbox, the task thread may still in data consumption loop and if it reads a new checkpoint barrier from another channel it will not return to the mailbox and instead it will continue to read data until a all data consumed or we have a full record, meanwhile, the data after checkpoint barrier may be read and consumed which lead to inconsistency.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAlignerTestBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="1757" opendate="2015-3-20 00:00:00" fixdate="2015-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ClassCastException is thrown while summing Short values on window</summary>
      <description>java.lang.ClassCastException is thrown while summing Short values on windowStack Trace:Caused by: java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short at org.apache.flink.streaming.api.streamvertex.OutputHandler.invokeUserFunction(OutputHandler.java:232) at org.apache.flink.streaming.api.streamvertex.StreamVertex.invoke(StreamVertex.java:121) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:205) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short at org.apache.flink.api.common.typeutils.base.ShortSerializer.copy(ShortSerializer.java:27) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.copy(TupleSerializer.java:95) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.copy(TupleSerializer.java:30) at org.apache.flink.streaming.api.invokable.StreamInvokable.copy(StreamInvokable.java:166) at org.apache.flink.streaming.api.invokable.SinkInvokable.collect(SinkInvokable.java:46) at org.apache.flink.streaming.api.collector.DirectedCollectorWrapper.collect(DirectedCollectorWrapper.java:95) at org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokable.reduce(GroupedReduceInvokable.java:47) at org.apache.flink.streaming.api.invokable.operator.StreamReduceInvokable.invoke(StreamReduceInvokable.java:39) at org.apache.flink.streaming.api.streamvertex.StreamVertex.invokeUserFunction(StreamVertex.java:85) at org.apache.flink.streaming.api.streamvertex.OutputHandler.invokeUserFunction(OutputHandler.java:229)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.SumFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="17701" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk:tools dependency from all Hadoop dependencies for Java 9+ compatibility</summary>
      <description>Hadoop transitively pulls the system dependency jdk:tools which is not longer available on Java 9+. This causes errors when importing the code into an IDE with runs Java 11.This dependency is anyways not needed when running the code, because the classes are always present. It can be safely excluded form the transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17702" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorCoordinators must be notified of tasks cancelled as part of failover</summary>
      <description>The OperatorCoordinators are currently only notified of tasks that directly fail.However, tasks that are cancelled (as part of the regional failover) must be handled the same was and also send notifications to the OperatorCoordinator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17931" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="18507" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="1907" opendate="2015-4-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala Interactive Shell</summary>
      <description>Build an interactive Shell for the Scala api.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.FlinkTestBase.scala</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19070" opendate="2020-8-28 00:00:00" fixdate="2020-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19077" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import process time temporal join operator</summary>
      <description>import TemporalProcessTimeJoinOperator for Processing-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.LegacyTemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacyTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="19078" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import rowtime join temporal operator</summary>
      <description>import TemporalRowTimeJoinOperator for EventTime-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19079" opendate="2020-8-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support row time deduplicate operator</summary>
      <description>To convert a insert-only table to versioned table, the recommended way is use deduplicate query as following, the converted versioned_view owns primary key and event time and thus can be a versioned table.CREATE VIEW versioned_rates ASSELECT currency, rate, currency_timeFROM (      SELECT *,      ROW_NUMBER() OVER (PARTITION BY currency -- inferred primary key ORDER BY currency_time  -- the event time  DESC) AS rowNum FROM rates)WHERE rowNum = 1;But currently deduplicate operator only support on process time, this issue aims to support deduplicate on Event time. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecChangelogNormalize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20695" opendate="2020-12-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper node under leader and leaderlatch is not deleted after job finished</summary>
      <description>I used flink 1.11 in standalone cluster mode for batch job. The enviornment was configured as zookeeper HA mode.After job was commited, flink runtime created nodes under /flink/default/leader and /flink/default/leaderlatch with job id.  Though jobs were finished, these nodes  were remaining in zookeeper path forever. After a period of running, more and more jobs had been executed and there were a greate number of nodes under /flink/default/leader and slowed down the performance of zookeeper. Why not delete the nodes after job finished? Flink runtime could get job status by listeners and delete the leader nodes for job immidiately.</description>
      <version>1.9.0,1.11.3,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.1,1.12.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingManualHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.AbstractNonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="5340" opendate="2016-12-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric exposing jobs uptimes</summary>
      <description>I would like the job manager to expose a metric indicating how long each job has been up. This way I can grab this number and measure the health of my job.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
